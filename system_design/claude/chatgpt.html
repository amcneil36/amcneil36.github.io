<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>System Design: ChatGPT</title>
<script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
<style>
  :root {
    --bg: #fdfdfd;
    --text: #1a1a1a;
    --accent: #2563eb;
    --border: #e5e7eb;
    --code-bg: #f3f4f6;
    --heading: #0f172a;
    --card-bg: #ffffff;
    --card-shadow: 0 1px 3px rgba(0,0,0,0.08);
    --example-bg: #eff6ff;
    --example-border: #93c5fd;
    --warn-bg: #fffbeb;
    --warn-border: #fcd34d;
  }
  * { box-sizing: border-box; margin: 0; padding: 0; }
  body {
    font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif;
    background: var(--bg);
    color: var(--text);
    line-height: 1.7;
    padding: 2rem;
    max-width: 1100px;
    margin: 0 auto;
  }
  h1 { font-size: 2.2rem; color: var(--heading); margin-bottom: 0.5rem; border-bottom: 3px solid var(--accent); padding-bottom: 0.5rem; }
  h2 { font-size: 1.6rem; color: var(--heading); margin-top: 2.5rem; margin-bottom: 1rem; border-bottom: 2px solid var(--border); padding-bottom: 0.3rem; }
  h3 { font-size: 1.25rem; color: var(--heading); margin-top: 1.8rem; margin-bottom: 0.7rem; }
  h4 { font-size: 1.05rem; color: var(--heading); margin-top: 1.3rem; margin-bottom: 0.5rem; }
  p { margin-bottom: 0.9rem; }
  ul, ol { margin-bottom: 1rem; padding-left: 1.6rem; }
  li { margin-bottom: 0.35rem; }
  code {
    background: var(--code-bg);
    padding: 0.15rem 0.4rem;
    border-radius: 4px;
    font-size: 0.92em;
    font-family: 'SF Mono', 'Fira Code', 'Consolas', monospace;
  }
  pre {
    background: #1e293b;
    color: #e2e8f0;
    padding: 1rem 1.2rem;
    border-radius: 8px;
    overflow-x: auto;
    margin-bottom: 1rem;
    font-size: 0.9rem;
    line-height: 1.5;
  }
  pre code { background: none; padding: 0; color: inherit; }
  .diagram-container {
    background: var(--card-bg);
    border: 1px solid var(--border);
    border-radius: 10px;
    padding: 1.5rem;
    margin: 1.2rem 0;
    box-shadow: var(--card-shadow);
    overflow-x: auto;
  }
  .example-box {
    background: var(--example-bg);
    border-left: 4px solid var(--example-border);
    padding: 1rem 1.2rem;
    border-radius: 0 8px 8px 0;
    margin: 1rem 0;
  }
  .example-box strong { color: var(--accent); }
  .warn-box {
    background: var(--warn-bg);
    border-left: 4px solid var(--warn-border);
    padding: 1rem 1.2rem;
    border-radius: 0 8px 8px 0;
    margin: 1rem 0;
  }
  table {
    width: 100%;
    border-collapse: collapse;
    margin: 1rem 0;
    font-size: 0.95rem;
  }
  th, td {
    border: 1px solid var(--border);
    padding: 0.6rem 0.8rem;
    text-align: left;
  }
  th { background: var(--code-bg); font-weight: 600; }
  .toc { background: var(--card-bg); border: 1px solid var(--border); border-radius: 10px; padding: 1.5rem; margin: 1.5rem 0; }
  .toc a { color: var(--accent); text-decoration: none; }
  .toc a:hover { text-decoration: underline; }
  .toc ul { list-style: none; padding-left: 0; }
  .toc li { margin-bottom: 0.3rem; }
  .toc li li { padding-left: 1.2rem; }
  .badge {
    display: inline-block;
    padding: 0.1rem 0.5rem;
    border-radius: 4px;
    font-size: 0.8rem;
    font-weight: 600;
    color: white;
  }
  .badge-pk { background: #7c3aed; }
  .badge-fk { background: #059669; }
  .badge-idx { background: #d97706; }
  .badge-sk { background: #2563eb; }
</style>
</head>
<body>

<h1>System Design: ChatGPT</h1>
<p><em>A conversational AI assistant that allows users to interact with a large language model (LLM) through multi-turn text conversations, with real-time token streaming.</em></p>

<!-- TABLE OF CONTENTS -->
<div class="toc">
<strong>Table of Contents</strong>
<ul>
  <li><a href="#fr">1. Functional Requirements</a></li>
  <li><a href="#nfr">2. Non-Functional Requirements</a></li>
  <li><a href="#flow1">3. Flow 1 — Send Message &amp; Receive Streamed Response</a></li>
  <li><a href="#flow2">4. Flow 2 — Load Conversation History</a></li>
  <li><a href="#flow3">5. Flow 3 — Create New Conversation</a></li>
  <li><a href="#combined">6. Combined Overall Flow Diagram</a></li>
  <li><a href="#schema">7. Database Schema</a></li>
  <li><a href="#cdn-cache">8. CDN &amp; Caching Deep Dive</a></li>
  <li><a href="#sse">9. Streaming Deep Dive — SSE</a></li>
  <li><a href="#grpc">10. gRPC Deep Dive (Inter-Service Communication)</a></li>
  <li><a href="#mq">11. Message Queue Deep Dive</a></li>
  <li><a href="#inference">12. GPU Inference Deep Dive</a></li>
  <li><a href="#lb">13. Load Balancer Deep Dive</a></li>
  <li><a href="#scaling">14. Scaling Considerations</a></li>
  <li><a href="#tradeoffs">15. Tradeoffs &amp; Deep Dives</a></li>
  <li><a href="#alternatives">16. Alternative Approaches</a></li>
  <li><a href="#additional">17. Additional Considerations</a></li>
  <li><a href="#vendors">18. Vendor Recommendations</a></li>
</ul>
</div>

<!-- ═══════════════════════════════════════════════ -->
<h2 id="fr">1. Functional Requirements</h2>
<ol>
  <li><strong>Send a text prompt</strong> — Users can type a message and send it to the AI model.</li>
  <li><strong>Receive a streamed response</strong> — The model's reply is delivered token-by-token in real-time (not all at once).</li>
  <li><strong>Multi-turn conversations</strong> — Each conversation maintains context so the model can reference prior messages.</li>
  <li><strong>Create a new conversation</strong> — Users can start a fresh conversation at any time.</li>
  <li><strong>View past conversations</strong> — Users can see a list of their previous conversations and reopen them.</li>
  <li><strong>Stop generation</strong> — Users can halt a response mid-stream.</li>
  <li><strong>Regenerate a response</strong> — Users can ask the model to regenerate its last answer.</li>
  <li><strong>Auto-generate conversation titles</strong> — After the first exchange, a title is automatically created from the conversation content.</li>
  <li><strong>Model selection</strong> — Users can choose between different model versions (e.g., a faster model vs. a more capable model).</li>
</ol>

<!-- ═══════════════════════════════════════════════ -->
<h2 id="nfr">2. Non-Functional Requirements</h2>
<ol>
  <li><strong>Low latency for first token</strong> — Time-to-first-token (TTFT) should be &lt; 1 second for most requests. Users should perceive an immediate response.</li>
  <li><strong>High throughput streaming</strong> — The system should sustain smooth token delivery at 30–80 tokens/second per stream.</li>
  <li><strong>High availability</strong> — 99.9%+ uptime. The chat service and conversation storage must be fault-tolerant.</li>
  <li><strong>Scalability</strong> — Support millions of concurrent users and tens of thousands of simultaneous inference requests.</li>
  <li><strong>Consistency</strong> — Conversation history must be durably persisted; users should never lose messages.</li>
  <li><strong>Rate limiting</strong> — Enforce per-user limits based on subscription tier (free vs. paid) to prevent abuse and manage GPU costs.</li>
  <li><strong>Security &amp; privacy</strong> — Encrypt data in transit (TLS) and at rest. Users' conversation data must be isolated.</li>
  <li><strong>Cost efficiency</strong> — GPU compute is extremely expensive; the system must maximize GPU utilization through batching and scheduling.</li>
</ol>

<!-- ═══════════════════════════════════════════════ -->
<h2 id="flow1">3. Flow 1 — Send Message &amp; Receive Streamed Response</h2>
<p>This is the <strong>core flow</strong> of the system. The user types a message, the system runs LLM inference, and tokens are streamed back in real-time.</p>

<div class="diagram-container">
<pre class="mermaid">
graph LR
    subgraph Client
        A["Client<br/>(Web / Mobile)"]
    end

    A -->|"① HTTP POST /chat/completions<br/>Accept: text/event-stream"| B["Load Balancer"]
    B --> C["API Gateway<br/>+ Rate Limiter"]
    C --> D["Chat Service"]

    D -->|"② Read conversation<br/>history"| E[("Cache")]
    E -->|"③ Cache miss"| F[("Conversation Store<br/>(NoSQL)")]

    D -->|"④ gRPC Bidirectional<br/>Stream"| G["Inference Service"]
    G --> H["Request Scheduler<br/>+ Priority Queue"]
    H --> I["GPU Cluster<br/>(Model Workers)"]

    I -->|"⑤ Tokens"| G
    G -->|"⑥ gRPC Token Stream"| D
    D -->|"⑦ SSE Token Stream"| A

    D -->|"⑧ Async: enqueue<br/>completed message"| J["Message Queue"]
    J --> K["Persistence Worker"]
    K -->|"⑨ Write message +<br/>update conversation"| F
    K -->|"⑩ Record usage"| L[("Usage Store<br/>(Time-Series)")]
</pre>
</div>

<h3>Examples</h3>

<div class="example-box">
<strong>Example 1 — Happy Path (Paid User):</strong><br/>
User "Alice" (a Plus subscriber) types <em>"Explain quantum entanglement in simple terms"</em> in a conversation she already has open (conversation_id = <code>conv_abc123</code>). She clicks Send.<br/><br/>
① The client sends an HTTP POST to <code>/api/chat/completions</code> with body <code>{ conversation_id: "conv_abc123", message: "Explain quantum entanglement in simple terms", model: "gpt-4" }</code> and header <code>Accept: text/event-stream</code>. The request passes through the Load Balancer → API Gateway (which validates Alice's JWT, checks her rate limit — she's well within her paid-tier limit of 40 messages/3 hours).<br/><br/>
② The Chat Service looks up conversation <code>conv_abc123</code> in Cache. Cache hit — it retrieves Alice's prior 6 messages in this conversation.<br/><br/>
③ (Skipped — cache hit.)<br/><br/>
④ The Chat Service constructs the prompt: [system prompt] + [6 prior messages] + [new user message]. It opens a gRPC bidirectional stream to the Inference Service, sending the full prompt and parameters (temperature=0.7, max_tokens=2048).<br/><br/>
⑤ The Inference Service's Request Scheduler places Alice's request in the priority queue. Since she's a paid user, she gets higher priority. A GPU worker picks up the request, runs the prefill phase on all input tokens, then begins autoregressive token generation.<br/><br/>
⑥⑦ Each generated token (e.g., "Quantum", " entanglement", " is", " when", ...) is streamed back via gRPC to the Chat Service, which immediately forwards it to Alice's client via SSE: <code>data: {"token":"Quantum"}\n\n</code>. Alice sees the words appear one by one in the UI.<br/><br/>
⑧⑨⑩ After the model emits the stop token, the Chat Service sends <code>data: [DONE]\n\n</code> to Alice and closes the SSE connection. The full assistant response (850 tokens) is enqueued on the Message Queue. A Persistence Worker dequeues it, writes the new user message and assistant response to the Conversation Store, updates the conversation's <code>updated_at</code> timestamp, writes the cache through, and records 850 tokens of usage to the Usage Store for billing.
</div>

<div class="example-box">
<strong>Example 2 — Rate-Limited (Free User):</strong><br/>
User "Bob" (free tier) sends his 15th message within 1 hour. The API Gateway's Rate Limiter checks Bob's recent usage, finds he has exceeded the free-tier limit of 10 messages/hour. The API Gateway returns HTTP <code>429 Too Many Requests</code> with body <code>{ error: "rate_limit_exceeded", message: "You've reached the free tier limit. Please wait or upgrade to Plus." }</code>. The client displays a rate-limit banner. No inference is performed; GPU resources are preserved.
</div>

<div class="example-box">
<strong>Example 3 — Stop Generation:</strong><br/>
User "Carol" sends a complex prompt and the model starts generating a very long response. After reading the first 3 paragraphs (about 200 tokens), Carol decides she has enough and clicks the <strong>Stop Generating</strong> button.<br/><br/>
The client closes the SSE connection. The Chat Service detects the connection closure and immediately sends a cancellation signal via the gRPC stream to the Inference Service. The Inference Service's GPU worker aborts generation, frees the KV cache memory for that request, and returns the resources to the pool. The Chat Service enqueues the <em>partial</em> response (200 tokens, with <code>finish_reason: "stop"</code>) to the Message Queue for persistence. Carol sees the partial response saved in her conversation.
</div>

<div class="example-box">
<strong>Example 4 — Regenerate Response:</strong><br/>
User "Dave" reads the model's response and is unsatisfied. He clicks <strong>Regenerate</strong>.<br/><br/>
The client sends a new HTTP POST to <code>/api/chat/completions</code> with the same <code>conversation_id</code> and a flag <code>regenerate: true</code>, along with the <code>message_id</code> of the response to replace. The Chat Service re-sends the same prompt to the Inference Service (potentially with a different random seed or slightly higher temperature to produce variation). The new response streams back via SSE. Upon completion, the old assistant message is marked as replaced (soft-deleted) and the new response is persisted as the current response.
</div>

<div class="example-box">
<strong>Example 5 — Context Window Overflow:</strong><br/>
User "Eve" has a very long conversation spanning 200 messages (~120K tokens). She sends a new message. The Chat Service retrieves the conversation history, calculates the total token count, and discovers it exceeds the model's 128K context window.<br/><br/>
The Chat Service applies a <strong>truncation strategy</strong>: it keeps the system prompt + the most recent N messages that fit within the context window, dropping the oldest messages. Alternatively, for very long conversations, it may use a <strong>summarization strategy</strong>: a lightweight model summarizes the older messages into a condensed context block, which is prepended to the recent messages. Eve's new prompt proceeds to inference with the truncated/summarized context.
</div>

<h3>Component Deep Dive — Flow 1</h3>

<h4>Client (Web / Mobile)</h4>
<p>The front-end application (React web app, iOS/Android native app). Responsible for rendering the chat interface, managing SSE connections for streaming, and local state management. Sends HTTP requests to the API layer and renders tokens as they arrive via SSE.</p>

<h4>Load Balancer</h4>
<p>Sits at the edge of the backend infrastructure. Distributes incoming HTTP requests across multiple API Gateway instances. Operates at <strong>Layer 7 (HTTP)</strong> to support routing based on path and headers. Performs SSL/TLS termination. Uses <strong>least-connections</strong> algorithm to balance load, which is important because SSE connections are long-lived and round-robin would not account for active connection counts. See <a href="#lb">Load Balancer Deep Dive</a> for more detail.</p>

<h4>API Gateway + Rate Limiter</h4>
<p>The API Gateway serves as the single entry point. It handles:</p>
<ul>
  <li><strong>Authentication</strong>: validates JWT tokens on every request.</li>
  <li><strong>Rate limiting</strong>: uses a sliding window counter per user, backed by an in-memory store. Free users: ~10 messages/hour. Paid users: ~40 messages/3 hours. The rate limiter also enforces a per-user <em>token</em> budget (total tokens consumed per day).</li>
  <li><strong>Request validation</strong>: checks required fields (<code>conversation_id</code>, <code>message</code>, <code>model</code>).</li>
  <li><strong>Routing</strong>: forwards valid requests to the Chat Service.</li>
</ul>
<p><strong>Protocol</strong>: HTTP/2 (supports SSE streaming natively with multiplexing).</p>

<h4>Chat Service</h4>
<p>The core orchestration service. Stateless; can be horizontally scaled.</p>
<table>
  <tr><th>Endpoint</th><th>Method</th><th>Input</th><th>Output</th><th>Description</th></tr>
  <tr>
    <td><code>/api/chat/completions</code></td>
    <td>POST</td>
    <td><code>{ conversation_id, message, model, [regenerate, target_message_id] }</code></td>
    <td>SSE stream of tokens, ending with <code>[DONE]</code></td>
    <td>Send a message and receive a streamed response. The connection stays open for the duration of inference.</td>
  </tr>
</table>
<p><strong>Internal behavior:</strong></p>
<ol>
  <li>Persists the user's message to the Conversation Store (synchronous write — the user's message must be durably stored before inference begins).</li>
  <li>Reads conversation history from Cache (or Conversation Store on miss).</li>
  <li>Constructs the full prompt (system prompt + history + new message). Applies context window truncation/summarization if needed.</li>
  <li>Opens a gRPC bidirectional streaming connection to the Inference Service.</li>
  <li>Forwards each token received from gRPC to the client via SSE.</li>
  <li>On completion (or cancellation), enqueues the assistant response to the Message Queue for async persistence.</li>
</ol>

<h4>Cache (In-Memory Store)</h4>
<p>Stores recently accessed conversation messages and metadata to reduce reads against the Conversation Store. See <a href="#cdn-cache">CDN &amp; Caching Deep Dive</a> for details on strategies and eviction policies.</p>

<h4>Conversation Store (NoSQL)</h4>
<p>The primary durable store for conversations and messages. See <a href="#schema">Database Schema</a> for full details.</p>

<h4>Inference Service</h4>
<p>Manages GPU resources and model inference. This is the most complex and expensive component. See <a href="#inference">GPU Inference Deep Dive</a> for a complete deep dive.</p>
<ul>
  <li><strong>Protocol</strong>: gRPC with Protocol Buffers (server-side streaming RPC).</li>
  <li><strong>Input</strong>: prompt tokens, generation parameters (temperature, top_p, max_tokens, stop sequences).</li>
  <li><strong>Output</strong>: stream of generated tokens, finishing with a completion signal (stop reason).</li>
</ul>

<h4>Request Scheduler + Priority Queue</h4>
<p>An internal component of the Inference Service. Manages the queue of inference requests waiting for GPU resources:</p>
<ul>
  <li>Paid users get higher priority than free users.</li>
  <li>Implements <strong>continuous batching</strong> — dynamically adds new requests to a currently running batch and removes completed requests, maximizing GPU utilization.</li>
  <li>Enforces max queue depth; returns HTTP <code>503 Service Unavailable</code> when the system is overloaded.</li>
</ul>

<h4>GPU Cluster (Model Workers)</h4>
<p>A fleet of GPU-equipped servers running the LLM. Each server may host one or more model instances depending on model size. For very large models (100B+ parameters), a single model instance may be sharded across multiple GPUs using <strong>tensor parallelism</strong> (splitting layers across GPUs) and/or <strong>pipeline parallelism</strong> (splitting model stages across GPUs). Uses <strong>paged attention</strong> for efficient KV cache memory management.</p>

<h4>Message Queue</h4>
<p>Decouples the streaming response path from persistence and analytics. See <a href="#mq">Message Queue Deep Dive</a>.</p>

<h4>Persistence Worker</h4>
<p>Consumes messages from the queue and performs:</p>
<ul>
  <li>Writing the assistant's completed (or partial) response to the Conversation Store.</li>
  <li>Updating the conversation's <code>updated_at</code> timestamp and <code>last_message_preview</code>.</li>
  <li>Updating the Cache (write-through).</li>
  <li>Recording token usage to the Usage Store for billing and rate limiting.</li>
  <li>Triggering auto-title generation (if this is the first message exchange in the conversation): enqueues a lightweight inference request to generate a conversation title.</li>
</ul>

<h4>Usage Store (Time-Series)</h4>
<p>Records per-user token consumption over time. Optimized for time-range queries (e.g., "How many tokens did user X use in the last 24 hours?"). Used for billing, rate limiting, and analytics dashboards.</p>


<!-- ═══════════════════════════════════════════════ -->
<h2 id="flow2">4. Flow 2 — Load Conversation History</h2>
<p>When a user opens the app or selects a past conversation, they need to see their conversation list and the messages within a specific conversation.</p>

<div class="diagram-container">
<pre class="mermaid">
graph LR
    A["Client<br/>(Web / Mobile)"] -->|"① GET /conversations<br/>or GET /conversations/:id/messages"| B["Load Balancer"]
    B --> C["API Gateway"]
    C --> D["Chat Service"]
    D -->|"② Read"| E[("Cache")]
    E -->|"③ Cache miss"| F[("Conversation Store<br/>(NoSQL)")]
    F -->|"④ Return data"| D
    D -->|"⑤ Populate cache"| E
    D -->|"⑥ JSON response"| A
</pre>
</div>

<h3>Examples</h3>

<div class="example-box">
<strong>Example 1 — User Opens the App (List Conversations):</strong><br/>
User "Alice" opens the ChatGPT app. The client sends <code>GET /api/conversations?limit=50&sort=updated_at:desc</code>.<br/><br/>
① The request passes through Load Balancer → API Gateway (JWT validated).<br/>
② The Chat Service checks the Cache for Alice's conversation list (key: <code>user:alice:conversations</code>). <strong>Cache hit</strong> — returns the cached list of 50 conversations with their titles, <code>updated_at</code> timestamps, and <code>last_message_preview</code>.<br/>
⑥ The Chat Service returns the JSON array to Alice's client. The sidebar populates with her conversation list. No database read was needed.
</div>

<div class="example-box">
<strong>Example 2 — User Opens a Specific Conversation (Cache Miss):</strong><br/>
Alice clicks on a conversation titled "Quantum Physics Questions" (<code>conv_abc123</code>) that she hasn't opened in several days. The client sends <code>GET /api/conversations/conv_abc123/messages?limit=100</code>.<br/><br/>
② The Chat Service checks the Cache for <code>conv:conv_abc123:messages</code>. <strong>Cache miss</strong> — this conversation was evicted due to TTL expiration.<br/>
③ The Chat Service queries the Conversation Store for messages where <code>conversation_id = conv_abc123</code>, ordered by <code>sequence_number ASC</code>, limited to the most recent 100 messages.<br/>
④ The Conversation Store returns 42 messages.<br/>
⑤ The Chat Service populates the cache with these messages (TTL = 15 minutes).<br/>
⑥ The Chat Service returns the messages as a JSON array. Alice's client renders the chat history.
</div>

<div class="example-box">
<strong>Example 3 — Pagination for a Very Long Conversation:</strong><br/>
Alice scrolls to the top of a conversation with 500+ messages. The initial load returned the most recent 100. As she scrolls up, the client sends <code>GET /api/conversations/conv_abc123/messages?limit=100&before_sequence=101</code>. The Chat Service fetches the next 100 older messages from the Conversation Store (using the <code>sequence_number</code> as a cursor), returns them, and Alice's client prepends them to the chat view.
</div>

<h3>Component Deep Dive — Flow 2</h3>

<h4>Chat Service — Read Endpoints</h4>
<table>
  <tr><th>Endpoint</th><th>Method</th><th>Input</th><th>Output</th><th>Description</th></tr>
  <tr>
    <td><code>/api/conversations</code></td>
    <td>GET</td>
    <td>Query params: <code>limit</code>, <code>sort</code>, <code>cursor</code></td>
    <td>JSON array of conversation summaries (id, title, updated_at, last_message_preview, model)</td>
    <td>List the authenticated user's conversations, sorted by <code>updated_at</code> descending.</td>
  </tr>
  <tr>
    <td><code>/api/conversations/:id/messages</code></td>
    <td>GET</td>
    <td>Path param: <code>conversation_id</code>. Query params: <code>limit</code>, <code>before_sequence</code></td>
    <td>JSON array of messages (id, role, content, created_at, model, finish_reason)</td>
    <td>Retrieve messages within a conversation, using cursor-based pagination on <code>sequence_number</code>.</td>
  </tr>
</table>

<p>All other components (Load Balancer, API Gateway, Cache, Conversation Store) behave as described in Flow 1. The key difference is that this is a <strong>read-only</strong> flow with no GPU inference involved.</p>

<!-- ═══════════════════════════════════════════════ -->
<h2 id="flow3">5. Flow 3 — Create New Conversation</h2>
<p>When a user clicks "New Chat", a fresh conversation is created.</p>

<div class="diagram-container">
<pre class="mermaid">
graph LR
    A["Client<br/>(Web / Mobile)"] -->|"① POST /conversations<br/>{model: 'gpt-4'}"| B["Load Balancer"]
    B --> C["API Gateway"]
    C --> D["Chat Service"]
    D -->|"② Insert new<br/>conversation record"| F[("Conversation Store<br/>(NoSQL)")]
    D -->|"③ Invalidate user's<br/>conversation list cache"| E[("Cache")]
    D -->|"④ JSON { conversation_id }"| A
</pre>
</div>

<h3>Examples</h3>

<div class="example-box">
<strong>Example 1 — New Chat with Default Model:</strong><br/>
User "Alice" clicks the <strong>+ New Chat</strong> button. The client sends <code>POST /api/conversations</code> with body <code>{ model: "gpt-4" }</code>.<br/><br/>
① The request passes through Load Balancer → API Gateway.<br/>
② The Chat Service generates a new <code>conversation_id</code> (UUID), creates a record in the Conversation Store with <code>user_id = alice</code>, <code>title = "New Conversation"</code> (placeholder), <code>model_version = "gpt-4"</code>, <code>created_at = now()</code>.<br/>
③ The Chat Service invalidates Alice's cached conversation list so the next read picks up the new conversation.<br/>
④ The Chat Service returns <code>{ conversation_id: "conv_xyz789" }</code>. The client navigates to the new empty chat. Alice can now type her first message, which will trigger Flow 1.
</div>

<div class="example-box">
<strong>Example 2 — First Message Triggers Auto-Title:</strong><br/>
After creating the conversation, Alice sends her first message: <em>"Help me write a cover letter for a software engineering position."</em> Flow 1 executes. After the assistant's response is fully generated, the Persistence Worker detects this is the first exchange in the conversation and enqueues a lightweight title-generation inference request. A GPU worker processes the prompt <em>"Generate a short title for this conversation: [user message + assistant response]"</em> and returns <em>"Software Engineering Cover Letter"</em>. The Persistence Worker updates the conversation's title in the Conversation Store and invalidates the cache. The next time Alice views her conversation list, she sees the new title.
</div>

<h3>Component Deep Dive — Flow 3</h3>

<h4>Chat Service — Create Endpoint</h4>
<table>
  <tr><th>Endpoint</th><th>Method</th><th>Input</th><th>Output</th><th>Description</th></tr>
  <tr>
    <td><code>/api/conversations</code></td>
    <td>POST</td>
    <td><code>{ model }</code></td>
    <td><code>{ conversation_id }</code></td>
    <td>Create a new conversation for the authenticated user.</td>
  </tr>
</table>
<p>This is a lightweight write operation. No inference is involved. The only database write is inserting a single row in the Conversations table.</p>


<!-- ═══════════════════════════════════════════════ -->
<h2 id="combined">6. Combined Overall Flow Diagram</h2>
<p>This diagram shows all three flows in a single unified architecture view.</p>

<div class="diagram-container">
<pre class="mermaid">
graph TB
    subgraph Clients
        WEB["Web Client<br/>(React)"]
        MOB["Mobile Client<br/>(iOS / Android)"]
    end

    subgraph Edge
        LB["Load Balancer<br/>(L7, least-connections)"]
        GW["API Gateway<br/>+ Rate Limiter<br/>+ Auth (JWT)"]
    end

    subgraph Application Tier
        CS["Chat Service<br/>(Stateless)"]
    end

    subgraph Inference Tier
        IS["Inference Service"]
        SCHED["Request Scheduler<br/>+ Priority Queue"]
        GPU1["GPU Worker 1"]
        GPU2["GPU Worker 2"]
        GPUN["GPU Worker N"]
    end

    subgraph Async Processing
        MQ["Message Queue"]
        PW["Persistence<br/>Worker(s)"]
    end

    subgraph Data Stores
        CACHE[("Cache<br/>(In-Memory)")]
        CONVDB[("Conversation Store<br/>(NoSQL)")]
        USERDB[("User Store<br/>(SQL)")]
        USAGEDB[("Usage Store<br/>(Time-Series)")]
    end

    subgraph Static Assets
        CDN["CDN<br/>(JS, CSS, Images)"]
    end

    WEB & MOB -->|"HTTPS"| LB
    WEB -.->|"Static assets"| CDN
    LB --> GW
    GW -->|"Auth check"| USERDB
    GW --> CS
    CS --> CACHE
    CACHE --> CONVDB
    CS -->|"gRPC Stream"| IS
    IS --> SCHED
    SCHED --> GPU1 & GPU2 & GPUN
    GPU1 & GPU2 & GPUN -->|"Tokens"| IS
    IS -->|"gRPC Token Stream"| CS
    CS -->|"SSE Stream"| LB
    CS --> MQ
    MQ --> PW
    PW --> CONVDB
    PW --> CACHE
    PW --> USAGEDB
</pre>
</div>

<h3>Examples — Combined Flow</h3>

<div class="example-box">
<strong>Example — Full User Session:</strong><br/>
1. Alice opens <code>chat.example.com</code> in her browser. Static assets (JS, CSS, fonts, images) are served from the <strong>CDN</strong>.<br/><br/>
2. The React app loads and Alice logs in. The client sends <code>POST /api/auth/login</code> → Load Balancer → API Gateway → validates credentials against the <strong>User Store (SQL)</strong> → returns a JWT.<br/><br/>
3. Alice sees her sidebar. The client sends <code>GET /api/conversations</code> (<strong>Flow 2</strong>) → Load Balancer → API Gateway → Chat Service → <strong>Cache</strong> (hit) → returns her 30 most recent conversations with titles and previews.<br/><br/>
4. Alice clicks <strong>+ New Chat</strong> (<strong>Flow 3</strong>) → <code>POST /api/conversations</code> → Chat Service creates a new record in the <strong>Conversation Store</strong>, invalidates Alice's conversation list cache → returns <code>conv_new456</code>.<br/><br/>
5. Alice types <em>"Write a Python function to merge two sorted lists"</em> and hits Enter (<strong>Flow 1</strong>) → <code>POST /api/chat/completions</code> with SSE → Chat Service persists Alice's message (sync write), constructs the prompt, opens gRPC stream to Inference Service → Request Scheduler places it in priority queue → GPU Worker 3 picks it up → begins generating tokens → tokens flow: GPU Worker → Inference Service (gRPC) → Chat Service (SSE) → Alice's browser. Words appear letter by letter.<br/><br/>
6. Generation finishes (200 tokens, <code>finish_reason: "stop"</code>). The Chat Service sends <code>[DONE]</code> via SSE, closes the connection, enqueues the response on the <strong>Message Queue</strong> → Persistence Worker writes the assistant message to the Conversation Store, updates cache, records 200 tokens to the <strong>Usage Store</strong>, and generates a title "Merge Sorted Lists Python" for the conversation.<br/><br/>
7. Alice sends a follow-up: <em>"Now make it handle duplicates"</em>. Flow 1 repeats, but this time the Chat Service retrieves the prior 2 messages from <strong>Cache</strong> (warm from step 5) and appends the new message to the prompt context before sending to inference.
</div>


<!-- ═══════════════════════════════════════════════ -->
<h2 id="schema">7. Database Schema</h2>

<h3>7.1 Users Table — SQL</h3>
<table>
  <tr><th>Column</th><th>Type</th><th>Constraints</th><th>Notes</th></tr>
  <tr><td><code>user_id</code></td><td>UUID</td><td><span class="badge badge-pk">PK</span></td><td>Unique user identifier</td></tr>
  <tr><td><code>email</code></td><td>VARCHAR(255)</td><td>UNIQUE, NOT NULL, <span class="badge badge-idx">INDEX</span></td><td>Login identifier. <strong>Hash index</strong> on this column for O(1) lookups during login.</td></tr>
  <tr><td><code>username</code></td><td>VARCHAR(100)</td><td>NOT NULL</td><td>Display name</td></tr>
  <tr><td><code>password_hash</code></td><td>VARCHAR(255)</td><td>NOT NULL</td><td>Bcrypt/Argon2 hash</td></tr>
  <tr><td><code>subscription_tier</code></td><td>ENUM('free','plus','enterprise')</td><td>DEFAULT 'free'</td><td>Determines rate limits and model access</td></tr>
  <tr><td><code>created_at</code></td><td>TIMESTAMP</td><td>NOT NULL</td><td></td></tr>
  <tr><td><code>updated_at</code></td><td>TIMESTAMP</td><td>NOT NULL</td><td></td></tr>
</table>

<p><strong>Why SQL?</strong> The Users table has a well-defined, rigid schema that rarely changes. User data is relational (could join with billing, subscription, or organization tables). ACID compliance is essential for authentication operations — we cannot tolerate eventual consistency when validating credentials or updating subscription tiers. The dataset is relatively small (millions of rows, not billions), so SQL handles it efficiently.</p>

<p><strong>Index:</strong> <code>Hash index on email</code> — used during login for exact-match lookups by email. Hash indexes provide O(1) average lookups, which is ideal since email lookups are always exact-match (we never do range queries on email).</p>

<p><strong>Sharding:</strong> For large scale (100M+ users), shard by <code>user_id</code> using consistent hashing. Since each user's data is self-contained and queries always include <code>user_id</code> or <code>email</code>, this ensures single-shard queries.</p>

<p><strong>Read events:</strong> User login (validate credentials), API Gateway auth check (load subscription tier for rate limiting).<br/>
<strong>Write events:</strong> User registration, subscription tier upgrade/downgrade, profile updates.</p>


<h3>7.2 Conversations Table — NoSQL (Document Store)</h3>
<table>
  <tr><th>Field</th><th>Type</th><th>Constraints</th><th>Notes</th></tr>
  <tr><td><code>conversation_id</code></td><td>UUID</td><td><span class="badge badge-pk">PK</span> (Partition Key)</td><td>Unique conversation identifier</td></tr>
  <tr><td><code>user_id</code></td><td>UUID</td><td><span class="badge badge-fk">FK → Users</span>, <span class="badge badge-idx">GSI</span></td><td>Owner of the conversation. <strong>Global Secondary Index</strong> on <code>(user_id, updated_at DESC)</code> — a <strong>B-tree composite index</strong> to efficiently list a user's conversations sorted by most recently updated.</td></tr>
  <tr><td><code>title</code></td><td>STRING</td><td></td><td>Auto-generated or user-edited conversation title</td></tr>
  <tr><td><code>model_version</code></td><td>STRING</td><td>NOT NULL</td><td>Which model is being used (e.g., "gpt-4", "gpt-4-mini")</td></tr>
  <tr><td><code>last_message_preview</code></td><td>STRING(200)</td><td></td><td><strong>Denormalized</strong> — first 200 chars of the last message, for sidebar display</td></tr>
  <tr><td><code>message_count</code></td><td>INTEGER</td><td>DEFAULT 0</td><td><strong>Denormalized</strong> — avoids counting messages in a separate query</td></tr>
  <tr><td><code>is_archived</code></td><td>BOOLEAN</td><td>DEFAULT false</td><td>Soft-archive a conversation</td></tr>
  <tr><td><code>created_at</code></td><td>TIMESTAMP</td><td>NOT NULL</td><td></td></tr>
  <tr><td><code>updated_at</code></td><td>TIMESTAMP</td><td>NOT NULL</td><td>Updated every time a new message is added</td></tr>
</table>

<p><strong>Why NoSQL (Document Store)?</strong> Conversations are independent, self-contained documents with no complex joins. The schema may evolve (adding fields like <code>plugins_used</code>, <code>shared_with</code>, etc.) and a document store accommodates schema flexibility without migrations. Write throughput is critical — every message exchange updates the conversation record. NoSQL scales horizontally via partitioning far more naturally than SQL. The access pattern is simple: (1) get conversation by ID, and (2) list conversations by user_id sorted by updated_at — both of which NoSQL handles well with partition keys and indexes.</p>

<p><strong>Denormalization explanation:</strong></p>
<ul>
  <li><code>last_message_preview</code>: Stored directly on the conversation record so that when listing conversations in the sidebar, we do NOT need to join or query the Messages table for each conversation. This avoids an N+1 query problem. The tradeoff is a small write amplification (updating this field on every message), but sidebar loads are far more frequent than message sends.</li>
  <li><code>message_count</code>: Stored directly to avoid a <code>COUNT(*)</code> aggregation query on messages, which would be expensive in NoSQL. Updated atomically (increment) when a message is persisted.</li>
</ul>

<p><strong>Index:</strong> <code>Global Secondary Index on (user_id, updated_at DESC)</code> — a B-tree composite index. The <code>user_id</code> component enables partition-level filtering (all conversations for one user), and the <code>updated_at DESC</code> component enables sorted retrieval without a full scan. B-tree is chosen because we need ordered traversal (sorting by recency), which hash indexes cannot provide.</p>

<p><strong>Sharding:</strong> Partition by <code>user_id</code> using consistent hashing. This co-locates all of a user's conversations on the same partition, making the "list my conversations" query a single-partition scan. Users with extremely high conversation counts (power users) are unlikely to cause hot partitions because typical conversation counts are in the hundreds, not millions.</p>

<p><strong>Read events:</strong> User opens app (list conversations), user opens a specific conversation (read metadata).<br/>
<strong>Write events:</strong> New conversation created, message sent (updates <code>updated_at</code>, <code>last_message_preview</code>, <code>message_count</code>), title auto-generated, user renames conversation, user archives conversation.</p>


<h3>7.3 Messages Table — NoSQL (Wide-Column / Document Store)</h3>
<table>
  <tr><th>Field</th><th>Type</th><th>Constraints</th><th>Notes</th></tr>
  <tr><td><code>conversation_id</code></td><td>UUID</td><td><span class="badge badge-pk">PK</span> (Partition Key)</td><td>Groups all messages for a conversation on the same partition</td></tr>
  <tr><td><code>sequence_number</code></td><td>INTEGER</td><td><span class="badge badge-sk">Sort Key</span></td><td>Monotonically increasing within a conversation. Enables ordered retrieval and cursor-based pagination.</td></tr>
  <tr><td><code>message_id</code></td><td>UUID</td><td>UNIQUE</td><td>Globally unique message identifier (for regeneration, soft-delete, etc.)</td></tr>
  <tr><td><code>role</code></td><td>ENUM('system','user','assistant')</td><td>NOT NULL</td><td>Who sent this message</td></tr>
  <tr><td><code>content</code></td><td>TEXT</td><td>NOT NULL</td><td>The message body. Can range from 1 token to 100K+ tokens for long assistant responses.</td></tr>
  <tr><td><code>token_count</code></td><td>INTEGER</td><td>NOT NULL</td><td>Number of tokens in this message (for billing and context window calculation)</td></tr>
  <tr><td><code>model_version</code></td><td>STRING</td><td></td><td><strong>Denormalized</strong> from Conversations table — avoids a join to know which model generated a specific response</td></tr>
  <tr><td><code>finish_reason</code></td><td>ENUM('stop','length','cancelled')</td><td></td><td>Only for assistant messages. 'stop' = natural completion, 'length' = hit max_tokens, 'cancelled' = user stopped generation.</td></tr>
  <tr><td><code>is_active</code></td><td>BOOLEAN</td><td>DEFAULT true</td><td>Set to false when a message is replaced by a regenerated response</td></tr>
  <tr><td><code>created_at</code></td><td>TIMESTAMP</td><td>NOT NULL</td><td></td></tr>
</table>

<p><strong>Why NoSQL (Wide-Column / Document)?</strong> Messages are append-heavy with very high write throughput (millions of messages per hour across all users). The content field is variable-size (from a few bytes to hundreds of kilobytes). The primary access pattern — "get all messages in a conversation, ordered by sequence" — maps perfectly to a partition key (<code>conversation_id</code>) + sort key (<code>sequence_number</code>) model. No joins are needed. NoSQL's horizontal scalability via partitioning is essential for this table, which will be the largest in the system by orders of magnitude.</p>

<p><strong>Denormalization explanation:</strong> <code>model_version</code> is duplicated from the Conversations table onto each message. This allows the client to display per-message model badges (e.g., "Generated by GPT-4") without joining the Conversations table. Since model_version rarely changes within a conversation (and never retroactively on old messages), the write amplification is negligible.</p>

<p><strong>Index:</strong> The <code>(conversation_id, sequence_number)</code> partition + sort key combination acts as the primary index. This is a B-tree/LSM-tree sorted index that supports:</p>
<ul>
  <li>Efficient range scans: "get messages 1–100 in conv_abc" → single partition, sorted scan.</li>
  <li>Cursor-based pagination: "get messages before sequence 101" → seek + scan.</li>
  <li>No additional secondary indexes needed for the primary access pattern.</li>
</ul>

<p><strong>Sharding:</strong> Partition by <code>conversation_id</code>. All messages in a single conversation reside on the same shard, which means retrieving a conversation's messages never requires cross-shard queries. The consistent hashing of <code>conversation_id</code> (UUID) provides excellent distribution. No hot-partition risk because individual conversations rarely exceed thousands of messages.</p>

<p><strong>Read events:</strong> User opens a conversation (retrieves messages), Chat Service constructs a prompt for inference (reads recent messages for context).<br/>
<strong>Write events:</strong> User sends a message (sync write of user message), Persistence Worker writes assistant response (async write), user regenerates (old message soft-deleted, new message written).</p>


<h3>7.4 Usage Table — Time-Series Database</h3>
<table>
  <tr><th>Field</th><th>Type</th><th>Constraints</th><th>Notes</th></tr>
  <tr><td><code>user_id</code></td><td>UUID</td><td><span class="badge badge-pk">PK</span> (Partition Key)</td><td></td></tr>
  <tr><td><code>timestamp</code></td><td>TIMESTAMP</td><td><span class="badge badge-sk">Sort Key</span></td><td>When the usage event occurred</td></tr>
  <tr><td><code>model_version</code></td><td>STRING</td><td>NOT NULL</td><td>Which model was used</td></tr>
  <tr><td><code>prompt_tokens</code></td><td>INTEGER</td><td>NOT NULL</td><td>Number of tokens in the prompt</td></tr>
  <tr><td><code>completion_tokens</code></td><td>INTEGER</td><td>NOT NULL</td><td>Number of tokens generated</td></tr>
  <tr><td><code>conversation_id</code></td><td>UUID</td><td><span class="badge badge-fk">FK → Conversations</span></td><td>For traceability</td></tr>
</table>

<p><strong>Why Time-Series?</strong> Usage data is naturally time-ordered, append-only, and write-heavy. Queries are always time-range based ("How many tokens did user X use in the past 3 hours?" for rate limiting, or "Total usage this billing period" for billing). Time-series databases excel at these patterns with built-in time-based partitioning, efficient compression of sequential data, and fast aggregation functions. They also support automatic data retention policies (e.g., auto-delete raw data older than 90 days while keeping daily rollups).</p>

<p><strong>Read events:</strong> Rate limiter checks recent token usage before allowing a new request; billing system aggregates monthly usage.<br/>
<strong>Write events:</strong> Every completed inference (Persistence Worker records prompt + completion tokens).</p>


<!-- ═══════════════════════════════════════════════ -->
<h2 id="cdn-cache">8. CDN &amp; Caching Deep Dive</h2>

<h3>8.1 CDN</h3>
<p>A CDN is <strong>appropriate for static assets</strong> in this system — the web client's JavaScript bundles, CSS stylesheets, images, fonts, and any static documentation pages. These assets are:</p>
<ul>
  <li>Identical for all users (no personalization).</li>
  <li>Read-heavy (every page load fetches them).</li>
  <li>Change infrequently (only on deployments).</li>
</ul>
<p>A CDN is <strong>NOT appropriate for API responses</strong> (chat messages, conversation lists) because:</p>
<ul>
  <li>API responses are personalized per user.</li>
  <li>Chat responses are dynamically generated in real-time.</li>
  <li>SSE streams cannot be cached by a CDN.</li>
</ul>
<p>CDN cache headers for static assets: <code>Cache-Control: public, max-age=31536000, immutable</code> (with content-hashed filenames for cache busting on deploys).</p>

<h3>8.2 In-Memory Cache</h3>
<p>An in-memory cache sits between the Chat Service and the Conversation Store. It is <strong>essential</strong> for this system because:</p>
<ul>
  <li>Users frequently reload the conversation list (every app open, every tab switch).</li>
  <li>Active conversations are repeatedly read (every new message requires reading the conversation history to construct the prompt).</li>
  <li>The same conversation data is read far more often than it's written (high read:write ratio during a chat session).</li>
</ul>

<h4>Cache Entries</h4>
<table>
  <tr><th>Cache Key Pattern</th><th>Data Cached</th><th>Strategy</th><th>TTL</th><th>Eviction</th></tr>
  <tr>
    <td><code>user:{user_id}:conversations</code></td>
    <td>List of conversation summaries (title, updated_at, preview)</td>
    <td><strong>Cache-aside (lazy loading)</strong></td>
    <td>5 minutes</td>
    <td>LRU</td>
  </tr>
  <tr>
    <td><code>conv:{conversation_id}:messages</code></td>
    <td>Ordered list of messages in a conversation</td>
    <td><strong>Write-through</strong></td>
    <td>15 minutes</td>
    <td>LRU</td>
  </tr>
  <tr>
    <td><code>user:{user_id}:session</code></td>
    <td>Session data (user_id, subscription_tier, preferences)</td>
    <td><strong>Write-through</strong></td>
    <td>1 hour (matches session duration)</td>
    <td>LRU</td>
  </tr>
  <tr>
    <td><code>ratelimit:{user_id}</code></td>
    <td>Sliding window message count and token count</td>
    <td><strong>Write-through</strong></td>
    <td>Matches rate limit window (e.g., 3 hours)</td>
    <td>TTL-based</td>
  </tr>
</table>

<h4>Strategy Explanations</h4>
<ul>
  <li><strong>Cache-aside (conversation list):</strong> On a cache miss, the Chat Service queries the Conversation Store, then populates the cache. We use cache-aside here (instead of write-through) because the conversation list changes with every message sent, but is only read when the user navigates to the sidebar. Writing through on every message would cause unnecessary cache writes for users who aren't currently viewing their list.</li>
  <li><strong>Write-through (conversation messages):</strong> When a new message is persisted by the Persistence Worker, it simultaneously updates the cache. We use write-through here because during an active chat session, the Chat Service needs the latest messages to construct the next prompt immediately after a response is persisted. Without write-through, the very next user message would cause a cache miss on the just-completed conversation.</li>
</ul>

<h4>Eviction Policy: LRU (Least Recently Used)</h4>
<p>LRU is chosen because access patterns in a chat application have strong temporal locality — users typically focus on 1–3 conversations at a time, then move on. Conversations that haven't been accessed recently are good candidates for eviction. LRU naturally prioritizes keeping active conversations warm.</p>

<h4>Expiration Policy (TTL)</h4>
<ul>
  <li><strong>5 minutes for conversation lists:</strong> Short TTL because the list changes whenever any conversation is updated (new message, title change). 5 minutes balances freshness with read reduction.</li>
  <li><strong>15 minutes for conversation messages:</strong> Longer TTL because messages are immutable once written (only soft-deleted on regeneration). During an active session, the write-through strategy keeps the cache fresh. The 15-minute TTL handles the case where a user returns to an old conversation after a brief absence.</li>
</ul>

<!-- ═══════════════════════════════════════════════ -->
<h2 id="sse">9. Streaming Deep Dive — Server-Sent Events (SSE)</h2>

<h3>Why SSE?</h3>
<p>SSE is the chosen protocol for streaming tokens from server to client. Here's why:</p>
<table>
  <tr><th>Protocol</th><th>Direction</th><th>Overhead</th><th>Reconnection</th><th>Infra Compatibility</th><th>Verdict</th></tr>
  <tr>
    <td><strong>SSE</strong></td>
    <td>Server → Client (unidirectional)</td>
    <td>Low (HTTP-based)</td>
    <td>Built-in auto-reconnect</td>
    <td>Works with standard HTTP load balancers, proxies, CDNs</td>
    <td><strong>✅ Chosen</strong></td>
  </tr>
  <tr>
    <td>WebSocket</td>
    <td>Bidirectional</td>
    <td>Medium (upgrade handshake, frame overhead)</td>
    <td>Manual implementation needed</td>
    <td>Requires WebSocket-aware LBs and proxies; stateful connections</td>
    <td>❌ Overkill</td>
  </tr>
  <tr>
    <td>Long Polling</td>
    <td>Simulated push</td>
    <td>High (repeated HTTP requests)</td>
    <td>Inherent (each poll is a new request)</td>
    <td>Fully compatible</td>
    <td>❌ Too slow</td>
  </tr>
  <tr>
    <td>HTTP/2 Server Push</td>
    <td>Server → Client</td>
    <td>Low</td>
    <td>N/A (being deprecated by browsers)</td>
    <td>Limited browser support</td>
    <td>❌ Deprecated</td>
  </tr>
</table>

<h3>Why NOT WebSocket?</h3>
<p>WebSockets provide full-duplex bidirectional communication, but ChatGPT's token streaming is fundamentally <strong>unidirectional</strong>: the user sends one HTTP request (with their message), and the server streams back N tokens. There is no need for the server and client to send messages concurrently in both directions.</p>
<p>WebSockets also introduce complexity:</p>
<ul>
  <li><strong>Connection state management:</strong> WebSocket connections are stateful and long-lived. If a Chat Service instance goes down, all its WebSocket connections are lost and must be re-established. With SSE, the standard <code>EventSource</code> API auto-reconnects.</li>
  <li><strong>Infrastructure friction:</strong> Many load balancers, proxies, and firewalls require special WebSocket configuration. SSE works over standard HTTP/1.1 or HTTP/2 without any special infrastructure.</li>
  <li><strong>No benefit:</strong> The only client-to-server message is the initial prompt, which is sent as a regular HTTP POST. There's nothing else the client needs to send while the response is streaming.</li>
</ul>

<h3>Why NOT Long Polling?</h3>
<p>Long polling simulates server push by having the client repeatedly make HTTP requests. For token streaming at 30–80 tokens/second, this would mean dozens of HTTP request/response cycles per second per user — extremely wasteful in terms of latency overhead and server resource consumption.</p>

<h3>How SSE Works in This System</h3>
<ol>
  <li><strong>Connection Establishment:</strong> The client sends <code>POST /api/chat/completions</code> with header <code>Accept: text/event-stream</code>. The server responds with <code>Content-Type: text/event-stream</code> and keeps the HTTP connection open.</li>
  <li><strong>Token Delivery:</strong> As each token is generated by the GPU, the server writes an SSE event:
<pre><code>data: {"id":"msg_123","object":"chat.completion.chunk","choices":[{"delta":{"content":"Hello"},"index":0}]}

</code></pre>
  (Each event is terminated by two newlines <code>\n\n</code>.)</li>
  <li><strong>Completion:</strong> When the model finishes generating, the server sends:
<pre><code>data: [DONE]

</code></pre></li>
  <li><strong>Connection Closure:</strong> After <code>[DONE]</code>, the server closes the HTTP connection. The client detects the closure and knows the response is complete.</li>
  <li><strong>Error Handling:</strong> If the server encounters an error mid-stream, it sends an SSE error event:
<pre><code>data: {"error":{"message":"Internal error","type":"server_error"}}

</code></pre>
  and closes the connection. The client can display the partial response and offer a "Retry" option.</li>
  <li><strong>Client-Initiated Cancellation (Stop):</strong> If the user clicks "Stop", the client calls <code>AbortController.abort()</code> which closes the underlying HTTP connection. The Chat Service detects the broken connection and propagates a cancellation to the Inference Service.</li>
</ol>


<!-- ═══════════════════════════════════════════════ -->
<h2 id="grpc">10. gRPC Deep Dive (Inter-Service Communication)</h2>

<h3>Why gRPC Between Chat Service ↔ Inference Service?</h3>
<p>Internal service-to-service communication uses <strong>gRPC with Protocol Buffers</strong> rather than HTTP/REST or GraphQL for the following reasons:</p>
<ul>
  <li><strong>Native streaming support:</strong> gRPC supports server-side streaming, client-side streaming, and bidirectional streaming natively. This maps perfectly to the inference use case: send a prompt (unary), receive a stream of tokens (server-side streaming).</li>
  <li><strong>Efficient binary serialization:</strong> Protocol Buffers encode data in a compact binary format, ~10x smaller than JSON. When streaming millions of token events per second across the cluster, this significantly reduces network bandwidth and serialization overhead.</li>
  <li><strong>Strong typing:</strong> Protobuf service definitions provide compile-time type safety and auto-generated client/server stubs, reducing integration bugs between the Chat Service and Inference Service.</li>
  <li><strong>Built on HTTP/2:</strong> gRPC uses HTTP/2 under the hood, providing multiplexing (multiple streams on one TCP connection), header compression, and flow control.</li>
  <li><strong>Deadline/timeout propagation:</strong> gRPC supports deadline propagation, which is critical for enforcing max generation time limits and propagating cancellation signals from client all the way to GPU workers.</li>
</ul>

<h3>Why NOT REST/HTTP Between Internal Services?</h3>
<p>REST over HTTP/1.1 would require establishing a new TCP connection for each request (or managing keep-alive pools) and does not natively support streaming. Polling-based approaches to simulate streaming would add significant latency. While HTTP/2 could support streaming with REST, gRPC provides a more ergonomic and battle-tested framework for this pattern.</p>

<h3>Protobuf Service Definition (Simplified)</h3>
<pre><code>service InferenceService {
  // Send a prompt, receive a stream of tokens
  rpc GenerateStream(GenerateRequest) returns (stream TokenChunk);
}

message GenerateRequest {
  repeated Message messages = 1;
  string model = 2;
  float temperature = 3;
  float top_p = 4;
  int32 max_tokens = 5;
  repeated string stop_sequences = 6;
  string user_id = 7;         // for priority scheduling
  string tier = 8;            // "free" | "plus" | "enterprise"
}

message Message {
  string role = 1;            // "system" | "user" | "assistant"
  string content = 2;
}

message TokenChunk {
  string token = 1;
  bool is_finished = 2;
  string finish_reason = 3;   // "stop" | "length" | "cancelled"
  int32 prompt_tokens = 4;    // only set on the last chunk
  int32 completion_tokens = 5; // only set on the last chunk
}
</code></pre>


<!-- ═══════════════════════════════════════════════ -->
<h2 id="mq">11. Message Queue Deep Dive</h2>

<h3>Why a Message Queue?</h3>
<p>The Message Queue decouples the <strong>real-time streaming path</strong> (latency-critical) from the <strong>persistence and analytics path</strong> (not latency-critical). Without it, the Chat Service would need to synchronously write the assistant's response to the database before closing the SSE connection, adding hundreds of milliseconds of latency to the perceived response time.</p>

<h3>What Goes on the Queue?</h3>
<p>After a response is fully generated (or partially generated due to cancellation), the Chat Service publishes a message to the queue:</p>
<pre><code>{
  "event_type": "response_completed",
  "conversation_id": "conv_abc123",
  "message_id": "msg_456",
  "role": "assistant",
  "content": "Quantum entanglement is when two particles...",
  "token_count": 850,
  "model_version": "gpt-4",
  "finish_reason": "stop",
  "user_id": "user_alice",
  "timestamp": "2025-01-15T10:30:00Z"
}</code></pre>

<h3>How Messages Are Consumed</h3>
<ul>
  <li><strong>Persistence Worker(s):</strong> A pool of consumers reads from the queue. Each consumer processes one message at a time:
    <ol>
      <li>Writes the assistant message to the Messages table in the Conversation Store.</li>
      <li>Updates the Conversations table: <code>updated_at</code>, <code>last_message_preview</code>, <code>message_count++</code>.</li>
      <li>Updates the Cache (write-through) with the new message and updated conversation metadata.</li>
      <li>Writes to the Usage Store (token count).</li>
      <li>If <code>message_count == 2</code> (first user-assistant exchange), triggers auto-title generation.</li>
    </ol>
  </li>
  <li><strong>Acknowledgment:</strong> After all writes succeed, the consumer acknowledges the message, removing it from the queue.</li>
  <li><strong>Failure handling:</strong> If any write fails, the message is NOT acknowledged. It becomes visible again after a visibility timeout and is retried. After 3 retries, it's moved to a <strong>Dead Letter Queue (DLQ)</strong> for manual inspection.</li>
</ul>

<h3>Why NOT Synchronous Writes?</h3>
<p>If the Chat Service wrote the response to the database synchronously (before sending <code>[DONE]</code> to the client), it would add 50–200ms of latency to every response completion. Multiplied across millions of requests, this creates unnecessary load on the database and degrades user experience. The message queue provides <strong>at-least-once delivery</strong> guarantees, ensuring that even if a Persistence Worker crashes mid-write, the message will be retried.</p>

<h3>Why NOT Pub/Sub?</h3>
<p>A publish/subscribe system is designed for fan-out — one message, multiple subscribers. In our case, each completed response needs to be processed exactly once by a single Persistence Worker. A message queue with competing consumers (consumer group pattern) is the correct primitive. Pub/Sub would introduce unnecessary complexity and the risk of duplicate processing across subscribers.</p>


<!-- ═══════════════════════════════════════════════ -->
<h2 id="inference">12. GPU Inference Deep Dive</h2>

<h3>Architecture of the Inference Service</h3>
<p>The Inference Service is the most complex and expensive component in the system. It manages GPU resources to serve LLM inference requests with low latency and high throughput.</p>

<div class="diagram-container">
<pre class="mermaid">
graph TB
    CS["Chat Service"] -->|"gRPC Stream"| IS["Inference Gateway"]
    IS --> SCHED["Request Scheduler"]
    SCHED --> PQ["Priority Queue<br/>(Paid > Free)"]
    PQ --> BATCH["Continuous Batcher"]
    BATCH --> GPU1["GPU Node 1<br/>(Model Shard)"]
    BATCH --> GPU2["GPU Node 2<br/>(Model Shard)"]
    BATCH --> GPUN["GPU Node N<br/>(Model Shard)"]
    GPU1 & GPU2 & GPUN --> KV["KV Cache<br/>Manager"]
    GPU1 & GPU2 & GPUN --> STREAM["Token<br/>Streamer"]
    STREAM --> IS
    IS -->|"gRPC Token Stream"| CS
    HEALTH["Health Monitor"] --> GPU1 & GPU2 & GPUN
</pre>
</div>

<h4>Request Scheduler + Priority Queue</h4>
<p>Every incoming inference request is placed in a priority queue. Priority is determined by:</p>
<ol>
  <li><strong>Subscription tier:</strong> Enterprise > Plus > Free.</li>
  <li><strong>Wait time:</strong> Within the same tier, requests that have waited longer get higher priority (starvation prevention).</li>
  <li><strong>Request size:</strong> Shorter prompts may be prioritized over very long ones to improve overall throughput (optional optimization).</li>
</ol>
<p>When the system is at capacity, free-tier requests may be queued (with the client showing "You're in queue, position #X") while paid users continue to get immediate service.</p>

<h4>Continuous Batching</h4>
<p>Traditional batching waits until the longest request in a batch finishes before accepting new requests. <strong>Continuous batching</strong> (also called "inflight batching") is a critical optimization:</p>
<ul>
  <li>New requests can be added to a running batch at any iteration step.</li>
  <li>Completed requests are immediately removed, freeing their KV cache memory.</li>
  <li>This increases GPU utilization from ~30% (naive batching) to ~80%+ because the GPU is never idle waiting for the longest request.</li>
</ul>

<h4>KV Cache Manager</h4>
<p>During autoregressive generation, the model stores <strong>key-value pairs</strong> from the attention mechanism for all previously generated tokens. This KV cache consumes significant GPU memory (often more than the model weights themselves for long contexts).</p>
<ul>
  <li><strong>Paged Attention:</strong> Instead of pre-allocating contiguous memory for the maximum context length, memory is allocated in fixed-size "pages" on demand. This eliminates memory waste from over-allocation and reduces fragmentation.</li>
  <li><strong>Eviction:</strong> When GPU memory is exhausted, the scheduler pauses low-priority requests, evicts their KV cache to CPU memory (swapping), and resumes them later.</li>
</ul>

<h4>Token Generation Pipeline</h4>
<ol>
  <li><strong>Prefill Phase:</strong> Process all input tokens in parallel. This is the compute-heavy phase — the model runs a forward pass on the entire prompt. For a 10K-token prompt, this can take 1–5 seconds depending on model size and GPU count.</li>
  <li><strong>Decode Phase:</strong> Generate output tokens one at a time (autoregressive). Each decode step takes ~20–50ms, producing one token. This is the memory-bandwidth-bound phase.</li>
  <li><strong>Sampling:</strong> After each forward pass, a sampling strategy (temperature, top-p, top-k) is applied to the logits to select the next token.</li>
  <li><strong>Token Streaming:</strong> Each generated token is immediately sent back via gRPC, not buffered until generation is complete.</li>
</ol>

<h4>Model Sharding (for Large Models)</h4>
<p>A model with 100B+ parameters does not fit on a single GPU (80GB VRAM). Sharding strategies:</p>
<ul>
  <li><strong>Tensor Parallelism:</strong> Each layer's weight matrices are split across N GPUs. Each GPU computes a portion of each layer, then results are communicated via all-reduce. Low latency but requires high-bandwidth interconnect (NVLink). Typically 2–8 GPUs.</li>
  <li><strong>Pipeline Parallelism:</strong> Different layers are placed on different GPUs. Token generation flows through GPUs sequentially. Introduces pipeline bubbles but allows scaling to more GPUs.</li>
  <li><strong>Typically both are combined:</strong> Tensor parallelism within a server (NVLink-connected GPUs), pipeline parallelism across servers.</li>
</ul>


<!-- ═══════════════════════════════════════════════ -->
<h2 id="lb">13. Load Balancer Deep Dive</h2>

<h3>Where Load Balancers Are Placed</h3>
<ol>
  <li><strong>Edge Load Balancer (Client → API Gateway):</strong> Faces the public internet. Handles all incoming user traffic. Terminates TLS.</li>
  <li><strong>Internal Load Balancer (Chat Service → Inference Service):</strong> Distributes inference requests across Inference Service instances. Internal only; no TLS termination needed (internal network is trusted).</li>
</ol>

<h3>Edge Load Balancer Configuration</h3>
<table>
  <tr><th>Property</th><th>Value</th><th>Rationale</th></tr>
  <tr><td>Layer</td><td>Layer 7 (HTTP)</td><td>Needs to inspect HTTP headers for routing (e.g., <code>Accept: text/event-stream</code>), path-based routing, and WebSocket upgrade (if ever needed).</td></tr>
  <tr><td>Algorithm</td><td>Least Connections</td><td>SSE connections are <strong>long-lived</strong> (30 seconds to several minutes during generation). Round-robin would not account for servers with many active streams, leading to uneven load. Least-connections routes new requests to the server with the fewest active connections.</td></tr>
  <tr><td>TLS Termination</td><td>Yes</td><td>Terminate TLS at the LB so backend services handle plaintext HTTP, reducing their CPU load.</td></tr>
  <tr><td>Health Checks</td><td>HTTP GET /health every 10s</td><td>Removes unhealthy API Gateway instances from the pool.</td></tr>
  <tr><td>Connection Timeout</td><td>5 minutes</td><td>Long timeout to accommodate slow generation for complex prompts. Standard 30-second timeouts would kill SSE connections prematurely.</td></tr>
  <tr><td>Sticky Sessions</td><td>Not needed</td><td>All backend services are stateless; any instance can handle any request.</td></tr>
</table>

<h3>Internal Load Balancer (Chat Service → Inference Service)</h3>
<table>
  <tr><th>Property</th><th>Value</th><th>Rationale</th></tr>
  <tr><td>Layer</td><td>Layer 4 (TCP) for gRPC</td><td>gRPC manages its own multiplexing over HTTP/2. Layer 4 load balancing on TCP connections is simpler and sufficient. (Note: for HTTP/2-aware routing, Layer 7 may be preferred to balance individual streams rather than connections.)</td></tr>
  <tr><td>Algorithm</td><td>Least Connections</td><td>Inference requests have variable duration; least-connections distributes load based on active inference streams.</td></tr>
  <tr><td>Health Checks</td><td>gRPC health check protocol every 5s</td><td>Standard gRPC health checking service.</td></tr>
</table>


<!-- ═══════════════════════════════════════════════ -->
<h2 id="scaling">14. Scaling Considerations</h2>

<h3>14.1 Horizontal Scaling of Stateless Services</h3>
<ul>
  <li><strong>API Gateway</strong>: Stateless — scale horizontally behind the edge Load Balancer. Add instances based on request rate (requests/second). Auto-scale trigger: CPU > 60% or request queue depth > threshold.</li>
  <li><strong>Chat Service</strong>: Stateless — scale horizontally. Each instance manages a set of SSE connections. Scale based on active connection count. Auto-scale trigger: active connections > 10K per instance.</li>
  <li><strong>Persistence Workers</strong>: Scale based on message queue depth. If the queue grows beyond threshold, add more workers. Scale down when the queue drains.</li>
</ul>

<h3>14.2 GPU Cluster Scaling (The Primary Bottleneck)</h3>
<p>GPU compute is the most expensive and scarce resource. Scaling strategies:</p>
<ul>
  <li><strong>Continuous batching:</strong> Maximize utilization of existing GPUs (described in inference deep dive).</li>
  <li><strong>Model quantization:</strong> Run models in INT8 or INT4 precision instead of FP16. Reduces memory usage by 2–4x and increases throughput at the cost of marginal quality loss. Appropriate for free-tier users.</li>
  <li><strong>Model distillation:</strong> Offer smaller, faster models (e.g., "GPT-4-mini") for latency-sensitive or cost-sensitive use cases.</li>
  <li><strong>Heterogeneous GPU fleet:</strong> Mix high-end GPUs (for large models, paid users) with cheaper GPUs (for smaller models, free users).</li>
  <li><strong>Autoscaling GPU nodes:</strong> Add/remove GPU nodes based on inference queue depth and wait time. Free-tier traffic can be shed during peak load.</li>
  <li><strong>Geographic distribution:</strong> Deploy GPU clusters in multiple regions to reduce latency for global users and provide redundancy.</li>
</ul>

<h3>14.3 Database Scaling</h3>
<ul>
  <li><strong>Conversation Store (NoSQL):</strong> Already horizontally partitioned by <code>conversation_id</code> and <code>user_id</code>. Adding nodes rebalances partitions automatically. For read scaling, add read replicas.</li>
  <li><strong>User Store (SQL):</strong> At moderate scale (~10M users), a single primary with read replicas suffices. At 100M+ users, implement horizontal sharding by <code>user_id</code>.</li>
  <li><strong>Usage Store (Time-Series):</strong> Scales by adding nodes and time-based partitioning. Old data can be down-sampled (aggregate to daily/weekly) to reduce storage.</li>
</ul>

<h3>14.4 Cache Scaling</h3>
<ul>
  <li>The in-memory cache can be scaled by running a <strong>distributed cache cluster</strong> with consistent hashing across nodes.</li>
  <li>Scale based on cache hit rate — if hit rate drops below 80%, add capacity (more memory/nodes).</li>
</ul>

<h3>14.5 Message Queue Scaling</h3>
<ul>
  <li>Partition the queue by <code>conversation_id</code> hash to parallelize consumption.</li>
  <li>Add partitions and consumers as throughput demand grows.</li>
</ul>

<h3>14.6 Load Balancer Placement Summary</h3>
<table>
  <tr><th>Location</th><th>Type</th><th>Purpose</th></tr>
  <tr><td>Internet → API Gateway</td><td>Edge LB (L7)</td><td>Distribute external traffic, TLS termination, long-lived SSE connections</td></tr>
  <tr><td>Chat Service → Inference Service</td><td>Internal LB (L4)</td><td>Distribute inference requests across Inference Service instances</td></tr>
  <tr><td>Internet → CDN → Origin</td><td>CDN (acts as LB for static assets)</td><td>Distribute static asset requests globally</td></tr>
</table>


<!-- ═══════════════════════════════════════════════ -->
<h2 id="tradeoffs">15. Tradeoffs &amp; Deep Dives</h2>

<h3>15.1 Sync vs. Async Persistence of Assistant Responses</h3>
<p><strong>Tradeoff:</strong> Persisting the assistant's response synchronously (before sending <code>[DONE]</code>) guarantees durability immediately, but adds latency. Persisting asynchronously via a message queue reduces latency but introduces a window where the response exists only in the client's memory.</p>
<p><strong>Decision:</strong> We chose <strong>async persistence</strong> because: (a) the user already has the response rendered in their client, so there's no data loss from the user's perspective; (b) the message queue provides at-least-once delivery, so the response will eventually be persisted; (c) in the rare case of a total message queue failure, the client can retry sending the response content back to the server.</p>

<h3>15.2 Context Window Management: Truncation vs. Summarization</h3>
<p><strong>Tradeoff:</strong> When a conversation exceeds the model's context window:</p>
<ul>
  <li><strong>Truncation</strong> (drop oldest messages): simple, no extra compute, but loses information.</li>
  <li><strong>Summarization</strong> (summarize old messages into a condensed block): preserves more information but requires an extra inference call (additional latency and GPU cost).</li>
</ul>
<p><strong>Decision:</strong> Use <strong>truncation by default</strong> for cost efficiency. Offer summarization as a premium feature for paid users who need long-context conversations. In practice, most conversations are short enough (< 10 messages) that context overflow is rare.</p>

<h3>15.3 Denormalization Tradeoffs</h3>
<p><strong>Tradeoff:</strong> <code>last_message_preview</code> and <code>message_count</code> on the Conversations table introduce write amplification (every message updates the conversation record) in exchange for eliminating read-time joins.</p>
<p><strong>Decision:</strong> The write amplification is acceptable because: (a) conversation updates are already happening on every message (updating <code>updated_at</code>), so the additional fields add minimal overhead; (b) the conversation list is the most frequently loaded view, and removing the N+1 query dramatically reduces read latency.</p>

<h3>15.4 Strong Consistency vs. Eventual Consistency</h3>
<p><strong>Tradeoff:</strong> The Conversation Store (NoSQL) typically offers eventual consistency by default. For chat, eventual consistency means a user might briefly not see their most recent message after a page refresh.</p>
<p><strong>Decision:</strong> Use <strong>strong consistency for reads immediately after writes</strong> (read-after-write consistency). The write-through cache ensures that the Chat Service always has the latest state. For cross-device scenarios (user sends on mobile, opens web), eventual consistency with a short propagation delay (< 1 second) is acceptable.</p>

<h3>15.5 Single Large Model vs. Multiple Specialized Models</h3>
<p><strong>Tradeoff:</strong> Running one massive model (e.g., 200B parameters) provides the highest quality but is expensive and slow. Running multiple specialized smaller models (coding model, creative writing model, general model) is cheaper but adds routing complexity.</p>
<p><strong>Decision:</strong> Offer <strong>multiple model tiers</strong> (e.g., a fast/cheap model and a large/capable model). Let users choose. Use the fast model by default and the large model for users who explicitly select it or for paid tiers. This optimizes cost while preserving quality for those who want it.</p>


<!-- ═══════════════════════════════════════════════ -->
<h2 id="alternatives">16. Alternative Approaches</h2>

<h3>16.1 WebSocket Instead of SSE for Token Streaming</h3>
<p><strong>Approach:</strong> Establish a persistent WebSocket connection per user session. Send the prompt over the WebSocket, receive tokens over the same connection.</p>
<p><strong>Why not chosen:</strong> WebSockets are bidirectional, but ChatGPT's streaming is unidirectional (server → client). WebSocket introduces connection management complexity (tracking connection state, handling reconnections, WebSocket-aware load balancers). SSE is simpler, works over standard HTTP, auto-reconnects, and is sufficient for the use case. OpenAI's actual ChatGPT product uses SSE.</p>

<h3>16.2 REST (Non-Streaming) Instead of SSE</h3>
<p><strong>Approach:</strong> Client sends the prompt, waits for the entire response to be generated, then receives it all at once as a single JSON response.</p>
<p><strong>Why not chosen:</strong> For a model that takes 10–60 seconds to generate a full response, the user would stare at a loading spinner the entire time. Streaming provides immediate feedback (time-to-first-token < 1s) and a better user experience. Streaming also allows users to stop generation early, saving GPU resources.</p>

<h3>16.3 SQL for All Data (Including Messages)</h3>
<p><strong>Approach:</strong> Use a single SQL database for users, conversations, and messages with traditional joins.</p>
<p><strong>Why not chosen:</strong> The Messages table will be by far the largest table (billions of rows). SQL databases struggle with this scale without significant sharding complexity. The access patterns (append-heavy writes, partition-scoped reads) map naturally to NoSQL. SQL's join capability is not needed — we never join Messages with other tables in application queries. The flexibility of a document model also accommodates future schema changes (e.g., adding tool-call metadata, image references).</p>

<h3>16.4 Polling the Inference Service Instead of Streaming gRPC</h3>
<p><strong>Approach:</strong> Chat Service submits an inference job, then polls the Inference Service periodically to check for new tokens.</p>
<p><strong>Why not chosen:</strong> Polling introduces latency between token generation and delivery (up to the polling interval). At 50 tokens/second generation speed, even 50ms polling intervals would noticeably degrade the smoothness of the streaming experience. gRPC streaming delivers tokens with near-zero additional latency.</p>

<h3>16.5 Event-Driven Architecture with Pub/Sub Everywhere</h3>
<p><strong>Approach:</strong> All inter-service communication goes through a pub/sub system. The Chat Service publishes "inference requested" events, the Inference Service subscribes, publishes "token generated" events, etc.</p>
<p><strong>Why not chosen:</strong> Pub/sub adds an indirection layer that increases latency for the real-time streaming path. The direct gRPC connection between Chat Service and Inference Service provides the lowest possible latency for token delivery. Pub/sub is appropriate for the async persistence path (which we use as a message queue), but not for the latency-critical inference path.</p>

<h3>16.6 Serverless / FaaS for Inference</h3>
<p><strong>Approach:</strong> Use serverless functions to handle inference, spinning up GPU instances on-demand for each request.</p>
<p><strong>Why not chosen:</strong> GPU cold starts are extremely slow (loading a 100B+ parameter model into GPU memory takes minutes). LLM inference requires persistent, pre-loaded model instances. Serverless architectures don't support this well. A dedicated GPU cluster with pre-loaded models and continuous batching is far more efficient.</p>


<!-- ═══════════════════════════════════════════════ -->
<h2 id="additional">17. Additional Considerations</h2>

<h3>17.1 Security</h3>
<ul>
  <li><strong>Input sanitization:</strong> User prompts must be sanitized to prevent prompt injection attacks (where malicious input attempts to override the system prompt).</li>
  <li><strong>Output filtering:</strong> Model outputs should pass through a content safety filter before delivery to detect and block harmful, biased, or policy-violating content.</li>
  <li><strong>Data isolation:</strong> Users must only be able to access their own conversations. Every database query must be scoped by <code>user_id</code>.</li>
  <li><strong>Encryption:</strong> TLS 1.3 for all external traffic. Encryption at rest for all data stores. GPU-to-GPU traffic within the cluster uses encrypted interconnects.</li>
  <li><strong>Audit logging:</strong> Log all API requests (without message content) for security auditing. Optionally allow users to export or delete their data (GDPR compliance).</li>
</ul>

<h3>17.2 Monitoring &amp; Observability</h3>
<ul>
  <li><strong>Key metrics:</strong> Time-to-first-token (TTFT), tokens-per-second (TPS), inference queue depth, GPU utilization, cache hit rate, error rate, p50/p99 latencies.</li>
  <li><strong>Distributed tracing:</strong> Trace a request from client → LB → API Gateway → Chat Service → Inference Service → GPU → back. Essential for debugging latency issues.</li>
  <li><strong>Alerting:</strong> Alert on: TTFT p99 > 3s, inference queue depth > 1000, GPU utilization < 50% (underutilized, wasting money) or > 95% (at risk of queueing), error rate > 1%.</li>
</ul>

<h3>17.3 Multi-Modal Support (Future)</h3>
<p>To support image/file inputs (vision models), the architecture would add:</p>
<ul>
  <li><strong>Object storage:</strong> For uploaded images/files.</li>
  <li><strong>Upload Service:</strong> Handles file uploads, generates pre-signed URLs, validates file types/sizes.</li>
  <li>The Chat Service would include file references in the prompt sent to a multi-modal model.</li>
</ul>

<h3>17.4 Plugin / Tool Use</h3>
<p>If the model calls external tools (code execution, web browsing, API calls), the Chat Service would need a <strong>Tool Execution Service</strong> that:</p>
<ol>
  <li>Detects tool-call tokens in the model output.</li>
  <li>Pauses token streaming.</li>
  <li>Executes the tool (e.g., runs Python code in a sandboxed environment).</li>
  <li>Injects the tool result back into the prompt.</li>
  <li>Resumes inference with the augmented context.</li>
</ol>

<h3>17.5 Data Retention &amp; Compliance</h3>
<ul>
  <li>Allow users to delete individual conversations or all data (right to be forgotten).</li>
  <li>Implement configurable data retention policies (e.g., enterprise customers may require 90-day retention; free-tier conversations may be auto-deleted after 30 days of inactivity).</li>
  <li>Anonymize conversation data used for model training (if opt-in). Provide users a clear opt-out mechanism.</li>
  <li>GDPR, CCPA, SOC 2 compliance for enterprise offerings.</li>
</ul>

<h3>17.6 Graceful Degradation</h3>
<ul>
  <li><strong>GPU cluster overload:</strong> When inference queue depth exceeds capacity, degrade gracefully: (a) queue free-tier users with a "high demand" message and estimated wait time, (b) reduce max_tokens for free tier, (c) route free-tier users to a smaller/faster model automatically.</li>
  <li><strong>Database outage:</strong> If the Conversation Store is unavailable, the Chat Service can still accept new messages (persisting them in the message queue for later replay). The user can continue chatting with context from the cache, but "load past conversations" will fail gracefully with a retry prompt.</li>
  <li><strong>Cache outage:</strong> All reads fall through to the Conversation Store. Latency increases but functionality is preserved.</li>
</ul>

<h3>17.7 Cost Optimization</h3>
<ul>
  <li><strong>Prompt caching:</strong> If multiple users send identical or nearly identical prompts (e.g., "What is the capital of France?"), the Inference Service can cache the KV states for common system prompt prefixes to avoid redundant prefill computation. This is especially impactful for the system prompt, which is identical across all requests for the same model.</li>
  <li><strong>Speculative decoding:</strong> Use a small "draft" model to generate candidate tokens quickly, then verify them in batches with the large model. This can increase tokens/second by 2–3x with no quality loss.</li>
  <li><strong>Right-sizing GPU allocation:</strong> Run smaller models on cheaper GPUs; reserve premium GPUs for the largest models.</li>
</ul>


<!-- ═══════════════════════════════════════════════ -->
<h2 id="vendors">18. Vendor Recommendations</h2>
<p>The architecture above is vendor-agnostic. Below are potential vendors for each infrastructure component, should a concrete implementation be needed.</p>

<h3>18.1 Conversation Store (NoSQL / Document / Wide-Column)</h3>
<table>
  <tr><th>Vendor</th><th>Rationale</th></tr>
  <tr><td>Amazon DynamoDB</td><td>Fully managed, auto-scaling, native partition + sort key model maps perfectly to our schema. Single-digit millisecond latency at any scale. Built-in Global Secondary Indexes.</td></tr>
  <tr><td>Apache Cassandra</td><td>Open-source, proven at massive write-heavy scale (used by Discord, Netflix). Partition + clustering key model aligns with our conversation_id + sequence_number pattern. Multi-region replication.</td></tr>
  <tr><td>ScyllaDB</td><td>Cassandra-compatible but written in C++ with significantly better per-node performance. Lower operational cost for the same throughput.</td></tr>
</table>

<h3>18.2 User Store (SQL)</h3>
<table>
  <tr><th>Vendor</th><th>Rationale</th></tr>
  <tr><td>PostgreSQL</td><td>Open-source, ACID-compliant, excellent for structured relational data with complex queries. Strong ecosystem (extensions, tooling). Handles millions of users easily.</td></tr>
  <tr><td>MySQL / Aurora</td><td>Battle-tested, widely supported. Aurora provides managed scaling and multi-AZ high availability.</td></tr>
  <tr><td>CockroachDB</td><td>Distributed SQL with horizontal scaling and strong consistency. Good choice if the user base grows to hundreds of millions and sharding Postgres becomes complex.</td></tr>
</table>

<h3>18.3 Usage Store (Time-Series)</h3>
<table>
  <tr><th>Vendor</th><th>Rationale</th></tr>
  <tr><td>InfluxDB</td><td>Purpose-built time-series database with built-in retention policies, down-sampling, and fast aggregation queries.</td></tr>
  <tr><td>TimescaleDB</td><td>PostgreSQL extension for time-series. Benefits from PostgreSQL ecosystem while providing time-series optimizations (hypertables, continuous aggregates).</td></tr>
  <tr><td>Amazon Timestream</td><td>Fully managed, auto-scaling, built-in retention tiers (in-memory for recent data, magnetic for historical).</td></tr>
</table>

<h3>18.4 In-Memory Cache</h3>
<table>
  <tr><th>Vendor</th><th>Rationale</th></tr>
  <tr><td>Redis</td><td>Industry standard for caching. Sub-millisecond latency, rich data structures (sorted sets for ordered conversation lists, hashes for conversation metadata). Redis Cluster for horizontal scaling.</td></tr>
  <tr><td>Memcached</td><td>Simpler key-value cache. Higher throughput for simple GET/SET patterns. Less feature-rich than Redis but lower overhead.</td></tr>
  <tr><td>Dragonfly</td><td>Modern Redis-compatible cache with better multi-threaded performance and memory efficiency.</td></tr>
</table>

<h3>18.5 Message Queue</h3>
<table>
  <tr><th>Vendor</th><th>Rationale</th></tr>
  <tr><td>Apache Kafka</td><td>High-throughput, durable, log-based. Supports consumer groups for competing consumers. Excellent for our use case of post-inference persistence. Also useful for event sourcing and analytics pipelines.</td></tr>
  <tr><td>Amazon SQS</td><td>Fully managed, no infrastructure to operate. Built-in dead letter queues, visibility timeouts, and exactly-once processing (FIFO queues). Simpler than Kafka for pure message queue semantics.</td></tr>
  <tr><td>RabbitMQ</td><td>Mature, feature-rich message broker with flexible routing. Good for lower-throughput scenarios with complex routing requirements.</td></tr>
</table>

<h3>18.6 Object Storage (for future multi-modal support)</h3>
<table>
  <tr><th>Vendor</th><th>Rationale</th></tr>
  <tr><td>Amazon S3</td><td>Industry standard. Highly durable (11 nines), scalable, cost-effective for large binary objects (images, files).</td></tr>
  <tr><td>Google Cloud Storage</td><td>Comparable to S3 with strong multi-region capabilities.</td></tr>
  <tr><td>MinIO</td><td>S3-compatible open-source object storage. Good for self-hosted/on-premise deployments.</td></tr>
</table>

<h3>18.7 GPU Infrastructure / Model Serving</h3>
<table>
  <tr><th>Vendor</th><th>Rationale</th></tr>
  <tr><td>NVIDIA A100 / H100 GPUs</td><td>Industry-leading GPUs for LLM inference. H100s provide 2–3x throughput improvement over A100s for transformer models. NVLink for high-bandwidth inter-GPU communication.</td></tr>
  <tr><td>vLLM</td><td>Open-source high-throughput inference engine with PagedAttention, continuous batching, and gRPC serving. Best-in-class for LLM serving.</td></tr>
  <tr><td>NVIDIA TensorRT-LLM</td><td>Optimized inference runtime from NVIDIA. Provides kernel-level optimizations for NVIDIA GPUs.</td></tr>
  <tr><td>Text Generation Inference (TGI) by Hugging Face</td><td>Open-source, production-ready inference server with continuous batching, quantization support, and streaming.</td></tr>
</table>

<h3>18.8 Load Balancer</h3>
<table>
  <tr><th>Vendor</th><th>Rationale</th></tr>
  <tr><td>NGINX</td><td>High-performance reverse proxy and load balancer. Excellent SSE/long-connection support. Open-source with commercial NGINX Plus option.</td></tr>
  <tr><td>HAProxy</td><td>Extremely high-performance L4/L7 load balancer. Battle-tested for millions of concurrent connections.</td></tr>
  <tr><td>AWS ALB / GCP Cloud Load Balancing</td><td>Fully managed, auto-scaling. Natively supports HTTP/2, gRPC, and long-lived connections.</td></tr>
  <tr><td>Envoy Proxy</td><td>Modern L7 proxy designed for microservices. Excellent gRPC support, used in service meshes (Istio). Good for the internal Chat Service → Inference Service LB.</td></tr>
</table>

<h3>18.9 CDN</h3>
<table>
  <tr><th>Vendor</th><th>Rationale</th></tr>
  <tr><td>Cloudflare</td><td>Global edge network, DDoS protection, fast cache invalidation. Also provides edge workers for potential future edge logic.</td></tr>
  <tr><td>AWS CloudFront</td><td>Integrated with AWS ecosystem. Global PoPs, supports custom cache behaviors.</td></tr>
  <tr><td>Fastly</td><td>Real-time cache purging, edge compute (Compute@Edge), excellent for dynamic content at the edge.</td></tr>
</table>

<hr style="margin-top: 3rem; margin-bottom: 1rem;"/>
<p style="color: #6b7280; font-size: 0.9rem; text-align: center;"><em>End of System Design Document — ChatGPT</em></p>

<script>
  mermaid.initialize({ startOnLoad: true, theme: 'default', securityLevel: 'loose' });
</script>

</body>
</html>