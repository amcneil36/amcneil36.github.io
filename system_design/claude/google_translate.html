<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Google Translate - System Design</title>
<style>
:root{--bg:#0f0f0f;--surface:#1a1a1a;--border:#2a2a2a;--text:#e0e0e0;--accent:#4285F4;--accent2:#34A853;}
*{margin:0;padding:0;box-sizing:border-box}
body{background:var(--bg);color:var(--text);font-family:'Segoe UI',system-ui,sans-serif;line-height:1.7;padding:2rem;max-width:1400px;margin:0 auto}
h1{color:var(--accent);font-size:2.2rem;border-bottom:3px solid var(--accent);padding-bottom:.5rem;margin-bottom:1.5rem}
h2{color:var(--accent2);margin:2rem 0 1rem;font-size:1.5rem}
h3{color:#81D4FA;margin:1.5rem 0 .75rem}
h4{color:#CE93D8;margin:1rem 0 .5rem}
.card{background:var(--surface);border:1px solid var(--border);border-radius:12px;padding:1.5rem;margin:1rem 0}
.grid{display:grid;grid-template-columns:repeat(auto-fit,minmax(340px,1fr));gap:1rem}
table{width:100%;border-collapse:collapse;margin:1rem 0}
th,td{border:1px solid var(--border);padding:.75rem;text-align:left;font-size:.95rem}
th{background:#1a3a5c;color:var(--accent)}
pre{background:#1e1e1e;padding:1rem;border-radius:8px;overflow-x:auto;margin:.5rem 0}
code{color:#89DDFF;font-size:.9rem}
svg text{font-family:'Segoe UI',system-ui,sans-serif}
.tradeoff-grid{display:grid;grid-template-columns:1fr 1fr;gap:1rem}
.pro{border-left:4px solid #4CAF50;padding-left:1rem}
.con{border-left:4px solid #f44336;padding-left:1rem}
details{background:var(--surface);border:1px solid var(--border);border-radius:8px;padding:1rem;margin:.5rem 0}
summary{cursor:pointer;font-weight:600;color:var(--accent)}
.step{margin:.5rem 0;padding:.5rem 1rem;border-left:3px solid var(--accent2)}
.tag-pk{background:#E91E63;color:#fff;padding:2px 8px;border-radius:4px;font-size:.8rem}
.tag-idx{background:#FF9800;color:#fff;padding:2px 8px;border-radius:4px;font-size:.8rem}
.tag-shard{background:#9C27B0;color:#fff;padding:2px 8px;border-radius:4px;font-size:.8rem}
</style>
</head>
<body>

<h1>üåê Google Translate ‚Äî Machine Translation System</h1>
<p>Design a system that translates text, documents, and speech across 130+ languages using neural machine translation, supporting 1B+ daily translations with low latency.</p>

<h2>üìã Functional Requirements</h2>
<div class="card">
<ul>
<li><strong>Text translation</strong> ‚Äî translate text between 130+ language pairs with context-aware quality</li>
<li><strong>Language detection</strong> ‚Äî auto-detect source language from input text</li>
<li><strong>Document translation</strong> ‚Äî translate PDF, DOCX, PPTX while preserving formatting</li>
<li><strong>Speech translation</strong> ‚Äî speech-to-text ‚Üí translate ‚Üí text-to-speech pipeline</li>
<li><strong>Camera/image translation</strong> ‚Äî OCR on camera input, translate overlaid text in real-time</li>
<li><strong>Conversation mode</strong> ‚Äî real-time bidirectional speech translation</li>
<li><strong>Offline translation</strong> ‚Äî download language packs for offline use on mobile</li>
</ul>
</div>

<h2>üìã Non-Functional Requirements</h2>
<div class="card">
<ul>
<li><strong>Latency</strong> ‚Äî &lt;200ms for text translation, &lt;500ms for speech-to-text per utterance</li>
<li><strong>Throughput</strong> ‚Äî 1B+ translations/day, 100K+ QPS at peak</li>
<li><strong>Quality</strong> ‚Äî BLEU score &gt;40 for major language pairs, continuous improvement via user feedback</li>
<li><strong>Availability</strong> ‚Äî 99.99% uptime globally</li>
<li><strong>Coverage</strong> ‚Äî 130+ languages including low-resource languages</li>
<li><strong>Cost efficiency</strong> ‚Äî optimize GPU/TPU inference cost per translation</li>
</ul>
</div>

<h2>üîÑ Flow 1: Text Translation (Neural Machine Translation)</h2>
<svg viewBox="0 0 1000 300" style="width:100%;background:#111;border-radius:12px;margin:1rem 0">
<defs><marker id="a1" markerWidth="10" markerHeight="7" refX="10" refY="3.5" orient="auto"><polygon points="0 0,10 3.5,0 7" fill="#4285F4"/></marker></defs>
<rect x="20" y="60" width="110" height="50" rx="8" fill="#2E7D32"/><text x="75" y="90" text-anchor="middle" fill="white" font-size="13">Client</text>
<rect x="175" y="60" width="130" height="50" rx="8" fill="#E65100"/><text x="240" y="90" text-anchor="middle" fill="white" font-size="12">API Gateway</text>
<rect x="350" y="30" width="130" height="50" rx="8" fill="#1565C0"/><text x="415" y="60" text-anchor="middle" fill="white" font-size="12">Language Detect</text>
<rect x="350" y="100" width="130" height="50" rx="8" fill="#6A1B9A"/><text x="415" y="120" text-anchor="middle" fill="white" font-size="12">Translation Cache</text><text x="415" y="135" text-anchor="middle" fill="white" font-size="10">(Redis)</text>
<rect x="530" y="60" width="140" height="50" rx="8" fill="#1565C0"/><text x="600" y="80" text-anchor="middle" fill="white" font-size="12">NMT Model Server</text><text x="600" y="95" text-anchor="middle" fill="white" font-size="10">(TPU/GPU)</text>
<rect x="720" y="30" width="130" height="50" rx="8" fill="#4E342E"/><text x="785" y="60" text-anchor="middle" fill="white" font-size="12">Model Registry</text>
<rect x="720" y="100" width="130" height="50" rx="8" fill="#00695C"/><text x="785" y="130" text-anchor="middle" fill="white" font-size="12">Post-Processing</text>
<rect x="530" y="200" width="140" height="50" rx="8" fill="#6A1B9A"/><text x="600" y="230" text-anchor="middle" fill="white" font-size="12">Feedback Logger</text>
<line x1="130" y1="85" x2="170" y2="85" stroke="#4285F4" stroke-width="2" marker-end="url(#a1)"/>
<line x1="305" y1="75" x2="345" y2="55" stroke="#4285F4" stroke-width="2" marker-end="url(#a1)"/>
<line x1="305" y1="95" x2="345" y2="125" stroke="#4285F4" stroke-width="2" marker-end="url(#a1)"/>
<line x1="480" y1="125" x2="525" y2="95" stroke="#4285F4" stroke-width="2" marker-end="url(#a1)"/>
<line x1="670" y1="85" x2="715" y2="55" stroke="#81D4FA" stroke-width="2" marker-end="url(#a1)"/>
<line x1="670" y1="95" x2="715" y2="125" stroke="#81D4FA" stroke-width="2" marker-end="url(#a1)"/>
<line x1="600" y1="110" x2="600" y2="195" stroke="#FF9800" stroke-width="2" marker-end="url(#a1)"/>
<text x="415" y="170" fill="#aaa" font-size="10">cache miss</text>
<text x="620" y="160" fill="#aaa" font-size="10">log for training</text>
</svg>

<div class="step"><strong>Step 1:</strong> Client sends text with optional source/target language. API Gateway authenticates, rate-limits, routes</div>
<div class="step"><strong>Step 2:</strong> Language Detection runs if source language not specified (fastText classifier, &lt;1ms)</div>
<div class="step"><strong>Step 3:</strong> Translation Cache checked ‚Äî common phrases/sentences cached (Redis, ~40% hit rate)</div>
<div class="step"><strong>Step 4:</strong> On cache miss, NMT Model Server runs Transformer inference on TPU. Text tokenized ‚Üí encoded ‚Üí decoded ‚Üí detokenized</div>
<div class="step"><strong>Step 5:</strong> Post-processing: fix capitalization, restore named entities, format numbers/dates for target locale</div>

<details><summary>Deep Dive: Transformer Architecture for Translation</summary>
<div class="card">
<h4>Encoder-Decoder Transformer (Google's NMT)</h4>
<pre><code>// Evolution of Google Translate:
// 2006: Statistical MT (phrase-based, BLEU ~25)
// 2016: Neural MT (GNMT - LSTM encoder-decoder, BLEU ~35)
// 2017: Transformer (attention is all you need, BLEU ~40+)
// 2023: PaLM-based / large multilingual models

// Transformer architecture for translation:
Input: "The cat sat on the mat"
  ‚Üì Tokenization (SentencePiece - subword units)
  ["‚ñÅThe", "‚ñÅcat", "‚ñÅsat", "‚ñÅon", "‚ñÅthe", "‚ñÅmat"]
  ‚Üì Embedding + Positional Encoding
  ‚Üì Encoder (6 layers of self-attention + FFN)
  ‚Üí Context-aware representation of source sentence
  ‚Üì Decoder (6 layers of masked self-attention + cross-attention + FFN)
  ‚Üí Autoregressive generation, one token at a time
Output: "Le chat √©tait assis sur le tapis"

// Key innovation: Multi-head attention
// Q, K, V = linear projections of input
// Attention(Q,K,V) = softmax(QK^T / ‚àöd_k) √ó V
// 8 heads ‚Üí 8 different "perspectives" on relationships

// For 130+ languages: Multilingual model
// Single model handles ALL language pairs
// Zero-shot: can translate X‚ÜíY even if only trained on X‚ÜíEN and EN‚ÜíY
// Massively multilingual: shared representations improve low-resource languages</code></pre>
</div>
</details>

<details><summary>Deep Dive: Language Detection</summary>
<div class="card">
<pre><code>// Language detection must be fast (&lt;1ms) and accurate
// Approach: Character n-gram classifier (fastText)

// 1. Extract character trigrams from input
// "Hello" ‚Üí ["Hel", "ell", "llo"]
// Each language has distinctive character patterns

// 2. fastText linear classifier
// - Trained on Wikipedia text in 130+ languages
// - Hash n-grams to fixed vocabulary (buckets)
// - Single matrix multiply ‚Üí softmax over languages
// - 99.5% accuracy for &gt;20 character inputs

// 3. Challenges:
// - Short text ("ok", "taxi") ‚Äî ambiguous across languages
// - Code-mixed text ("I want ‡§ï‡•â‡§´‡§º‡•Ä") ‚Äî multilingual input
// - Script detection helps: Devanagari ‚Üí Hindi/Marathi/Sanskrit
// - Fallback: user's locale as prior probability

// 4. CLD3 (Compact Language Detector 3)
// Google's production detector
// Neural network on character sequences
// Supports 107 languages, runs in ~0.5ms</code></pre>
</div>
</details>

<h2>üîÑ Flow 2: Speech Translation (Real-Time Conversation)</h2>
<svg viewBox="0 0 1000 280" style="width:100%;background:#111;border-radius:12px;margin:1rem 0">
<defs><marker id="a2" markerWidth="10" markerHeight="7" refX="10" refY="3.5" orient="auto"><polygon points="0 0,10 3.5,0 7" fill="#34A853"/></marker></defs>
<rect x="20" y="50" width="110" height="50" rx="8" fill="#2E7D32"/><text x="75" y="80" text-anchor="middle" fill="white" font-size="12">Microphone</text>
<rect x="170" y="50" width="130" height="50" rx="8" fill="#1565C0"/><text x="235" y="70" text-anchor="middle" fill="white" font-size="12">Speech-to-Text</text><text x="235" y="85" text-anchor="middle" fill="white" font-size="10">(Streaming ASR)</text>
<rect x="340" y="50" width="120" height="50" rx="8" fill="#6A1B9A"/><text x="400" y="80" text-anchor="middle" fill="white" font-size="12">NMT Engine</text>
<rect x="500" y="50" width="130" height="50" rx="8" fill="#1565C0"/><text x="565" y="70" text-anchor="middle" fill="white" font-size="12">Text-to-Speech</text><text x="565" y="85" text-anchor="middle" fill="white" font-size="10">(WaveNet/Tacotron)</text>
<rect x="670" y="50" width="110" height="50" rx="8" fill="#2E7D32"/><text x="725" y="80" text-anchor="middle" fill="white" font-size="12">Speaker</text>
<rect x="170" y="160" width="130" height="50" rx="8" fill="#E65100"/><text x="235" y="180" text-anchor="middle" fill="white" font-size="12">WebSocket /</text><text x="235" y="195" text-anchor="middle" fill="white" font-size="10">gRPC Streaming</text>
<line x1="130" y1="75" x2="165" y2="75" stroke="#34A853" stroke-width="2" marker-end="url(#a2)"/>
<line x1="300" y1="75" x2="335" y2="75" stroke="#34A853" stroke-width="2" marker-end="url(#a2)"/>
<line x1="460" y1="75" x2="495" y2="75" stroke="#34A853" stroke-width="2" marker-end="url(#a2)"/>
<line x1="630" y1="75" x2="665" y2="75" stroke="#34A853" stroke-width="2" marker-end="url(#a2)"/>
<line x1="235" y1="100" x2="235" y2="155" stroke="#FF9800" stroke-width="2" marker-end="url(#a2)"/>
<text x="130" y="45" fill="#aaa" font-size="10">audio chunks</text>
<text x="310" y="45" fill="#aaa" font-size="10">partial text</text>
<text x="470" y="45" fill="#aaa" font-size="10">translated</text>
<text x="640" y="45" fill="#aaa" font-size="10">synthesized audio</text>
</svg>

<div class="step"><strong>Step 1:</strong> Audio streamed via WebSocket/gRPC in 100ms chunks (16kHz PCM or Opus codec)</div>
<div class="step"><strong>Step 2:</strong> Streaming ASR produces partial transcripts in real-time (Conformer model), finalizes on silence detection</div>
<div class="step"><strong>Step 3:</strong> Finalized text segments sent to NMT engine for translation</div>
<div class="step"><strong>Step 4:</strong> Translated text synthesized to speech via WaveNet/Tacotron TTS model</div>
<div class="step"><strong>Step 5:</strong> Audio response streamed back to client for playback</div>

<details><summary>Deep Dive: Streaming ASR & End-of-Utterance Detection</summary>
<div class="card">
<pre><code>// Streaming Speech Recognition Pipeline:
// Audio ‚Üí Feature Extraction ‚Üí Acoustic Model ‚Üí Language Model ‚Üí Text

// 1. Feature extraction: 25ms windows, 10ms hop
//    Mel-frequency cepstral coefficients (MFCC) or log-mel filterbanks
//    80 mel bins ‚Üí 80-dim feature vector per frame

// 2. Acoustic model: Conformer (Conv + Transformer)
//    - Convolution captures local audio patterns
//    - Transformer captures long-range dependencies
//    - Streaming mode: causal attention (can't look ahead)
//    - Emits partial hypotheses every ~200ms

// 3. Endpointer: detect when user stops speaking
//    - Voice Activity Detection (VAD): classify frames as speech/silence
//    - End-of-query model: predict if user is done or just pausing
//    - Typical threshold: 700ms silence ‚Üí finalize
//    - Challenge: premature cutoff vs excessive wait

// 4. Protocol: bidirectional gRPC streaming
//    Client ‚Üí Server: audio chunks (StreamingRecognizeRequest)
//    Server ‚Üí Client: partial + final results (StreamingRecognizeResponse)
//    { "is_final": false, "transcript": "how do you" }  // partial
//    { "is_final": true,  "transcript": "how do you say hello" }  // final</code></pre>
</div>
</details>

<h2>üîÑ Flow 3: Camera/Image Translation (Visual Translation)</h2>
<svg viewBox="0 0 1000 280" style="width:100%;background:#111;border-radius:12px;margin:1rem 0">
<defs><marker id="a3" markerWidth="10" markerHeight="7" refX="10" refY="3.5" orient="auto"><polygon points="0 0,10 3.5,0 7" fill="#FBBC04"/></marker></defs>
<rect x="20" y="60" width="110" height="50" rx="8" fill="#2E7D32"/><text x="75" y="90" text-anchor="middle" fill="white" font-size="12">Camera Feed</text>
<rect x="175" y="60" width="120" height="50" rx="8" fill="#1565C0"/><text x="235" y="80" text-anchor="middle" fill="white" font-size="12">Text Detection</text><text x="235" y="95" text-anchor="middle" fill="white" font-size="10">(CRAFT/EAST)</text>
<rect x="340" y="60" width="120" height="50" rx="8" fill="#6A1B9A"/><text x="400" y="80" text-anchor="middle" fill="white" font-size="12">OCR Engine</text><text x="400" y="95" text-anchor="middle" fill="white" font-size="10">(Tesseract/CNN)</text>
<rect x="505" y="60" width="120" height="50" rx="8" fill="#1565C0"/><text x="565" y="90" text-anchor="middle" fill="white" font-size="12">NMT Engine</text>
<rect x="670" y="60" width="130" height="50" rx="8" fill="#E65100"/><text x="735" y="80" text-anchor="middle" fill="white" font-size="12">AR Overlay</text><text x="735" y="95" text-anchor="middle" fill="white" font-size="10">(Inpainting+Render)</text>
<line x1="130" y1="85" x2="170" y2="85" stroke="#FBBC04" stroke-width="2" marker-end="url(#a3)"/>
<line x1="295" y1="85" x2="335" y2="85" stroke="#FBBC04" stroke-width="2" marker-end="url(#a3)"/>
<line x1="460" y1="85" x2="500" y2="85" stroke="#FBBC04" stroke-width="2" marker-end="url(#a3)"/>
<line x1="625" y1="85" x2="665" y2="85" stroke="#FBBC04" stroke-width="2" marker-end="url(#a3)"/>
</svg>

<div class="step"><strong>Step 1:</strong> Camera captures frame. Text Detection model (CRAFT/EAST) locates text regions as bounding boxes</div>
<div class="step"><strong>Step 2:</strong> OCR engine recognizes characters within each bounding box (CNN + CTC decoder or attention-based)</div>
<div class="step"><strong>Step 3:</strong> Recognized text translated via NMT engine</div>
<div class="step"><strong>Step 4:</strong> AR overlay: inpaint original text region (remove source text), render translated text with matched font/color/perspective</div>

<details><summary>Deep Dive: On-Device vs Cloud Translation</summary>
<div class="card">
<table>
<tr><th>Aspect</th><th>On-Device</th><th>Cloud</th></tr>
<tr><td>Latency</td><td>~50ms (no network)</td><td>~200ms (network RTT)</td></tr>
<tr><td>Quality</td><td>Good (compressed model)</td><td>Best (full model)</td></tr>
<tr><td>Model size</td><td>~50MB per language pair</td><td>~1GB+ per model</td></tr>
<tr><td>Offline</td><td>Yes</td><td>No</td></tr>
<tr><td>Privacy</td><td>Data stays on device</td><td>Data sent to server</td></tr>
<tr><td>Languages</td><td>~59 (downloadable packs)</td><td>130+</td></tr>
</table>
<pre><code>// On-device model optimization:
// 1. Quantization: FP32 ‚Üí INT8 (4x smaller, 2-3x faster)
// 2. Distillation: large teacher ‚Üí small student model
// 3. Pruning: remove low-magnitude weights (50-90% sparsity)
// 4. TFLite / ONNX Runtime for mobile inference
// 5. Shared encoder across languages, tiny per-language decoder heads

// Google Translate offline packs:
// ~50MB per language pair (download once)
// Uses Transformer-based model with 6 encoder + 2 decoder layers
// Runs on mobile CPU (no GPU required) at ~100 tokens/sec</code></pre>
</div>
</details>

<h2>üèóÔ∏è Combined Architecture</h2>
<svg viewBox="0 0 1100 500" style="width:100%;background:#111;border-radius:12px;margin:1rem 0">
<defs><marker id="ac" markerWidth="10" markerHeight="7" refX="10" refY="3.5" orient="auto"><polygon points="0 0,10 3.5,0 7" fill="#4285F4"/></marker></defs>
<text x="550" y="25" text-anchor="middle" fill="#4285F4" font-size="16" font-weight="bold">Google Translate Architecture</text>
<rect x="20" y="50" width="120" height="40" rx="8" fill="#2E7D32"/><text x="80" y="75" text-anchor="middle" fill="white" font-size="11">Web/Mobile/API</text>
<rect x="200" y="50" width="130" height="40" rx="8" fill="#E65100"/><text x="265" y="75" text-anchor="middle" fill="white" font-size="11">API Gateway + LB</text>
<rect x="400" y="40" width="130" height="35" rx="8" fill="#1565C0"/><text x="465" y="63" text-anchor="middle" fill="white" font-size="11">Lang Detection</text>
<rect x="400" y="85" width="130" height="35" rx="8" fill="#00695C"/><text x="465" y="108" text-anchor="middle" fill="white" font-size="11">Translation Cache</text>
<rect x="400" y="130" width="130" height="35" rx="8" fill="#6A1B9A"/><text x="465" y="153" text-anchor="middle" fill="white" font-size="11">NMT Model Serving</text>
<rect x="400" y="175" width="130" height="35" rx="8" fill="#1565C0"/><text x="465" y="198" text-anchor="middle" fill="white" font-size="11">Speech-to-Text</text>
<rect x="400" y="220" width="130" height="35" rx="8" fill="#1565C0"/><text x="465" y="243" text-anchor="middle" fill="white" font-size="11">Text-to-Speech</text>
<rect x="400" y="265" width="130" height="35" rx="8" fill="#1565C0"/><text x="465" y="288" text-anchor="middle" fill="white" font-size="11">OCR / Vision</text>
<rect x="600" y="50" width="140" height="40" rx="8" fill="#4E342E"/><text x="670" y="75" text-anchor="middle" fill="white" font-size="11">Model Registry</text>
<rect x="600" y="110" width="140" height="40" rx="8" fill="#607D8B"/><text x="670" y="135" text-anchor="middle" fill="white" font-size="11">TPU Cluster</text>
<rect x="600" y="170" width="140" height="40" rx="8" fill="#4E342E"/><text x="670" y="195" text-anchor="middle" fill="white" font-size="11">Training Pipeline</text>
<rect x="600" y="230" width="140" height="40" rx="8" fill="#6A1B9A"/><text x="670" y="255" text-anchor="middle" fill="white" font-size="11">Feedback / RLHF</text>
<rect x="830" y="50" width="140" height="40" rx="8" fill="#4E342E"/><text x="900" y="75" text-anchor="middle" fill="white" font-size="11">Parallel Corpora DB</text>
<rect x="830" y="110" width="140" height="40" rx="8" fill="#4E342E"/><text x="900" y="135" text-anchor="middle" fill="white" font-size="11">Monolingual Data</text>
<rect x="200" y="370" width="130" height="40" rx="8" fill="#00695C"/><text x="265" y="395" text-anchor="middle" fill="white" font-size="11">CDN (Static Assets)</text>
<rect x="400" y="370" width="130" height="40" rx="8" fill="#00695C"/><text x="465" y="395" text-anchor="middle" fill="white" font-size="11">Redis Cluster</text>
<rect x="600" y="370" width="140" height="40" rx="8" fill="#6A1B9A"/><text x="670" y="395" text-anchor="middle" fill="white" font-size="11">Kafka Event Bus</text>
<line x1="140" y1="70" x2="195" y2="70" stroke="#4285F4" stroke-width="2" marker-end="url(#ac)"/>
<line x1="330" y1="70" x2="395" y2="57" stroke="#4285F4" stroke-width="2" marker-end="url(#ac)"/>
<line x1="330" y1="70" x2="395" y2="102" stroke="#4285F4" stroke-width="2" marker-end="url(#ac)"/>
<line x1="330" y1="70" x2="395" y2="147" stroke="#4285F4" stroke-width="2" marker-end="url(#ac)"/>
<line x1="530" y1="147" x2="595" y2="70" stroke="#81D4FA" stroke-width="2" marker-end="url(#ac)"/>
<line x1="530" y1="147" x2="595" y2="130" stroke="#81D4FA" stroke-width="2" marker-end="url(#ac)"/>
<line x1="740" y1="190" x2="825" y2="70" stroke="#FF9800" stroke-width="2" marker-end="url(#ac)"/>
<line x1="740" y1="190" x2="825" y2="130" stroke="#FF9800" stroke-width="2" marker-end="url(#ac)"/>
<line x1="670" y1="210" x2="670" y2="225" stroke="#FF9800" stroke-width="2" marker-end="url(#ac)"/>
</svg>

<h2>üíæ Database Schema</h2>
<div class="grid">
<div class="card">
<h3>Translation Cache (Redis)</h3>
<pre><code>// Key design for translation cache:
// Key: hash(source_lang + target_lang + normalized_text)
// Value: translated_text + metadata

SET translate:en:fr:sha256(text) {
  "translation": "Bonjour le monde",
  "model_version": "nmt-v4.2",
  "quality_score": 0.95,
  "cached_at": 1707500000
} EX 86400  // 24h TTL

// Cache segmentation:
// - Hot phrases: permanent cache (top 10M translations)
// - Session cache: 24h TTL for recent translations
// - Document cache: keyed by (doc_hash + lang_pair), 7d TTL

// Hit rate optimization:
// - Normalize whitespace, casing before hashing
// - Sentence-level caching (split paragraphs into sentences)
// - ~40% cache hit rate for text, ~60% for common phrases</code></pre>
</div>
<div class="card">
<h3>Training Data Store</h3>
<pre><code>-- Parallel Corpora (Bigtable / Spanner)
-- Billions of aligned sentence pairs
Key: (language_pair, sentence_hash)
Columns: {
  source_text: STRING,
  target_text: STRING,
  source: STRING, -- "web_crawl"|"europarl"|"user_feedback"
  quality_score: FLOAT, -- 0-1 filtering threshold
  domain: STRING, -- "medical"|"legal"|"general"
  created_at: TIMESTAMP
}

-- User Feedback (for model improvement)
Key: (feedback_id)
Columns: {
  source_text: STRING,
  machine_translation: STRING,
  user_correction: STRING,
  source_lang: STRING,
  target_lang: STRING,
  timestamp: TIMESTAMP,
  approved: BOOLEAN -- human review for training inclusion
}

-- Model metadata stored in Spanner (globally consistent)
-- Model artifacts stored in GCS (Cloud Storage)</code></pre>
</div>
</div>

<h2>‚ö° Cache & CDN Deep Dive</h2>
<div class="card">
<table>
<tr><th>Cache Layer</th><th>What</th><th>Strategy</th><th>TTL</th></tr>
<tr><td>Browser/App</td><td>Recent translations</td><td>LRU local storage</td><td>Session / 7 days</td></tr>
<tr><td>CDN Edge</td><td>Static assets, language packs</td><td>Pull-based caching</td><td>30 days</td></tr>
<tr><td>Redis (Regional)</td><td>Translation results</td><td>Write-through on translate</td><td>24 hours</td></tr>
<tr><td>Model Cache</td><td>Loaded model weights in GPU/TPU memory</td><td>LRU across language pairs</td><td>Permanent (hot)</td></tr>
</table>
<h4>Offline Language Packs (CDN Distribution)</h4>
<pre><code>// Language packs distributed via CDN:
// - ~50MB per language pair (quantized INT8 model)
// - Delta updates: only download changed weights (~5MB)
// - Stored: GCS ‚Üí CloudFront/Akamai CDN ‚Üí device
// - Version pinning: app requests specific model version
// - Background download on Wi-Fi, prompt on cellular</code></pre>
</div>

<h2>üìà Scaling Considerations</h2>
<div class="card">
<ul>
<li><strong>Model serving at scale:</strong> TPU pods (v4: 4096 chips) for batch inference. GPU clusters (A100/H100) for real-time. Batching: group requests by language pair for efficient GPU utilization (dynamic batching with max 50ms wait).</li>
<li><strong>Language pair routing:</strong> not all 130√ó129 = 16,770 pairs have direct models. Use pivot translation through English: X ‚Üí EN ‚Üí Y. Trade quality for coverage.</li>
<li><strong>Auto-scaling:</strong> scale model servers based on QPS per language pair. Popular pairs (EN‚ÜîES, EN‚ÜîZH) get dedicated capacity. Rare pairs share multi-language model instances.</li>
<li><strong>Global deployment:</strong> model replicas in each GCP region. Route to nearest region. Language-aware routing: Asian language pairs served from asia-east, European from europe-west.</li>
</ul>
</div>

<h2>‚öñÔ∏è Tradeoffs</h2>
<div class="tradeoff-grid">
<div class="card pro"><h4>Multilingual Single Model</h4><ul><li>One model handles all 130+ languages</li><li>Zero-shot translation (unseen pairs)</li><li>Shared representations help low-resource langs</li></ul></div>
<div class="card con"><h4>Multilingual Single Model</h4><ul><li>"Curse of multilinguality" ‚Äî quality drops as langs increase</li><li>Large model = higher inference cost</li><li>High-resource languages subsidize low-resource</li></ul></div>
<div class="card pro"><h4>Pivot Through English</h4><ul><li>Only need N models (X‚ÜíEN) instead of N¬≤ pairs</li><li>English has most training data</li></ul></div>
<div class="card con"><h4>Pivot Through English</h4><ul><li>Error compounds (two translations)</li><li>English-centric bias in translations</li><li>Double latency</li></ul></div>
</div>

<h2>üîÑ Alternative Approaches</h2>
<div class="grid">
<div class="card"><h4>LLM-Based Translation (GPT-4, PaLM)</h4><p>Use general-purpose LLMs for translation. Better context handling, idiom translation, and style matching. Higher latency and cost per token. Competitive quality for high-resource pairs, but specialized NMT still wins on low-resource and speed.</p></div>
<div class="card"><h4>Retrieval-Augmented Translation</h4><p>Retrieve similar translated sentences from a translation memory (TM), use as additional context for NMT model. Especially useful for domain-specific translation (medical, legal). Combines the consistency of TM with flexibility of NMT.</p></div>
</div>

<h2>üìö Additional Information</h2>
<div class="card">
<ul>
<li><strong>BLEU score:</strong> standard metric for translation quality (0-100). Measures n-gram overlap with reference translation. BLEU 40+ = high quality for most language pairs. Human evaluation (MQM) used for nuanced quality assessment.</li>
<li><strong>SentencePiece tokenization:</strong> language-agnostic subword tokenizer. Handles all scripts without language-specific preprocessing. Shared vocabulary across all languages (~64K tokens).</li>
<li><strong>Back-translation:</strong> data augmentation for low-resource languages. Translate target monolingual text ‚Üí source using existing model. Creates synthetic parallel data. 2-3x quality improvement for low-resource pairs.</li>
<li><strong>Adaptive translation:</strong> user corrections feed back into model fine-tuning (with human review). Google Community contributions for quality improvement.</li>
</ul>
</div>

</body>
</html>
