<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>System Design: Credit Karma</title>
<script src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script>
<script>mermaid.initialize({startOnLoad:true, theme:'default', securityLevel:'loose'});</script>
<style>
  body { font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; line-height: 1.7; color: #1a1a2e; max-width: 1100px; margin: 0 auto; padding: 30px; background: #f8f9fa; }
  h1 { color: #0f3460; border-bottom: 4px solid #00b894; padding-bottom: 12px; font-size: 2.2em; }
  h2 { color: #16213e; border-bottom: 2px solid #e17055; padding-bottom: 8px; margin-top: 50px; font-size: 1.6em; }
  h3 { color: #2d3436; margin-top: 30px; font-size: 1.3em; }
  h4 { color: #636e72; margin-top: 20px; }
  ul, ol { margin-left: 20px; }
  li { margin-bottom: 6px; }
  .diagram-container { background: #fff; border: 1px solid #dfe6e9; border-radius: 10px; padding: 25px; margin: 20px 0; box-shadow: 0 2px 8px rgba(0,0,0,0.06); overflow-x: auto; }
  .example-box { background: #dfe6e9; border-left: 5px solid #0984e3; padding: 18px 22px; margin: 16px 0; border-radius: 6px; }
  .example-box strong { color: #0984e3; }
  .deep-dive { background: #ffeaa7; border-left: 5px solid #fdcb6e; padding: 18px 22px; margin: 16px 0; border-radius: 6px; }
  .deep-dive strong { color: #e17055; }
  table { border-collapse: collapse; width: 100%; margin: 16px 0; background: #fff; border-radius: 8px; overflow: hidden; box-shadow: 0 1px 4px rgba(0,0,0,0.08); }
  th { background: #0f3460; color: #fff; padding: 12px 16px; text-align: left; }
  td { padding: 10px 16px; border-bottom: 1px solid #dfe6e9; }
  tr:hover { background: #f1f2f6; }
  code { background: #dfe6e9; padding: 2px 7px; border-radius: 4px; font-size: 0.95em; }
  .toc { background: #fff; border: 1px solid #dfe6e9; border-radius: 10px; padding: 25px 35px; margin: 30px 0; box-shadow: 0 2px 8px rgba(0,0,0,0.06); }
  .toc a { text-decoration: none; color: #0984e3; }
  .toc a:hover { text-decoration: underline; }
  .toc ol { counter-reset: toc-counter; list-style: none; padding-left: 0; }
  .toc > ol > li { counter-increment: toc-counter; margin-bottom: 4px; }
  .toc > ol > li::before { content: counter(toc-counter) ". "; color: #e17055; font-weight: bold; }
  .alert-box { background: #fab1a0; border-left: 5px solid #e17055; padding: 14px 20px; margin: 16px 0; border-radius: 6px; }
  .info-box { background: #74b9ff; border-left: 5px solid #0984e3; padding: 14px 20px; margin: 16px 0; border-radius: 6px; color: #fff; }
  .section-divider { border: none; border-top: 2px dashed #b2bec3; margin: 50px 0; }
</style>
</head>
<body>

<h1>üè¶ System Design: Credit Karma</h1>
<p><em>A free consumer credit and financial management platform providing credit scores, credit monitoring, personalized financial product recommendations, and financial insights to 100M+ users.</em></p>

<div class="toc">
<h3>üìë Table of Contents</h3>
<ol>
  <li><a href="#fr">Functional Requirements</a></li>
  <li><a href="#nfr">Non-Functional Requirements</a></li>
  <li><a href="#flow1">Flow 1 ‚Äî Credit Data Ingestion</a></li>
  <li><a href="#flow2">Flow 2 ‚Äî Credit Score &amp; Report Display</a></li>
  <li><a href="#flow3">Flow 3 ‚Äî Credit Monitoring &amp; Alerts</a></li>
  <li><a href="#flow4">Flow 4 ‚Äî Product Recommendation &amp; Referral</a></li>
  <li><a href="#combined">Combined Overall Flow</a></li>
  <li><a href="#schema">Database Schema</a></li>
  <li><a href="#caching">CDN &amp; Caching Strategy</a></li>
  <li><a href="#mq">Message Queue Deep Dive</a></li>
  <li><a href="#push">Push Notifications Deep Dive</a></li>
  <li><a href="#lb">Load Balancers</a></li>
  <li><a href="#scaling">Scaling Considerations</a></li>
  <li><a href="#tradeoffs">Tradeoffs &amp; Deep Dives</a></li>
  <li><a href="#alternatives">Alternative Approaches</a></li>
  <li><a href="#additional">Additional Considerations</a></li>
  <li><a href="#vendors">Vendor Recommendations</a></li>
</ol>
</div>

<hr class="section-divider">

<!-- ===================== FUNCTIONAL REQUIREMENTS ===================== -->
<h2 id="fr">1. Functional Requirements</h2>
<ol>
  <li><strong>User Registration &amp; Identity Verification</strong> ‚Äî Users can sign up by providing PII (name, DOB, address, last 4 of SSN). The system performs Knowledge-Based Authentication (KBA) via a credit bureau to verify identity. A <em>soft pull</em> (does not affect credit score) is initiated once verified.</li>
  <li><strong>Credit Score Display</strong> ‚Äî Users can view their free credit scores from two major bureaus (e.g., TransUnion and Equifax). Scores are updated weekly.</li>
  <li><strong>Credit Report Details</strong> ‚Äî Users can view a breakdown of their credit report: open accounts, payment history, credit utilization, hard inquiries, derogatory marks, and age of credit history.</li>
  <li><strong>Credit Score Factors</strong> ‚Äî Users can see the top factors positively and negatively affecting their score (e.g., "High credit utilization ‚Äî 72%").</li>
  <li><strong>Credit Monitoring &amp; Alerts</strong> ‚Äî Users receive alerts when: score changes, new account appears, hard inquiry detected, address change, potential fraud, or account enters collections.</li>
  <li><strong>Personalized Product Recommendations</strong> ‚Äî Users see a curated feed of credit cards, personal loans, auto loans, mortgages, and insurance products matched to their credit profile. Each recommendation shows estimated approval odds.</li>
  <li><strong>Product Referral &amp; Click-Through</strong> ‚Äî Users can click on a recommended product to be redirected to the partner's application page. The system tracks click-through and conversion for revenue attribution.</li>
  <li><strong>Credit Score Simulator</strong> ‚Äî Users can model "what-if" scenarios (e.g., "What if I pay off $5,000 of credit card debt?") and see estimated score impact.</li>
  <li><strong>Notification Preferences</strong> ‚Äî Users can configure which alert types they want and via which channels (push, email, SMS).</li>
  <li><strong>Score History</strong> ‚Äî Users can view a time-series chart of their credit score over months/years.</li>
</ol>

<!-- ===================== NON-FUNCTIONAL REQUIREMENTS ===================== -->
<h2 id="nfr">2. Non-Functional Requirements</h2>
<ol>
  <li><strong>Security</strong> ‚Äî All PII and financial data must be encrypted at rest (AES-256) and in transit (TLS 1.3). The system must comply with SOC 2 Type II, PCI-DSS, and FCRA regulations. SSNs are stored as salted hashes ‚Äî never in plaintext.</li>
  <li><strong>High Availability</strong> ‚Äî 99.95%+ uptime. The platform must gracefully degrade if a credit bureau API is temporarily unreachable (serve cached/stale data).</li>
  <li><strong>Low Latency</strong> ‚Äî Credit score retrieval &lt; 300ms (p99) from cache. Full credit report &lt; 800ms (p99).</li>
  <li><strong>Scalability</strong> ‚Äî Support 100M+ registered users, with 10M+ DAU. Handle peak traffic (e.g., Monday mornings, tax season, post-New Year resolutions).</li>
  <li><strong>Data Freshness</strong> ‚Äî Credit scores and reports refreshed at least weekly. Monitoring alerts generated within 24 hours of bureau data update.</li>
  <li><strong>Eventual Consistency</strong> ‚Äî Recommendations and analytics can be eventually consistent (minutes to hours). Credit scores and alerts should be consistent within one batch cycle.</li>
  <li><strong>Fault Tolerance</strong> ‚Äî If one bureau feed fails, the system should still display available data from the other bureau. Message queue ensures no alert events are lost.</li>
  <li><strong>Regulatory Compliance</strong> ‚Äî Must comply with the Fair Credit Reporting Act (FCRA), GDPR (for EU users if applicable), CCPA, and state-level privacy laws.</li>
  <li><strong>Auditability</strong> ‚Äî All data access (especially PII) must be logged for audit trails.</li>
</ol>

<hr class="section-divider">

<!-- ===================== FLOW 1: CREDIT DATA INGESTION ===================== -->
<h2 id="flow1">3. Flow 1 ‚Äî Credit Data Ingestion</h2>
<p>This flow describes how raw credit data is ingested from external credit bureaus, processed, and stored in our system. This is a <strong>backend batch process</strong> ‚Äî no direct user interaction triggers it.</p>

<div class="diagram-container">
<div class="mermaid">
graph LR
    CB["üèõÔ∏è Credit Bureaus<br/>(TransUnion, Equifax)"] -->|"Batch Files<br/>(SFTP / Secure API)"| OS[("Object Storage<br/>(Encrypted)")]
    OS --> BPS["Batch Processing<br/>Service"]
    BPS --> DPS["Data Parsing &amp;<br/>Transform Service"]
    DPS -->|"Parsed Scores"| SDB[("Credit Score DB<br/>(Time-Series NoSQL)")]
    DPS -->|"Parsed Reports"| RDB[("Credit Report DB<br/>(Document NoSQL)")]
    DPS -->|"Change Events"| MQ(["Message Queue<br/>(Change Events)"])
    MQ --> MS["Monitoring Service<br/>(Flow 3)"]
    MQ --> REC["Recommendation<br/>Pipeline (Flow 4)"]
    CRON["‚è∞ Scheduler<br/>(Cron Job)"] -->|"Triggers Daily"| BPS
</div>
</div>

<h3>Examples</h3>

<div class="example-box">
<strong>Example 1 ‚Äî Normal Weekly Batch Ingestion:</strong><br>
Every Tuesday at 2:00 AM UTC, TransUnion delivers an encrypted batch file containing updated credit data for ~15 million users to our secure SFTP endpoint. The file lands in Object Storage (encrypted at rest). At 3:00 AM, the Scheduler triggers the Batch Processing Service, which reads the batch file, decompresses it, and decrypts it. The Data Parsing &amp; Transform Service normalizes the bureau-specific format into our internal canonical schema. For each user, the new credit score is written to the Credit Score DB (time-series) and the full credit report is upserted into the Credit Report DB (document store). Simultaneously, a change event is published to the Message Queue for every user whose score or report changed compared to the previous batch. The Monitoring Service (Flow 3) consumes these events to generate alerts, and the Recommendation Pipeline (Flow 4) uses them to refresh product recommendations.
</div>

<div class="example-box">
<strong>Example 2 ‚Äî Partial Bureau Failure:</strong><br>
Equifax's weekly batch file is delayed by 6 hours due to an outage on their side. The Scheduler fires the Batch Processing Service at the usual time, but finds no new Equifax file in Object Storage. The service logs a warning, sends an operational alert to the on-call team, and skips the Equifax batch. TransUnion data is still processed normally. Users who open the app will see their TransUnion score updated but their Equifax score will show the last available data with a label: "Last updated: Feb 6, 2026." Once Equifax delivers the file later, the system detects the late arrival (via object storage event trigger) and processes it immediately.
</div>

<div class="example-box">
<strong>Example 3 ‚Äî New User's First Data Pull:</strong><br>
A new user (user_42) just completed identity verification. The system performs a real-time soft-pull API call to TransUnion and Equifax (not a batch ‚Äî this is an on-demand pull). The response is parsed by the Data Parsing &amp; Transform Service and stored in both databases. No change event is published to the Message Queue because there is no previous data to compare against. The user now sees their credit scores for the first time on their dashboard.
</div>

<h3>Component Deep Dive</h3>

<div class="deep-dive">
<strong>Credit Bureaus (External)</strong><br>
TransUnion and Equifax are external third-party credit bureaus. They provide credit data via two mechanisms: (1) <strong>Batch file delivery</strong> ‚Äî encrypted files delivered weekly via SFTP or a secure file transfer API. Each file contains updated credit records for all enrolled users. (2) <strong>Real-time soft-pull API</strong> ‚Äî HTTPS REST API (mutual TLS) used for on-demand pulls during user registration. Response is JSON/XML containing the credit score and full report. Protocol: HTTPS with mutual TLS (mTLS). The bureau authenticates us via client certificates, and we authenticate the bureau via server certificates.
</div>

<div class="deep-dive">
<strong>Object Storage (Encrypted)</strong><br>
Stores raw batch files from credit bureaus. Files are encrypted at rest using AES-256 with keys managed by a Key Management Service (KMS). Access is restricted via IAM policies ‚Äî only the Batch Processing Service has read access. Files are retained for 90 days for audit purposes, then archived to cold storage. An event notification is configured so that when a new file is uploaded, it can trigger the Batch Processing Service (as a backup to the cron scheduler).
</div>

<div class="deep-dive">
<strong>Scheduler (Cron Job)</strong><br>
A distributed cron service that triggers the Batch Processing Service on a configurable schedule (e.g., daily at 3:00 AM UTC). Uses leader election to avoid duplicate triggers in a multi-node deployment. If the scheduled trigger fails, the Object Storage event notification serves as a fallback trigger.
</div>

<div class="deep-dive">
<strong>Batch Processing Service</strong><br>
Reads raw batch files from Object Storage, decompresses and decrypts them, and streams records to the Data Parsing &amp; Transform Service. For large files (tens of GBs), it processes records in parallel using a map-reduce pattern: the file is split into chunks, each chunk is processed by a worker, and results are aggregated. Protocol: Internal gRPC for communication with Data Parsing Service (low latency, efficient binary serialization). Input: Raw encrypted batch file path. Output: Stream of raw credit records.
</div>

<div class="deep-dive">
<strong>Data Parsing &amp; Transform Service</strong><br>
Receives raw credit records and normalizes them from bureau-specific formats (each bureau has its own schema) into our internal canonical data model. It performs:<br>
- Schema mapping (bureau fields ‚Üí internal fields)<br>
- Data validation (score range checks, field format validation)<br>
- Change detection (compares new data with last stored snapshot to identify changes)<br>
- Writes parsed scores to Credit Score DB and parsed reports to Credit Report DB<br>
- Publishes change events to the Message Queue<br>
Protocol: Internal gRPC (receives from Batch Processing Service). Writes to databases via their native client protocols. Publishes to Message Queue via producer API.<br>
Input: Stream of raw credit records. Output: Parsed &amp; stored records + change events on the queue.
</div>

<div class="deep-dive">
<strong>Message Queue (Change Events)</strong><br>
A durable, distributed message queue that holds change events. Each event contains: <code>{ user_id, bureau, change_type, previous_value, new_value, timestamp }</code>. Consumers include the Monitoring Service (for alert generation) and the Recommendation Pipeline (for refreshing recommendations). Messages are retained for 7 days. At-least-once delivery with idempotent consumers. See <a href="#mq">Message Queue Deep Dive</a> for full details.
</div>

<hr class="section-divider">

<!-- ===================== FLOW 2: CREDIT SCORE & REPORT DISPLAY ===================== -->
<h2 id="flow2">4. Flow 2 ‚Äî Credit Score &amp; Report Display</h2>
<p>This flow describes the user-facing path: a user opens the app or website and views their credit score and/or detailed credit report.</p>

<div class="diagram-container">
<div class="mermaid">
graph LR
    U["üë§ User<br/>(Mobile / Web)"] -->|"HTTPS"| CDN["CDN<br/>(Static Assets)"]
    U -->|"HTTPS"| LB["Load Balancer<br/>(L7)"]
    LB --> AG["API Gateway<br/>(Auth, Rate Limit)"]
    AG -->|"GET /scores"| CSS["Credit Score<br/>Service"]
    AG -->|"GET /report"| CRS["Credit Report<br/>Service"]
    CSS --> CACHE[("In-Memory Cache<br/>(Scores)")]
    CACHE -->|"Cache Miss"| SDB[("Credit Score DB<br/>(Time-Series NoSQL)")]
    CRS --> RCACHE[("In-Memory Cache<br/>(Reports)")]
    RCACHE -->|"Cache Miss"| RDB[("Credit Report DB<br/>(Document NoSQL)")]
    AG -->|"GET /score-history"| CSS
    CSS -->|"Time-range query"| SDB
</div>
</div>

<h3>Examples</h3>

<div class="example-box">
<strong>Example 1 ‚Äî User Views Credit Score (Cache Hit):</strong><br>
User opens the Credit Karma mobile app on their iPhone. The app makes an <code>HTTPS GET /api/v1/scores?user_id=user_42</code> request. The request hits the Load Balancer, which routes it to an available API Gateway instance. The API Gateway validates the user's JWT access token, checks rate limits, and forwards the request to the Credit Score Service. The Credit Score Service looks up <code>user_42</code> in the In-Memory Cache ‚Äî it's a cache hit (the score was cached when the weekly batch ran 2 days ago). The cached response <code>{ "transunion": 742, "equifax": 738, "as_of": "2026-02-10" }</code> is returned in ~80ms. The app displays both scores on the dashboard.
</div>

<div class="example-box">
<strong>Example 2 ‚Äî User Views Credit Score (Cache Miss):</strong><br>
User_99 logs in for the first time in 3 weeks. Their score has been evicted from cache (LRU eviction). The Credit Score Service checks the In-Memory Cache ‚Äî cache miss. It queries the Credit Score DB with <code>user_id = user_99, ORDER BY score_date DESC, LIMIT 2</code> (one per bureau). The DB returns the two most recent scores. The service writes them to the cache with a 4-hour TTL, then returns the response to the user in ~250ms.
</div>

<div class="example-box">
<strong>Example 3 ‚Äî User Views Detailed Credit Report:</strong><br>
User_42 taps "View Full Report" on their TransUnion score card. The app makes <code>HTTPS GET /api/v1/report?user_id=user_42&amp;bureau=transunion</code>. The Credit Report Service checks the In-Memory Cache (reports cache). Cache hit ‚Äî the full report JSON (accounts, inquiries, payment history, etc.) is returned. The app renders sections: 12 open accounts, 2 hard inquiries in the last 2 years, 98% on-time payments, 32% credit utilization, and 1 derogatory mark (a medical collection from 2019). The user can expand each section for details.
</div>

<div class="example-box">
<strong>Example 4 ‚Äî User Views Score History:</strong><br>
User_42 navigates to the "Score History" tab. The app makes <code>HTTPS GET /api/v1/score-history?user_id=user_42&amp;bureau=transunion&amp;range=12months</code>. The Credit Score Service queries the Credit Score DB for all TransUnion scores in the last 12 months for user_42. The DB returns ~52 data points (weekly scores). The response is returned as a time-series array, and the app renders a line chart showing the score trend over the past year.
</div>

<h3>Component Deep Dive</h3>

<div class="deep-dive">
<strong>CDN (Static Assets Only)</strong><br>
The CDN serves static assets: JavaScript bundles, CSS, images, educational content articles, and marketing pages. It does NOT serve any dynamic or personalized credit data (that would be a security risk). Assets are cache-busted via content-hash filenames (e.g., <code>main.a3f8b2c.js</code>). TTL: 30 days for immutable assets, 1 hour for HTML shells. See <a href="#caching">CDN &amp; Caching Strategy</a> for details.
</div>

<div class="deep-dive">
<strong>Load Balancer (L7)</strong><br>
Layer 7 (application-level) load balancer that distributes incoming HTTPS requests across multiple API Gateway instances. Uses a <strong>least-connections</strong> algorithm (routes to the gateway instance with the fewest active connections). Performs TLS termination so internal traffic uses plain HTTP/gRPC over a private network. Health checks every 10 seconds; unhealthy instances are removed from the pool. See <a href="#lb">Load Balancers</a> section.
</div>

<div class="deep-dive">
<strong>API Gateway</strong><br>
The single entry point for all client API requests. Responsibilities:<br>
- <strong>Authentication:</strong> Validates JWT access tokens. Rejects expired or invalid tokens with <code>401 Unauthorized</code>.<br>
- <strong>Rate Limiting:</strong> Per-user rate limit of 60 requests/minute to prevent abuse.<br>
- <strong>Request Routing:</strong> Routes requests to the appropriate microservice based on URL path.<br>
- <strong>Request/Response Transformation:</strong> Translates external REST API calls into internal gRPC calls.<br>
- <strong>Logging &amp; Metrics:</strong> Logs every request for audit and emits latency/error metrics.<br>
Protocol: Accepts HTTPS REST from clients. Communicates with backend services via internal gRPC.
</div>

<div class="deep-dive">
<strong>Credit Score Service</strong><br>
Responsible for serving credit scores and score history.<br>
<strong>Endpoints:</strong><br>
<table>
<tr><th>Method</th><th>Endpoint</th><th>Input</th><th>Output</th></tr>
<tr><td>GET</td><td>/api/v1/scores</td><td>user_id (from JWT)</td><td>Latest scores from each bureau: <code>[{bureau, score, score_date, factors}]</code></td></tr>
<tr><td>GET</td><td>/api/v1/score-history</td><td>user_id, bureau, range (e.g., 6months, 12months)</td><td>Array of <code>{score, score_date}</code> for the time range</td></tr>
</table>
Protocol: Internal gRPC (receives from API Gateway). Reads from In-Memory Cache (cache-aside) and falls back to Credit Score DB. Stateless ‚Äî can be horizontally scaled.
</div>

<div class="deep-dive">
<strong>Credit Report Service</strong><br>
Responsible for serving detailed credit reports.<br>
<strong>Endpoints:</strong><br>
<table>
<tr><th>Method</th><th>Endpoint</th><th>Input</th><th>Output</th></tr>
<tr><td>GET</td><td>/api/v1/report</td><td>user_id (from JWT), bureau</td><td>Full credit report JSON: accounts, inquiries, payment_history, utilization, derogatory_marks, public_records</td></tr>
</table>
Protocol: Internal gRPC. Reads from In-Memory Cache (reports cache) and falls back to Credit Report DB. Reports are large documents (~5-50KB each), so caching is important to avoid expensive document DB reads.
</div>

<div class="deep-dive">
<strong>In-Memory Cache (Scores &amp; Reports)</strong><br>
Two logical cache namespaces backed by a distributed in-memory cache cluster. See <a href="#caching">CDN &amp; Caching Strategy</a> for full details on strategies, eviction, and expiration.
</div>

<hr class="section-divider">

<!-- ===================== FLOW 3: CREDIT MONITORING & ALERTS ===================== -->
<h2 id="flow3">5. Flow 3 ‚Äî Credit Monitoring &amp; Alerts</h2>
<p>This flow describes how the system detects changes in a user's credit data and delivers alerts. It has two sub-paths: (A) alert generation (background) and (B) alert retrieval (user-initiated).</p>

<div class="diagram-container">
<div class="mermaid">
graph LR
    subgraph "A: Alert Generation (Background)"
        MQ(["Message Queue<br/>(Change Events)"]) --> MS["Monitoring<br/>Service"]
        MS -->|"Evaluate Rules"| RE["Rules Engine"]
        RE -->|"Alert Needed"| ADB[("Alert DB<br/>(NoSQL)")]
        RE -->|"Alert Payload"| NS["Notification<br/>Service"]
        NS --> PN["üì± Push<br/>Notification"]
        NS --> EM["üìß Email<br/>Service"]
        NS --> SMS["üí¨ SMS<br/>Service"]
        MS --> PREF[("User Preferences<br/>DB")]
    end

    subgraph "B: Alert Retrieval (User-Initiated)"
        U["üë§ User"] -->|"HTTPS"| LB["Load Balancer"]
        LB --> AG["API Gateway"]
        AG -->|"GET /alerts"| AS["Alert Service"]
        AS --> ADB2[("Alert DB<br/>(NoSQL)")]
        AG -->|"PATCH /alerts/:id"| AS
    end
</div>
</div>

<h3>Examples</h3>

<div class="example-box">
<strong>Example 1 ‚Äî Score Change Alert (Background Generation):</strong><br>
The Data Parsing Service (Flow 1) detects that user_42's TransUnion score changed from 742 ‚Üí 755 (a +13 increase). It publishes a change event to the Message Queue: <code>{ user_id: "user_42", bureau: "transunion", change_type: "score_change", previous: 742, new: 755, timestamp: "2026-02-10T03:15:00Z" }</code>. The Monitoring Service consumes this event, looks up user_42's notification preferences from the User Preferences DB (user_42 has push + email enabled for score changes). The Rules Engine evaluates the event and determines that a score change of +13 exceeds the notification threshold (default: any change). The Monitoring Service writes an alert record to the Alert DB: <code>{ alert_id: "a_001", user_id: "user_42", type: "score_change", data: {...}, is_read: false, created_at: "..." }</code>. It then sends the alert payload to the Notification Service, which dispatches a push notification ("Your TransUnion score went up 13 points to 755! üéâ") and an email with more detail.
</div>

<div class="example-box">
<strong>Example 2 ‚Äî New Hard Inquiry Alert:</strong><br>
The batch data shows a new hard inquiry on user_88's Equifax report from "Chase Bank" dated Feb 8. A change event is published: <code>{ user_id: "user_88", bureau: "equifax", change_type: "new_inquiry", data: { creditor: "Chase Bank", date: "2026-02-08" } }</code>. The Monitoring Service consumes this, checks preferences (user_88 has push enabled for new inquiries), and the Rules Engine triggers an alert. A push notification is sent: "A new hard inquiry from Chase Bank appeared on your Equifax report." If user_88 did NOT apply for anything at Chase, this could indicate potential fraud ‚Äî the notification includes a "Dispute" action button.
</div>

<div class="example-box">
<strong>Example 3 ‚Äî User Retrieves Alerts:</strong><br>
User_42 opens the app and navigates to the Alerts tab. The app makes <code>HTTPS GET /api/v1/alerts?user_id=user_42&amp;limit=20&amp;offset=0</code>. The API Gateway routes this to the Alert Service, which queries the Alert DB for user_42's most recent 20 alerts (sorted by <code>created_at DESC</code>). The response includes the score change alert from Example 1 and 3 other alerts. The app displays them in a feed. When user_42 taps on the score change alert, the app sends <code>HTTPS PATCH /api/v1/alerts/a_001</code> with body <code>{ is_read: true }</code> to mark it as read.
</div>

<div class="example-box">
<strong>Example 4 ‚Äî User Has Alerts Disabled:</strong><br>
A change event for user_77 is consumed by the Monitoring Service. It checks user_77's preferences and finds that user_77 has disabled all notifications. The Rules Engine still writes the alert to the Alert DB (so it's visible when the user opens the app), but does NOT send any push notification, email, or SMS. The alert is available for in-app retrieval only.
</div>

<h3>Component Deep Dive</h3>

<div class="deep-dive">
<strong>Monitoring Service</strong><br>
Consumes change events from the Message Queue and orchestrates the alert generation process. For each event:<br>
1. Looks up user's notification preferences<br>
2. Passes the event + preferences to the Rules Engine<br>
3. If an alert is warranted, writes to Alert DB and dispatches to Notification Service<br>
Runs as a consumer group with multiple instances for parallel processing. Each partition of the queue is assigned to one consumer instance (no duplicate processing). Protocol: Consumes from Message Queue via consumer API. Writes to Alert DB via native DB client. Calls Notification Service via internal gRPC.
</div>

<div class="deep-dive">
<strong>Rules Engine</strong><br>
A configurable rule evaluation engine embedded within the Monitoring Service. Rules are defined as JSON configurations and can be updated without code deploys. Example rules:<br>
- "If score changes by any amount ‚Üí trigger score_change alert"<br>
- "If a new hard inquiry appears ‚Üí trigger new_inquiry alert"<br>
- "If a new account is opened ‚Üí trigger new_account alert"<br>
- "If an account enters collections ‚Üí trigger collections alert"<br>
- "If score drops by > 50 points ‚Üí trigger major_score_drop alert (high priority)"<br>
Rules can be prioritized and combined. The engine outputs a decision: { should_alert: true/false, alert_type, priority, message_template }.
</div>

<div class="deep-dive">
<strong>Notification Service</strong><br>
Receives alert payloads and dispatches notifications via the user's preferred channels.<br>
<strong>Channels:</strong><br>
- <strong>Push Notification:</strong> Uses platform-native push services (APNs for iOS, FCM for Android). Stores device tokens in a Device Token DB.<br>
- <strong>Email:</strong> Sends via an SMTP relay / email delivery service. Emails include richer content (score chart, report summary).<br>
- <strong>SMS:</strong> Sends via an SMS gateway. Used sparingly (high-priority alerts only, e.g., potential fraud).<br>
Protocol: Internal gRPC (receives from Monitoring Service). Outbound: HTTPS to push services, SMTP to email relay, HTTPS to SMS gateway.<br>
The Notification Service is idempotent ‚Äî it deduplicates based on <code>(user_id, alert_id)</code> to prevent sending the same notification twice if the Message Queue delivers the event more than once.
</div>

<div class="deep-dive">
<strong>Alert Service</strong><br>
Serves user-facing alert retrieval and management endpoints.<br>
<strong>Endpoints:</strong><br>
<table>
<tr><th>Method</th><th>Endpoint</th><th>Input</th><th>Output</th></tr>
<tr><td>GET</td><td>/api/v1/alerts</td><td>user_id (JWT), limit, offset, filter (optional: unread_only)</td><td>Paginated list of alerts: <code>[{alert_id, type, data, is_read, created_at}]</code></td></tr>
<tr><td>PATCH</td><td>/api/v1/alerts/:alert_id</td><td>alert_id, is_read (boolean)</td><td>Updated alert record</td></tr>
<tr><td>GET</td><td>/api/v1/alerts/unread-count</td><td>user_id (JWT)</td><td><code>{unread_count: 5}</code></td></tr>
</table>
Protocol: Internal gRPC (receives from API Gateway). Reads/writes to Alert DB.
</div>

<hr class="section-divider">

<!-- ===================== FLOW 4: PRODUCT RECOMMENDATION & REFERRAL ===================== -->
<h2 id="flow4">6. Flow 4 ‚Äî Product Recommendation &amp; Referral</h2>
<p>This flow describes how personalized financial product recommendations are generated, displayed to users, and how referral clicks are tracked. This is the <strong>core revenue-generating flow</strong> ‚Äî Credit Karma earns referral fees when users click through and are approved for partner products.</p>

<div class="diagram-container">
<div class="mermaid">
graph LR
    subgraph "A: Recommendation Generation (Background)"
        SDB[("Credit Score DB")] --> ML["ML Recommendation<br/>Pipeline"]
        RDB[("Credit Report DB")] --> ML
        PCAT[("Product Catalog DB<br/>(SQL)")] --> ML
        CLICK[("Click/Conversion<br/>Analytics DB")] --> ML
        ML -->|"Ranked Offers"| RECDB[("Recommendation DB<br/>(NoSQL)")]
        ML --> RECCACHE[("In-Memory Cache<br/>(Recommendations)")]
    end

    subgraph "B: Recommendation Display &amp; Referral"
        U["üë§ User"] -->|"HTTPS"| LB["Load Balancer"]
        LB --> AG["API Gateway"]
        AG -->|"GET /recommendations"| RS["Recommendation<br/>Service"]
        RS --> RECCACHE2[("In-Memory Cache")]
        RECCACHE2 -->|"Miss"| RECDB2[("Recommendation DB")]
        U -->|"Clicks Offer"| LB
        LB --> AG
        AG -->|"POST /referral/click"| CTS["Click Tracking<br/>Service"]
        CTS --> CLICK2[("Analytics DB<br/>(NoSQL)")]
        CTS -->|"302 Redirect"| PARTNER["üè¶ Partner<br/>Application Page"]
    end
</div>
</div>

<h3>Examples</h3>

<div class="example-box">
<strong>Example 1 ‚Äî Background Recommendation Generation:</strong><br>
The ML Recommendation Pipeline runs nightly at 4:00 AM UTC. For user_42 (credit score: 755, low utilization, no derogatory marks), the pipeline reads their credit profile from the Score DB and Report DB, fetches all active products from the Product Catalog DB, and also ingests historical click/conversion data from the Analytics DB for collaborative filtering. The ML model (a combination of gradient-boosted trees for approval odds + collaborative filtering for user preference) scores each product. Results: "Premium Travel Rewards Card" (approval odds: 92%, relevance: 0.95), "Cash Back Card" (approval odds: 88%, relevance: 0.87), "Low APR Personal Loan" (approval odds: 78%, relevance: 0.62). These ranked recommendations are written to the Recommendation DB and the In-Memory Cache for fast retrieval.
</div>

<div class="example-box">
<strong>Example 2 ‚Äî User Browses Recommendations:</strong><br>
User_42 taps the "Recommendations" tab in the app. The app makes <code>HTTPS GET /api/v1/recommendations?user_id=user_42&amp;category=credit_cards&amp;limit=10</code>. The API Gateway routes this to the Recommendation Service. The service checks the In-Memory Cache ‚Äî cache hit. It returns the top 10 credit card recommendations, each including: product name, card image URL, key features, APR range, annual fee, estimated approval odds, and a brief reason (e.g., "Great match ‚Äî your excellent credit score qualifies you for top rewards"). The app renders the feed with each card as a tappable card UI element.
</div>

<div class="example-box">
<strong>Example 3 ‚Äî User Clicks a Recommended Product (Referral):</strong><br>
User_42 taps "Apply Now" on the "Premium Travel Rewards Card." The app makes <code>HTTPS POST /api/v1/referral/click</code> with body <code>{ user_id: "user_42", product_id: "prod_101", recommendation_rank: 1, session_id: "sess_abc" }</code>. The API Gateway routes this to the Click Tracking Service, which:<br>
1. Logs the click event to the Analytics DB (for conversion tracking and ML model feedback).<br>
2. Generates a partner-specific referral URL with tracking parameters (e.g., <code>https://partner-bank.com/apply?ref=ck_user42_prod101&amp;tracking_id=trk_xyz</code>).<br>
3. Returns a <code>302 Redirect</code> to the partner's application page.<br>
The user's browser/app follows the redirect and lands on the partner's application page. If the user is approved, the partner sends a conversion callback to our Conversion Tracking webhook (async), and Credit Karma earns a $75 referral fee.
</div>

<div class="example-box">
<strong>Example 4 ‚Äî User With Low Credit Score Sees Different Recommendations:</strong><br>
User_55 has a credit score of 580 (fair). The ML pipeline generates recommendations tailored to their profile: "Credit Builder Card" (approval odds: 85%, no annual fee, secured card), "Debt Consolidation Personal Loan" (approval odds: 70%), and "Free Credit Monitoring Upgrade." The system does NOT show premium rewards cards (approval odds would be &lt;5%). This personalization is key to user trust ‚Äî showing products users are likely to be approved for increases conversion rates and user satisfaction.
</div>

<h3>Component Deep Dive</h3>

<div class="deep-dive">
<strong>ML Recommendation Pipeline</strong><br>
A batch ML pipeline that runs nightly (or more frequently during high-traffic periods). Architecture:<br>
1. <strong>Feature Engineering:</strong> Extracts features from credit profiles (score, utilization, account age, inquiry count, etc.) and user behavior (past clicks, time on page, categories viewed).<br>
2. <strong>Approval Odds Model:</strong> A gradient-boosted tree model trained on historical application outcomes. Predicts P(approval | user_features, product_features) for each user-product pair.<br>
3. <strong>Relevance Ranking:</strong> Collaborative filtering model that ranks products by estimated user interest, combining approval odds with behavioral signals.<br>
4. <strong>Business Rules Layer:</strong> Applies constraints (e.g., don't show a product the user already has, respect partner daily impression caps, ensure regulatory compliance in product advertising).<br>
Output: For each user, a ranked list of ~20 recommended products stored in the Recommendation DB.<br>
The pipeline can also be triggered on-demand when a user's credit profile changes significantly (via the Message Queue from Flow 1), though full re-computation is batched.
</div>

<div class="deep-dive">
<strong>Recommendation Service</strong><br>
Serves personalized product recommendations to users.<br>
<strong>Endpoints:</strong><br>
<table>
<tr><th>Method</th><th>Endpoint</th><th>Input</th><th>Output</th></tr>
<tr><td>GET</td><td>/api/v1/recommendations</td><td>user_id (JWT), category (optional: credit_cards, loans, insurance), limit, offset</td><td>Ranked list of products: <code>[{product_id, name, image_url, features, apr_range, annual_fee, approval_odds, reason}]</code></td></tr>
</table>
Protocol: Internal gRPC. Reads from In-Memory Cache (write-through from ML pipeline) and falls back to Recommendation DB. Stateless.
</div>

<div class="deep-dive">
<strong>Click Tracking Service</strong><br>
Tracks user interactions with recommended products for analytics and revenue attribution.<br>
<strong>Endpoints:</strong><br>
<table>
<tr><th>Method</th><th>Endpoint</th><th>Input</th><th>Output</th></tr>
<tr><td>POST</td><td>/api/v1/referral/click</td><td>user_id, product_id, recommendation_rank, session_id</td><td>302 Redirect to partner URL with tracking parameters</td></tr>
<tr><td>POST</td><td>/api/v1/referral/conversion (webhook)</td><td>tracking_id, conversion_type (approved/funded), revenue</td><td>200 OK</td></tr>
</table>
Protocol: Internal gRPC (from API Gateway). Writes to Analytics DB. Generates partner redirect URLs. The conversion webhook is a public HTTPS endpoint authenticated via HMAC signatures from partners.
</div>

<div class="deep-dive">
<strong>Product Catalog DB (SQL)</strong><br>
Stores all available financial products from partner companies. This is a relatively small dataset (~10K products) that changes infrequently (partners add/update products weekly). SQL is used because:<br>
- Structured, relational data (products belong to partners, have categories)<br>
- Needs ACID transactions for product updates (e.g., deactivating a product)<br>
- Complex queries needed by the ML pipeline (filter by score range, category, partner)<br>
See <a href="#schema">Schema</a> section for full table definitions.
</div>

<div class="deep-dive">
<strong>Analytics DB (NoSQL)</strong><br>
Stores click events, impression events, and conversion events. High write volume (~millions of events/day), append-only, time-series access patterns. NoSQL chosen for:<br>
- High write throughput<br>
- Time-range queries for analytics dashboards<br>
- No complex joins needed<br>
See <a href="#schema">Schema</a> section for full table definitions.
</div>

<hr class="section-divider">

<!-- ===================== COMBINED OVERALL FLOW ===================== -->
<h2 id="combined">7. Combined Overall Flow</h2>
<p>This diagram combines all four flows into a single unified architecture view, showing how data flows from credit bureaus all the way to the user's screen and back to partner referrals.</p>

<div class="diagram-container">
<div class="mermaid">
graph TB
    CB["üèõÔ∏è Credit Bureaus"] -->|"Batch Files (SFTP)"| OS[("Object Storage")]
    CB -->|"Real-time Soft Pull<br/>(New Users)"| DPS

    OS --> BPS["Batch Processing Service"]
    BPS --> DPS["Data Parsing &amp; Transform"]

    DPS --> SDB[("Credit Score DB<br/>(Time-Series NoSQL)")]
    DPS --> RDB[("Credit Report DB<br/>(Document NoSQL)")]
    DPS --> MQ(["Message Queue"])

    MQ --> MS["Monitoring Service"]
    MQ --> MLPIPE["ML Recommendation Pipeline"]

    MS --> RE["Rules Engine"]
    MS --> PREFDB[("User Prefs DB")]
    RE --> ADB[("Alert DB<br/>(NoSQL)")]
    RE --> NS["Notification Service"]
    NS --> PN["üì± Push"]
    NS --> EM["üìß Email"]

    MLPIPE --> PCAT[("Product Catalog DB<br/>(SQL)")]
    MLPIPE --> CLICKDB[("Analytics DB<br/>(NoSQL)")]
    MLPIPE --> RECDB[("Recommendation DB<br/>(NoSQL)")]

    U["üë§ User (Mobile/Web)"] -->|"HTTPS"| CDN_NODE["CDN (Static Assets)"]
    U -->|"HTTPS"| LB["Load Balancer (L7)"]
    LB --> AG["API Gateway"]

    AG --> CSS["Credit Score Service"]
    AG --> CRS["Credit Report Service"]
    AG --> AS["Alert Service"]
    AG --> RS["Recommendation Service"]
    AG --> CTS["Click Tracking Service"]

    CSS --> SCACHE[("Score Cache")]
    SCACHE --> SDB
    CRS --> RCACHE[("Report Cache")]
    RCACHE --> RDB
    AS --> ADB
    RS --> RECCACHE[("Rec Cache")]
    RECCACHE --> RECDB
    CTS --> CLICKDB
    CTS -->|"302 Redirect"| PARTNER["üè¶ Partner Site"]

    USERDB[("User DB<br/>(SQL)")] --> AG
</div>
</div>

<h3>Combined Flow Examples</h3>

<div class="example-box">
<strong>Example 1 ‚Äî End-to-End: From Bureau Update to User Dashboard:</strong><br>
On Tuesday at 2 AM, TransUnion delivers its weekly batch file to Object Storage. At 3 AM, the Scheduler triggers the Batch Processing Service, which reads, decompresses, and streams records to the Data Parsing &amp; Transform Service. For user_42, the parser detects a score increase from 742 ‚Üí 755 and a new "Excellent" rating. It writes the new score to the Credit Score DB and the updated report to the Credit Report DB. A change event <code>{user_id: "user_42", change_type: "score_change", old: 742, new: 755}</code> is published to the Message Queue.<br><br>

The Monitoring Service consumes this event, checks user_42's preferences (push + email enabled), evaluates it against the Rules Engine (score change ‚Üí alert), writes an alert to the Alert DB, and dispatches a push notification: "Your TransUnion score went up 13 points to 755! üéâ"<br><br>

Simultaneously, the ML Recommendation Pipeline (which also consumes change events for high-value users) notes that user_42 now qualifies for premium credit cards. It re-ranks user_42's recommendations and updates the Recommendation DB and cache.<br><br>

Later that morning, user_42 receives the push notification on their iPhone and opens the app. The app loads the dashboard by calling <code>GET /scores</code> ‚Äî the Credit Score Service returns the new 755 score from cache. User_42 is pleased and navigates to the Recommendations tab. <code>GET /recommendations</code> returns the newly ranked offers, now featuring the "Premium Travel Rewards Card" at the top with 92% approval odds. User_42 taps "Apply Now," which triggers <code>POST /referral/click</code> ‚Äî the Click Tracking Service logs the event and redirects user_42 to the partner bank's application page. User_42 applies and is approved. The partner sends a conversion webhook, and Credit Karma earns a $75 referral fee. üí∞
</div>

<div class="example-box">
<strong>Example 2 ‚Äî New User Onboarding (Real-time Pull):</strong><br>
New user signs up on the Credit Karma website. They enter their name, DOB, address, and last 4 of SSN. The API Gateway calls the Identity Verification Service, which makes a real-time KBA request to TransUnion. TransUnion returns 4 security questions (e.g., "Which of these streets have you lived on?"). The user answers correctly. The system then performs a real-time soft pull from both TransUnion and Equifax. The Data Parsing &amp; Transform Service processes the responses and stores scores + reports in the respective databases. The user's dashboard loads with their first-ever credit scores. No monitoring alerts are generated (no previous data to compare). The ML pipeline will pick up this user in the next nightly run to generate personalized recommendations. In the meantime, the Recommendation Service falls back to a default set of "popular products for your score range" recommendations.
</div>

<div class="example-box">
<strong>Example 3 ‚Äî Potential Fraud Detection:</strong><br>
The weekly batch for user_88 shows a new hard inquiry from "Unknown Lender" and a new credit card account opened that user_88 didn't authorize. Two change events are published. The Monitoring Service processes them and the Rules Engine flags both as high-priority (new inquiry + new account in the same batch is a fraud signal). A high-priority push notification is sent: "‚ö†Ô∏è Potential fraud detected: A new account was opened in your name. Review now." An email is also sent with details and a "Freeze Credit" action link. When user_88 opens the app, the Alerts tab shows both alerts with a red "Action Required" badge. The app provides one-tap links to dispute the inquiry and freeze credit with both bureaus.
</div>

<hr class="section-divider">

<!-- ===================== DATABASE SCHEMA ===================== -->
<h2 id="schema">8. Database Schema</h2>

<h3>SQL Tables</h3>

<h4>8.1 ‚Äî <code>users</code> Table (SQL ‚Äî Relational Database)</h4>
<table>
<tr><th>Column</th><th>Type</th><th>Constraints</th><th>Description</th></tr>
<tr><td><strong>user_id</strong></td><td>UUID</td><td>PRIMARY KEY</td><td>Unique user identifier</td></tr>
<tr><td>email</td><td>VARCHAR(255)</td><td>UNIQUE, NOT NULL</td><td>User's email (used for login)</td></tr>
<tr><td>password_hash</td><td>VARCHAR(255)</td><td>NOT NULL</td><td>Bcrypt-hashed password</td></tr>
<tr><td>full_name</td><td>VARCHAR(255)</td><td>NOT NULL</td><td>User's legal name</td></tr>
<tr><td>ssn_hash</td><td>VARCHAR(255)</td><td>UNIQUE, NOT NULL</td><td>Salted SHA-256 hash of full SSN</td></tr>
<tr><td>date_of_birth</td><td>DATE</td><td>NOT NULL</td><td>For identity verification</td></tr>
<tr><td>address</td><td>TEXT</td><td>NOT NULL</td><td>Encrypted current address</td></tr>
<tr><td>phone</td><td>VARCHAR(20)</td><td></td><td>Encrypted phone number</td></tr>
<tr><td>identity_verified</td><td>BOOLEAN</td><td>DEFAULT false</td><td>Whether KBA verification passed</td></tr>
<tr><td>created_at</td><td>TIMESTAMP</td><td>NOT NULL</td><td>Account creation timestamp</td></tr>
<tr><td>updated_at</td><td>TIMESTAMP</td><td>NOT NULL</td><td>Last update timestamp</td></tr>
</table>

<p><strong>Why SQL:</strong> User data is highly structured and relational. ACID transactions are required for account creation (ensuring uniqueness of email and SSN hash atomically). The user table is read on every authenticated request (via JWT validation that references user_id), and written to during registration and profile updates.</p>

<p><strong>Indexes:</strong></p>
<ul>
  <li><strong>B-tree index on <code>email</code></strong> ‚Äî Used for login lookups (<code>SELECT * FROM users WHERE email = ?</code>). B-tree supports equality and range queries; email lookups are equality-based. A hash index would also work, but B-tree is more versatile and supports prefix searches if needed.</li>
  <li><strong>Hash index on <code>ssn_hash</code></strong> ‚Äî Used during registration to check for duplicate SSNs. Pure equality lookups make hash index optimal (O(1) lookup). No need for range queries on SSN hashes.</li>
</ul>

<p><strong>Read/Write Triggers:</strong></p>
<ul>
  <li><em>Write:</em> User registers (INSERT), user updates profile (UPDATE)</li>
  <li><em>Read:</em> Every authenticated API call (to validate user exists), login (email lookup), registration (SSN dedup check)</li>
</ul>

<p><strong>Sharding:</strong> Shard by <code>user_id</code> using consistent hashing. User lookups are almost always by <code>user_id</code> (from JWT). For email-based lookups (login), a global secondary index or a lookup table mapping email ‚Üí user_id ‚Üí shard is used. Consistent hashing ensures even distribution and minimal resharding when nodes are added/removed.</p>

<hr>

<h4>8.2 ‚Äî <code>partners</code> Table (SQL ‚Äî Relational Database)</h4>
<table>
<tr><th>Column</th><th>Type</th><th>Constraints</th><th>Description</th></tr>
<tr><td><strong>partner_id</strong></td><td>UUID</td><td>PRIMARY KEY</td><td>Unique partner identifier</td></tr>
<tr><td>partner_name</td><td>VARCHAR(255)</td><td>NOT NULL</td><td>E.g., "Chase," "Discover"</td></tr>
<tr><td>partner_type</td><td>ENUM</td><td>NOT NULL</td><td>bank, credit_union, insurer, lender</td></tr>
<tr><td>api_endpoint</td><td>VARCHAR(500)</td><td></td><td>Partner's application redirect base URL</td></tr>
<tr><td>webhook_secret</td><td>VARCHAR(255)</td><td></td><td>HMAC secret for conversion webhooks</td></tr>
<tr><td>is_active</td><td>BOOLEAN</td><td>DEFAULT true</td><td>Whether partner is currently active</td></tr>
<tr><td>created_at</td><td>TIMESTAMP</td><td>NOT NULL</td><td>Partner onboarding date</td></tr>
</table>

<p><strong>Why SQL:</strong> Small, structured dataset (~500 partners). Relational ‚Äî products belong to partners (FK relationship). Needs ACID for partner status changes. Read by the ML pipeline and joined with products.</p>

<p><strong>Indexes:</strong> Primary key index on <code>partner_id</code> (B-tree). No additional indexes needed ‚Äî table is small enough for full scans.</p>

<p><strong>Sharding:</strong> Not needed ‚Äî table is small (hundreds of rows). Replicated across read replicas for availability.</p>

<p><strong>Read/Write Triggers:</strong></p>
<ul>
  <li><em>Write:</em> Admin onboards new partner (INSERT), admin updates partner status (UPDATE)</li>
  <li><em>Read:</em> ML pipeline reads all active partners nightly; Recommendation Service joins with products</li>
</ul>

<hr>

<h4>8.3 ‚Äî <code>partner_products</code> Table (SQL ‚Äî Relational Database)</h4>
<table>
<tr><th>Column</th><th>Type</th><th>Constraints</th><th>Description</th></tr>
<tr><td><strong>product_id</strong></td><td>UUID</td><td>PRIMARY KEY</td><td>Unique product identifier</td></tr>
<tr><td>partner_id</td><td>UUID</td><td>FOREIGN KEY ‚Üí partners.partner_id</td><td>The partner offering this product</td></tr>
<tr><td>product_type</td><td>ENUM</td><td>NOT NULL</td><td>credit_card, personal_loan, auto_loan, mortgage, insurance</td></tr>
<tr><td>product_name</td><td>VARCHAR(255)</td><td>NOT NULL</td><td>E.g., "Sapphire Preferred"</td></tr>
<tr><td>description</td><td>TEXT</td><td></td><td>Product description</td></tr>
<tr><td>image_url</td><td>VARCHAR(500)</td><td></td><td>URL to product image (served via CDN)</td></tr>
<tr><td>apr_min</td><td>DECIMAL(5,2)</td><td></td><td>Minimum APR (e.g., 18.49)</td></tr>
<tr><td>apr_max</td><td>DECIMAL(5,2)</td><td></td><td>Maximum APR (e.g., 25.49)</td></tr>
<tr><td>annual_fee</td><td>DECIMAL(8,2)</td><td></td><td>Annual fee (0 = no fee)</td></tr>
<tr><td>min_credit_score</td><td>INT</td><td></td><td>Minimum score for eligibility</td></tr>
<tr><td>max_credit_score</td><td>INT</td><td></td><td>Maximum score (for targeting, e.g., credit builder cards target &lt;650)</td></tr>
<tr><td>referral_fee</td><td>DECIMAL(10,2)</td><td></td><td>Fee Credit Karma earns per approved referral</td></tr>
<tr><td>features</td><td>JSON</td><td></td><td>Key features list (e.g., ["2x points on travel", "No foreign transaction fees"])</td></tr>
<tr><td>is_active</td><td>BOOLEAN</td><td>DEFAULT true</td><td>Whether product is currently offered</td></tr>
<tr><td>created_at</td><td>TIMESTAMP</td><td>NOT NULL</td><td>Product creation date</td></tr>
<tr><td>updated_at</td><td>TIMESTAMP</td><td>NOT NULL</td><td>Last update</td></tr>
</table>

<p><strong>Why SQL:</strong> Structured data with relational foreign key to partners. The ML pipeline needs complex queries: <code>SELECT * FROM partner_products WHERE is_active = true AND min_credit_score <= 755 AND product_type = 'credit_card'</code>. SQL's query optimizer handles these efficiently with proper indexes. ACID needed for product updates.</p>

<p><strong>Indexes:</strong></p>
<ul>
  <li><strong>Composite B-tree index on <code>(product_type, min_credit_score, is_active)</code></strong> ‚Äî The ML pipeline queries products by type and score range. This composite index supports: <code>WHERE product_type = 'credit_card' AND min_credit_score <= 755 AND is_active = true</code>. B-tree chosen because it supports both equality (product_type, is_active) and range queries (min_credit_score <=). Column ordering matters: highest-selectivity column first.</li>
  <li><strong>B-tree index on <code>partner_id</code></strong> ‚Äî For JOIN queries between products and partners.</li>
</ul>

<p><strong>Sharding:</strong> Not needed ‚Äî table is small (~10K products). Replicated read replicas suffice.</p>

<p><strong>Read/Write Triggers:</strong></p>
<ul>
  <li><em>Write:</em> Partner manager adds/updates product (INSERT/UPDATE), admin deactivates expired products (UPDATE)</li>
  <li><em>Read:</em> ML pipeline reads all active products nightly; Recommendation Service may read product details for display enrichment</li>
</ul>

<hr>

<h4>8.4 ‚Äî <code>user_preferences</code> Table (SQL ‚Äî Relational Database)</h4>
<table>
<tr><th>Column</th><th>Type</th><th>Constraints</th><th>Description</th></tr>
<tr><td><strong>user_id</strong></td><td>UUID</td><td>PRIMARY KEY, FOREIGN KEY ‚Üí users.user_id</td><td>One-to-one with users</td></tr>
<tr><td>push_enabled</td><td>BOOLEAN</td><td>DEFAULT true</td><td>Global push notification toggle</td></tr>
<tr><td>email_enabled</td><td>BOOLEAN</td><td>DEFAULT true</td><td>Global email notification toggle</td></tr>
<tr><td>sms_enabled</td><td>BOOLEAN</td><td>DEFAULT false</td><td>SMS notifications (opt-in)</td></tr>
<tr><td>alert_score_change</td><td>BOOLEAN</td><td>DEFAULT true</td><td>Alert on score changes</td></tr>
<tr><td>alert_new_account</td><td>BOOLEAN</td><td>DEFAULT true</td><td>Alert on new accounts</td></tr>
<tr><td>alert_new_inquiry</td><td>BOOLEAN</td><td>DEFAULT true</td><td>Alert on hard inquiries</td></tr>
<tr><td>alert_fraud</td><td>BOOLEAN</td><td>DEFAULT true</td><td>Alert on potential fraud (always recommended on)</td></tr>
<tr><td>updated_at</td><td>TIMESTAMP</td><td>NOT NULL</td><td>Last preference update</td></tr>
</table>

<p><strong>Why SQL:</strong> One-to-one with users table (FK). Structured boolean flags. Read by the Monitoring Service for every alert decision. Low write frequency (users rarely change preferences). ACID ensures consistent preference state.</p>

<p><strong>Indexes:</strong> Primary key on <code>user_id</code> (B-tree). No additional indexes ‚Äî all queries are by user_id.</p>

<p><strong>Sharding:</strong> Co-located with <code>users</code> table ‚Äî same shard key (<code>user_id</code>). This enables efficient local joins.</p>

<p><strong>Read/Write Triggers:</strong></p>
<ul>
  <li><em>Write:</em> User updates notification preferences in Settings screen (UPDATE)</li>
  <li><em>Read:</em> Monitoring Service reads preferences for every change event to decide notification channels</li>
</ul>

<hr>

<h3>NoSQL Tables</h3>

<h4>8.5 ‚Äî <code>credit_scores</code> Table (Time-Series NoSQL)</h4>
<table>
<tr><th>Column</th><th>Type</th><th>Key</th><th>Description</th></tr>
<tr><td><strong>user_id</strong></td><td>UUID</td><td>PARTITION KEY</td><td>User this score belongs to</td></tr>
<tr><td><strong>bureau_score_date</strong></td><td>STRING</td><td>SORT KEY</td><td>Composite: <code>{bureau}#{score_date}</code> (e.g., "transunion#2026-02-10")</td></tr>
<tr><td>bureau</td><td>STRING</td><td></td><td>"transunion" or "equifax"</td></tr>
<tr><td>score</td><td>INT</td><td></td><td>Credit score (300-850)</td></tr>
<tr><td>score_model</td><td>STRING</td><td></td><td>"VantageScore 3.0"</td></tr>
<tr><td>score_date</td><td>DATE</td><td></td><td>Date the score was calculated</td></tr>
<tr><td>factors</td><td>LIST&lt;MAP&gt;</td><td></td><td>Top factors: [{factor: "High utilization", impact: "negative", detail: "72% utilization"}]</td></tr>
<tr><td>ingested_at</td><td>TIMESTAMP</td><td></td><td>When we ingested this record</td></tr>
</table>

<p><strong>Why Time-Series NoSQL:</strong></p>
<ul>
  <li>Credit scores are inherently time-series data (weekly snapshots over years).</li>
  <li>Write pattern is append-only (new scores are added, never updated).</li>
  <li>Read patterns are: (a) latest score per bureau for a user, (b) score history over a time range ‚Äî both optimally served by a sort key on date.</li>
  <li>High write volume during batch ingestion (100M+ users √ó 2 bureaus = 200M+ writes in a batch window).</li>
  <li>No complex joins needed ‚Äî always queried by user_id.</li>
</ul>

<p><strong>Indexes:</strong> The partition key (user_id) + sort key (bureau_score_date) combination naturally supports:</p>
<ul>
  <li><code>WHERE user_id = 'user_42' AND bureau_score_date BEGINS_WITH 'transunion' ORDER BY bureau_score_date DESC LIMIT 1</code> ‚Üí Latest TransUnion score</li>
  <li><code>WHERE user_id = 'user_42' AND bureau_score_date BETWEEN 'transunion#2025-02-01' AND 'transunion#2026-02-10'</code> ‚Üí Score history for charting</li>
</ul>

<p><strong>Sharding:</strong> Partition key (<code>user_id</code>) serves as the shard key. Consistent hashing distributes users evenly across shards. Each shard stores all time-series data for its assigned users. This ensures that all queries for a single user hit a single shard (no scatter-gather).</p>

<p><strong>Read/Write Triggers:</strong></p>
<ul>
  <li><em>Write:</em> Batch Processing Service writes new scores during weekly ingestion; real-time soft-pull for new users</li>
  <li><em>Read:</em> Credit Score Service reads latest scores (on dashboard load) and score history (on history tab)</li>
</ul>

<hr>

<h4>8.6 ‚Äî <code>credit_reports</code> Table (Document NoSQL)</h4>
<table>
<tr><th>Column</th><th>Type</th><th>Key</th><th>Description</th></tr>
<tr><td><strong>user_id</strong></td><td>UUID</td><td>PARTITION KEY</td><td>User this report belongs to</td></tr>
<tr><td><strong>bureau_report_date</strong></td><td>STRING</td><td>SORT KEY</td><td>Composite: <code>{bureau}#{report_date}</code></td></tr>
<tr><td>bureau</td><td>STRING</td><td></td><td>"transunion" or "equifax"</td></tr>
<tr><td>report_date</td><td>DATE</td><td></td><td>Date of the report</td></tr>
<tr><td>accounts</td><td>LIST&lt;MAP&gt;</td><td></td><td>[{account_name, type, balance, limit, status, opened_date, payment_history: [...]}]</td></tr>
<tr><td>inquiries</td><td>LIST&lt;MAP&gt;</td><td></td><td>[{creditor, date, type: "hard"/"soft"}]</td></tr>
<tr><td>collections</td><td>LIST&lt;MAP&gt;</td><td></td><td>[{creditor, amount, date, status}]</td></tr>
<tr><td>public_records</td><td>LIST&lt;MAP&gt;</td><td></td><td>[{type: "bankruptcy", date, status}]</td></tr>
<tr><td>utilization</td><td>MAP</td><td></td><td>{total_balance, total_limit, utilization_pct}</td></tr>
<tr><td>summary</td><td>MAP</td><td></td><td>{total_accounts, avg_age, oldest_account_age, on_time_pct}</td></tr>
<tr><td>ingested_at</td><td>TIMESTAMP</td><td></td><td>When we ingested this record</td></tr>
</table>

<p><strong>Why Document NoSQL:</strong></p>
<ul>
  <li>Credit reports are deeply nested, hierarchical documents (accounts contain payment histories, each with 24+ months of data).</li>
  <li>A single report can be 5-50KB of JSON ‚Äî relational normalization would require 6+ tables and expensive JOINs to reconstruct.</li>
  <li>The document model allows storing and retrieving the entire report in a single read operation.</li>
  <li>Schema varies slightly between bureaus ‚Äî a flexible document model accommodates this without schema migrations.</li>
  <li>Read pattern is always "get the entire report for user X from bureau Y" ‚Äî no partial queries needed.</li>
</ul>

<p><strong>Denormalization Note:</strong> The <code>summary</code> field is a denormalized aggregation of the report data (total_accounts, avg_age, on_time_pct). This is intentionally denormalized to avoid computing these aggregations on every read. The summary is computed once during ingestion by the Data Parsing &amp; Transform Service. This is acceptable because the data only changes weekly and the computation cost is negligible at write time but would add latency on every read if done on-the-fly.</p>

<p><strong>Sharding:</strong> Same as credit_scores ‚Äî partition key (<code>user_id</code>) with consistent hashing.</p>

<p><strong>Read/Write Triggers:</strong></p>
<ul>
  <li><em>Write:</em> Batch Processing Service writes/upserts reports during weekly ingestion; real-time soft-pull for new users</li>
  <li><em>Read:</em> Credit Report Service reads report when user taps "View Full Report"</li>
</ul>

<hr>

<h4>8.7 ‚Äî <code>alerts</code> Table (NoSQL ‚Äî Wide-Column/Key-Value)</h4>
<table>
<tr><th>Column</th><th>Type</th><th>Key</th><th>Description</th></tr>
<tr><td><strong>user_id</strong></td><td>UUID</td><td>PARTITION KEY</td><td>User this alert belongs to</td></tr>
<tr><td><strong>created_at</strong></td><td>TIMESTAMP</td><td>SORT KEY (DESC)</td><td>Alert creation time (descending for "newest first")</td></tr>
<tr><td>alert_id</td><td>UUID</td><td></td><td>Unique alert ID</td></tr>
<tr><td>alert_type</td><td>STRING</td><td></td><td>score_change, new_inquiry, new_account, fraud_warning, collections</td></tr>
<tr><td>priority</td><td>STRING</td><td></td><td>low, medium, high, critical</td></tr>
<tr><td>title</td><td>STRING</td><td></td><td>Alert title text</td></tr>
<tr><td>body</td><td>STRING</td><td></td><td>Alert detail text</td></tr>
<tr><td>data</td><td>MAP</td><td></td><td>Alert-specific payload (e.g., {old_score: 742, new_score: 755})</td></tr>
<tr><td>is_read</td><td>BOOLEAN</td><td></td><td>Whether user has viewed this alert</td></tr>
<tr><td>bureau</td><td>STRING</td><td></td><td>Which bureau triggered this alert</td></tr>
</table>

<p><strong>Why NoSQL (Wide-Column):</strong></p>
<ul>
  <li>High write volume ‚Äî millions of alerts generated per batch cycle.</li>
  <li>Simple access pattern: always queried by <code>user_id</code>, sorted by <code>created_at DESC</code>.</li>
  <li>No joins needed. Each alert is self-contained.</li>
  <li>TTL-based expiration: alerts older than 1 year can be automatically expired by the DB's TTL feature.</li>
</ul>

<p><strong>Indexes:</strong></p>
<ul>
  <li><strong>Global Secondary Index (GSI) on <code>(user_id, is_read, created_at)</code></strong> ‚Äî Supports the "unread alerts" query: <code>WHERE user_id = 'user_42' AND is_read = false ORDER BY created_at DESC</code>. This GSI enables the unread count badge on the app without scanning all alerts.</li>
</ul>

<p><strong>Sharding:</strong> Partition key (<code>user_id</code>) with consistent hashing. Same strategy as other user-centric tables.</p>

<p><strong>Read/Write Triggers:</strong></p>
<ul>
  <li><em>Write:</em> Monitoring Service writes new alerts when change events are processed (INSERT); Alert Service updates <code>is_read</code> when user views an alert (UPDATE)</li>
  <li><em>Read:</em> Alert Service reads alerts when user opens Alerts tab (GET); reads unread count for badge on dashboard</li>
</ul>

<hr>

<h4>8.8 ‚Äî <code>user_recommendations</code> Table (NoSQL ‚Äî Key-Value/Wide-Column)</h4>
<table>
<tr><th>Column</th><th>Type</th><th>Key</th><th>Description</th></tr>
<tr><td><strong>user_id</strong></td><td>UUID</td><td>PARTITION KEY</td><td>User these recommendations are for</td></tr>
<tr><td><strong>rank</strong></td><td>INT</td><td>SORT KEY</td><td>Recommendation rank (1 = top recommendation)</td></tr>
<tr><td>product_id</td><td>UUID</td><td></td><td>FK reference to partner_products (not enforced ‚Äî NoSQL)</td></tr>
<tr><td>product_name</td><td>STRING</td><td></td><td><em>Denormalized</em> from partner_products for fast reads</td></tr>
<tr><td>product_type</td><td>STRING</td><td></td><td><em>Denormalized</em> ‚Äî credit_card, loan, insurance</td></tr>
<tr><td>image_url</td><td>STRING</td><td></td><td><em>Denormalized</em> ‚Äî product image URL</td></tr>
<tr><td>approval_odds</td><td>FLOAT</td><td></td><td>ML-predicted approval probability (0.0 - 1.0)</td></tr>
<tr><td>relevance_score</td><td>FLOAT</td><td></td><td>ML-computed relevance/ranking score</td></tr>
<tr><td>reason</td><td>STRING</td><td></td><td>Human-readable reason (e.g., "Great match for your excellent credit")</td></tr>
<tr><td>features</td><td>LIST&lt;STRING&gt;</td><td></td><td><em>Denormalized</em> ‚Äî key product features</td></tr>
<tr><td>apr_range</td><td>STRING</td><td></td><td><em>Denormalized</em> ‚Äî "18.49% - 25.49%"</td></tr>
<tr><td>annual_fee</td><td>STRING</td><td></td><td><em>Denormalized</em> ‚Äî "$0" or "$95"</td></tr>
<tr><td>generated_at</td><td>TIMESTAMP</td><td></td><td>When the ML pipeline generated this recommendation</td></tr>
<tr><td>expires_at</td><td>TIMESTAMP</td><td></td><td>TTL ‚Äî recommendations expire after 48 hours (force refresh)</td></tr>
</table>

<p><strong>Why NoSQL:</strong></p>
<ul>
  <li>Access pattern is simple: "Get top N recommendations for user X" ‚Äî always by user_id, sorted by rank.</li>
  <li>High write volume during ML pipeline batch runs.</li>
  <li>No joins needed at read time (denormalized data).</li>
  <li>Recommendations are ephemeral ‚Äî replaced nightly by the ML pipeline.</li>
</ul>

<p><strong>Denormalization Explanation:</strong> Product details (product_name, product_type, image_url, features, apr_range, annual_fee) are <em>denormalized</em> (copied) from the <code>partner_products</code> SQL table into each recommendation record. This eliminates the need for a cross-database JOIN between NoSQL (recommendations) and SQL (products) at read time. The tradeoff is data duplication and potential staleness ‚Äî if a product's APR changes, the recommendation record may show the old APR until the ML pipeline re-runs. This is acceptable because: (1) product details change infrequently, (2) the ML pipeline refreshes recommendations nightly, (3) the Recommendation Service screen shows a "View full details" link that fetches live product data from the SQL DB.</p>

<p><strong>Sharding:</strong> Partition key (<code>user_id</code>) with consistent hashing.</p>

<p><strong>Read/Write Triggers:</strong></p>
<ul>
  <li><em>Write:</em> ML Recommendation Pipeline writes/replaces all recommendations for a user after each nightly run</li>
  <li><em>Read:</em> Recommendation Service reads when user opens Recommendations tab</li>
</ul>

<hr>

<h4>8.9 ‚Äî <code>click_events</code> Table (NoSQL ‚Äî Time-Series / Append-Only)</h4>
<table>
<tr><th>Column</th><th>Type</th><th>Key</th><th>Description</th></tr>
<tr><td><strong>event_id</strong></td><td>UUID</td><td>PRIMARY KEY (auto-generated)</td><td>Unique event identifier</td></tr>
<tr><td>user_id</td><td>UUID</td><td></td><td>User who performed the action</td></tr>
<tr><td>product_id</td><td>UUID</td><td></td><td>Product interacted with</td></tr>
<tr><td>event_type</td><td>STRING</td><td></td><td>impression, click, apply_start, conversion</td></tr>
<tr><td>recommendation_rank</td><td>INT</td><td></td><td>Position of product in recommendation list when clicked</td></tr>
<tr><td>session_id</td><td>STRING</td><td></td><td>User's app session ID</td></tr>
<tr><td>partner_tracking_id</td><td>STRING</td><td></td><td>Tracking ID sent to partner for attribution</td></tr>
<tr><td>revenue</td><td>DECIMAL</td><td></td><td>Referral revenue (populated on conversion events only)</td></tr>
<tr><td>metadata</td><td>MAP</td><td></td><td>Additional event-specific data</td></tr>
<tr><td>created_at</td><td>TIMESTAMP</td><td></td><td>Event timestamp</td></tr>
</table>

<p><strong>Why Time-Series NoSQL:</strong></p>
<ul>
  <li>Extremely high write volume ‚Äî millions of impressions and clicks per day.</li>
  <li>Append-only ‚Äî events are never updated (immutable log).</li>
  <li>Analytics queries are time-range based ("clicks in the last 7 days for product X").</li>
  <li>The ML pipeline reads historical events for collaborative filtering (batch reads, not real-time).</li>
</ul>

<p><strong>Indexes:</strong></p>
<ul>
  <li><strong>GSI on <code>(user_id, created_at)</code></strong> ‚Äî For querying a user's click history (used by ML pipeline for personalization).</li>
  <li><strong>GSI on <code>(product_id, created_at)</code></strong> ‚Äî For querying click/conversion rates per product (used by analytics dashboard and business rules).</li>
</ul>

<p><strong>Sharding:</strong> Shard by <code>event_id</code> (random distribution) for write scaling, since this table is write-heavy and reads are done via GSIs. Alternatively, shard by <code>created_at</code> (time-based range partitioning) if analytics queries by time range dominate.</p>

<p><strong>Read/Write Triggers:</strong></p>
<ul>
  <li><em>Write:</em> Click Tracking Service writes on every user interaction ‚Äî impression (product shown), click (user taps), conversion (partner webhook)</li>
  <li><em>Read:</em> ML pipeline reads historical events for model training; analytics dashboards query aggregated metrics</li>
</ul>

<hr>

<h4>8.10 ‚Äî <code>device_tokens</code> Table (NoSQL ‚Äî Key-Value)</h4>
<table>
<tr><th>Column</th><th>Type</th><th>Key</th><th>Description</th></tr>
<tr><td><strong>user_id</strong></td><td>UUID</td><td>PARTITION KEY</td><td>User who owns the device</td></tr>
<tr><td><strong>device_id</strong></td><td>STRING</td><td>SORT KEY</td><td>Unique device identifier</td></tr>
<tr><td>platform</td><td>STRING</td><td></td><td>"ios" or "android"</td></tr>
<tr><td>push_token</td><td>STRING</td><td></td><td>APNs token (iOS) or FCM token (Android)</td></tr>
<tr><td>is_active</td><td>BOOLEAN</td><td></td><td>Whether token is still valid</td></tr>
<tr><td>updated_at</td><td>TIMESTAMP</td><td></td><td>Last token refresh</td></tr>
</table>

<p><strong>Why NoSQL (Key-Value):</strong> Simple key-value access (lookup by user_id to get all device tokens). High read frequency (every push notification). Low write frequency (token changes when user reinstalls app or token expires). No joins needed.</p>

<p><strong>Read/Write Triggers:</strong></p>
<ul>
  <li><em>Write:</em> App startup sends device token to server (INSERT/UPDATE); token invalidation on push failure (UPDATE)</li>
  <li><em>Read:</em> Notification Service reads device tokens when sending push notifications</li>
</ul>

<hr class="section-divider">

<!-- ===================== CDN & CACHING ===================== -->
<h2 id="caching">9. CDN &amp; Caching Strategy</h2>

<h3>9.1 ‚Äî CDN (Content Delivery Network)</h3>

<p><strong>Appropriate?</strong> ‚úÖ Yes, for static assets. ‚ùå Not for dynamic credit data.</p>

<p><strong>What is served via CDN:</strong></p>
<ul>
  <li>JavaScript bundles, CSS stylesheets (web app)</li>
  <li>Product card images (credit card artwork, partner logos)</li>
  <li>Educational articles and financial tips (static HTML content)</li>
  <li>Marketing pages and landing pages</li>
  <li>App update packages (if applicable)</li>
</ul>

<p><strong>What is NOT served via CDN:</strong></p>
<ul>
  <li>Credit scores, reports, alerts ‚Äî these are personalized and sensitive (PII). Serving via CDN would risk data leakage through shared caches.</li>
  <li>Recommendations ‚Äî personalized per user.</li>
  <li>Any user-specific data.</li>
</ul>

<p><strong>CDN Strategy:</strong></p>
<ul>
  <li><strong>Caching:</strong> Immutable assets (JS bundles, images) are served with <code>Cache-Control: public, max-age=2592000, immutable</code> (30 days). Filenames include content hashes for cache busting (e.g., <code>app.3fa8b2.js</code>).</li>
  <li><strong>Origin:</strong> Object Storage serves as the origin for static assets.</li>
  <li><strong>Edge Locations:</strong> CDN edges distributed globally for low-latency static asset delivery.</li>
  <li><strong>HTTPS Only:</strong> All CDN content served over HTTPS.</li>
</ul>

<h3>9.2 ‚Äî In-Memory Cache</h3>

<p><strong>Appropriate?</strong> ‚úÖ Absolutely. Credit Karma is extremely read-heavy (users check scores far more often than scores update). Caching is critical for performance and database load reduction.</p>

<h4>Cache Namespace: Credit Scores</h4>
<table>
<tr><th>Property</th><th>Value</th><th>Rationale</th></tr>
<tr><td><strong>Caching Strategy</strong></td><td>Cache-Aside (Lazy Loading)</td><td>Scores are read-heavy, write-infrequent (weekly). Cache-aside avoids caching data that's never read (some users are inactive). On cache miss, the service reads from DB, writes to cache, then returns. This is simpler than write-through and avoids caching data for inactive users who may never read it.</td></tr>
<tr><td><strong>Population</strong></td><td>On cache miss (first read after eviction/expiration). <strong>Additionally</strong>, the Batch Processing Service proactively warms the cache for recently active users after each batch ingestion cycle (pre-warming).</td><td>Pre-warming for active users eliminates cold-start latency for the most common users. Inactive users' data is loaded on demand.</td></tr>
<tr><td><strong>Eviction Policy</strong></td><td>LRU (Least Recently Used)</td><td>Users who haven't checked their score recently are the best candidates for eviction. LRU ensures that the most active users' scores remain cached. With 100M users but only ~10M DAU, LRU effectively keeps the "hot" 10-15M users in cache.</td></tr>
<tr><td><strong>Expiration Policy</strong></td><td>TTL = 4 hours</td><td>Credit scores change at most weekly, so a 4-hour TTL provides a good balance: fresh enough that users see updates within hours of a batch run, but long enough to absorb most traffic without DB hits. After a batch run at 3 AM, the cache naturally expires and repopulates with fresh data by the time users open the app in the morning.</td></tr>
<tr><td><strong>Cache Key</strong></td><td><code>score:{user_id}</code></td><td>Simple, unique per user. Returns both bureau scores in one cache entry to avoid multiple lookups.</td></tr>
</table>

<h4>Cache Namespace: Credit Reports</h4>
<table>
<tr><th>Property</th><th>Value</th><th>Rationale</th></tr>
<tr><td><strong>Caching Strategy</strong></td><td>Cache-Aside (Lazy Loading)</td><td>Reports are large (5-50KB) and not all users view their full report. Cache-aside ensures we only cache reports that are actually requested.</td></tr>
<tr><td><strong>Population</strong></td><td>On cache miss only (no pre-warming)</td><td>Reports are large. Pre-warming would consume excessive cache memory. Only ~20% of DAU views full reports ‚Äî lazy loading is efficient.</td></tr>
<tr><td><strong>Eviction Policy</strong></td><td>LRU (Least Recently Used)</td><td>Same rationale as scores ‚Äî evict least-accessed reports first. Reports are larger, so the cache holds fewer of them; LRU maximizes hit rate for the available memory.</td></tr>
<tr><td><strong>Expiration Policy</strong></td><td>TTL = 24 hours</td><td>Reports change weekly and are expensive to retrieve from Document DB. A 24-hour TTL reduces DB load while ensuring updates are visible within a day.</td></tr>
<tr><td><strong>Cache Key</strong></td><td><code>report:{user_id}:{bureau}</code></td><td>Separate cache entries per bureau to allow independent retrieval.</td></tr>
</table>

<h4>Cache Namespace: Recommendations</h4>
<table>
<tr><th>Property</th><th>Value</th><th>Rationale</th></tr>
<tr><td><strong>Caching Strategy</strong></td><td>Write-Through</td><td>The ML pipeline writes recommendations to both the DB and the cache simultaneously. This ensures the cache is always populated with the latest recommendations after each pipeline run, eliminating cold-start latency for the most important revenue-generating feature. Write-through is chosen over cache-aside because recommendations are always generated in bulk by the pipeline (not by user requests), so we can guarantee the cache is warm.</td></tr>
<tr><td><strong>Population</strong></td><td>Populated by the ML Recommendation Pipeline during each nightly run. The pipeline writes each user's ranked recommendations to both the Recommendation DB and the cache atomically.</td><td>Ensures zero cold-start misses for the revenue-critical recommendations feed. Every user who opens the Recommendations tab gets a cache hit.</td></tr>
<tr><td><strong>Eviction Policy</strong></td><td>LRU (Least Recently Used)</td><td>If cache memory is full, evict recommendations for the least recently active users. Since the pipeline writes for all users but only ~10M DAU read them, LRU naturally keeps active users' recs in cache.</td></tr>
<tr><td><strong>Expiration Policy</strong></td><td>TTL = 48 hours</td><td>Recommendations are regenerated nightly. A 48-hour TTL provides a safety net: if the nightly pipeline fails, users still see yesterday's recommendations (slightly stale but better than nothing). After 48 hours, stale recommendations are evicted to avoid showing very outdated approval odds.</td></tr>
<tr><td><strong>Cache Key</strong></td><td><code>recs:{user_id}</code></td><td>Returns the full ranked list for the user. Category filtering is done in-memory by the Recommendation Service after cache retrieval.</td></tr>
</table>

<h4>Cache Namespace: User Sessions</h4>
<table>
<tr><th>Property</th><th>Value</th><th>Rationale</th></tr>
<tr><td><strong>Caching Strategy</strong></td><td>Cache-Aside</td><td>Session tokens are created at login and validated on every request. Cache-aside with short TTL provides fast lookups.</td></tr>
<tr><td><strong>Population</strong></td><td>On login ‚Äî session data is written to cache.</td><td>Login is the natural trigger. Every subsequent request reads from cache.</td></tr>
<tr><td><strong>Eviction Policy</strong></td><td>LRU</td><td>Inactive sessions are the first to be evicted.</td></tr>
<tr><td><strong>Expiration Policy</strong></td><td>TTL = 30 minutes (sliding window)</td><td>Each API request refreshes the TTL. If no activity for 30 minutes, the session expires for security. Sliding window ensures active users are not logged out.</td></tr>
<tr><td><strong>Cache Key</strong></td><td><code>session:{session_id}</code></td><td>Session ID from JWT or session cookie.</td></tr>
</table>

<hr class="section-divider">

<!-- ===================== MESSAGE QUEUE DEEP DIVE ===================== -->
<h2 id="mq">10. Message Queue Deep Dive</h2>

<h3>Why a Message Queue?</h3>
<p>The Credit Karma system has several <strong>asynchronous processing needs</strong> that are perfectly suited for a message queue:</p>
<ul>
  <li><strong>Decoupling:</strong> The Data Parsing Service (producer) should not need to know about or wait for the Monitoring Service, Recommendation Pipeline, or any other consumer. Adding a new consumer (e.g., a future fraud detection service) requires zero changes to the producer.</li>
  <li><strong>Burst Handling:</strong> During batch ingestion, the Data Parsing Service generates 100M+ change events in a few hours. Downstream consumers (Monitoring Service, Notification Service) cannot process them all in real-time. The queue acts as a buffer, allowing consumers to process at their own pace.</li>
  <li><strong>Reliability:</strong> At-least-once delivery ensures no alert events are lost, even if a consumer crashes mid-processing.</li>
  <li><strong>Retry:</strong> Failed messages are retried with exponential backoff. After max retries, messages go to a Dead Letter Queue (DLQ) for manual inspection.</li>
</ul>

<h3>Why Not Alternatives?</h3>
<table>
<tr><th>Alternative</th><th>Why Not Chosen</th></tr>
<tr><td><strong>Synchronous HTTP calls</strong></td><td>Would tightly couple the Data Parsing Service to all downstream consumers. If the Monitoring Service is slow or down, it would block the entire batch ingestion pipeline. Unacceptable for a batch processing 100M+ records.</td></tr>
<tr><td><strong>Pub/Sub (fan-out messaging)</strong></td><td>Actually, the message queue here IS configured with pub/sub-like topic-based routing. Each change event is published to a "credit-change-events" topic. Multiple consumer groups (Monitoring Service, Recommendation Pipeline) each have their own subscription/consumer group, so each event is delivered to all consumers independently. This is a pub/sub pattern implemented on top of the message queue.</td></tr>
<tr><td><strong>WebSockets</strong></td><td>WebSockets are for client-server real-time communication. They are irrelevant for inter-service backend communication. The message queue is for service-to-service async processing.</td></tr>
<tr><td><strong>Polling (consumers poll DB for changes)</strong></td><td>Polling the database for changes would add read load to the DB, introduce latency (polling interval), and be inefficient (most polls would find no changes). A push-based message queue is more efficient and lower latency.</td></tr>
</table>

<h3>How Messages Flow</h3>
<ol>
  <li><strong>Production:</strong> The Data Parsing &amp; Transform Service publishes messages to the queue after processing each credit record. Message format:
    <pre><code>{
  "event_id": "evt_abc123",
  "user_id": "user_42",
  "bureau": "transunion",
  "change_type": "score_change",  // or "new_inquiry", "new_account", "account_closed", etc.
  "previous_value": { "score": 742 },
  "new_value": { "score": 755 },
  "timestamp": "2026-02-10T03:15:00Z"
}</code></pre>
  Messages are published with a partition key of <code>user_id</code> to ensure all events for the same user are processed in order by each consumer.</li>
  <li><strong>Topic/Routing:</strong> Messages are published to a topic called <code>credit-change-events</code>. Consumer groups subscribe to this topic.</li>
  <li><strong>Consumption:</strong> Each consumer group (e.g., "monitoring-service-group", "recommendation-pipeline-group") receives a copy of every message. Within a consumer group, messages are distributed across consumer instances by partition (user_id hash). Each partition is consumed by exactly one instance in the group (no duplicate processing within a group).</li>
  <li><strong>Acknowledgment:</strong> After a consumer successfully processes a message (e.g., Monitoring Service writes alert to DB and dispatches notification), it sends an ACK to the queue. The message is then marked as processed and eventually removed. If no ACK is received within 60 seconds, the message is redelivered to another consumer instance (retry).</li>
  <li><strong>Dead Letter Queue (DLQ):</strong> Messages that fail processing after 3 retries are moved to a DLQ. An operational alert is triggered for the on-call team to investigate. Common DLQ causes: malformed data from bureau, DB write timeout, notification service outage.</li>
</ol>

<h3>Configuration</h3>
<table>
<tr><th>Property</th><th>Value</th></tr>
<tr><td>Delivery Guarantee</td><td>At-least-once (consumers must be idempotent)</td></tr>
<tr><td>Ordering</td><td>Per-partition (events for same user_id are ordered)</td></tr>
<tr><td>Partitions</td><td>256 (supports up to 256 consumer instances per group)</td></tr>
<tr><td>Retention</td><td>7 days (allows replay if a consumer needs to reprocess)</td></tr>
<tr><td>Max Message Size</td><td>1 MB</td></tr>
<tr><td>ACK Timeout</td><td>60 seconds</td></tr>
<tr><td>Max Retries</td><td>3 (then DLQ)</td></tr>
</table>

<p><strong>Idempotency:</strong> Because delivery is at-least-once, consumers must handle duplicate messages. The Monitoring Service uses the <code>event_id</code> as an idempotency key ‚Äî before writing an alert, it checks if an alert with this event_id already exists. If so, it skips the duplicate.</p>

<hr class="section-divider">

<!-- ===================== PUSH NOTIFICATIONS DEEP DIVE ===================== -->
<h2 id="push">11. Push Notifications Deep Dive</h2>

<h3>Why Push Notifications (Not WebSocket/SSE/Polling)?</h3>
<p>Credit Karma's alert delivery has a unique characteristic: <strong>low frequency, high importance</strong>. Users receive at most a few alerts per week (when scores update), not continuous real-time streams. This makes push notifications the ideal delivery mechanism:</p>

<table>
<tr><th>Mechanism</th><th>Why Used / Not Used</th></tr>
<tr><td><strong>Push Notifications ‚úÖ</strong></td><td>Ideal for low-frequency, high-importance alerts. Works even when the app is closed. Battery-efficient (no persistent connection). Users expect credit alerts as push notifications. Platform-native (APNs for iOS, FCM for Android) with reliable delivery infrastructure.</td></tr>
<tr><td><strong>WebSocket ‚ùå</strong></td><td>WebSockets maintain a persistent bidirectional connection. This is overkill for Credit Karma ‚Äî we don't need real-time bidirectional communication (no chat, no live streaming). Maintaining WebSocket connections for 10M+ concurrent users would be expensive and wasteful when alerts come at most weekly. WebSockets drain mobile battery due to persistent connections.</td></tr>
<tr><td><strong>Server-Sent Events (SSE) ‚ùå</strong></td><td>SSE is unidirectional (server ‚Üí client), which fits our use case better than WebSocket. However, SSE requires the app to be open and maintaining an HTTP connection. Since users check their credit score maybe once a week, maintaining an SSE connection would be wasteful. Push notifications work when the app is closed.</td></tr>
<tr><td><strong>Long Polling ‚ùå</strong></td><td>Long polling has the same limitation as SSE (requires app to be active) but with more overhead (repeated HTTP requests). Inefficient for low-frequency events.</td></tr>
</table>

<h3>How Push Notifications Work</h3>
<ol>
  <li><strong>Device Registration:</strong> When the user installs the app and grants notification permission, the app requests a push token from the OS (APNs token on iOS, FCM token on Android). The app sends this token to our backend via <code>POST /api/v1/devices/register</code> with body <code>{ device_id, platform, push_token }</code>. The token is stored in the <code>device_tokens</code> table.</li>
  <li><strong>Token Refresh:</strong> Push tokens can change (e.g., after app reinstall, OS update). The app re-registers on every launch. If the stored token differs, it's updated.</li>
  <li><strong>Notification Dispatch:</strong> When the Notification Service receives an alert payload from the Monitoring Service:
    <ul>
      <li>It looks up the user's device tokens from the <code>device_tokens</code> table.</li>
      <li>For each active device, it constructs a platform-specific push payload (APNs JSON or FCM JSON).</li>
      <li>It sends the payload to the platform's push service (APNs or FCM) via HTTPS.</li>
      <li>The platform's push service delivers the notification to the user's device.</li>
    </ul>
  </li>
  <li><strong>Failure Handling:</strong> If the push service returns an error indicating the token is invalid (e.g., user uninstalled the app), the Notification Service marks the token as inactive in the <code>device_tokens</code> table. Future notifications for this device are skipped until a new token is registered.</li>
  <li><strong>Email Fallback:</strong> For users who have email enabled, the Notification Service also sends an email via SMTP relay. Emails include richer content: score chart, detailed breakdown, and CTA buttons.</li>
</ol>

<hr class="section-divider">

<!-- ===================== LOAD BALANCERS ===================== -->
<h2 id="lb">12. Load Balancers</h2>

<h3>Where Load Balancers Are Placed</h3>
<ol>
  <li><strong>LB-1: Internet ‚Üí API Gateway</strong> ‚Äî The primary external-facing load balancer. Distributes incoming HTTPS requests from mobile apps and web browsers across multiple API Gateway instances.</li>
  <li><strong>LB-2: API Gateway ‚Üí Microservices</strong> ‚Äî Internal load balancers (or service mesh) that distribute requests from the API Gateway to instances of each microservice (Credit Score Service, Credit Report Service, Alert Service, Recommendation Service, Click Tracking Service). In practice, this is often implemented as a service mesh (e.g., Envoy sidecars) or internal L4 load balancers rather than a dedicated appliance.</li>
</ol>

<h3>Deep Dive: LB-1 (External Load Balancer)</h3>
<table>
<tr><th>Property</th><th>Value</th><th>Rationale</th></tr>
<tr><td><strong>Layer</strong></td><td>L7 (Application Layer)</td><td>Needs to inspect HTTP headers for routing, perform TLS termination, and apply HTTP-level health checks. L7 enables path-based routing (e.g., <code>/api/v1/scores</code> vs <code>/api/v1/recommendations</code>).</td></tr>
<tr><td><strong>Algorithm</strong></td><td>Least Connections</td><td>Routes each request to the API Gateway instance with the fewest active connections. This handles uneven request processing times better than round-robin (e.g., a <code>GET /report</code> takes longer than <code>GET /scores</code>, so an instance handling many report requests should receive fewer new requests).</td></tr>
<tr><td><strong>TLS Termination</strong></td><td>Yes ‚Äî at the load balancer</td><td>Decrypts HTTPS at the edge. Internal traffic between LB and API Gateway uses plain HTTP over a private VPC network. This reduces CPU overhead on API Gateway instances (no TLS handshake per request).</td></tr>
<tr><td><strong>Health Checks</strong></td><td>HTTP GET /health every 10s</td><td>Each API Gateway instance exposes a <code>/health</code> endpoint. If an instance fails 3 consecutive health checks (30s), it's removed from the pool. When it passes again, it's re-added.</td></tr>
<tr><td><strong>Sticky Sessions</strong></td><td>Not used</td><td>All services are stateless. Any API Gateway instance can handle any request. No need for session affinity.</td></tr>
<tr><td><strong>Rate Limiting</strong></td><td>Performed at LB level (global) and API Gateway level (per-user)</td><td>The LB applies a global rate limit (e.g., 100K req/s total) to protect against DDoS. The API Gateway applies per-user rate limits (60 req/min) to protect against abuse by individual users.</td></tr>
<tr><td><strong>Auto-Scaling Integration</strong></td><td>Automatically registers/deregisters API Gateway instances as they scale up/down.</td><td>During peak traffic (e.g., Monday mornings), new API Gateway instances are launched and automatically added to the LB pool.</td></tr>
</table>

<h3>Deep Dive: LB-2 (Internal Service Mesh / Load Balancers)</h3>
<table>
<tr><th>Property</th><th>Value</th><th>Rationale</th></tr>
<tr><td><strong>Layer</strong></td><td>L4 (Transport Layer) or Service Mesh</td><td>Internal traffic is gRPC (HTTP/2). L4 load balancing is sufficient and lower overhead. Alternatively, a service mesh handles load balancing, retries, and circuit breaking at the sidecar level.</td></tr>
<tr><td><strong>Algorithm</strong></td><td>Round Robin with health-aware routing</td><td>Internal service instances are generally uniform in processing time. Round robin provides even distribution. Health-aware routing skips unhealthy instances.</td></tr>
<tr><td><strong>Circuit Breaking</strong></td><td>Enabled ‚Äî trips after 50% error rate over 30s window</td><td>If a downstream service starts failing, the circuit breaker trips and returns an error immediately instead of sending more requests to a failing service. This prevents cascade failures.</td></tr>
</table>

<hr class="section-divider">

<!-- ===================== SCALING CONSIDERATIONS ===================== -->
<h2 id="scaling">13. Scaling Considerations</h2>

<h3>Traffic Patterns</h3>
<ul>
  <li><strong>Peak Hours:</strong> 7-10 AM local time (users check credit on their morning commute). Monday mornings are the weekly peak.</li>
  <li><strong>Seasonal Peaks:</strong> January (New Year's resolutions), tax season (March-April), and post-holiday (November-December, users checking credit after shopping).</li>
  <li><strong>Batch Ingestion:</strong> 2-5 AM UTC weekly ‚Äî massive write spike to databases.</li>
</ul>

<h3>Scaling Strategies by Component</h3>
<table>
<tr><th>Component</th><th>Scaling Strategy</th><th>Details</th></tr>
<tr><td><strong>API Gateway</strong></td><td>Horizontal auto-scaling</td><td>Stateless ‚Äî add more instances behind LB-1 during peak hours. Scale based on CPU utilization (target: 60%) and request queue depth. Min: 10 instances, Max: 100 instances.</td></tr>
<tr><td><strong>Credit Score Service</strong></td><td>Horizontal auto-scaling</td><td>Stateless. Scale based on request latency (target p99 &lt; 300ms). Most load is absorbed by the cache; the service itself is lightweight.</td></tr>
<tr><td><strong>Credit Report Service</strong></td><td>Horizontal auto-scaling</td><td>Stateless. Reports are larger, so slightly more CPU per request. Scale based on request count and latency.</td></tr>
<tr><td><strong>Alert Service</strong></td><td>Horizontal auto-scaling</td><td>Stateless. Moderate load ‚Äî only active when users open the Alerts tab.</td></tr>
<tr><td><strong>Recommendation Service</strong></td><td>Horizontal auto-scaling</td><td>Stateless. High priority for scaling ‚Äî this is the revenue-generating service. Scale aggressively during peak hours.</td></tr>
<tr><td><strong>Click Tracking Service</strong></td><td>Horizontal auto-scaling</td><td>Stateless. Must handle burst clicks when a popular product is featured.</td></tr>
<tr><td><strong>Monitoring Service</strong></td><td>Horizontal auto-scaling (consumer group)</td><td>Scales by adding consumer instances to the consumer group. Max instances = number of queue partitions (256). Each instance processes one or more partitions.</td></tr>
<tr><td><strong>Notification Service</strong></td><td>Horizontal auto-scaling</td><td>Must handle bursts during batch processing (millions of notifications in a few hours). Scale based on notification queue depth.</td></tr>
<tr><td><strong>Batch Processing Service</strong></td><td>Vertical + Horizontal</td><td>Map-reduce pattern: split batch files across worker nodes. Scale workers based on file size. Vertical scaling for memory (large files need in-memory buffering).</td></tr>
<tr><td><strong>ML Recommendation Pipeline</strong></td><td>Horizontal (compute cluster)</td><td>Distributed ML pipeline running on a compute cluster. Scale by adding compute nodes. Uses spot/preemptible instances for cost savings (batch job can tolerate restarts).</td></tr>
<tr><td><strong>Credit Score DB</strong></td><td>Horizontal (sharding) + Read Replicas</td><td>Sharded by user_id (consistent hashing). Add shards as data grows. Read replicas in each shard for read scaling during peak hours.</td></tr>
<tr><td><strong>Credit Report DB</strong></td><td>Horizontal (sharding) + Read Replicas</td><td>Same as Score DB. Documents are larger, so storage per shard is higher.</td></tr>
<tr><td><strong>Alert DB</strong></td><td>Horizontal (sharding) + TTL expiration</td><td>Sharded by user_id. TTL expiration (1 year) prevents unbounded growth. Old alerts are automatically deleted.</td></tr>
<tr><td><strong>In-Memory Cache</strong></td><td>Horizontal (cluster sharding)</td><td>Distributed cache cluster. Add nodes to increase cache capacity. Consistent hashing ensures minimal cache invalidation when nodes are added/removed.</td></tr>
<tr><td><strong>Message Queue</strong></td><td>Horizontal (add partitions + brokers)</td><td>Increase partitions to support more parallel consumers. Add broker nodes for throughput.</td></tr>
<tr><td><strong>Load Balancer</strong></td><td>Managed auto-scaling</td><td>Cloud-managed load balancers automatically scale based on traffic. No manual intervention needed.</td></tr>
</table>

<h3>Read/Write Ratio &amp; Optimization</h3>
<p>Credit Karma has an extremely high read-to-write ratio:</p>
<ul>
  <li><strong>Writes:</strong> Weekly batch ingestion (100M+ records √ó 2 bureaus = ~200M writes). This is a burst, not continuous.</li>
  <li><strong>Reads:</strong> 10M DAU √ó ~5 requests per session = ~50M reads/day, spread across 16 hours of peak.</li>
  <li><strong>Read:Write Ratio:</strong> ~175:1 (on a weekly average), making aggressive caching highly effective.</li>
</ul>

<p>Optimization: During batch ingestion (write burst), read replicas are temporarily promoted to handle read traffic while the primary handles writes. After ingestion, replicas sync and resume serving reads. This isolates write load from read performance.</p>

<hr class="section-divider">

<!-- ===================== TRADEOFFS & DEEP DIVES ===================== -->
<h2 id="tradeoffs">14. Tradeoffs &amp; Deep Dives</h2>

<h3>Tradeoff 1: Batch Ingestion vs. Real-Time Streaming</h3>
<p><strong>Chosen:</strong> Batch file ingestion with weekly updates.</p>
<p><strong>Tradeoff:</strong> Users see data that is up to 7 days old. However, credit data inherently changes slowly (most users' scores don't change week to week). Real-time streaming from bureaus would require complex streaming infrastructure, higher costs, and bureaus may not support it for 100M+ users. The weekly batch model aligns with how credit bureaus actually operate and is significantly simpler to build and maintain.</p>

<h3>Tradeoff 2: Denormalization in Recommendations Table</h3>
<p><strong>Chosen:</strong> Denormalize product details into the recommendations table.</p>
<p><strong>Tradeoff:</strong> Data duplication (~10KB per user √ó 100M users = ~1TB of duplicated product data). Risk of stale product details (e.g., if a product's APR changes mid-day, recommendations show the old APR until the next pipeline run). However, this eliminates a cross-database JOIN between NoSQL (recommendations) and SQL (products) on every read, reducing p99 latency from ~400ms to ~100ms. The staleness window is bounded (max 24 hours) and acceptable for the use case ‚Äî the "Apply" page shows live product data anyway.</p>

<h3>Tradeoff 3: Cache-Aside vs. Write-Through for Scores</h3>
<p><strong>Chosen:</strong> Cache-aside for scores (with pre-warming for active users).</p>
<p><strong>Tradeoff:</strong> Cache-aside has a cold-start problem ‚Äî the first request after cache expiration hits the DB. We mitigate this with pre-warming for active users. Write-through would eliminate cold starts entirely but would cache data for 100M users, including the ~90M who are inactive ‚Äî wasting cache memory. Cache-aside + pre-warming for 10M active users is a better use of cache resources.</p>

<h3>Tradeoff 4: Microservices vs. Monolith</h3>
<p><strong>Chosen:</strong> Microservices architecture.</p>
<p><strong>Tradeoff:</strong> Increased operational complexity (service discovery, distributed tracing, network latency between services, deployment coordination). However, the benefits outweigh the costs for Credit Karma's scale: independent scaling (Credit Score Service needs more instances than Click Tracking Service), independent deployment (Recommendation team can deploy without affecting Alert team), technology flexibility (time-series DB for scores, document DB for reports), and fault isolation (if the Recommendation Service crashes, users can still see their scores).</p>

<h3>Tradeoff 5: Time-Series NoSQL vs. SQL for Credit Scores</h3>
<p><strong>Chosen:</strong> Time-series NoSQL.</p>
<p><strong>Tradeoff:</strong> Lose ACID transactions and complex query capability. However, credit scores are append-only (never updated), always queried by user_id + time range, and need to handle 200M+ writes in a batch window. Time-series NoSQL is purpose-built for this pattern. SQL would require manual partitioning by time, struggle with the write burst, and the query patterns don't need JOINs.</p>

<h3>Tradeoff 6: Approval Odds Accuracy vs. Latency</h3>
<p><strong>Chosen:</strong> Pre-computed approval odds (batch ML pipeline).</p>
<p><strong>Tradeoff:</strong> Pre-computed odds may be slightly stale (up to 24 hours old). A real-time ML inference approach would give the most current odds but would add 200-500ms latency per recommendation and require a GPU inference cluster for serving. Pre-computation allows sub-100ms reads from cache at the cost of slight staleness ‚Äî acceptable because credit profiles change weekly.</p>

<h3>Deep Dive: Identity Verification (KBA)</h3>
<p>Knowledge-Based Authentication is the most sensitive part of the signup flow. The user provides PII, we query the bureau for security questions, the user answers, and the bureau verifies. Key security considerations:</p>
<ul>
  <li>All KBA communication is over HTTPS with mutual TLS.</li>
  <li>SSN is never stored in plaintext ‚Äî only a salted SHA-256 hash for deduplication.</li>
  <li>Failed KBA attempts are rate-limited (3 attempts per SSN per 24 hours) to prevent brute-force attacks.</li>
  <li>KBA questions/answers are not stored ‚Äî only the pass/fail result and a verification ID from the bureau.</li>
  <li>The soft-pull performed after verification does NOT affect the user's credit score (this is a key selling point of Credit Karma).</li>
</ul>

<h3>Deep Dive: Data Encryption</h3>
<ul>
  <li><strong>At Rest:</strong> All databases and object storage use AES-256 encryption with keys managed by a centralized Key Management Service (KMS). Keys are rotated every 90 days.</li>
  <li><strong>In Transit:</strong> TLS 1.3 for all external communication (client ‚Üî LB, services ‚Üî bureaus). Internal service-to-service communication uses mTLS in a zero-trust network model.</li>
  <li><strong>Application-Level Encryption:</strong> Sensitive fields (SSN, address, phone) are encrypted at the application level before being written to the DB, using envelope encryption. This provides defense-in-depth ‚Äî even if the DB is compromised, the attacker needs both the DB and the KMS to decrypt the data.</li>
</ul>

<hr class="section-divider">

<!-- ===================== ALTERNATIVE APPROACHES ===================== -->
<h2 id="alternatives">15. Alternative Approaches</h2>

<h3>Alternative 1: Monolithic Architecture</h3>
<p><strong>Description:</strong> Build the entire system as a single monolithic application with a single database.</p>
<p><strong>Why Not Chosen:</strong> At Credit Karma's scale (100M+ users, 10M+ DAU), a monolith cannot scale individual components independently. The batch ingestion pipeline has very different scaling needs (bursty write-heavy) than the score display service (steady read-heavy). A monolith would also create a single point of failure ‚Äî a bug in the recommendation engine could take down the entire platform, including credit score display. Team velocity would also suffer ‚Äî multiple teams working on different features would create merge conflicts and deployment bottlenecks in a single codebase.</p>

<h3>Alternative 2: Real-Time Bureau API Pulls (No Batch)</h3>
<p><strong>Description:</strong> Instead of batch files, pull credit data from bureaus in real-time when a user requests their score.</p>
<p><strong>Why Not Chosen:</strong> At 10M+ DAU, this would generate 10M+ API calls per day to each bureau. Bureau APIs are rate-limited and charge per-pull. This would be prohibitively expensive (~$0.50-$2.00 per pull √ó 10M pulls/day = $5M-$20M/day). Additionally, bureau API latency is 1-3 seconds, which would make the user experience unacceptably slow. The batch model costs a fraction (flat fee per batch) and allows us to serve scores from cache in &lt;100ms.</p>

<h3>Alternative 3: Event Sourcing for All Data</h3>
<p><strong>Description:</strong> Store all credit data changes as an immutable event log and derive the current state from the event stream.</p>
<p><strong>Why Not Chosen:</strong> While event sourcing has benefits (full audit trail, ability to replay events), it adds significant complexity. Credit data doesn't need the full power of event sourcing ‚Äî we already have an immutable time-series of scores and weekly report snapshots. The append-only time-series DB achieves similar benefits without the complexity of event replay, projections, and snapshot management. Event sourcing would be over-engineering for this use case.</p>

<h3>Alternative 4: GraphQL Instead of REST</h3>
<p><strong>Description:</strong> Use GraphQL for the client API instead of REST.</p>
<p><strong>Why Not Chosen:</strong> The Credit Karma API has well-defined, stable resources (scores, reports, alerts, recommendations) with predictable data needs per screen. REST's fixed endpoints map cleanly to these resources. GraphQL's flexibility (client chooses which fields to fetch) is more beneficial for rapidly evolving APIs with many different clients that need different data shapes ‚Äî which isn't the primary concern here. Additionally, REST is simpler to cache (HTTP caching at the CDN/LB level), which is critical for our read-heavy workload. GraphQL would require application-level caching. That said, GraphQL could be a reasonable choice if the mobile and web clients had significantly different data needs per screen.</p>

<h3>Alternative 5: SQL for All Databases</h3>
<p><strong>Description:</strong> Use a single SQL database for everything (scores, reports, alerts, recommendations).</p>
<p><strong>Why Not Chosen:</strong> Credit reports are deeply nested hierarchical documents ‚Äî normalizing them into SQL tables would require 6+ tables and complex JOINs to reconstruct a single report. Credit scores are time-series data with append-only writes ‚Äî SQL would need manual time-based partitioning. The write burst during batch ingestion (200M+ writes in hours) would overwhelm a traditional SQL database. Using purpose-built databases (time-series for scores, document DB for reports, wide-column for alerts) leverages each database's strengths for its specific access pattern.</p>

<h3>Alternative 6: WebSockets for Real-Time Alerts</h3>
<p><strong>Description:</strong> Maintain persistent WebSocket connections to deliver alerts instantly when they're generated.</p>
<p><strong>Why Not Chosen:</strong> Maintaining WebSocket connections for 10M+ concurrent users requires a massive WebSocket server fleet and connection state management. The connection-to-alert ratio would be extremely poor ‚Äî each user might receive 1-2 alerts per week, meaning connections sit idle 99.99% of the time. Push notifications achieve the same result (immediate delivery) without the infrastructure overhead. WebSockets would make sense for a real-time feature like live score updates, but credit scores update weekly ‚Äî not real-time.</p>

<hr class="section-divider">

<!-- ===================== ADDITIONAL CONSIDERATIONS ===================== -->
<h2 id="additional">16. Additional Considerations</h2>

<h3>Security &amp; Compliance</h3>
<ul>
  <li><strong>FCRA Compliance:</strong> The Fair Credit Reporting Act requires that credit data be handled with strict privacy controls. Users must be able to dispute inaccuracies. The system must maintain audit logs of all data access.</li>
  <li><strong>SOC 2 Type II:</strong> Annual audits verifying security controls for data handling. All services must log access, implement least-privilege IAM, and encrypt sensitive data.</li>
  <li><strong>CCPA/GDPR:</strong> Users can request data export or deletion. The system must support a "right to be forgotten" workflow that deletes all user data across all databases, caches, message queues, and backups within 30 days.</li>
  <li><strong>Penetration Testing:</strong> Regular penetration testing of all public-facing endpoints. Bug bounty program for external security researchers.</li>
</ul>

<h3>Disaster Recovery</h3>
<ul>
  <li><strong>Multi-Region Deployment:</strong> Active-passive multi-region setup. Primary region handles all traffic. Secondary region has read replicas and can be promoted within 15 minutes (RTO). Database replication lag &lt; 1 second (RPO).</li>
  <li><strong>Backups:</strong> Daily database snapshots stored in Object Storage (different region). 30-day retention for SQL, 90-day for NoSQL (regulatory requirement).</li>
  <li><strong>Runbook:</strong> Documented runbooks for common failure scenarios: bureau feed failure, database failover, cache cluster failure, notification service outage.</li>
</ul>

<h3>Observability</h3>
<ul>
  <li><strong>Metrics:</strong> Request latency (p50, p95, p99), error rates, cache hit rates, queue depth, consumer lag, batch processing duration.</li>
  <li><strong>Logging:</strong> Structured JSON logs from all services, centralized in a log aggregation system. PII fields are masked in logs.</li>
  <li><strong>Distributed Tracing:</strong> Each request gets a trace ID that propagates across all services. Enables end-to-end latency analysis (e.g., "this request was slow because the report cache missed and the document DB took 600ms").</li>
  <li><strong>Alerting:</strong> Operational alerts for: p99 latency exceeding SLA, error rate &gt; 1%, cache hit rate &lt; 80%, queue consumer lag &gt; 10 minutes, batch processing failure.</li>
</ul>

<h3>Credit Score Simulator</h3>
<p>The simulator allows users to model "what-if" scenarios. It uses a lightweight ML model (separate from the recommendation model) trained on historical score change data. The user inputs an action (e.g., "pay off $5,000 credit card debt"), the model predicts the likely score impact (e.g., "+20 to +35 points"), and the result is displayed. This is a stateless computation ‚Äî no data is persisted. The model runs on the Credit Score Service and is served via:</p>
<table>
<tr><th>Method</th><th>Endpoint</th><th>Input</th><th>Output</th></tr>
<tr><td>POST</td><td>/api/v1/score-simulator</td><td>user_id (JWT), action_type ("pay_debt", "open_account", "close_account"), parameters (amount, account_type)</td><td><code>{ predicted_score_change: { min: 20, max: 35 }, current_score: 755, projected_score: { min: 775, max: 790 } }</code></td></tr>
</table>

<h3>A/B Testing for Recommendations</h3>
<p>The recommendation algorithm directly impacts revenue. A/B testing is critical:</p>
<ul>
  <li>Users are randomly assigned to experiment groups during recommendation generation.</li>
  <li>Different ranking algorithms or model versions are tested per group.</li>
  <li>Metrics tracked: click-through rate (CTR), application rate, approval rate, revenue per user.</li>
  <li>Experiments run for 2-4 weeks to achieve statistical significance given weekly user behavior patterns.</li>
</ul>

<h3>Rate Limiting Strategy</h3>
<ul>
  <li><strong>Global:</strong> 100K requests/second at the load balancer (DDoS protection).</li>
  <li><strong>Per-User:</strong> 60 requests/minute at the API Gateway (abuse prevention).</li>
  <li><strong>Per-Endpoint:</strong> Sensitive endpoints (e.g., identity verification) have stricter limits: 5 requests/hour per IP.</li>
  <li><strong>Implementation:</strong> Token bucket algorithm with the bucket state stored in the in-memory cache (fast, distributed). Each request decrements the bucket; if empty, return <code>429 Too Many Requests</code>.</li>
</ul>

<hr class="section-divider">

<!-- ===================== VENDOR RECOMMENDATIONS ===================== -->
<h2 id="vendors">17. Vendor Recommendations</h2>
<p><em>The system design above is vendor-agnostic. Below are potential vendor choices with rationale, should a specific implementation be needed.</em></p>

<table>
<tr><th>Component</th><th>Potential Vendors</th><th>Rationale</th></tr>
<tr><td><strong>SQL Database</strong></td><td>PostgreSQL, Amazon Aurora, Google Cloud Spanner</td><td>PostgreSQL is the industry standard for relational data with excellent JSON support (useful for the <code>features</code> column). Aurora provides managed PostgreSQL with auto-scaling read replicas. Cloud Spanner offers global distribution with strong consistency, useful if multi-region active-active is needed.</td></tr>
<tr><td><strong>Time-Series NoSQL (Scores)</strong></td><td>Apache Cassandra, Amazon DynamoDB, ScyllaDB</td><td>Cassandra excels at time-series append-only workloads with high write throughput. Its partition key + clustering key model maps perfectly to user_id + bureau_score_date. DynamoDB offers fully managed experience with auto-scaling. ScyllaDB is a Cassandra-compatible option with better per-node performance.</td></tr>
<tr><td><strong>Document NoSQL (Reports)</strong></td><td>MongoDB, Amazon DocumentDB, Couchbase</td><td>MongoDB's document model natively stores nested JSON structures like credit reports. Built-in sharding and replication. DocumentDB provides managed MongoDB-compatible service. Couchbase offers built-in caching + document storage in one system.</td></tr>
<tr><td><strong>In-Memory Cache</strong></td><td>Redis, Memcached, Amazon ElastiCache</td><td>Redis offers rich data structures (hashes for user scores, sorted sets for recommendations) and TTL-based expiration. Cluster mode supports horizontal scaling. Memcached is simpler and faster for pure key-value caching but lacks Redis's data structure flexibility. ElastiCache provides managed Redis/Memcached.</td></tr>
<tr><td><strong>Message Queue</strong></td><td>Apache Kafka, Amazon Kinesis, Apache Pulsar</td><td>Kafka's partitioned, replicated log model with consumer groups is ideal for our pub/sub-like change event distribution. High throughput (millions of events/sec), configurable retention (7-day replay), and strong ordering guarantees per partition. Kinesis is Kafka-like but fully managed. Pulsar offers multi-tenancy and tiered storage.</td></tr>
<tr><td><strong>Object Storage</strong></td><td>Amazon S3, Google Cloud Storage, Azure Blob Storage</td><td>S3 is the industry standard for storing large batch files. Offers encryption at rest, lifecycle policies (archive to Glacier after 90 days), and event notifications (trigger processing on file upload). Cross-region replication for disaster recovery.</td></tr>
<tr><td><strong>CDN</strong></td><td>Cloudflare, Amazon CloudFront, Fastly</td><td>Cloudflare offers global edge network with DDoS protection built-in. CloudFront integrates tightly with S3 for origin. Fastly offers real-time purging and edge compute capabilities (useful for A/B testing at the edge).</td></tr>
<tr><td><strong>Push Notifications</strong></td><td>Apple APNs (iOS), Firebase Cloud Messaging (Android), OneSignal</td><td>APNs and FCM are required for iOS and Android push delivery respectively. OneSignal provides a unified SDK for both platforms with analytics and segmentation.</td></tr>
<tr><td><strong>Email Delivery</strong></td><td>SendGrid, Amazon SES, Mailgun</td><td>SendGrid has high deliverability rates and rich template management. SES is cost-effective at scale. All provide DKIM/SPF authentication to avoid spam filters.</td></tr>
<tr><td><strong>Container Orchestration</strong></td><td>Kubernetes, Amazon ECS, Google GKE</td><td>Kubernetes provides auto-scaling, rolling deployments, and service mesh integration (Istio/Envoy for internal load balancing). GKE and ECS offer managed Kubernetes with less operational overhead.</td></tr>
<tr><td><strong>ML Pipeline</strong></td><td>Apache Spark, Amazon SageMaker, Google Vertex AI</td><td>Spark for distributed feature engineering over 100M+ user profiles. SageMaker/Vertex AI for model training, versioning, and batch inference jobs. MLflow for experiment tracking.</td></tr>
</table>

<hr class="section-divider">

<p style="text-align: center; color: #636e72; margin-top: 40px; font-size: 0.9em;">
  <em>Credit Karma System Design Document ‚Äî Generated February 2026</em>
</p>

</body>
</html>
