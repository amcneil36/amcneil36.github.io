<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>System Design: Tinder</title>
  <script src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script>
  <style>
    :root {
      --bg: #0f1117;
      --surface: #1a1d27;
      --border: #2a2d3a;
      --accent: #fd5068;
      --accent2: #ff7854;
      --text: #e2e4ea;
      --text-muted: #9196a1;
      --code-bg: #252833;
      --success: #36d399;
      --info: #5b9bf8;
      --warn: #fbbf24;
    }
    * { box-sizing: border-box; margin: 0; padding: 0; }
    body {
      font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
      background: var(--bg);
      color: var(--text);
      line-height: 1.75;
      padding: 2rem;
      max-width: 1200px;
      margin: 0 auto;
    }
    h1 {
      font-size: 2.4rem;
      background: linear-gradient(135deg, var(--accent), var(--accent2));
      -webkit-background-clip: text;
      -webkit-text-fill-color: transparent;
      margin-bottom: 0.5rem;
      border-bottom: 2px solid var(--accent);
      padding-bottom: 1rem;
    }
    h2 {
      font-size: 1.7rem;
      color: var(--accent);
      margin-top: 3rem;
      margin-bottom: 1rem;
      border-left: 4px solid var(--accent);
      padding-left: 0.75rem;
    }
    h3 {
      font-size: 1.3rem;
      color: var(--accent2);
      margin-top: 2rem;
      margin-bottom: 0.75rem;
    }
    h4 {
      font-size: 1.1rem;
      color: var(--info);
      margin-top: 1.5rem;
      margin-bottom: 0.5rem;
    }
    p, li { color: var(--text); margin-bottom: 0.6rem; }
    ul, ol { padding-left: 1.5rem; margin-bottom: 1rem; }
    li { margin-bottom: 0.4rem; }
    code {
      background: var(--code-bg);
      padding: 0.15rem 0.4rem;
      border-radius: 4px;
      font-size: 0.9em;
      color: var(--accent2);
    }
    pre {
      background: var(--code-bg);
      padding: 1rem;
      border-radius: 8px;
      overflow-x: auto;
      margin: 1rem 0;
      border: 1px solid var(--border);
    }
    pre code { background: none; padding: 0; color: var(--text); }
    table {
      width: 100%;
      border-collapse: collapse;
      margin: 1rem 0;
      background: var(--surface);
      border-radius: 8px;
      overflow: hidden;
    }
    th {
      background: var(--code-bg);
      color: var(--accent);
      padding: 0.75rem 1rem;
      text-align: left;
      font-weight: 600;
      border-bottom: 2px solid var(--accent);
    }
    td {
      padding: 0.6rem 1rem;
      border-bottom: 1px solid var(--border);
      color: var(--text);
    }
    tr:hover td { background: rgba(253, 80, 104, 0.05); }
    .diagram-container {
      background: var(--surface);
      border-radius: 12px;
      padding: 1.5rem;
      margin: 1.5rem 0;
      border: 1px solid var(--border);
      overflow-x: auto;
    }
    .example-box {
      background: rgba(91, 155, 248, 0.08);
      border-left: 4px solid var(--info);
      padding: 1rem 1.25rem;
      border-radius: 0 8px 8px 0;
      margin: 1rem 0;
    }
    .example-box strong { color: var(--info); }
    .callout {
      background: rgba(253, 80, 104, 0.08);
      border-left: 4px solid var(--accent);
      padding: 1rem 1.25rem;
      border-radius: 0 8px 8px 0;
      margin: 1rem 0;
    }
    .callout strong { color: var(--accent); }
    .warn-box {
      background: rgba(251, 191, 36, 0.08);
      border-left: 4px solid var(--warn);
      padding: 1rem 1.25rem;
      border-radius: 0 8px 8px 0;
      margin: 1rem 0;
    }
    .warn-box strong { color: var(--warn); }
    .success-box {
      background: rgba(54, 211, 153, 0.08);
      border-left: 4px solid var(--success);
      padding: 1rem 1.25rem;
      border-radius: 0 8px 8px 0;
      margin: 1rem 0;
    }
    .success-box strong { color: var(--success); }
    .mermaid { text-align: center; }
    hr { border: none; border-top: 1px solid var(--border); margin: 2rem 0; }
    .badge {
      display: inline-block;
      padding: 0.15rem 0.6rem;
      border-radius: 12px;
      font-size: 0.8rem;
      font-weight: 600;
    }
    .badge-sql { background: rgba(91,155,248,0.2); color: var(--info); }
    .badge-nosql { background: rgba(253,80,104,0.2); color: var(--accent); }
    .badge-cache { background: rgba(54,211,153,0.2); color: var(--success); }
  </style>
</head>
<body>

<h1>üî• System Design: Tinder</h1>
<p style="color: var(--text-muted); font-size: 0.95rem; margin-bottom: 2rem;">
  A location-based dating and social discovery platform that enables users to browse profiles, express interest through swiping, match with mutual interests, and communicate through real-time messaging.
</p>

<!-- ============================================================ -->
<h2>1. Functional Requirements</h2>
<!-- ============================================================ -->
<ol>
  <li><strong>User Profile Management</strong> ‚Äî Users can create, edit, and delete their profile including bio, age, gender, preferences, and photos.</li>
  <li><strong>Photo Upload &amp; Management</strong> ‚Äî Users can upload, reorder, and delete photos. Photos are served globally with low latency.</li>
  <li><strong>Location Tracking</strong> ‚Äî The app continuously or periodically updates the user's geographic location for proximity-based discovery.</li>
  <li><strong>Discovery / Recommendation Feed</strong> ‚Äî Users see a queue of nearby profiles filtered by their preferences (age range, distance, gender preference). Already-swiped profiles are excluded.</li>
  <li><strong>Swiping (Like / Dislike / Super Like)</strong> ‚Äî Users can swipe right (like), left (dislike), or up (super like) on profiles in the feed.</li>
  <li><strong>Match Detection</strong> ‚Äî When two users both swipe right on each other, a mutual match is created and both users are notified.</li>
  <li><strong>Real-time Messaging</strong> ‚Äî Matched users can exchange text messages in real time.</li>
  <li><strong>Notifications</strong> ‚Äî Users receive push notifications for new matches and new messages (when app is backgrounded).</li>
  <li><strong>Unmatch / Block / Report</strong> ‚Äî Users can unmatch, block, or report other users.</li>
</ol>

<!-- ============================================================ -->
<h2>2. Non-Functional Requirements</h2>
<!-- ============================================================ -->
<ol>
  <li><strong>Low Latency</strong> ‚Äî Swiping must feel instant (&lt;100 ms perceived). Discovery feed should load within 200‚Äì300 ms. Messages should be delivered in &lt;200 ms end-to-end.</li>
  <li><strong>High Availability</strong> ‚Äî Target 99.9%+ uptime. Users expect the app to work at any time of day.</li>
  <li><strong>Scalability</strong> ‚Äî Must support ~75M+ monthly active users globally, with peaks during evening hours. Swipe volume can reach billions per day.</li>
  <li><strong>Consistency Model</strong> ‚Äî Eventual consistency is acceptable for discovery feeds and notification delivery. Strong consistency is required for match detection (no duplicate or missed matches).</li>
  <li><strong>Data Privacy &amp; Security</strong> ‚Äî Location data is sensitive. End-to-end encryption for messages. GDPR/CCPA compliance.</li>
  <li><strong>Global Reach</strong> ‚Äî Serve users across all continents with low latency via CDN and regional deployments.</li>
  <li><strong>Fault Tolerance</strong> ‚Äî No single point of failure. Graceful degradation under load (e.g., slower recommendations before complete outage).</li>
</ol>

<!-- ============================================================ -->
<h2>3. Back-of-the-Envelope Estimation</h2>
<!-- ============================================================ -->
<ul>
  <li><strong>DAU:</strong> ~25 million</li>
  <li><strong>Swipes per user per day:</strong> ~100</li>
  <li><strong>Total swipes per day:</strong> 25M √ó 100 = <strong>2.5 billion swipes/day</strong> ‚âà ~29,000 swipes/sec average, ~100K/sec peak</li>
  <li><strong>Matches per day:</strong> ~25 million (roughly 1% of swipes result in a match)</li>
  <li><strong>Messages per day:</strong> ~500 million</li>
  <li><strong>Photos per user:</strong> ~5 photos, ~2 MB each = 10 MB/user</li>
  <li><strong>Total photo storage:</strong> 75M users √ó 10 MB = ~750 TB</li>
  <li><strong>Location updates:</strong> Every 5 minutes while app is active = ~25M √ó 12/hr √ó 4hr/day = ~1.2 billion location writes/day</li>
</ul>

<!-- ============================================================ -->
<h2>4. Flows &amp; Diagrams</h2>
<!-- ============================================================ -->

<!-- ===================== FLOW 1 ===================== -->
<h3>Flow 1: User Profile Creation &amp; Photo Upload</h3>

<div class="diagram-container">
  <div class="mermaid">
graph LR
    Client["üì± Client<br/>(iOS/Android)"]
    LB1["‚öñÔ∏è Load<br/>Balancer"]
    US["üßë User<br/>Service"]
    MS["üì∏ Media<br/>Service"]
    UserDB[("üë§ User DB<br/>(SQL)")]
    ObjStore[("üóÑÔ∏è Object<br/>Storage")]
    CDN["üåê CDN"]
    Cache["‚ö° Cache"]

    Client -->|"1. POST /users<br/>(profile data)"| LB1
    LB1 --> US
    US -->|"2. Write profile"| UserDB
    US -->|"3. Populate cache"| Cache
    US -->|"4. Return user_id"| Client

    Client -->|"5. POST /users/{id}/photos<br/>(multipart upload)"| LB1
    LB1 --> MS
    MS -->|"6. Store photo"| ObjStore
    MS -->|"7. Return CDN URL"| Client
    ObjStore -.->|"Origin pull"| CDN
  </div>
</div>

<div class="example-box">
  <strong>Example 1 ‚Äî New User Sign-Up:</strong><br/>
  Alice downloads Tinder and creates an account. She fills in her name, age (25), gender (female), bio ("Love hiking and coffee ‚òï"), and sets preferences: interested in men, ages 24‚Äì32, within 15 miles. The client sends an <code>HTTP POST /api/v1/users</code> with this data to the Load Balancer, which forwards it to the User Service. The User Service validates the input, hashes any sensitive data, writes the profile to the SQL User DB, populates the profile cache, and returns a <code>201 Created</code> with Alice's <code>user_id</code>.
</div>

<div class="example-box">
  <strong>Example 2 ‚Äî Photo Upload:</strong><br/>
  Alice then uploads 4 photos. Each photo is sent as a multipart <code>HTTP POST /api/v1/users/{user_id}/photos</code> to the Load Balancer ‚Üí Media Service. The Media Service generates a unique key, compresses/resizes the image into multiple resolutions (thumbnail, medium, full), uploads all versions to Object Storage, records the photo metadata (URL, order) in the User DB, and returns the CDN URL for each resolution. When other users later view Alice's profile, their clients fetch photos directly from the CDN, which either serves from edge cache or pulls from the Object Storage origin.
</div>

<div class="example-box">
  <strong>Example 3 ‚Äî Profile Edit:</strong><br/>
  Alice later changes her bio. The client sends an <code>HTTP PATCH /api/v1/users/{user_id}</code> with <code>{"bio": "Hiking enthusiast üèîÔ∏è"}</code>. The User Service updates the SQL record and invalidates/updates the profile in the cache.
</div>

<h4>Deep Dive: Flow 1 Components</h4>

<table>
  <tr><th>Component</th><th>Description</th></tr>
  <tr>
    <td><strong>Client (iOS/Android)</strong></td>
    <td>Native mobile app. Handles photo selection, compression before upload, and displays CDN-served images. Communicates over HTTPS/TLS.</td>
  </tr>
  <tr>
    <td><strong>Load Balancer</strong></td>
    <td>Layer 7 (Application) load balancer. Distributes incoming HTTP requests across User Service and Media Service instances using round-robin or least-connections. Terminates TLS. Performs health checks.</td>
  </tr>
  <tr>
    <td><strong>User Service</strong></td>
    <td>
      Stateless microservice responsible for all user CRUD operations.<br/>
      <strong>Protocol:</strong> HTTP/REST over TLS.<br/>
      <strong>Endpoints:</strong>
      <ul>
        <li><code>POST /api/v1/users</code> ‚Äî Create profile. Input: <code>{name, email, age, gender, bio, preferences}</code>. Output: <code>{user_id, created_at}</code>.</li>
        <li><code>GET /api/v1/users/{id}</code> ‚Äî Fetch profile. Output: <code>{user_id, name, age, bio, photos[], ...}</code>.</li>
        <li><code>PATCH /api/v1/users/{id}</code> ‚Äî Update profile fields. Input: partial user object. Output: <code>{updated fields}</code>.</li>
        <li><code>DELETE /api/v1/users/{id}</code> ‚Äî Delete account.</li>
      </ul>
    </td>
  </tr>
  <tr>
    <td><strong>Media Service</strong></td>
    <td>
      Handles photo upload, processing (resize, compress, generate thumbnails), and storage.<br/>
      <strong>Protocol:</strong> HTTP/REST, accepts <code>multipart/form-data</code>.<br/>
      <strong>Endpoints:</strong>
      <ul>
        <li><code>POST /api/v1/users/{id}/photos</code> ‚Äî Upload photo. Input: image binary + order. Output: <code>{photo_id, cdn_urls: {thumbnail, medium, full}}</code>.</li>
        <li><code>DELETE /api/v1/users/{id}/photos/{photo_id}</code> ‚Äî Delete a photo.</li>
        <li><code>PUT /api/v1/users/{id}/photos/reorder</code> ‚Äî Reorder photos. Input: <code>{photo_ids: [ordered list]}</code>.</li>
      </ul>
    </td>
  </tr>
  <tr>
    <td><strong>User DB (SQL)</strong></td>
    <td>Relational database storing structured user profile data, preferences, and photo metadata. SQL chosen for strong consistency, relational integrity (foreign keys between users ‚Üî preferences ‚Üî photos), and transactional guarantees on profile updates.</td>
  </tr>
  <tr>
    <td><strong>Object Storage</strong></td>
    <td>Distributed blob storage for actual photo binary files. Stores multiple resolutions per photo. High durability (11 nines). Serves as CDN origin.</td>
  </tr>
  <tr>
    <td><strong>CDN</strong></td>
    <td>Content Delivery Network with global edge presence. Caches photo files at edge locations close to users. Dramatically reduces latency for photo loading and offloads traffic from Object Storage. Cache TTL of 24 hours; invalidation on photo delete/update.</td>
  </tr>
  <tr>
    <td><strong>Cache</strong></td>
    <td>In-memory key-value cache storing frequently accessed user profiles. Reduces read load on SQL. Write-through strategy on profile create/update. LRU eviction. TTL of 1 hour. Key format: <code>user:{user_id}</code>.</td>
  </tr>
</table>

<hr/>

<!-- ===================== FLOW 2 ===================== -->
<h3>Flow 2: Location Update &amp; Discovery / Recommendation Feed</h3>

<div class="diagram-container">
  <div class="mermaid">
graph LR
    Client["üì± Client"]
    LB["‚öñÔ∏è Load<br/>Balancer"]
    LS["üìç Location<br/>Service"]
    DS["üîç Discovery<br/>Service"]
    LocDB[("üìç Location DB<br/>(NoSQL)")]
    UserDB[("üë§ User DB<br/>(SQL)")]
    SwipeDB[("üëÜ Swipe DB<br/>(NoSQL)")]
    RecCache["‚ö° Recommendation<br/>Cache"]
    ProfileCache["‚ö° Profile<br/>Cache"]

    Client -->|"1. PUT /location<br/>(lat, lng)"| LB
    LB --> LS
    LS -->|"2. Compute geohash<br/>& write"| LocDB

    Client -->|"3. GET /discovery/feed"| LB
    LB --> DS
    DS -->|"4. Check cache"| RecCache
    DS -->|"5. Query nearby<br/>(geohash prefix)"| LocDB
    DS -->|"6. Fetch preferences<br/>& filter"| UserDB
    DS -->|"7. Exclude<br/>already-swiped"| SwipeDB
    DS -->|"8. Fetch profiles"| ProfileCache
    DS -->|"9. Ranked<br/>profile list"| Client
  </div>
</div>

<div class="example-box">
  <strong>Example 1 ‚Äî Location Update:</strong><br/>
  Bob opens Tinder while in downtown San Francisco. His phone's GPS provides coordinates (37.7749, -122.4194). The client sends <code>HTTP PUT /api/v1/location</code> with <code>{lat: 37.7749, lng: -122.4194}</code> to the Load Balancer ‚Üí Location Service. The Location Service computes a geohash (<code>"9q8yyk"</code>) from the coordinates and writes/upserts <code>{user_id, lat, lng, geohash, updated_at}</code> to the Location DB (NoSQL). This update happens every ~5 minutes while the app is foregrounded.
</div>

<div class="example-box">
  <strong>Example 2 ‚Äî Discovery Feed (Cache Miss):</strong><br/>
  Bob taps the discovery tab. The client sends <code>HTTP GET /api/v1/discovery/feed?limit=50</code>. The Discovery Service first checks the Recommendation Cache for a pre-built feed for Bob ‚Äî cache miss. It then: (1) queries the Location DB for all user IDs within Bob's geohash and neighboring geohashes (covering his 15-mile radius), (2) fetches Bob's preferences from the User DB (women, ages 22‚Äì30), (3) filters the candidate set by preferences, (4) queries the Swipe DB to exclude anyone Bob has already swiped on, (5) ranks remaining candidates by a score (profile completeness, recent activity, desirability score), (6) fetches full profile data from the Profile Cache for the top 50 candidates, and (7) returns the ranked list. The result is also stored in the Recommendation Cache with a 5-minute TTL.
</div>

<div class="example-box">
  <strong>Example 3 ‚Äî Discovery Feed (Cache Hit):</strong><br/>
  Bob swipes through 10 profiles and scrolls down. The client requests the next batch. The Discovery Service finds the remaining pre-computed candidates in the Recommendation Cache and returns the next 20 profiles instantly without hitting the databases.
</div>

<div class="example-box">
  <strong>Example 4 ‚Äî Significant Location Change:</strong><br/>
  Bob flies to New York. His new GPS coordinates produce a drastically different geohash. The Location Service detects the large distance change, updates the Location DB, and invalidates Bob's Recommendation Cache entry so his next discovery feed request returns profiles near New York instead of San Francisco.
</div>

<h4>Deep Dive: Flow 2 Components</h4>

<table>
  <tr><th>Component</th><th>Description</th></tr>
  <tr>
    <td><strong>Location Service</strong></td>
    <td>
      Receives GPS coordinates from the client, converts lat/lng to a geohash string, and writes to the Location DB.<br/>
      <strong>Protocol:</strong> HTTP/REST.<br/>
      <strong>Endpoints:</strong>
      <ul>
        <li><code>PUT /api/v1/location</code> ‚Äî Update user location. Input: <code>{user_id (from auth token), lat, lng}</code>. Output: <code>200 OK</code>.</li>
      </ul>
      Also responsible for invalidating recommendation cache when a significant location change is detected (distance &gt; 5 miles from last known position).
    </td>
  </tr>
  <tr>
    <td><strong>Discovery Service</strong></td>
    <td>
      Core recommendation engine. Orchestrates the discovery pipeline: geospatial query ‚Üí preference filtering ‚Üí swipe exclusion ‚Üí ranking ‚Üí profile hydration.<br/>
      <strong>Protocol:</strong> HTTP/REST.<br/>
      <strong>Endpoints:</strong>
      <ul>
        <li><code>GET /api/v1/discovery/feed?limit=N&offset=M</code> ‚Äî Fetch discovery feed. Input: user_id (from auth), pagination params. Output: <code>{profiles: [{user_id, name, age, bio, photos[], distance_miles}, ...]}</code>.</li>
      </ul>
      <strong>Ranking factors:</strong> profile completeness score, mutual preference overlap, recent activity (last active within 24 hours ranked higher), desirability score (derived from swipe-right ratio on the user).
    </td>
  </tr>
  <tr>
    <td><strong>Location DB (NoSQL)</strong></td>
    <td>NoSQL key-value/document store optimized for geospatial queries. Stores <code>{user_id, lat, lng, geohash, last_updated}</code>. NoSQL chosen because: extremely high write throughput (1.2B writes/day), simple key-value access pattern, geohash prefix queries map well to range scans, no complex joins needed. Uses a <strong>geohash index</strong> for efficient proximity lookups ‚Äî querying a geohash prefix returns all users in that geographic cell and its neighbors.</td>
  </tr>
  <tr>
    <td><strong>Swipe DB (NoSQL)</strong></td>
    <td>Stores all swipe records. Used here for exclusion filtering ("has user X already swiped on user Y?"). NoSQL chosen due to extremely high write volume (2.5B/day) and simple lookup pattern. Detailed further in Flow 3.</td>
  </tr>
  <tr>
    <td><strong>Recommendation Cache</strong></td>
    <td>In-memory cache storing pre-computed discovery feeds per user. <strong>Strategy:</strong> Cache-aside (populate on miss, serve on hit). <strong>TTL:</strong> 5 minutes (recommendations should reflect recent activity and location). <strong>Eviction:</strong> LRU. <strong>Key:</strong> <code>rec:{user_id}</code>. Invalidated on significant location change or preference update. Short TTL is appropriate because the feed depends on dynamic, frequently-changing data (other users' locations, swipe history).</td>
  </tr>
</table>

<hr/>

<!-- ===================== FLOW 3 ===================== -->
<h3>Flow 3: Swiping &amp; Match Detection</h3>

<div class="diagram-container">
  <div class="mermaid">
graph LR
    Client["üì± Client"]
    LB["‚öñÔ∏è Load<br/>Balancer"]
    SS["üëÜ Swipe<br/>Service"]
    SwipeDB[("üëÜ Swipe DB<br/>(NoSQL)")]
    MatchDB[("üíï Match DB<br/>(NoSQL)")]
    MQ["üì¨ Message<br/>Queue"]
    NS["üîî Notification<br/>Service"]
    APNS["üì≤ Push Gateway<br/>(APNs / FCM)"]

    Client -->|"1. POST /swipe<br/>{target_id, action}"| LB
    LB --> SS
    SS -->|"2. Write swipe"| SwipeDB
    SS -->|"3. Check reverse<br/>swipe exists?"| SwipeDB
    SS -->|"4. If mutual:<br/>write match"| MatchDB
    SS -->|"5. If mutual:<br/>enqueue notification"| MQ
    MQ --> NS
    NS -->|"6. Push<br/>notification"| APNS
    APNS -->|"7. Push to<br/>both users"| Client
  </div>
</div>

<div class="example-box">
  <strong>Example 1 ‚Äî Right Swipe, No Match Yet:</strong><br/>
  Alice sees Bob's profile and swipes right (like). The client sends <code>HTTP POST /api/v1/swipe</code> with <code>{target_user_id: "bob_123", action: "like"}</code>. The Swipe Service writes <code>{swiper: "alice_456", swiped: "bob_123", action: "like", timestamp}</code> to the Swipe DB. It then checks: "Has bob_123 already swiped right on alice_456?" ‚Äî query returns no result. The Swipe Service returns <code>200 OK {matched: false}</code>. No match is created.
</div>

<div class="example-box">
  <strong>Example 2 ‚Äî Right Swipe, Mutual Match:</strong><br/>
  Later, Bob sees Alice in his discovery feed and swipes right. The client sends <code>HTTP POST /api/v1/swipe</code> with <code>{target_user_id: "alice_456", action: "like"}</code>. The Swipe Service writes Bob's swipe to the Swipe DB. It then checks: "Has alice_456 already swiped right on bob_123?" ‚Äî query returns Alice's earlier right swipe. <strong>It's a match!</strong> The Swipe Service: (1) creates a match record <code>{match_id, user1: "alice_456", user2: "bob_123", matched_at}</code> in the Match DB, (2) enqueues a <code>match_notification</code> message onto the Message Queue with both user IDs, and (3) returns <code>200 OK {matched: true, match_id}</code> to Bob's client (which shows the match animation). The Notification Service consumes the queue message and sends push notifications via APNs (iOS) and FCM (Android) to both Alice and Bob: "You have a new match! üéâ".
</div>

<div class="example-box">
  <strong>Example 3 ‚Äî Left Swipe (Dislike):</strong><br/>
  Alice sees Charlie's profile and swipes left. The client sends <code>HTTP POST /api/v1/swipe</code> with <code>{target_user_id: "charlie_789", action: "dislike"}</code>. The Swipe Service records the dislike in the Swipe DB. No reverse-swipe check is needed for dislikes. Returns <code>200 OK {matched: false}</code>. Charlie will be excluded from Alice's future discovery feeds.
</div>

<div class="example-box">
  <strong>Example 4 ‚Äî Super Like:</strong><br/>
  Alice super-likes David. The flow is identical to a right swipe, but the swipe action is <code>"super_like"</code>. If David hasn't swiped Alice yet, when David later sees Alice in his feed, Alice's profile is tagged with a special "Super Liked you" indicator, increasing the chance David swipes right.
</div>

<h4>Deep Dive: Flow 3 Components</h4>

<table>
  <tr><th>Component</th><th>Description</th></tr>
  <tr>
    <td><strong>Swipe Service</strong></td>
    <td>
      Handles all swipe actions and performs synchronous match detection.<br/>
      <strong>Protocol:</strong> HTTP/REST.<br/>
      <strong>Endpoints:</strong>
      <ul>
        <li><code>POST /api/v1/swipe</code> ‚Äî Record a swipe. Input: <code>{target_user_id, action: "like"|"dislike"|"super_like"}</code>. Output: <code>{matched: bool, match_id?: string}</code>.</li>
      </ul>
      <strong>Match detection logic:</strong> On a "like" or "super_like", query Swipe DB for <code>{swiper: target_user_id, swiped: current_user_id, action: "like"|"super_like"}</code>. If found ‚Üí create match and enqueue notification. This check must be <strong>atomic</strong> ‚Äî the write-then-check is done within a single operation or uses optimistic concurrency to prevent race conditions where both users swipe simultaneously.
    </td>
  </tr>
  <tr>
    <td><strong>Swipe DB (NoSQL)</strong></td>
    <td>
      High-throughput NoSQL store for swipe records.<br/>
      <strong>Data model:</strong> <code>{swipe_id (PK), swiper_id, swiped_id, action, created_at}</code>.<br/>
      <strong>Indexes:</strong> Composite index on <code>(swiped_id, swiper_id)</code> for reverse-swipe lookups. Also indexed on <code>(swiper_id)</code> for "get all swipes by user" (used in discovery exclusion).<br/>
      NoSQL chosen for: extreme write throughput (2.5B swipes/day = ~29K writes/sec avg), simple access patterns, no joins, append-heavy workload.
    </td>
  </tr>
  <tr>
    <td><strong>Match DB (NoSQL)</strong></td>
    <td>
      Stores confirmed mutual matches.<br/>
      <strong>Data model:</strong> <code>{match_id (PK), user1_id, user2_id, matched_at, is_active}</code>.<br/>
      <strong>Indexes:</strong> Secondary index on <code>user1_id</code> and <code>user2_id</code> for "get all matches for user X".<br/>
      NoSQL chosen for: high write throughput (~25M new matches/day), simple query patterns, no complex joins.
    </td>
  </tr>
  <tr>
    <td><strong>Message Queue</strong></td>
    <td>
      Durable, ordered message queue for asynchronous notification delivery. When a match is detected, the Swipe Service <strong>enqueues</strong> a message: <code>{type: "match", user1_id, user2_id, match_id, timestamp}</code>. The Notification Service <strong>dequeues/consumes</strong> messages, processes them (looks up device tokens, formats notification payload), and sends push notifications. Messages are acknowledged after successful delivery; failed messages are retried with exponential backoff and sent to a dead-letter queue after max retries.<br/><br/>
      <strong>Why a Message Queue instead of synchronous notification?</strong> (1) Decouples match detection from notification delivery ‚Äî Swipe Service responds to the user immediately without waiting for push delivery. (2) Absorbs spikes ‚Äî notification sending can be rate-limited without affecting swipe throughput. (3) Reliability ‚Äî if the Notification Service is temporarily down, messages are retained and processed when it recovers. (4) Dead-letter queue handles poison messages.
    </td>
  </tr>
  <tr>
    <td><strong>Notification Service</strong></td>
    <td>
      Consumes messages from the Message Queue and dispatches push notifications.<br/>
      <strong>Protocol:</strong> Consumes from Message Queue (pull-based). Sends to APNs (Apple Push Notification service) for iOS devices and FCM (Firebase Cloud Messaging) for Android devices over HTTPS.<br/>
      Looks up the target user's device token from a Device Token store (or User DB), formats the notification payload (title, body, deep link), and delivers it. Handles token invalidation (uninstalled apps).
    </td>
  </tr>
</table>

<div class="callout">
  <strong>Why Message Queue over Pub/Sub for Notifications?</strong><br/>
  A message queue guarantees exactly-once (or at-least-once) processing per message via acknowledgment and dead-letter mechanisms. Each match notification must be processed exactly once. Pub/Sub would be more appropriate if multiple independent consumers needed to react to the same event (fan-out). Here, only the Notification Service consumes match events, making a point-to-point queue the better fit. If we later add analytics or recommendation score updates triggered by matches, we could switch to a pub/sub topic with multiple subscriber queues.
</div>

<hr/>

<!-- ===================== FLOW 4 ===================== -->
<h3>Flow 4: Real-Time Messaging</h3>

<div class="diagram-container">
  <div class="mermaid">
graph LR
    ClientA["üì± Alice's<br/>Client"]
    ClientB["üì± Bob's<br/>Client"]
    WSG["üîå WebSocket<br/>Gateway"]
    MsgSvc["üí¨ Message<br/>Service"]
    MsgDB[("üí¨ Message DB<br/>(NoSQL)")]
    PubSub["üì° Pub/Sub"]
    ConnStore["‚ö° Connection<br/>Store (Cache)"]
    WSG2["üîå WebSocket<br/>Gateway<br/>(Server 2)"]
    NS["üîî Notification<br/>Service"]

    ClientA <-->|"1. WebSocket<br/>connection"| WSG
    WSG -->|"2. Register<br/>connection"| ConnStore
    ClientA -->|"3. Send message<br/>via WS"| WSG
    WSG -->|"4. Forward to<br/>Message Service"| MsgSvc
    MsgSvc -->|"5. Persist<br/>message"| MsgDB
    MsgSvc -->|"6. Publish to<br/>recipient channel"| PubSub
    PubSub -->|"7a. Route to<br/>correct gateway"| WSG2
    WSG2 -->|"8. Push to<br/>recipient via WS"| ClientB

    MsgSvc -->|"7b. If offline:<br/>push notification"| NS

    ClientB <-->|"WS connection"| WSG2
    WSG2 -->|"Register"| ConnStore
  </div>
</div>

<div class="example-box">
  <strong>Example 1 ‚Äî Both Users Online (Different Servers):</strong><br/>
  Alice and Bob matched earlier. Alice opens the chat screen. Her client establishes a <strong>WebSocket connection</strong> to a WebSocket Gateway (Server 1). The gateway registers Alice's connection in the Connection Store: <code>{user_id: "alice_456", gateway_server: "ws-server-1", connection_id: "conn_abc"}</code>. Bob is also online, connected to WebSocket Gateway Server 2.<br/><br/>
  Alice types "Hey Bob! üëã" and sends. The message travels over her existing WebSocket to Gateway Server 1, which forwards it to the Message Service. The Message Service: (1) validates Alice and Bob are matched, (2) persists the message <code>{message_id, match_id, sender: alice, content: "Hey Bob! üëã", timestamp, status: "sent"}</code> in the Message DB, (3) looks up Bob's connection in the Connection Store ‚Üí finds Bob is on ws-server-2, (4) publishes the message to the Pub/Sub channel <code>"user:bob_123"</code>. WebSocket Gateway Server 2, which subscribes to Bob's channel, receives the message and pushes it to Bob's client over his WebSocket. Bob sees the message instantly. The Message Service returns an ack to Alice's client updating the message status to "delivered".
</div>

<div class="example-box">
  <strong>Example 2 ‚Äî Recipient Offline:</strong><br/>
  Alice sends "Are you free Friday?" but Bob has closed the app. The Message Service persists the message in the Message DB. It checks the Connection Store ‚Äî Bob has no active connection. The Message Service triggers a push notification via the Notification Service: "Alice: Are you free Friday?". When Bob later opens the app and re-establishes his WebSocket connection, the client fetches unread messages via <code>HTTP GET /api/v1/matches/{match_id}/messages?since={last_seen_timestamp}</code>, retrieves all messages sent while offline, and displays them.
</div>

<div class="example-box">
  <strong>Example 3 ‚Äî Connection Recovery:</strong><br/>
  Alice's phone temporarily loses cellular signal. Her WebSocket disconnects. When signal returns, the client automatically reconnects to a gateway (possibly a different server), re-registers in the Connection Store, and fetches any messages missed during the disconnection window using the last known message timestamp.
</div>

<h4>Deep Dive: Flow 4 Components</h4>

<table>
  <tr><th>Component</th><th>Description</th></tr>
  <tr>
    <td><strong>WebSocket Gateway</strong></td>
    <td>
      Maintains persistent, full-duplex WebSocket connections with clients. Each gateway server handles thousands of concurrent connections.<br/>
      <strong>Protocol:</strong> WebSocket (WSS ‚Äî encrypted via TLS).<br/>
      <strong>Connection Establishment:</strong> Client initiates an HTTP upgrade request: <code>GET /ws?token={auth_jwt}</code> with <code>Upgrade: websocket</code> header. The gateway validates the JWT, upgrades the connection, and registers it in the Connection Store.<br/>
      <strong>Heartbeat:</strong> Ping/pong frames every 30 seconds to detect dead connections. If no pong received within 10 seconds, connection is closed and removed from Connection Store.<br/><br/>
      <strong>Why WebSocket over alternatives?</strong>
      <ul>
        <li><strong>vs. HTTP Long Polling:</strong> Long polling has higher latency (new HTTP request per message), more overhead (HTTP headers each time), and does not support server-initiated messages as efficiently. WebSocket provides true bidirectional communication with minimal overhead after the initial handshake.</li>
        <li><strong>vs. Server-Sent Events (SSE):</strong> SSE is unidirectional (server ‚Üí client only). Chat requires bidirectional communication. Using SSE for receiving + HTTP POST for sending would double the connection overhead and add latency.</li>
        <li><strong>vs. HTTP/2 Push:</strong> HTTP/2 server push is designed for pushing resources (like CSS/JS), not arbitrary data streams. Not suitable for real-time messaging.</li>
      </ul>
    </td>
  </tr>
  <tr>
    <td><strong>Connection Store (Cache)</strong></td>
    <td>
      In-memory key-value store tracking which WebSocket Gateway server each online user is connected to.<br/>
      <strong>Data model:</strong> Key: <code>ws:user:{user_id}</code>, Value: <code>{gateway_server_id, connection_id, connected_at}</code>.<br/>
      <strong>TTL:</strong> 5 minutes (auto-expire stale connections). Refreshed on each heartbeat.<br/>
      <strong>Eviction:</strong> LRU, though TTL is the primary cleanup mechanism.<br/>
      <strong>Why cache (not DB)?</strong> This data is ephemeral ‚Äî connections come and go rapidly. Storing in a persistent database would be unnecessary overhead. In-memory cache provides sub-millisecond lookups needed for real-time routing.
    </td>
  </tr>
  <tr>
    <td><strong>Message Service</strong></td>
    <td>
      Core messaging logic: validation, persistence, and routing.<br/>
      <strong>Protocol:</strong> Internal gRPC for communication from WebSocket Gateways (low-latency, efficient binary serialization). Also exposes HTTP/REST for message history retrieval.<br/>
      <strong>Endpoints:</strong>
      <ul>
        <li><code>gRPC SendMessage(match_id, sender_id, content)</code> ‚Üí Persists and routes. Output: <code>{message_id, status, timestamp}</code>.</li>
        <li><code>GET /api/v1/matches/{match_id}/messages?since=timestamp&limit=N</code> ‚Äî Fetch message history. Output: paginated list of messages.</li>
      </ul>
    </td>
  </tr>
  <tr>
    <td><strong>Message DB (NoSQL)</strong></td>
    <td>
      Stores all chat messages. NoSQL chosen for: high write throughput (500M messages/day), time-series-like append pattern, queries are scoped to a single match (partition), no joins needed.<br/>
      <strong>Data model:</strong> <code>{message_id (PK), match_id (partition key), sender_id, content, created_at (sort key), status}</code>.<br/>
      Partitioned by <code>match_id</code> ‚Äî all messages for a conversation are co-located for efficient retrieval. Sorted by <code>created_at</code> within each partition for chronological display.
    </td>
  </tr>
  <tr>
    <td><strong>Pub/Sub</strong></td>
    <td>
      Used for routing messages between multiple WebSocket Gateway servers. Each gateway subscribes to channels for all users connected to it. When the Message Service needs to deliver a message to Bob, it publishes to channel <code>"user:bob_123"</code>. The gateway handling Bob's connection receives the event and pushes it over the WebSocket.<br/><br/>
      <strong>Why Pub/Sub for inter-gateway routing?</strong> With N gateway servers, the Message Service doesn't send directly to a specific gateway ‚Äî that would create tight coupling and require service discovery. Instead, Pub/Sub provides a decoupled fan-out mechanism. If Bob is online, the gateway subscribed to his channel receives the message. If Bob is offline (no subscriber), the message is simply not delivered via WebSocket (it's already persisted in the Message DB and a push notification is sent separately).<br/><br/>
      <strong>Topic/channel model:</strong> One channel per user (<code>user:{user_id}</code>). A gateway subscribes to channels for all users it serves. When a user disconnects, the gateway unsubscribes from their channel.
    </td>
  </tr>
</table>

<hr/>

<!-- ===================== FLOW 5 ===================== -->
<h3>Flow 5: Unmatch / Block / Report</h3>

<div class="diagram-container">
  <div class="mermaid">
graph LR
    Client["üì± Client"]
    LB["‚öñÔ∏è Load<br/>Balancer"]
    MatchSvc["üíï Match<br/>Service"]
    MatchDB[("üíï Match DB<br/>(NoSQL)")]
    MsgDB[("üí¨ Message DB<br/>(NoSQL)")]
    BlockDB[("üö´ Block DB<br/>(NoSQL)")]
    MQ["üì¨ Message<br/>Queue"]
    ModSvc["üõ°Ô∏è Moderation<br/>Service"]

    Client -->|"1. DELETE /matches/{id}<br/>or POST /block<br/>or POST /report"| LB
    LB --> MatchSvc
    MatchSvc -->|"2a. Deactivate match"| MatchDB
    MatchSvc -->|"2b. Soft-delete<br/>messages"| MsgDB
    MatchSvc -->|"2c. Write block<br/>record"| BlockDB
    MatchSvc -->|"3. If report:<br/>enqueue for review"| MQ
    MQ --> ModSvc
  </div>
</div>

<div class="example-box">
  <strong>Example 1 ‚Äî Unmatch:</strong><br/>
  Alice decides to unmatch Bob. She taps "Unmatch" in the chat. The client sends <code>HTTP DELETE /api/v1/matches/{match_id}</code>. The Match Service sets <code>is_active = false</code> on the match record in the Match DB, soft-deletes the conversation messages (marks them as deleted but retains for compliance), and returns <code>200 OK</code>. Bob's client, on next sync, sees the match is inactive and removes Alice from his match list.
</div>

<div class="example-box">
  <strong>Example 2 ‚Äî Block:</strong><br/>
  Alice blocks Charlie. <code>HTTP POST /api/v1/block</code> with <code>{blocked_user_id: "charlie_789"}</code>. The Match Service writes a block record to the Block DB: <code>{blocker: alice, blocked: charlie}</code>. If a match existed, it is deactivated. Charlie will never appear in Alice's discovery feed again, and Alice will never appear in Charlie's. The Discovery Service checks the Block DB during feed generation.
</div>

<div class="example-box">
  <strong>Example 3 ‚Äî Report:</strong><br/>
  Alice reports David for inappropriate behavior with <code>HTTP POST /api/v1/report</code> with <code>{reported_user_id: "david_012", reason: "inappropriate_messages", details: "..."}</code>. The Match Service records the report and enqueues it on the Message Queue for the Moderation Service to review asynchronously. If David accumulates multiple reports, the Moderation Service may automatically suspend his account.
</div>

<h4>Deep Dive: Flow 5 Components</h4>

<table>
  <tr><th>Component</th><th>Description</th></tr>
  <tr>
    <td><strong>Match Service</strong></td>
    <td>
      Manages match lifecycle including deactivation, blocking, and reporting.<br/>
      <strong>Protocol:</strong> HTTP/REST.<br/>
      <strong>Endpoints:</strong>
      <ul>
        <li><code>DELETE /api/v1/matches/{match_id}</code> ‚Äî Unmatch. Output: <code>200 OK</code>.</li>
        <li><code>GET /api/v1/matches?user_id=X</code> ‚Äî List active matches. Output: <code>{matches: [{match_id, other_user, matched_at}, ...]}</code>.</li>
        <li><code>POST /api/v1/block</code> ‚Äî Block user. Input: <code>{blocked_user_id}</code>.</li>
        <li><code>POST /api/v1/report</code> ‚Äî Report user. Input: <code>{reported_user_id, reason, details}</code>.</li>
      </ul>
    </td>
  </tr>
  <tr>
    <td><strong>Block DB (NoSQL)</strong></td>
    <td>Stores block relationships. <code>{block_id (PK), blocker_id, blocked_id, created_at}</code>. Indexed on <code>blocker_id</code> and <code>blocked_id</code> for bidirectional lookups. NoSQL for simple key-value access, high read throughput (checked on every discovery query).</td>
  </tr>
  <tr>
    <td><strong>Moderation Service</strong></td>
    <td>Consumes report events from the Message Queue. Applies automated scoring (NLP on reported messages, photo analysis) and queues for human review if needed. May auto-suspend accounts exceeding report thresholds.</td>
  </tr>
</table>

<hr/>

<!-- ===================== COMBINED FLOW ===================== -->
<h3>Combined Overall System Diagram</h3>

<div class="diagram-container">
  <div class="mermaid">
graph TB
    Client["üì± Client<br/>(iOS / Android)"]

    subgraph "API Layer"
        LB["‚öñÔ∏è Load Balancer"]
        WSG["üîå WebSocket<br/>Gateway Cluster"]
    end

    subgraph "Service Layer"
        US["üßë User Service"]
        MS["üì∏ Media Service"]
        LS["üìç Location Service"]
        DS["üîç Discovery Service"]
        SS["üëÜ Swipe Service"]
        MatchSvc["üíï Match Service"]
        MsgSvc["üí¨ Message Service"]
        NS["üîî Notification Service"]
        ModSvc["üõ°Ô∏è Moderation Service"]
    end

    subgraph "Async Layer"
        MQ["üì¨ Message Queue"]
        PS["üì° Pub/Sub"]
    end

    subgraph "Cache Layer"
        ProfileCache["‚ö° Profile Cache"]
        RecCache["‚ö° Recommendation Cache"]
        ConnStore["‚ö° Connection Store"]
    end

    subgraph "Data Layer"
        UserDB[("üë§ User DB<br/>(SQL)")]
        LocDB[("üìç Location DB<br/>(NoSQL)")]
        SwipeDB[("üëÜ Swipe DB<br/>(NoSQL)")]
        MatchDB[("üíï Match DB<br/>(NoSQL)")]
        MsgDB[("üí¨ Message DB<br/>(NoSQL)")]
        BlockDB[("üö´ Block DB<br/>(NoSQL)")]
    end

    subgraph "Storage & Delivery"
        ObjStore[("üóÑÔ∏è Object Storage")]
        CDN["üåê CDN"]
    end

    APNS["üì≤ Push Gateway<br/>(APNs / FCM)"]

    Client <-->|"WebSocket"| WSG
    Client -->|"HTTPS"| LB
    LB --> US
    LB --> MS
    LB --> LS
    LB --> DS
    LB --> SS
    LB --> MatchSvc

    US --> UserDB
    US --> ProfileCache
    MS --> ObjStore
    ObjStore -.-> CDN
    LS --> LocDB
    DS --> RecCache
    DS --> LocDB
    DS --> UserDB
    DS --> SwipeDB
    DS --> BlockDB
    DS --> ProfileCache
    SS --> SwipeDB
    SS --> MatchDB
    SS --> MQ
    MatchSvc --> MatchDB
    MatchSvc --> MsgDB
    MatchSvc --> BlockDB
    MatchSvc --> MQ

    WSG --> MsgSvc
    WSG --> ConnStore
    MsgSvc --> MsgDB
    MsgSvc --> PS
    MsgSvc --> NS
    PS --> WSG

    MQ --> NS
    MQ --> ModSvc
    NS --> APNS
    APNS --> Client

    CDN -->|"Photos"| Client
  </div>
</div>

<div class="example-box">
  <strong>Full End-to-End Example ‚Äî Alice &amp; Bob's Tinder Journey:</strong><br/><br/>
  <strong>Step 1 ‚Äî Sign Up:</strong> Alice creates her account (<code>POST /users</code> ‚Üí User Service ‚Üí User DB + Profile Cache). She uploads 4 photos (<code>POST /users/{id}/photos</code> ‚Üí Media Service ‚Üí Object Storage ‚Üí CDN). She sets preferences: men, 24‚Äì32, 15-mile radius.<br/><br/>
  <strong>Step 2 ‚Äî Go Online:</strong> Alice opens the app. Her client sends her GPS coordinates (<code>PUT /location</code> ‚Üí Location Service ‚Üí Location DB with geohash). Her client also establishes a WebSocket connection (WSS upgrade ‚Üí WebSocket Gateway ‚Üí Connection Store).<br/><br/>
  <strong>Step 3 ‚Äî Discovery:</strong> Alice taps the swipe card view (<code>GET /discovery/feed</code> ‚Üí Discovery Service). The service queries Location DB (geohash neighbors), applies Alice's preferences against User DB, excludes already-swiped users (Swipe DB) and blocked users (Block DB), ranks candidates, hydrates profiles from Profile Cache, and returns 50 profiles. Alice sees Bob's card first ‚Äî his photos load from the CDN.<br/><br/>
  <strong>Step 4 ‚Äî Alice Swipes Right:</strong> Alice likes Bob (<code>POST /swipe {target: bob, action: like}</code> ‚Üí Swipe Service ‚Üí Swipe DB). The service checks for Bob's reverse swipe ‚Äî none found. No match yet.<br/><br/>
  <strong>Step 5 ‚Äî Bob Swipes Right:</strong> Hours later, Bob sees Alice in his feed and swipes right. The Swipe Service finds Alice's earlier like. <strong>Match!</strong> A match record is written to Match DB, and a notification message is enqueued to the Message Queue. The Notification Service consumes the message and sends push notifications to both Alice ("New match with Bob! üéâ") and Bob (who sees the match animation in-app).<br/><br/>
  <strong>Step 6 ‚Äî Chat:</strong> Alice opens the chat with Bob. She types "Hey! üëã". The message flows: Alice's client ‚Üí WebSocket ‚Üí Gateway Server 1 ‚Üí Message Service ‚Üí persisted in Message DB ‚Üí published to Pub/Sub channel <code>user:bob</code> ‚Üí Gateway Server 2 (where Bob is connected) ‚Üí Bob's WebSocket ‚Üí Bob sees the message in real time.<br/><br/>
  <strong>Step 7 ‚Äî Bob Goes Offline:</strong> Bob closes the app. His WebSocket disconnects; his entry is removed from the Connection Store. Alice sends another message. The Message Service persists it but finds no active connection for Bob. A push notification is triggered: "Alice: Want to grab coffee? ‚òï". When Bob reopens the app, his client fetches missed messages via <code>GET /matches/{id}/messages?since=...</code>.
</div>

<hr/>

<!-- ============================================================ -->
<h2>5. Database Schema</h2>
<!-- ============================================================ -->

<!-- ===== SQL Tables ===== -->
<h3><span class="badge badge-sql">SQL</span> Users Table</h3>
<table>
  <tr><th>Column</th><th>Type</th><th>Constraints</th><th>Description</th></tr>
  <tr><td><code>user_id</code></td><td>UUID</td><td><strong>PRIMARY KEY</strong></td><td>Unique user identifier</td></tr>
  <tr><td><code>email</code></td><td>VARCHAR(255)</td><td>UNIQUE, NOT NULL</td><td>User's email (for auth)</td></tr>
  <tr><td><code>phone_hash</code></td><td>VARCHAR(64)</td><td>UNIQUE</td><td>Hashed phone number</td></tr>
  <tr><td><code>name</code></td><td>VARCHAR(100)</td><td>NOT NULL</td><td>Display name</td></tr>
  <tr><td><code>birth_date</code></td><td>DATE</td><td>NOT NULL</td><td>Date of birth (age derived)</td></tr>
  <tr><td><code>gender</code></td><td>ENUM</td><td>NOT NULL</td><td>User's gender</td></tr>
  <tr><td><code>bio</code></td><td>TEXT</td><td></td><td>Profile bio (max 500 chars)</td></tr>
  <tr><td><code>desirability_score</code></td><td>FLOAT</td><td>DEFAULT 0.5</td><td>Ranking score (0‚Äì1), updated by background job</td></tr>
  <tr><td><code>last_active_at</code></td><td>TIMESTAMP</td><td></td><td>Last activity timestamp</td></tr>
  <tr><td><code>is_active</code></td><td>BOOLEAN</td><td>DEFAULT true</td><td>Account active status</td></tr>
  <tr><td><code>created_at</code></td><td>TIMESTAMP</td><td>NOT NULL</td><td>Account creation time</td></tr>
  <tr><td><code>updated_at</code></td><td>TIMESTAMP</td><td>NOT NULL</td><td>Last profile update</td></tr>
</table>
<div class="success-box">
  <strong>Why SQL?</strong> User profiles are structured, relational data with strong consistency requirements (e.g., unique email constraint). Profile updates are transactional. Reads are frequent (every discovery query hydrates profiles) but well-served by the Profile Cache. SQL provides ACID guarantees for profile mutations and foreign key integrity with related tables (Preferences, Photos).<br/><br/>
  <strong>Indexes:</strong>
  <ul>
    <li><strong>B-tree index on <code>email</code></strong> ‚Äî For login lookups. B-tree supports equality and range queries; email lookups are equality-based.</li>
    <li><strong>B-tree index on <code>gender, birth_date</code></strong> ‚Äî Composite index for discovery filtering. Gender is equality match, birth_date is range query (age filter). B-tree handles both efficiently.</li>
    <li><strong>B-tree index on <code>last_active_at</code></strong> ‚Äî For ranking recently active users higher in discovery.</li>
  </ul>
  <strong>Sharding:</strong> Shard by <code>user_id</code> (hash-based). Distributes users uniformly across shards. All profile lookups are by user_id, so queries hit a single shard. Cross-shard queries (e.g., discovery filtering by gender/age) are handled by the Discovery Service querying the candidate set from the Location DB first, then fetching individual profiles ‚Äî so profile lookups are always by user_id, hitting the correct shard.<br/><br/>
  <strong>Read events:</strong> Profile viewed in discovery feed, profile opened in chat, profile edited by owner.<br/>
  <strong>Write events:</strong> Account creation, profile edit, desirability score update (background job), last_active_at update.
</div>

<h3><span class="badge badge-sql">SQL</span> User Preferences Table</h3>
<table>
  <tr><th>Column</th><th>Type</th><th>Constraints</th><th>Description</th></tr>
  <tr><td><code>preference_id</code></td><td>UUID</td><td><strong>PRIMARY KEY</strong></td><td>Unique preference record</td></tr>
  <tr><td><code>user_id</code></td><td>UUID</td><td><strong>FOREIGN KEY ‚Üí Users.user_id</strong>, UNIQUE</td><td>One-to-one with Users</td></tr>
  <tr><td><code>gender_preference</code></td><td>ENUM</td><td>NOT NULL</td><td>Interested in: male/female/both/all</td></tr>
  <tr><td><code>min_age</code></td><td>INT</td><td>NOT NULL</td><td>Minimum age preference</td></tr>
  <tr><td><code>max_age</code></td><td>INT</td><td>NOT NULL</td><td>Maximum age preference</td></tr>
  <tr><td><code>max_distance_miles</code></td><td>INT</td><td>NOT NULL, DEFAULT 50</td><td>Maximum discovery radius</td></tr>
</table>
<div class="success-box">
  <strong>Why SQL?</strong> Directly related to the Users table (1:1 foreign key). Always queried alongside user data during discovery filtering. Small, structured data. Joins with Users table are efficient. Stored in the same SQL database as Users for locality.<br/><br/>
  <strong>Why a separate table (Normalization)?</strong> Preferences are logically distinct from profile data. They are read/written at different frequencies (preferences rarely change, profiles change more often). Normalization avoids wide rows in the Users table and keeps the Users table scan efficient for columns frequently needed (name, gender, age, score).<br/><br/>
  <strong>Read events:</strong> Discovery Service fetches preferences when building a user's feed.<br/>
  <strong>Write events:</strong> User updates their discovery preferences in settings.
</div>

<h3><span class="badge badge-sql">SQL</span> Photos Table</h3>
<table>
  <tr><th>Column</th><th>Type</th><th>Constraints</th><th>Description</th></tr>
  <tr><td><code>photo_id</code></td><td>UUID</td><td><strong>PRIMARY KEY</strong></td><td>Unique photo identifier</td></tr>
  <tr><td><code>user_id</code></td><td>UUID</td><td><strong>FOREIGN KEY ‚Üí Users.user_id</strong></td><td>Photo owner</td></tr>
  <tr><td><code>url_thumbnail</code></td><td>VARCHAR(500)</td><td>NOT NULL</td><td>CDN URL for thumbnail</td></tr>
  <tr><td><code>url_medium</code></td><td>VARCHAR(500)</td><td>NOT NULL</td><td>CDN URL for medium resolution</td></tr>
  <tr><td><code>url_full</code></td><td>VARCHAR(500)</td><td>NOT NULL</td><td>CDN URL for full resolution</td></tr>
  <tr><td><code>display_order</code></td><td>SMALLINT</td><td>NOT NULL</td><td>Order in profile (0 = primary)</td></tr>
  <tr><td><code>created_at</code></td><td>TIMESTAMP</td><td>NOT NULL</td><td>Upload timestamp</td></tr>
</table>
<div class="success-box">
  <strong>Why SQL?</strong> Photos are relationally tied to Users (foreign key). The display_order needs transactional updates (reordering). Small metadata ‚Äî actual binary stored in Object Storage.<br/><br/>
  <strong>Index:</strong> <strong>B-tree index on <code>(user_id, display_order)</code></strong> ‚Äî Composite index for efficient retrieval of all photos for a user in display order. Most queries are "get all photos for user X ordered by display_order".<br/><br/>
  <strong>Read events:</strong> Profile viewed in discovery, chat opened.<br/>
  <strong>Write events:</strong> Photo uploaded, photo deleted, photos reordered.
</div>

<hr/>

<h3><span class="badge badge-nosql">NoSQL</span> Location Table</h3>
<table>
  <tr><th>Column</th><th>Type</th><th>Constraints</th><th>Description</th></tr>
  <tr><td><code>user_id</code></td><td>STRING</td><td><strong>PRIMARY KEY (Partition Key)</strong></td><td>User identifier</td></tr>
  <tr><td><code>latitude</code></td><td>DOUBLE</td><td>NOT NULL</td><td>GPS latitude</td></tr>
  <tr><td><code>longitude</code></td><td>DOUBLE</td><td>NOT NULL</td><td>GPS longitude</td></tr>
  <tr><td><code>geohash</code></td><td>STRING</td><td>NOT NULL</td><td>Geohash string (precision 6)</td></tr>
  <tr><td><code>updated_at</code></td><td>TIMESTAMP</td><td>NOT NULL</td><td>Last location update</td></tr>
</table>
<div class="callout">
  <strong>Why NoSQL?</strong> Extremely high write throughput: ~1.2 billion location updates/day (~14K writes/sec avg). Simple key-value access pattern ‚Äî no joins needed. Each record is independent. Geohash prefix queries map naturally to range scans in NoSQL key-value stores.<br/><br/>
  <strong>Index:</strong> <strong>Geohash prefix index</strong> ‚Äî A secondary index on the <code>geohash</code> column using a <strong>B-tree / range-scan</strong> approach. Since geohashes encode geographic proximity as string prefixes, querying for all users with geohash starting with <code>"9q8yy"</code> returns all users in that geographic cell. The Discovery Service queries the target geohash and its 8 neighboring geohashes to cover the full radius. This is effectively an <strong>R-tree-like spatial query implemented via geohash prefix matching</strong>.<br/><br/>
  <strong>Sharding:</strong> Shard by <code>geohash prefix</code> (first 3‚Äì4 characters). This provides geographic locality ‚Äî users in the same area are on the same shard, making discovery queries hit fewer shards. The downside is potential hot spots in dense urban areas (e.g., Manhattan). Mitigation: use virtual shards within popular geohash prefixes to distribute load.<br/><br/>
  <strong>Read events:</strong> Discovery Service queries for nearby users.<br/>
  <strong>Write events:</strong> Client sends location update (~every 5 minutes while app is foregrounded).
</div>

<h3><span class="badge badge-nosql">NoSQL</span> Swipes Table</h3>
<table>
  <tr><th>Column</th><th>Type</th><th>Constraints</th><th>Description</th></tr>
  <tr><td><code>swipe_id</code></td><td>UUID</td><td><strong>PRIMARY KEY</strong></td><td>Unique swipe identifier</td></tr>
  <tr><td><code>swiper_id</code></td><td>STRING</td><td><strong>Partition Key</strong>, NOT NULL</td><td>User who swiped</td></tr>
  <tr><td><code>swiped_id</code></td><td>STRING</td><td>NOT NULL</td><td>User who was swiped on</td></tr>
  <tr><td><code>action</code></td><td>ENUM</td><td>NOT NULL</td><td>like / dislike / super_like</td></tr>
  <tr><td><code>created_at</code></td><td>TIMESTAMP</td><td><strong>Sort Key</strong>, NOT NULL</td><td>Swipe timestamp</td></tr>
</table>
<div class="callout">
  <strong>Why NoSQL?</strong> Highest write throughput in the entire system: ~2.5 billion swipes/day (~29K writes/sec avg, ~100K peak). Append-heavy workload. Simple access patterns: (1) write a new swipe, (2) check if reverse swipe exists, (3) get all swipes by a user (for exclusion). No complex queries or joins.<br/><br/>
  <strong>Indexes:</strong>
  <ul>
    <li><strong>Primary index on <code>(swiper_id, created_at)</code></strong> ‚Äî Partition by swiper, sorted by time. Efficient for "get all swipes by user X" (discovery exclusion).</li>
    <li><strong>Global Secondary Index (GSI) on <code>(swiped_id, swiper_id, action)</code></strong> ‚Äî For reverse-swipe lookup: "Has user B swiped right on user A?". This is the match detection query. Hash index on swiped_id for fast equality lookup.</li>
  </ul>
  <strong>Sharding:</strong> Shard by <code>swiper_id</code> (hash-based). All swipes by a user are co-located on one shard ‚Äî efficient for the exclusion filter. The GSI for reverse lookups is distributed across shards by <code>swiped_id</code>.<br/><br/>
  <strong>Denormalization note:</strong> The <code>action</code> field in the GSI duplicates data for the reverse-lookup query. This denormalization avoids a two-step process (look up swipe_id from index ‚Üí fetch action from main table) and is justified by the extreme read frequency of match detection (every right swipe triggers this lookup).<br/><br/>
  <strong>Read events:</strong> (1) Match detection: every right swipe checks for reverse swipe. (2) Discovery exclusion: fetching all swiper_id's swiped targets.<br/>
  <strong>Write events:</strong> Every swipe action (like, dislike, super_like).
</div>

<h3><span class="badge badge-nosql">NoSQL</span> Matches Table</h3>
<table>
  <tr><th>Column</th><th>Type</th><th>Constraints</th><th>Description</th></tr>
  <tr><td><code>match_id</code></td><td>UUID</td><td><strong>PRIMARY KEY</strong></td><td>Unique match identifier</td></tr>
  <tr><td><code>user1_id</code></td><td>STRING</td><td>NOT NULL</td><td>First user (lexicographically smaller ID)</td></tr>
  <tr><td><code>user2_id</code></td><td>STRING</td><td>NOT NULL</td><td>Second user</td></tr>
  <tr><td><code>is_active</code></td><td>BOOLEAN</td><td>DEFAULT true</td><td>Match active status (false = unmatched)</td></tr>
  <tr><td><code>matched_at</code></td><td>TIMESTAMP</td><td>NOT NULL</td><td>When match was created</td></tr>
</table>
<div class="callout">
  <strong>Why NoSQL?</strong> High write volume (~25M matches/day). Simple access patterns: get all matches for user X, check if two users are matched. No complex joins.<br/><br/>
  <strong>Denormalization:</strong> Each match is stored once but we need efficient lookups for both users. We create two GSI entries: one partitioned by <code>user1_id</code> and one by <code>user2_id</code>, both sorted by <code>matched_at DESC</code>. Alternatively, we can store two records per match (one for each user) ‚Äî this is intentional denormalization to avoid scatter-gather queries. The trade-off is 2√ó storage but halved read latency, which is worthwhile since users frequently open their matches list.<br/><br/>
  <strong>Index:</strong> <strong>GSI on <code>(user1_id, matched_at DESC)</code></strong> and <strong>GSI on <code>(user2_id, matched_at DESC)</code></strong> ‚Äî For listing all matches for a user, sorted by most recent.<br/><br/>
  <strong>Read events:</strong> User opens matches/chat list.<br/>
  <strong>Write events:</strong> Mutual swipe creates match. Unmatch sets <code>is_active = false</code>.
</div>

<h3><span class="badge badge-nosql">NoSQL</span> Messages Table</h3>
<table>
  <tr><th>Column</th><th>Type</th><th>Constraints</th><th>Description</th></tr>
  <tr><td><code>message_id</code></td><td>UUID</td><td><strong>PRIMARY KEY</strong></td><td>Unique message identifier</td></tr>
  <tr><td><code>match_id</code></td><td>STRING</td><td><strong>Partition Key</strong>, NOT NULL</td><td>Match/conversation this belongs to</td></tr>
  <tr><td><code>sender_id</code></td><td>STRING</td><td>NOT NULL</td><td>User who sent the message</td></tr>
  <tr><td><code>content</code></td><td>TEXT</td><td>NOT NULL</td><td>Message text (encrypted)</td></tr>
  <tr><td><code>status</code></td><td>ENUM</td><td>DEFAULT 'sent'</td><td>sent / delivered / read</td></tr>
  <tr><td><code>created_at</code></td><td>TIMESTAMP</td><td><strong>Sort Key</strong>, NOT NULL</td><td>Message timestamp</td></tr>
</table>
<div class="callout">
  <strong>Why NoSQL?</strong> High write throughput (500M messages/day). Append-only workload. Access pattern is always scoped to a single conversation (match_id) ‚Äî perfect for partition-based NoSQL. Messages are fetched in chronological order within a conversation ‚Äî sort key on timestamp. No cross-conversation joins.<br/><br/>
  <strong>Partitioning by <code>match_id</code></strong> ensures all messages in a conversation are co-located on the same partition. Within each partition, messages are sorted by <code>created_at</code>. This enables efficient range queries like "get messages since timestamp X" (used for fetching missed messages after reconnection).<br/><br/>
  <strong>Sharding:</strong> Shard by <code>match_id</code> (hash-based). Each conversation is entirely on one shard. Even distribution since match_ids are UUIDs.<br/><br/>
  <strong>Read events:</strong> User opens chat, scrolls up for history, reconnects and fetches missed messages.<br/>
  <strong>Write events:</strong> User sends a message. Message status updated to delivered/read.
</div>

<h3><span class="badge badge-nosql">NoSQL</span> Blocks Table</h3>
<table>
  <tr><th>Column</th><th>Type</th><th>Constraints</th><th>Description</th></tr>
  <tr><td><code>block_id</code></td><td>UUID</td><td><strong>PRIMARY KEY</strong></td><td>Unique block record</td></tr>
  <tr><td><code>blocker_id</code></td><td>STRING</td><td><strong>Partition Key</strong>, NOT NULL</td><td>User who blocked</td></tr>
  <tr><td><code>blocked_id</code></td><td>STRING</td><td>NOT NULL</td><td>Blocked user</td></tr>
  <tr><td><code>created_at</code></td><td>TIMESTAMP</td><td>NOT NULL</td><td>When block was created</td></tr>
</table>
<div class="callout">
  <strong>Why NoSQL?</strong> Simple key-value lookups. Checked during every discovery query (high read volume). No complex queries.<br/><br/>
  <strong>Index:</strong> GSI on <code>blocked_id</code> ‚Äî "Who has blocked user X?" Needed so user X doesn't appear in any blocker's feed.<br/><br/>
  <strong>Read events:</strong> Discovery Service checks blocks during feed generation.<br/>
  <strong>Write events:</strong> User blocks another user.
</div>

<hr/>

<!-- ============================================================ -->
<h2>6. CDN &amp; Cache Deep Dive</h2>
<!-- ============================================================ -->

<h3>CDN</h3>
<div class="success-box">
  <strong>Why a CDN is appropriate:</strong> Tinder is a photo-heavy application. Every profile card in the discovery feed displays multiple photos. With 25M DAU each viewing ~100 profiles per session with ~5 photos each = <strong>~12.5 billion photo requests/day</strong>. Serving these from origin would be prohibitively expensive and slow. A CDN is essential for:<br/>
  <ul>
    <li><strong>Latency:</strong> Edge servers close to users serve photos in &lt;50ms vs. hundreds of ms from origin.</li>
    <li><strong>Throughput:</strong> CDN absorbs 95%+ of photo traffic, reducing origin load by orders of magnitude.</li>
    <li><strong>Global reach:</strong> Users worldwide get fast photo loading regardless of origin server location.</li>
    <li><strong>Cost:</strong> CDN bandwidth is cheaper than origin bandwidth at scale.</li>
  </ul>
  <strong>Caching Strategy:</strong> Origin-pull model. CDN edge requests from Object Storage on cache miss. Subsequent requests served from edge cache.<br/>
  <strong>TTL:</strong> 24 hours. Photos don't change often; when they do, a new URL is generated (cache busting via URL versioning).<br/>
  <strong>Invalidation:</strong> On photo deletion or update, the old URL becomes invalid. New photos get new URLs, so no explicit cache purging is needed (immutable URL pattern).<br/>
</div>

<h3>Profile Cache</h3>
<table>
  <tr><th>Aspect</th><th>Details</th></tr>
  <tr><td><strong>Purpose</strong></td><td>Reduces read load on User DB (SQL). User profiles are read far more frequently than written (every discovery feed hydration reads ~50 profiles).</td></tr>
  <tr><td><strong>Caching Strategy</strong></td><td><strong>Write-through cache.</strong> On every profile create/update, the User Service writes to both the SQL DB and the cache atomically. This ensures the cache is always consistent with the source of truth. Write-through was chosen over write-behind because profile updates are infrequent but reads are extremely frequent ‚Äî we want immediate consistency.</td></tr>
  <tr><td><strong>Eviction Policy</strong></td><td><strong>LRU (Least Recently Used).</strong> Profiles not accessed recently are evicted first. This keeps frequently viewed profiles (active users in popular areas) in cache. LRU is appropriate because profile access follows a Zipfian distribution ‚Äî a small percentage of active users receive the vast majority of profile views.</td></tr>
  <tr><td><strong>TTL / Expiration</strong></td><td><strong>1 hour.</strong> Even with write-through, TTL acts as a safety net to evict potentially stale entries (e.g., if a cache write fails silently). 1 hour balances freshness with cache hit rate.</td></tr>
  <tr><td><strong>Key Format</strong></td><td><code>profile:{user_id}</code></td></tr>
  <tr><td><strong>Populated by</strong></td><td>User Service on profile create, update, and on cache miss during read.</td></tr>
  <tr><td><strong>Cache Hit Rate</strong></td><td>Expected &gt;90%. Most discovery queries return profiles of recently active users who are already cached.</td></tr>
</table>

<h3>Recommendation Cache</h3>
<table>
  <tr><th>Aspect</th><th>Details</th></tr>
  <tr><td><strong>Purpose</strong></td><td>Stores pre-computed discovery feed results per user. Avoids re-running the expensive discovery pipeline (geospatial query + filtering + ranking) on every page request or scroll.</td></tr>
  <tr><td><strong>Caching Strategy</strong></td><td><strong>Cache-aside (Lazy loading).</strong> On cache miss, Discovery Service computes the feed and stores it. On cache hit, returns directly. Write-through is not suitable here because the feed depends on many external factors (other users' locations, new swipes) that change independently of the current user's actions.</td></tr>
  <tr><td><strong>Eviction Policy</strong></td><td><strong>LRU.</strong> Feeds for inactive users are evicted first.</td></tr>
  <tr><td><strong>TTL / Expiration</strong></td><td><strong>5 minutes.</strong> Short TTL is critical because discovery feeds are highly dynamic ‚Äî users move, new users join, existing users get swiped away. A stale feed would show profiles that have already been swiped on or have moved out of range.</td></tr>
  <tr><td><strong>Key Format</strong></td><td><code>rec:{user_id}</code></td></tr>
  <tr><td><strong>Invalidation</strong></td><td>Explicitly invalidated when: (1) User changes location significantly (&gt;5 miles), (2) User updates preferences, (3) TTL expires.</td></tr>
</table>

<h3>Connection Store (Cache)</h3>
<table>
  <tr><th>Aspect</th><th>Details</th></tr>
  <tr><td><strong>Purpose</strong></td><td>Tracks which WebSocket Gateway server each online user is connected to. Ephemeral data ‚Äî not persisted to disk.</td></tr>
  <tr><td><strong>Caching Strategy</strong></td><td><strong>Direct write.</strong> Gateways write directly on connection establish and delete on disconnect. No "source of truth" database ‚Äî the cache IS the source of truth for connection state.</td></tr>
  <tr><td><strong>Eviction Policy</strong></td><td><strong>TTL-based.</strong> Entries expire after 5 minutes if not refreshed. Heartbeats refresh the TTL every 30 seconds.</td></tr>
  <tr><td><strong>TTL / Expiration</strong></td><td><strong>5 minutes.</strong> Ensures stale connections (e.g., from a crashed gateway that didn't cleanly remove entries) are automatically cleaned up.</td></tr>
  <tr><td><strong>Key Format</strong></td><td><code>ws:user:{user_id}</code></td></tr>
</table>

<hr/>

<!-- ============================================================ -->
<h2>7. Scaling Considerations</h2>
<!-- ============================================================ -->

<h3>Load Balancers</h3>

<h4>Where Load Balancers are placed:</h4>
<ol>
  <li><strong>External Load Balancer (Client ‚Üí API Layer):</strong> Sits between clients and the API Gateway / service mesh. Handles all incoming HTTPS traffic. Distributes requests across service instances.</li>
  <li><strong>WebSocket Load Balancer (Client ‚Üí WebSocket Gateways):</strong> Separate load balancer for WebSocket connections. Uses <strong>least-connections</strong> algorithm (not round-robin) because WebSocket connections are long-lived ‚Äî round-robin would cause uneven connection distribution.</li>
  <li><strong>Internal Load Balancers (Service-to-Service):</strong> Between services (e.g., Discovery Service ‚Üí User Service, Message Service ‚Üí Notification Service). Can be implemented via client-side load balancing or service mesh sidecars for lower latency than a separate LB hop.</li>
</ol>

<h4>Load Balancer Deep Dive:</h4>
<table>
  <tr><th>Aspect</th><th>External LB</th><th>WebSocket LB</th><th>Internal LB</th></tr>
  <tr><td><strong>Layer</strong></td><td>Layer 7 (Application)</td><td>Layer 4 (Transport) or Layer 7</td><td>Layer 7 (via service mesh)</td></tr>
  <tr><td><strong>Algorithm</strong></td><td>Round-robin with health checks</td><td>Least connections</td><td>Round-robin or weighted</td></tr>
  <tr><td><strong>TLS</strong></td><td>TLS termination at LB</td><td>TLS termination at LB (WSS)</td><td>mTLS between services</td></tr>
  <tr><td><strong>Health Checks</strong></td><td>HTTP GET /health every 10s</td><td>TCP health check every 10s</td><td>gRPC health checks</td></tr>
  <tr><td><strong>Sticky Sessions</strong></td><td>Not needed (stateless services)</td><td>Not needed (connection tracked in Connection Store)</td><td>Not needed</td></tr>
  <tr><td><strong>Scaling</strong></td><td>Auto-scale based on requests/sec</td><td>Auto-scale based on active connections</td><td>Scales with service instances</td></tr>
</table>

<h3>Horizontal Scaling by Service</h3>
<table>
  <tr><th>Service</th><th>Scaling Strategy</th><th>Bottleneck</th></tr>
  <tr><td>User Service</td><td>Horizontally scale stateless instances behind LB. Most reads served from cache.</td><td>DB writes during sign-up spikes</td></tr>
  <tr><td>Media Service</td><td>Scale horizontally. Photo processing (resize, compress) is CPU-intensive ‚Äî scale based on CPU utilization.</td><td>CPU for image processing</td></tr>
  <tr><td>Location Service</td><td>Scale horizontally. Write-heavy ‚Äî scale based on write throughput to Location DB.</td><td>Location DB write throughput</td></tr>
  <tr><td>Discovery Service</td><td>Most compute-intensive service. Scale horizontally. Use recommendation cache aggressively to reduce compute.</td><td>Geospatial queries + ranking computation</td></tr>
  <tr><td>Swipe Service</td><td>Scale horizontally. Highest request volume service.</td><td>Swipe DB write throughput and match detection reads</td></tr>
  <tr><td>WebSocket Gateway</td><td>Scale based on concurrent connection count. Each server handles ~50K‚Äì100K concurrent connections. At 25M DAU with ~4 hours active = ~4M concurrent ‚Üí ~40‚Äì80 gateway instances.</td><td>Memory (per-connection state) and network I/O</td></tr>
  <tr><td>Message Service</td><td>Scale horizontally. Stateless processing.</td><td>Message DB write throughput</td></tr>
  <tr><td>Notification Service</td><td>Scale based on Message Queue depth. Scale up consumers when queue length grows.</td><td>Push gateway rate limits (APNs/FCM)</td></tr>
</table>

<h3>Database Scaling</h3>
<ul>
  <li><strong>User DB (SQL):</strong> Read replicas for read-heavy queries. Write to primary, read from replicas (with slight lag acceptable for discovery). Shard by user_id when single-node capacity is exceeded.</li>
  <li><strong>Location DB (NoSQL):</strong> Shard by geohash prefix. Add shards for hot geographic regions. Auto-rebalancing.</li>
  <li><strong>Swipe DB (NoSQL):</strong> Shard by swiper_id. Partition splitting when shards get hot. Consider TTL-based auto-deletion of old swipes (&gt;90 days) to manage storage growth.</li>
  <li><strong>Message DB (NoSQL):</strong> Shard by match_id. Grows linearly with conversations. Archive old conversations to cold storage after inactivity period.</li>
</ul>

<h3>Geographic Distribution</h3>
<ul>
  <li>Deploy service clusters in multiple regions (US-East, US-West, Europe, Asia).</li>
  <li>Route users to the nearest region via DNS-based global load balancing (geo-routing).</li>
  <li>Location DB and Swipe DB should be regionally deployed ‚Äî users primarily interact with other users in their region.</li>
  <li>User DB can be globally replicated with cross-region async replication for users who travel.</li>
</ul>

<hr/>

<!-- ============================================================ -->
<h2>8. Tradeoffs &amp; Deep Dives</h2>
<!-- ============================================================ -->

<h3>Match Detection: Synchronous vs. Asynchronous</h3>
<div class="warn-box">
  <strong>Chosen: Synchronous match detection within the Swipe Service.</strong><br/>
  When Bob swipes right on Alice, the Swipe Service immediately checks for Alice's reverse swipe and creates the match in the same request cycle. This means Bob sees the "It's a Match!" animation instantly.<br/><br/>
  <strong>Trade-off:</strong> Adds ~10‚Äì20ms of latency to every right-swipe (for the reverse-swipe lookup). But this is negligible compared to the swipe animation duration (~500ms). The alternative ‚Äî async match detection via a queue ‚Äî would mean Bob might not see the match for seconds or even minutes, degrading the iconic "match moment" experience. The synchronous approach is strongly preferred for UX.
</div>

<h3>Geohash vs. Quadtree vs. R-tree for Location</h3>
<div class="warn-box">
  <strong>Chosen: Geohash.</strong><br/>
  <ul>
    <li><strong>Geohash:</strong> Simple string-based encoding. Proximity ‚Üí prefix similarity. Works naturally with NoSQL range scans. Easy to implement. Slight inaccuracy at geohash cell boundaries (solved by querying neighbors). ‚úÖ Chosen for simplicity and NoSQL compatibility.</li>
    <li><strong>Quadtree:</strong> Tree structure that recursively divides 2D space. Great for in-memory spatial queries. But doesn't map well to distributed NoSQL storage. Requires a centralized tree structure. ‚ùå Rejected.</li>
    <li><strong>R-tree:</strong> Optimized for range queries on multi-dimensional data. Native to some SQL spatial extensions. But requires specialized database support and is harder to shard. ‚ùå Rejected for distributed NoSQL setup.</li>
  </ul>
</div>

<h3>Pre-computed Recommendation Queue vs. Real-time Computation</h3>
<div class="warn-box">
  <strong>Chosen: Hybrid approach.</strong><br/>
  <ul>
    <li>First request: Real-time computation, result cached for 5 minutes.</li>
    <li>Subsequent requests within TTL: Served from cache.</li>
    <li><strong>Alternative ‚Äî Fully pre-computed:</strong> A background job periodically computes each user's recommendation queue. Pros: instant feed loading. Cons: Stale recommendations (user may have moved or swiped), enormous compute cost to pre-compute for all 25M DAU, wastes compute for users who don't open the app. ‚ùå Rejected ‚Äî too wasteful.</li>
    <li><strong>Alternative ‚Äî Fully real-time:</strong> Every discovery request runs the full pipeline. Pros: Freshest results. Cons: High latency (~200‚Äì500ms per request), no benefit from temporal locality. ‚ùå Rejected alone ‚Äî too slow at scale.</li>
  </ul>
</div>

<h3>Desirability Score (ELO-like Ranking)</h3>
<div class="callout">
  Tinder famously uses a desirability/ELO score to rank profiles. The score is updated asynchronously by a <strong>background job</strong> that runs periodically (e.g., hourly) analyzing swipe data. Factors include: percentage of right-swipes received, quality of swipes received (high-score users swiping right counts more), profile completeness, and activity recency. This score is stored in the Users table (<code>desirability_score</code>) and used as a ranking signal in the Discovery Service.
</div>

<h3>Message Encryption</h3>
<div class="callout">
  Messages are encrypted in transit (TLS/WSS) and at rest (encryption at the database layer). For a higher security model, end-to-end encryption (E2EE) could be implemented using the Signal Protocol, where only the sender and recipient can decrypt messages. The server would store only ciphertext. Trade-off: E2EE prevents server-side content moderation, making it harder to detect harassment or abuse. Given Tinder's strong moderation requirements, <strong>server-side encryption at rest</strong> (not E2EE) is the pragmatic choice.
</div>

<hr/>

<!-- ============================================================ -->
<h2>9. Alternative Approaches</h2>
<!-- ============================================================ -->

<h3>Alternative 1: GraphQL Instead of REST</h3>
<div class="warn-box">
  <strong>Not chosen.</strong> GraphQL would allow clients to request exactly the fields they need (e.g., only name + photos for discovery cards, full profile on tap). This reduces over-fetching. However, Tinder's API has well-defined, predictable query patterns ‚Äî REST endpoints can be optimized for each use case (e.g., a lightweight <code>/discovery/feed</code> response vs. a full <code>/users/{id}</code> response). REST is simpler to cache (HTTP caching at CDN/LB level), easier to rate-limit per endpoint, and the team can move faster. GraphQL's flexibility adds complexity (query analysis, N+1 problems) without significant benefit for Tinder's relatively simple API surface.
</div>

<h3>Alternative 2: Single SQL Database for Everything</h3>
<div class="warn-box">
  <strong>Not chosen.</strong> Using a single SQL database would simplify the architecture and provide ACID guarantees everywhere. However, the write throughput required for Swipes (2.5B/day) and Location updates (1.2B/day) far exceeds what a single SQL database can handle, even with sharding. SQL's row-locking overhead for high-concurrency writes would create bottlenecks. NoSQL's horizontal scaling, eventual consistency, and tunable write performance make it the right choice for high-throughput, simple-access-pattern workloads.
</div>

<h3>Alternative 3: Long Polling Instead of WebSockets for Chat</h3>
<div class="warn-box">
  <strong>Not chosen.</strong> Long polling would work functionally but introduces: (1) Higher latency ‚Äî each message requires a new HTTP round-trip after the previous long-poll completes. (2) More server resources ‚Äî each poll ties up an HTTP connection and a thread/goroutine. (3) Higher bandwidth ‚Äî HTTP headers sent repeatedly. WebSocket's persistent connection, sub-50ms message delivery, and efficient binary framing make it strictly superior for real-time chat.
</div>

<h3>Alternative 4: Server-Sent Events (SSE) + HTTP POST for Chat</h3>
<div class="warn-box">
  <strong>Not chosen.</strong> SSE provides server-to-client push (good for receiving messages) but requires a separate HTTP POST channel for sending messages. This doubles the connection overhead and makes the architecture asymmetric. WebSocket provides both directions on a single connection with lower overhead.
</div>

<h3>Alternative 5: Pub/Sub Instead of Message Queue for Match Notifications</h3>
<div class="warn-box">
  <strong>Not chosen for match notifications.</strong> Pub/Sub is designed for fan-out (one message ‚Üí many consumers). Match notifications have exactly one consumer (Notification Service). A point-to-point Message Queue with acknowledgments and dead-letter queue provides stronger delivery guarantees and is the right pattern for this use case. Pub/Sub IS used for inter-gateway message routing (Flow 4), where the fan-out pattern fits naturally.
</div>

<h3>Alternative 6: Polling for Discovery Feed Refresh</h3>
<div class="warn-box">
  <strong>Not chosen.</strong> Instead of the client pulling the feed via polling, we could push updated recommendations. However, recommendations change infrequently (TTL 5 min), and pushing updates to 25M users whenever nearby users change would create enormous server-side overhead. Pull-based <code>GET /discovery/feed</code> on user action (opening the app, swiping to the end of the current batch) is more efficient and scalable.
</div>

<hr/>

<!-- ============================================================ -->
<h2>10. Additional Considerations</h2>
<!-- ============================================================ -->

<h3>Rate Limiting</h3>
<ul>
  <li><strong>Swipes:</strong> Free users are limited to ~100 right-swipes per 12-hour window. Enforced at the Swipe Service using a sliding window counter in the cache (<code>swipe_count:{user_id}</code> with 12-hour TTL). Premium users have unlimited or higher limits.</li>
  <li><strong>Location updates:</strong> Rate-limited to 1 per minute to prevent abuse and reduce load.</li>
  <li><strong>API-wide:</strong> Token bucket rate limiting at the Load Balancer level per user (~1000 requests/min).</li>
</ul>

<h3>Fraud &amp; Bot Detection</h3>
<ul>
  <li>Photo verification using ML-based face matching (selfie vs. profile photos).</li>
  <li>Behavioral analysis: bots exhibit abnormal swipe patterns (100% right-swipe, rapid-fire swiping). Flagged by a background anomaly detection job.</li>
  <li>Device fingerprinting and CAPTCHA for suspicious accounts.</li>
</ul>

<h3>Data Retention &amp; GDPR</h3>
<ul>
  <li>Swipes older than 90 days can be archived to cold storage (reduce hot data volume).</li>
  <li>Messages in inactive matches archived after 6 months.</li>
  <li>GDPR right-to-deletion: <code>DELETE /api/v1/users/{id}</code> triggers cascading deletion across all databases (swipes, matches, messages, location, photos).</li>
</ul>

<h3>Monitoring &amp; Observability</h3>
<ul>
  <li>Distributed tracing across services (trace a swipe from client ‚Üí Swipe Service ‚Üí Match DB ‚Üí Queue ‚Üí Notification).</li>
  <li>Real-time dashboards: swipe throughput, match rate, WebSocket connection count, message delivery latency.</li>
  <li>Alerting on: Swipe Service error rate &gt; 0.1%, Message delivery latency &gt; 500ms, WebSocket reconnection rate spike.</li>
</ul>

<h3>Failover</h3>
<ul>
  <li>If a WebSocket Gateway crashes, clients reconnect to another gateway automatically. Missed messages are fetched via HTTP on reconnection.</li>
  <li>If the Notification Service is down, the Message Queue retains messages until the service recovers (durable queue).</li>
  <li>If the Recommendation Cache is unavailable, the Discovery Service falls back to real-time computation (slower but functional ‚Äî graceful degradation).</li>
</ul>

<hr/>

<!-- ============================================================ -->
<h2>11. Vendor Recommendations</h2>
<!-- ============================================================ -->

<p>The system design above is vendor-agnostic. Below are potential vendor selections for production deployment:</p>

<table>
  <tr><th>Component</th><th>Vendor Options</th><th>Rationale</th></tr>
  <tr>
    <td>SQL Database</td>
    <td>PostgreSQL, CockroachDB, Amazon Aurora</td>
    <td>PostgreSQL: Mature, excellent spatial extensions (PostGIS), rich indexing. CockroachDB: Distributed SQL for global scale. Aurora: Managed, auto-scaling, MySQL/PostgreSQL compatible.</td>
  </tr>
  <tr>
    <td>NoSQL Database</td>
    <td>Apache Cassandra, Amazon DynamoDB, ScyllaDB</td>
    <td>Cassandra: Proven at extreme write throughput, tunable consistency, excellent for time-series-like data. DynamoDB: Fully managed, auto-scaling, built-in GSI support. ScyllaDB: Cassandra-compatible with better performance per node.</td>
  </tr>
  <tr>
    <td>In-Memory Cache</td>
    <td>Redis, Memcached, KeyDB</td>
    <td>Redis: Rich data structures (sorted sets for leaderboards, pub/sub built-in), persistence options. Memcached: Simpler, slightly faster for pure key-value. KeyDB: Multi-threaded Redis fork for higher throughput.</td>
  </tr>
  <tr>
    <td>Message Queue</td>
    <td>Apache Kafka, RabbitMQ, Amazon SQS</td>
    <td>Kafka: High throughput, durable, supports replay. Best for high-volume event streams (swipe events, notifications). RabbitMQ: Lower latency, rich routing. SQS: Fully managed, simple.</td>
  </tr>
  <tr>
    <td>Pub/Sub</td>
    <td>Redis Pub/Sub, Apache Kafka, NATS</td>
    <td>Redis Pub/Sub: Sub-millisecond latency, already deployed for caching (dual-use). Kafka: Durable pub/sub with consumer groups. NATS: Lightweight, extremely fast for ephemeral messaging.</td>
  </tr>
  <tr>
    <td>Object Storage</td>
    <td>Amazon S3, Google Cloud Storage, MinIO</td>
    <td>S3: Industry standard, 11 nines durability, lifecycle policies. GCS: Competitive pricing. MinIO: Self-hosted S3-compatible for on-prem.</td>
  </tr>
  <tr>
    <td>CDN</td>
    <td>Cloudflare, Amazon CloudFront, Fastly</td>
    <td>Cloudflare: Global edge network, DDoS protection, free tier. CloudFront: Tight S3 integration. Fastly: Instant cache purging, edge compute.</td>
  </tr>
  <tr>
    <td>WebSocket Gateway</td>
    <td>Custom (Go/Rust), Socket.IO, Centrifugo</td>
    <td>Custom Go/Rust: Maximum control and performance for long-lived connections. Socket.IO: Rapid development, fallback to long-polling. Centrifugo: Production-grade, scalable WebSocket server.</td>
  </tr>
</table>

<hr/>

<p style="color: var(--text-muted); text-align: center; margin-top: 3rem; font-size: 0.85rem;">
  System Design Document ‚Äî Tinder | Generated 2026-02-13
</p>

<script>
  mermaid.initialize({
    startOnLoad: true,
    theme: 'dark',
    themeVariables: {
      primaryColor: '#fd5068',
      primaryTextColor: '#e2e4ea',
      primaryBorderColor: '#fd5068',
      lineColor: '#9196a1',
      secondaryColor: '#1a1d27',
      tertiaryColor: '#252833',
      background: '#1a1d27',
      mainBkg: '#1a1d27',
      nodeBorder: '#fd5068',
      clusterBkg: '#1a1d2790',
      clusterBorder: '#2a2d3a',
      titleColor: '#e2e4ea',
      edgeLabelBackground: '#252833',
      fontSize: '14px'
    },
    flowchart: {
      useMaxWidth: true,
      htmlLabels: true,
      curve: 'basis'
    }
  });
</script>

</body>
</html>
