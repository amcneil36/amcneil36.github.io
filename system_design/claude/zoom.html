<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>System Design: Zoom (Video Conferencing)</title>
<script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
<style>
  :root { --bg: #ffffff; --fg: #1a1a2e; --accent: #2563eb; --accent2: #7c3aed; --border: #e2e8f0; --code-bg: #f1f5f9; --card-bg: #f8fafc; --green: #16a34a; --orange: #ea580c; --red: #dc2626; }
  * { box-sizing: border-box; margin: 0; padding: 0; }
  body { font-family: 'Segoe UI', system-ui, -apple-system, sans-serif; color: var(--fg); background: var(--bg); line-height: 1.7; max-width: 1100px; margin: 0 auto; padding: 2rem 2rem 4rem; }
  h1 { font-size: 2.2rem; margin: 0 0 0.5rem; color: var(--accent); border-bottom: 3px solid var(--accent); padding-bottom: 0.5rem; }
  h2 { font-size: 1.6rem; margin: 2.5rem 0 1rem; color: var(--accent2); border-bottom: 2px solid var(--border); padding-bottom: 0.3rem; }
  h3 { font-size: 1.25rem; margin: 1.8rem 0 0.7rem; color: var(--fg); }
  h4 { font-size: 1.05rem; margin: 1.2rem 0 0.5rem; color: #475569; }
  p, li { margin-bottom: 0.5rem; }
  ul, ol { padding-left: 1.5rem; margin-bottom: 1rem; }
  table { width: 100%; border-collapse: collapse; margin: 1rem 0; font-size: 0.92rem; }
  th, td { border: 1px solid var(--border); padding: 0.55rem 0.75rem; text-align: left; }
  th { background: var(--accent); color: #fff; font-weight: 600; }
  tr:nth-child(even) { background: var(--card-bg); }
  code { background: var(--code-bg); padding: 0.15rem 0.4rem; border-radius: 4px; font-size: 0.9em; }
  pre { background: var(--code-bg); padding: 1rem; border-radius: 6px; overflow-x: auto; margin: 1rem 0; }
  .card { background: var(--card-bg); border: 1px solid var(--border); border-radius: 8px; padding: 1.2rem; margin: 1rem 0; }
  .example { background: #eff6ff; border-left: 4px solid var(--accent); padding: 1rem 1.2rem; margin: 1rem 0; border-radius: 0 6px 6px 0; }
  .example strong { color: var(--accent); }
  .warn { background: #fef3c7; border-left: 4px solid var(--orange); padding: 1rem 1.2rem; margin: 1rem 0; border-radius: 0 6px 6px 0; }
  .mermaid { margin: 1.5rem 0; text-align: center; }
  .toc { background: var(--card-bg); border: 1px solid var(--border); border-radius: 8px; padding: 1.5rem 2rem; margin: 1.5rem 0; }
  .toc a { color: var(--accent); text-decoration: none; }
  .toc a:hover { text-decoration: underline; }
  .toc ol { margin-bottom: 0; }
  .toc li { margin-bottom: 0.25rem; }
  .badge { display: inline-block; padding: 0.15rem 0.5rem; border-radius: 4px; font-size: 0.8rem; font-weight: 600; }
  .badge-sql { background: #dbeafe; color: #1d4ed8; }
  .badge-nosql { background: #fce7f3; color: #be185d; }
  .badge-get { background: #dcfce7; color: var(--green); }
  .badge-post { background: #dbeafe; color: var(--accent); }
  .badge-put { background: #fef9c3; color: #a16207; }
  .badge-delete { background: #fee2e2; color: var(--red); }
  .badge-ws { background: #f3e8ff; color: var(--accent2); }
  .badge-udp { background: #ccfbf1; color: #0d9488; }
</style>
</head>
<body>

<h1>System Design: Zoom (Video Conferencing Platform)</h1>
<p><em>Designing a real-time video conferencing system supporting 1:1 calls, group meetings, screen sharing, in-meeting chat, and recording.</em></p>

<!-- TABLE OF CONTENTS -->
<div class="toc">
<strong>Table of Contents</strong>
<ol>
  <li><a href="#fr">Functional Requirements</a></li>
  <li><a href="#nfr">Non-Functional Requirements</a></li>
  <li><a href="#flow1">Flow 1 — Meeting Creation &amp; Scheduling</a></li>
  <li><a href="#flow2">Flow 2 — Joining a Meeting &amp; Real-Time Media</a></li>
  <li><a href="#flow3">Flow 3 — In-Meeting Chat</a></li>
  <li><a href="#flow4">Flow 4 — Screen Sharing</a></li>
  <li><a href="#flow5">Flow 5 — Meeting Recording</a></li>
  <li><a href="#overall">Combined Overall Architecture</a></li>
  <li><a href="#schema">Database Schema</a></li>
  <li><a href="#cache">Caching &amp; CDN Strategy</a></li>
  <li><a href="#scaling">Scaling Considerations</a></li>
  <li><a href="#lb">Load Balancing</a></li>
  <li><a href="#rtc">Real-Time Communication Deep Dive</a></li>
  <li><a href="#protocols">Protocol Decisions</a></li>
  <li><a href="#tradeoffs">Tradeoffs &amp; Deep Dives</a></li>
  <li><a href="#alternatives">Alternative Approaches</a></li>
  <li><a href="#additional">Additional Considerations</a></li>
  <li><a href="#vendors">Vendor Recommendations</a></li>
</ol>
</div>

<!-- ========================================== -->
<h2 id="fr">1. Functional Requirements</h2>
<ul>
  <li><strong>Create &amp; schedule meetings</strong> — A user can create an instant meeting or schedule one for a future time. A unique meeting link/code is generated.</li>
  <li><strong>Join a meeting</strong> — A user can join via meeting link or code, with optional password protection and a waiting room.</li>
  <li><strong>Real-time video &amp; audio</strong> — Participants send and receive live video and audio streams with low latency.</li>
  <li><strong>Screen sharing</strong> — A participant can share their screen (full screen or application window) to all other participants.</li>
  <li><strong>In-meeting text chat</strong> — Participants can send and receive text messages during a meeting.</li>
  <li><strong>Meeting recording</strong> — The host can start/stop cloud recording. The recording is stored and made available for download after the meeting.</li>
  <li><strong>Mute/unmute &amp; video on/off</strong> — Participants and hosts can control their own and (hosts) others' audio/video.</li>
  <li><strong>Host controls</strong> — Admit from waiting room, remove participant, mute all, lock meeting.</li>
  <li><strong>Participant list</strong> — View who is in the meeting.</li>
</ul>

<h2 id="nfr">2. Non-Functional Requirements</h2>
<ul>
  <li><strong>Low latency</strong> — End-to-end media latency under 200 ms for interactive communication. Ideally under 150 ms.</li>
  <li><strong>High availability</strong> — 99.99% uptime. Meetings must not drop unexpectedly.</li>
  <li><strong>Scalability</strong> — Support millions of concurrent meetings and tens of millions of concurrent participants. A single meeting should support up to 1,000 participants (or more for webinar mode).</li>
  <li><strong>Security</strong> — Media encrypted in transit (DTLS/SRTP). Meeting passwords and waiting rooms for access control.</li>
  <li><strong>Cross-platform</strong> — Web (browser), desktop (Windows, macOS, Linux), and mobile (iOS, Android).</li>
  <li><strong>Adaptive quality</strong> — Video quality adapts to participant bandwidth in real time (simulcast / adaptive bitrate).</li>
  <li><strong>Fault tolerance</strong> — If a media server fails, participants can be migrated to a healthy server with minimal disruption.</li>
  <li><strong>Geo-distribution</strong> — Media servers deployed globally so participants connect to the nearest region for lowest latency.</li>
</ul>

<!-- ========================================== -->
<h2 id="flow1">3. Flow 1 — Meeting Creation &amp; Scheduling</h2>

<div class="mermaid">
graph LR
    A["Client App<br/>(Web / Desktop / Mobile)"] -->|"HTTP POST<br/>/api/meetings"| B["API Gateway<br/>+ Load Balancer"]
    B --> C["Meeting Service"]
    C -->|"Write meeting record"| D[("SQL Database<br/>(Meetings)")]
    C -->|"If scheduled: enqueue invite job"| E["Message Queue"]
    E --> F["Notification Service"]
    F -->|"Email / Push notification"| G["Invitees"]
</div>

<h3>Examples</h3>

<div class="example">
<strong>Example 1 — Instant Meeting:</strong> Alice opens the Zoom app and clicks "New Meeting." The client sends an <code>HTTP POST /api/meetings</code> with body <code>{ type: "instant", host_user_id: "alice_123" }</code> to the API Gateway. The API Gateway routes to the Meeting Service, which generates a unique <code>meeting_id</code> and 10-digit <code>meeting_code</code>, writes the meeting record to the SQL Database with <code>status = "active"</code>, and returns the meeting link and code to Alice. No notification is sent since it is instant — Alice shares the link manually.
</div>

<div class="example">
<strong>Example 2 — Scheduled Meeting:</strong> Bob creates a meeting for tomorrow at 3 PM. The client sends <code>HTTP POST /api/meetings</code> with <code>{ type: "scheduled", host_user_id: "bob_456", scheduled_start: "2025-03-15T15:00:00Z", title: "Sprint Planning", invitees: ["carol@co.com", "dave@co.com"], settings: { waiting_room: true } }</code>. The Meeting Service writes the record with <code>status = "scheduled"</code>, then enqueues a notification job on the Message Queue. The Notification Service picks up the job and sends calendar invites via email to Carol and Dave. Fifteen minutes before the meeting, the Notification Service sends push notifications as reminders.
</div>

<h3>Component Deep Dive</h3>

<h4>Client App</h4>
<p>Native apps on iOS, Android, Windows, macOS, and a web app (browser-based via JavaScript). Responsible for capturing user input and rendering the UI. Communicates with the backend via HTTPS for REST calls and WebSocket for real-time signaling.</p>

<h4>API Gateway + Load Balancer</h4>
<p>Entry point for all HTTP REST traffic. Handles TLS termination, authentication (JWT token validation), rate limiting, and routes requests to the appropriate microservice. A load balancer (Layer 7) distributes traffic across multiple instances of each service using round-robin or least-connections algorithms.</p>

<h4>Meeting Service</h4>
<p>Core service managing the lifecycle of meetings: creation, joining, updating settings, ending. Stateless — any instance can handle any request.</p>
<table>
  <tr><th>Endpoint</th><th>Method</th><th>Input</th><th>Output</th></tr>
  <tr><td><code>/api/meetings</code></td><td><span class="badge badge-post">POST</span></td><td>host_user_id, type, title, scheduled_start, invitees, settings</td><td>meeting_id, meeting_code, join_url, status</td></tr>
  <tr><td><code>/api/meetings/:id</code></td><td><span class="badge badge-get">GET</span></td><td>meeting_id (path param), auth token</td><td>Meeting details (title, host, status, settings, SFU address)</td></tr>
  <tr><td><code>/api/meetings/:id</code></td><td><span class="badge badge-put">PUT</span></td><td>meeting_id, updated fields (title, settings, etc.)</td><td>Updated meeting record</td></tr>
  <tr><td><code>/api/meetings/:id/end</code></td><td><span class="badge badge-post">POST</span></td><td>meeting_id, host auth token</td><td>Confirmation, final participant list</td></tr>
</table>

<h4>SQL Database (Meetings)</h4>
<p>Relational database storing structured meeting data. Chosen for strong consistency (ACID), relational joins between meetings and users, and the need for complex queries (e.g., "find all upcoming meetings for user X"). Schema detailed in <a href="#schema">Section 9</a>.</p>

<h4>Message Queue</h4>
<p>Asynchronous job queue decoupling the Meeting Service from the Notification Service. When a scheduled meeting is created, a notification job is enqueued. This ensures the Meeting Service responds quickly to the client without waiting for emails/push notifications to be sent. Messages are enqueued by the Meeting Service (producer) and dequeued by the Notification Service (consumer). Uses at-least-once delivery with idempotent consumers to handle duplicates.</p>

<h4>Notification Service</h4>
<p>Consumes jobs from the Message Queue and sends email invitations (via SMTP / email provider), push notifications (via APNs for iOS, FCM for Android), and in-app notifications. Stateless, horizontally scalable.</p>

<!-- ========================================== -->
<h2 id="flow2">4. Flow 2 — Joining a Meeting &amp; Real-Time Media</h2>

<div class="mermaid">
graph TD
    A["Client App"] -->|"1. HTTP GET /api/meetings/:id<br/>(get meeting info + SFU address)"| B["API Gateway"]
    B --> C["Meeting Service"]
    C -->|"Read"| D[("SQL DB")]
    C -->|"Return meeting info,<br/>assigned SFU address"| A

    A -->|"2. WebSocket connect<br/>(signaling channel)"| E["Signaling Server"]
    E -->|"3. Verify meeting membership"| C

    A -->|"4. STUN binding request<br/>(discover public IP/port)"| F["STUN Server"]
    F -->|"Public IP/port"| A

    E -->|"5. Relay SDP offer from client"| G["SFU<br/>(Media Server)"]
    G -->|"6. SDP answer back via signaling"| E
    E -->|"SDP answer"| A

    A ===|"7. SRTP media streams<br/>over UDP"| G
    G ===|"8. Forward other participants'<br/>streams via SRTP/UDP"| A

    A -.->|"If direct path fails:<br/>relay via"| H["TURN Server"]
    H -.->|"Relayed media"| G
</div>

<h3>Examples</h3>

<div class="example">
<strong>Example 1 — Joining a Group Meeting (Happy Path):</strong> Carol clicks the meeting link. Her client sends <code>HTTP GET /api/meetings/abc123</code> to the API Gateway. The Meeting Service reads from the SQL DB, verifies Carol is an invitee, and returns meeting details including the address of the nearest SFU server (e.g., <code>sfu-us-west-2.internal</code>). Carol's client then opens a WebSocket connection to the Signaling Server and sends a <code>join</code> message with her meeting_id and auth token. The Signaling Server verifies with the Meeting Service. Next, Carol's client contacts a STUN server to discover her public IP:port for NAT traversal. The client generates an SDP offer (describing its media capabilities — codecs like VP9/Opus, ICE candidates) and sends it to the Signaling Server, which relays it to the SFU. The SFU responds with an SDP answer via the Signaling Server. A DTLS handshake occurs, and Carol's client begins sending her video/audio via SRTP over UDP to the SFU. The SFU simultaneously starts forwarding existing participants' streams (Dave, Eve) to Carol over SRTP/UDP. The Signaling Server notifies Dave and Eve that Carol has joined.
</div>

<div class="example">
<strong>Example 2 — NAT/Firewall Traversal via TURN:</strong> Frank is behind a strict corporate firewall that blocks UDP. His STUN request fails to establish a direct path to the SFU. The client falls back to a TURN server, which acts as a media relay. Frank's media is sent to the TURN server (over TCP or TLS if UDP is fully blocked), and the TURN server relays it to the SFU over UDP. This adds ~30-50ms latency but ensures connectivity.
</div>

<div class="example">
<strong>Example 3 — Waiting Room:</strong> Grace clicks the join link for a meeting with <code>waiting_room: true</code>. The Signaling Server places Grace in a "waiting" state — her WebSocket is connected but the SFU connection is NOT established. The host (Alice) receives a notification via WebSocket: "Grace is waiting." Alice clicks "Admit." The Meeting Service updates Grace's participant status, and the Signaling Server signals Grace's client to proceed with the SDP/media exchange with the SFU.
</div>

<h3>Component Deep Dive</h3>

<h4>Signaling Server</h4>
<p>Maintains persistent <strong>WebSocket</strong> connections with all active meeting participants. Responsibilities:</p>
<ul>
  <li>Relaying SDP offers/answers between clients and the SFU for WebRTC handshake.</li>
  <li>Exchanging ICE candidates.</li>
  <li>Sending real-time control events (participant joined/left, mute/unmute, waiting room admit, host controls).</li>
</ul>
<p><strong>Connection establishment:</strong> The client sends an HTTP Upgrade request to the Signaling Server. Upon successful upgrade, a persistent bidirectional WebSocket connection is established. The Signaling Server stores the connection in an in-memory connection map keyed by <code>(meeting_id, user_id)</code>.</p>
<p><strong>Horizontal scaling:</strong> To scale signaling across multiple server instances, a Pub/Sub system is used. Each Signaling Server subscribes to channels for the meetings it has active connections for. When Server A needs to send a message to a user connected to Server B, it publishes the message to the pub/sub channel, and Server B picks it up and delivers it via the local WebSocket. A consistent hashing or meeting-id-based routing strategy at the load balancer level helps minimize cross-server communication.</p>

<table>
  <tr><th>WebSocket Event</th><th>Direction</th><th>Payload</th></tr>
  <tr><td><code>join</code></td><td>Client → Server</td><td>meeting_id, auth_token</td></tr>
  <tr><td><code>sdp_offer</code></td><td>Client → Server</td><td>SDP offer object</td></tr>
  <tr><td><code>sdp_answer</code></td><td>Server → Client</td><td>SDP answer object</td></tr>
  <tr><td><code>ice_candidate</code></td><td>Bidirectional</td><td>ICE candidate object</td></tr>
  <tr><td><code>participant_joined</code></td><td>Server → Client</td><td>user_id, display_name</td></tr>
  <tr><td><code>participant_left</code></td><td>Server → Client</td><td>user_id</td></tr>
  <tr><td><code>mute_request</code></td><td>Server → Client</td><td>user_id, media_type (audio/video)</td></tr>
  <tr><td><code>waiting_room_admit</code></td><td>Server → Client</td><td>user_id</td></tr>
</table>

<h4>SFU (Selective Forwarding Unit) — Media Server</h4>
<p>The core real-time media component. Each participant uploads <strong>one</strong> set of media streams (audio + video) to the SFU, and the SFU selectively forwards each participant's stream to every other participant. The SFU does <strong>not</strong> transcode or mix streams (unlike an MCU), which dramatically reduces CPU usage and allows better scalability.</p>
<ul>
  <li><strong>Simulcast support:</strong> The client simultaneously sends multiple quality levels of the video (e.g., 720p, 360p, 180p). The SFU decides which quality level to forward to each receiver based on their available bandwidth, viewport size (e.g., pinned speaker gets high quality, gallery thumbnails get low quality).</li>
  <li><strong>Bandwidth estimation:</strong> The SFU uses REMB (Receiver Estimated Maximum Bitrate) or transport-cc feedback to adapt forwarding decisions in real time.</li>
  <li><strong>Protocol:</strong> SRTP (Secure Real-Time Transport Protocol) over UDP. DTLS is used for key exchange to establish SRTP encryption.</li>
  <li><strong>Scaling:</strong> One SFU instance handles meetings assigned to it. For large meetings (100+ participants), a <strong>cascaded SFU</strong> architecture is used where multiple SFU nodes share the load and forward streams between each other.</li>
</ul>

<h4>STUN Server</h4>
<p>Lightweight server that responds to STUN binding requests from clients. The client sends a UDP packet to the STUN server, which responds with the client's public IP address and port as seen from the internet. This information (a "server-reflexive candidate") is included in the client's ICE candidates during the WebRTC handshake. STUN is fast — a single round-trip — and stateless.</p>

<h4>TURN Server</h4>
<p>Fallback relay server for clients that cannot establish a direct UDP path to the SFU (due to symmetric NATs, corporate firewalls, etc.). The TURN server relays all media traffic between the client and SFU. It supports UDP, TCP, and TLS transport. TURN servers are resource-intensive (they relay all media bytes) so they should be provisioned generously and geo-distributed. Approximately 10-15% of connections require TURN relay in typical enterprise environments.</p>

<!-- ========================================== -->
<h2 id="flow3">5. Flow 3 — In-Meeting Chat</h2>

<div class="mermaid">
graph LR
    A["Sender Client"] -->|"WebSocket: chat_message"| B["Signaling Server"]
    B --> C["Chat Service"]
    C -->|"Write"| D[("NoSQL Database<br/>(Chat Messages)")]
    C -->|"Broadcast via signaling"| B
    B -->|"WebSocket: chat_message"| E["All Other Participants"]
</div>

<h3>Examples</h3>

<div class="example">
<strong>Example 1 — Normal Chat Message:</strong> During a meeting, Dave types "Can everyone see my screen?" and hits send. The client sends a <code>chat_message</code> event over the existing WebSocket connection to the Signaling Server with payload <code>{ meeting_id: "abc123", sender: "dave", content: "Can everyone see my screen?", timestamp: "2025-03-15T15:05:32Z" }</code>. The Signaling Server forwards this to the Chat Service, which writes the message to the NoSQL database (partitioned by <code>meeting_id</code>, sorted by <code>timestamp</code>). The Chat Service then instructs the Signaling Server to broadcast the message to all other participants in the meeting via their WebSocket connections. Carol, Eve, and Alice all see the message appear in real time.
</div>

<div class="example">
<strong>Example 2 — Chat History Retrieval:</strong> Frank joins the meeting 10 minutes late. Upon joining, his client sends <code>HTTP GET /api/meetings/abc123/chat?since=0</code> to fetch the chat history. The Chat Service reads all messages for <code>meeting_id = "abc123"</code> from the NoSQL database and returns them ordered by timestamp. Frank's client renders the full chat history.
</div>

<h3>Component Deep Dive</h3>

<h4>Chat Service</h4>
<p>Handles persistence and retrieval of in-meeting chat messages. Receives messages from the Signaling Server (internal RPC), writes them to the NoSQL database, and triggers broadcast back through the Signaling Server. Also exposes an HTTP endpoint for chat history retrieval.</p>

<table>
  <tr><th>Endpoint / Event</th><th>Method</th><th>Input</th><th>Output</th></tr>
  <tr><td><code>chat_message</code> (via WebSocket)</td><td><span class="badge badge-ws">WS</span></td><td>meeting_id, sender_id, content, timestamp</td><td>Broadcast to all participants</td></tr>
  <tr><td><code>/api/meetings/:id/chat</code></td><td><span class="badge badge-get">GET</span></td><td>meeting_id, since (timestamp), limit</td><td>Array of chat messages</td></tr>
</table>

<h4>NoSQL Database (Chat Messages)</h4>
<p>Wide-column / document store optimized for high write throughput and time-ordered retrieval. Partitioned by <code>meeting_id</code> so all messages for a meeting are co-located. Sort key is <code>timestamp</code> for efficient range queries. NoSQL chosen because chat messages are append-only, don't require complex joins, and need high write throughput during active meetings with many participants. Schema detailed in <a href="#schema">Section 9</a>.</p>

<!-- ========================================== -->
<h2 id="flow4">6. Flow 4 — Screen Sharing</h2>

<div class="mermaid">
graph LR
    A["Presenter Client"] -->|"1. WebSocket: screen_share_start"| B["Signaling Server"]
    B -->|"2. Notify all participants"| C["Other Clients"]
    A -->|"3. New SDP offer<br/>(screen share track)"| B
    B -->|"Relay to SFU"| D["SFU<br/>(Media Server)"]
    D -->|"SDP answer"| B
    B -->|"SDP answer"| A
    A ===|"4. SRTP screen share<br/>stream over UDP"| D
    D ===|"5. Forward screen share<br/>stream via SRTP/UDP"| C
</div>

<h3>Examples</h3>

<div class="example">
<strong>Example 1 — Starting Screen Share:</strong> Eve clicks "Share Screen" and selects her browser window. The client sends a <code>screen_share_start</code> event via WebSocket to the Signaling Server, which broadcasts a notification to all other participants so their UIs can prepare (e.g., switch to screen share layout). Eve's client then initiates a new media track negotiation — it generates a new SDP offer that includes the screen capture video track (in addition to her existing camera/audio tracks). This SDP offer is relayed via the Signaling Server to the SFU, which responds with an SDP answer. Eve's client begins sending the screen share stream over SRTP/UDP to the SFU, which forwards it to all other participants. The screen share is typically sent at a higher resolution but lower frame rate (e.g., 1080p at 5-10 fps) for clarity of text/UI content.
</div>

<div class="example">
<strong>Example 2 — Stopping Screen Share:</strong> Eve clicks "Stop Sharing." The client sends a <code>screen_share_stop</code> event via WebSocket. The client terminates the screen share media track. The SFU stops forwarding that stream. The Signaling Server notifies all participants to revert their layout back to the normal video grid.
</div>

<h3>Component Deep Dive</h3>
<p>Screen sharing reuses the same infrastructure as video/audio (the Signaling Server and SFU). The key difference is:</p>
<ul>
  <li><strong>Media capture:</strong> Instead of a camera, the client uses the <code>getDisplayMedia()</code> browser API (or native OS screen capture API on desktop) to capture screen content.</li>
  <li><strong>Codec optimization:</strong> Screen share content (text, UI) benefits from codecs that handle sharp edges well. VP9 or AV1 with screen-content coding tools are preferred over H.264 for this use case.</li>
  <li><strong>Bandwidth:</strong> Screen share streams are sent alongside camera streams. The SFU manages bandwidth allocation, often prioritizing screen share quality over camera thumbnails during active screen sharing.</li>
  <li><strong>One active screen share:</strong> Typically only one participant can share at a time. The Signaling Server enforces this — if another participant tries to share while one is active, the host is prompted or the new share replaces the old one.</li>
</ul>

<!-- ========================================== -->
<h2 id="flow5">7. Flow 5 — Meeting Recording</h2>

<div class="mermaid">
graph TD
    A["Host Client"] -->|"1. HTTP POST<br/>/api/meetings/:id/recording/start"| B["API Gateway"]
    B --> C["Meeting Service"]
    C -->|"2. Trigger recording"| D["Recording Service"]
    D -->|"3. Connect to SFU as<br/>receive-only participant"| E["SFU<br/>(Media Server)"]
    E -->|"4. Forward all participant<br/>streams via SRTP/UDP"| D
    D -->|"5. Mix, compose,<br/>encode to video file"| D
    D -->|"6. Upload finished<br/>recording"| F[("Object Storage")]
    D -->|"7. Update recording URL"| G[("SQL Database")]
    C -->|"8. Notify host"| H["Notification Service"]
    H -->|"Email with<br/>recording link"| A
</div>

<h3>Examples</h3>

<div class="example">
<strong>Example 1 — Starting and Completing Recording:</strong> Alice (host) clicks "Record to Cloud." Her client sends <code>HTTP POST /api/meetings/abc123/recording/start</code> to the API Gateway. The Meeting Service instructs the Recording Service to begin. The Recording Service connects to the SFU as a special receive-only participant — it does not send any media, only receives all participants' audio/video streams. The Recording Service mixes the audio tracks and composes the video tracks into a grid layout (or active-speaker layout), encoding the result in real time (e.g., H.264 + AAC into an MP4 container). When Alice ends the meeting (or clicks "Stop Recording"), the Recording Service finalizes the file, uploads it to Object Storage, and updates the meeting record in the SQL Database with the <code>recording_url</code>. The Notification Service then emails Alice a link to the recording.
</div>

<div class="example">
<strong>Example 2 — Recording Failure Recovery:</strong> During recording, the Recording Service instance crashes. The Meeting Service detects the failure (via heartbeat) and spins up a new Recording Service instance, which reconnects to the SFU. There will be a gap of a few seconds in the recording. The meeting itself is unaffected since the Recording Service is an independent consumer of media streams.
</div>

<h3>Component Deep Dive</h3>

<h4>Recording Service</h4>
<p>A specialized media consumer that joins the SFU as a receive-only participant. It receives all participants' raw media streams, then uses server-side media processing to compose them into a single recording.</p>
<ul>
  <li><strong>Processing:</strong> Audio mixing (combining all audio tracks) + video compositing (arranging video tiles into a grid layout). Encodes using H.264 (video) and AAC (audio) into MP4.</li>
  <li><strong>Resource-intensive:</strong> Requires significant CPU/GPU for real-time encoding. Typically runs on dedicated compute instances.</li>
  <li><strong>Output:</strong> The final video file is uploaded to Object Storage. Metadata (URL, duration, file size) is written to the SQL database.</li>
</ul>

<table>
  <tr><th>Endpoint</th><th>Method</th><th>Input</th><th>Output</th></tr>
  <tr><td><code>/api/meetings/:id/recording/start</code></td><td><span class="badge badge-post">POST</span></td><td>meeting_id, host auth token</td><td>recording_id, status: "recording"</td></tr>
  <tr><td><code>/api/meetings/:id/recording/stop</code></td><td><span class="badge badge-post">POST</span></td><td>meeting_id, host auth token</td><td>recording_id, status: "processing"</td></tr>
  <tr><td><code>/api/recordings/:id</code></td><td><span class="badge badge-get">GET</span></td><td>recording_id, auth token</td><td>recording_url, duration, status</td></tr>
</table>

<h4>Object Storage</h4>
<p>Blob/object storage for meeting recording files (MP4 video). Chosen for its scalability, durability (11 nines), and cost-effectiveness for large binary files. Recordings are accessed infrequently after creation, making object storage ideal. A CDN can be placed in front for faster downloads.</p>

<!-- ========================================== -->
<h2 id="overall">8. Combined Overall Architecture</h2>

<div class="mermaid">
graph TD
    subgraph Clients
        CL["Client Apps<br/>(Web / Desktop / Mobile)"]
    end

    subgraph Edge
        CDN["CDN<br/>(Static assets, recordings)"]
        LB["Load Balancer<br/>(L7)"]
        STUN["STUN Server"]
        TURN["TURN Server"]
    end

    subgraph API_Layer["API Layer"]
        AG["API Gateway"]
    end

    subgraph Services
        MS["Meeting Service"]
        CS["Chat Service"]
        RS["Recording Service"]
        NS["Notification Service"]
        US["User Service"]
    end

    subgraph Real_Time["Real-Time Layer"]
        SS["Signaling Server<br/>(WebSocket)"]
        PS["Pub/Sub<br/>(cross-server signaling)"]
        SFU["SFU Media Server<br/>(Selective Forwarding Unit)"]
    end

    subgraph Data_Layer["Data Layer"]
        SQL[("SQL Database<br/>(Users, Meetings,<br/>Participants, Recordings)")]
        NOSQL[("NoSQL Database<br/>(Chat Messages)")]
        OBJ[("Object Storage<br/>(Recording Files)")]
        CACHE["In-Memory Cache"]
        MQ["Message Queue"]
    end

    CL -->|"HTTPS REST"| LB
    CL -->|"WebSocket"| SS
    CL ===|"SRTP/UDP"| SFU
    CL -->|"STUN"| STUN
    CL -.->|"TURN relay"| TURN
    TURN -.->|"Relay to SFU"| SFU
    CL -->|"Download assets /<br/>recordings"| CDN

    LB --> AG
    AG --> MS
    AG --> CS
    AG --> US

    MS --> SQL
    MS --> CACHE
    MS --> MQ
    US --> SQL
    US --> CACHE
    CS --> NOSQL

    SS --> PS
    SS <--> MS
    SS <--> CS
    SS <--> SFU

    RS --> SFU
    RS --> OBJ
    RS --> SQL

    MQ --> NS
    MQ --> RS

    CDN --> OBJ
</div>

<h3>Examples (End-to-End Walkthrough)</h3>

<div class="example">
<strong>Full Lifecycle — Scheduled Meeting with Chat, Screen Share, and Recording:</strong><br/><br/>
<strong>1. Creation:</strong> Bob creates a scheduled meeting via <code>HTTP POST /api/meetings</code> → Load Balancer → API Gateway → Meeting Service → SQL DB. Notification jobs are enqueued in the Message Queue. The Notification Service sends email invites to Carol and Dave.<br/><br/>
<strong>2. Joining:</strong> At the scheduled time, Carol clicks the meeting link. <code>HTTP GET /api/meetings/:id</code> returns meeting info and an assigned SFU address (the Meeting Service reads from Cache, or on cache miss from SQL DB). Carol's client opens a WebSocket to the Signaling Server, performs a STUN check, exchanges SDP via the Signaling Server with the SFU, and begins sending/receiving SRTP media streams over UDP with the SFU. Dave and Bob join similarly. Each participant uploads one stream to the SFU and downloads N-1 streams.<br/><br/>
<strong>3. Chat:</strong> Carol sends "Hello everyone!" via WebSocket → Signaling Server → Chat Service → NoSQL DB. Chat Service broadcasts the message to Bob and Dave via the Signaling Server's WebSocket connections.<br/><br/>
<strong>4. Screen Sharing:</strong> Dave clicks "Share Screen." A <code>screen_share_start</code> event goes through the Signaling Server to notify Bob and Carol. Dave's client captures his screen, negotiates a new media track with the SFU (SDP renegotiation via Signaling Server), and starts sending the screen share stream over SRTP/UDP to the SFU, which forwards it to Bob and Carol.<br/><br/>
<strong>5. Recording:</strong> Bob (host) clicks "Record." <code>HTTP POST /api/meetings/:id/recording/start</code> → Meeting Service → Recording Service. The Recording Service connects to the SFU as a receive-only participant, receives all streams, mixes and encodes them in real time.<br/><br/>
<strong>6. End:</strong> Bob clicks "End Meeting." <code>HTTP POST /api/meetings/:id/end</code> → Meeting Service. The Meeting Service notifies the Signaling Server, which sends a <code>meeting_ended</code> event to all clients. WebSocket connections are closed. The SFU releases resources. The Recording Service finalizes the recording, uploads to Object Storage, and updates the SQL DB with the recording URL. The Notification Service emails Bob a link to the recording. Carol downloads the recording later — the CDN serves the file from Object Storage.
</div>

<!-- ========================================== -->
<h2 id="schema">9. Database Schema</h2>

<h3>SQL Tables</h3>

<h4>9.1 <code>users</code> <span class="badge badge-sql">SQL</span></h4>
<table>
  <tr><th>Column</th><th>Type</th><th>Constraints</th><th>Description</th></tr>
  <tr><td><code>user_id</code></td><td>UUID</td><td><strong>Primary Key</strong></td><td>Unique user identifier</td></tr>
  <tr><td><code>email</code></td><td>VARCHAR(255)</td><td>UNIQUE, NOT NULL</td><td>User's email address</td></tr>
  <tr><td><code>username</code></td><td>VARCHAR(100)</td><td>UNIQUE</td><td>Display username</td></tr>
  <tr><td><code>password_hash</code></td><td>VARCHAR(255)</td><td>NOT NULL</td><td>Hashed password (bcrypt/argon2)</td></tr>
  <tr><td><code>display_name</code></td><td>VARCHAR(255)</td><td></td><td>Name shown in meetings</td></tr>
  <tr><td><code>avatar_url</code></td><td>VARCHAR(512)</td><td></td><td>Profile picture URL</td></tr>
  <tr><td><code>created_at</code></td><td>TIMESTAMP</td><td>NOT NULL</td><td>Account creation time</td></tr>
  <tr><td><code>updated_at</code></td><td>TIMESTAMP</td><td>NOT NULL</td><td>Last update time</td></tr>
</table>
<p><strong>Why SQL:</strong> User data is highly structured and relational. ACID transactions ensure data integrity for account operations (e.g., email uniqueness). Supports complex queries like "find user by email." User data is read frequently (profile lookups) and written infrequently (account creation/updates), which suits SQL's read-optimized B-tree indexes.</p>
<p><strong>Indexes:</strong></p>
<ul>
  <li><code>email</code> — <strong>Hash index</strong> on <code>email</code> for O(1) lookup during login authentication. Email lookups are equality-based (exact match), making a hash index ideal.</li>
  <li><code>username</code> — <strong>B-tree index</strong> on <code>username</code> for both equality lookups and prefix-based search (e.g., autocomplete when inviting users).</li>
</ul>
<p><strong>Read events:</strong> User login (email lookup), joining a meeting (profile display), sending a meeting invite (user search).</p>
<p><strong>Write events:</strong> Account registration, profile update.</p>
<p><strong>Sharding:</strong> Shard by <code>user_id</code> using consistent hashing. User data is accessed independently per user, making user_id a natural partition key that distributes load evenly.</p>

<h4>9.2 <code>meetings</code> <span class="badge badge-sql">SQL</span></h4>
<table>
  <tr><th>Column</th><th>Type</th><th>Constraints</th><th>Description</th></tr>
  <tr><td><code>meeting_id</code></td><td>UUID</td><td><strong>Primary Key</strong></td><td>Unique meeting identifier</td></tr>
  <tr><td><code>host_user_id</code></td><td>UUID</td><td><strong>Foreign Key → users.user_id</strong>, NOT NULL</td><td>The meeting host</td></tr>
  <tr><td><code>title</code></td><td>VARCHAR(500)</td><td></td><td>Meeting title</td></tr>
  <tr><td><code>meeting_code</code></td><td>VARCHAR(20)</td><td>UNIQUE, NOT NULL</td><td>Human-readable join code (e.g., "123-456-7890")</td></tr>
  <tr><td><code>password_hash</code></td><td>VARCHAR(255)</td><td></td><td>Optional meeting password</td></tr>
  <tr><td><code>status</code></td><td>ENUM</td><td>NOT NULL</td><td>'scheduled', 'active', 'ended'</td></tr>
  <tr><td><code>scheduled_start</code></td><td>TIMESTAMP</td><td></td><td>Planned start time (null if instant)</td></tr>
  <tr><td><code>scheduled_end</code></td><td>TIMESTAMP</td><td></td><td>Planned end time</td></tr>
  <tr><td><code>actual_start</code></td><td>TIMESTAMP</td><td></td><td>When the meeting actually started</td></tr>
  <tr><td><code>actual_end</code></td><td>TIMESTAMP</td><td></td><td>When the meeting actually ended</td></tr>
  <tr><td><code>settings</code></td><td>JSON</td><td></td><td>Waiting room, mute on entry, etc.</td></tr>
  <tr><td><code>sfu_server_id</code></td><td>VARCHAR(255)</td><td></td><td>Assigned SFU server address (set when meeting becomes active)</td></tr>
  <tr><td><code>created_at</code></td><td>TIMESTAMP</td><td>NOT NULL</td><td>Record creation time</td></tr>
  <tr><td><code>updated_at</code></td><td>TIMESTAMP</td><td>NOT NULL</td><td>Last update time</td></tr>
</table>
<p><strong>Why SQL:</strong> Meeting data is structured and relational (references users). ACID guarantees are needed — for example, ensuring <code>meeting_code</code> uniqueness and atomic status transitions (scheduled → active → ended). We query meetings by host, by code, and by time range.</p>
<p><strong>Indexes:</strong></p>
<ul>
  <li><code>meeting_code</code> — <strong>Hash index</strong> for O(1) lookup when a user joins via meeting code. This is the most frequent read path.</li>
  <li><code>(host_user_id, scheduled_start)</code> — <strong>Composite B-tree index</strong> for querying "all upcoming meetings for a user" sorted by time. B-tree supports the range scan on <code>scheduled_start</code> after filtering by <code>host_user_id</code>.</li>
  <li><code>status</code> — <strong>B-tree index</strong> for filtering active meetings (used by admin dashboards and monitoring).</li>
</ul>
<p><strong>Read events:</strong> User clicking a join link (meeting_code lookup), user viewing "My Meetings" page (host_user_id query), SFU assignment during join.</p>
<p><strong>Write events:</strong> Meeting creation, meeting start (status → active, actual_start set), meeting end (status → ended, actual_end set), SFU assignment.</p>
<p><strong>Sharding:</strong> Shard by <code>meeting_id</code> using consistent hashing. Most meeting operations are keyed on meeting_id. The <code>meeting_code</code> lookup requires a global secondary index or a code-to-meeting_id mapping table that can be cached aggressively.</p>

<h4>9.3 <code>meeting_participants</code> <span class="badge badge-sql">SQL</span></h4>
<table>
  <tr><th>Column</th><th>Type</th><th>Constraints</th><th>Description</th></tr>
  <tr><td><code>participant_id</code></td><td>UUID</td><td><strong>Primary Key</strong></td><td>Unique record ID</td></tr>
  <tr><td><code>meeting_id</code></td><td>UUID</td><td><strong>Foreign Key → meetings.meeting_id</strong>, NOT NULL</td><td>Which meeting</td></tr>
  <tr><td><code>user_id</code></td><td>UUID</td><td><strong>Foreign Key → users.user_id</strong>, NULLABLE</td><td>Null for guest users</td></tr>
  <tr><td><code>display_name</code></td><td>VARCHAR(255)</td><td>NOT NULL</td><td>Name shown in meeting</td></tr>
  <tr><td><code>role</code></td><td>ENUM</td><td>NOT NULL</td><td>'host', 'co_host', 'participant'</td></tr>
  <tr><td><code>join_time</code></td><td>TIMESTAMP</td><td></td><td>When the user joined</td></tr>
  <tr><td><code>leave_time</code></td><td>TIMESTAMP</td><td></td><td>When the user left</td></tr>
  <tr><td><code>status</code></td><td>ENUM</td><td>NOT NULL</td><td>'waiting', 'in_meeting', 'left'</td></tr>
</table>
<p><strong>Why SQL:</strong> Join table linking meetings and users. Relational integrity (foreign keys to meetings and users) is important. Queries like "who is in meeting X?" and "which meetings has user Y attended?" benefit from SQL joins.</p>
<p><strong>Denormalization note:</strong> <code>display_name</code> is denormalized here (duplicated from <code>users.display_name</code>) because guest users (no user_id) still need a name, and it avoids a join to the users table on every participant list query — a hot read path during active meetings.</p>
<p><strong>Indexes:</strong></p>
<ul>
  <li><code>(meeting_id, status)</code> — <strong>Composite B-tree index</strong> for querying "all active participants in meeting X." This is the most frequent query during an active meeting (participant list updates).</li>
  <li><code>(user_id, join_time)</code> — <strong>Composite B-tree index</strong> for querying "meeting history for user Y," sorted by time.</li>
</ul>
<p><strong>Read events:</strong> Participant list display during meeting, meeting history page for user, host admitting from waiting room.</p>
<p><strong>Write events:</strong> User joining (insert), user admitted from waiting room (status update), user leaving (update leave_time + status).</p>
<p><strong>Sharding:</strong> Shard by <code>meeting_id</code> to co-locate all participants of the same meeting on the same shard. This optimizes the most common query pattern (list participants in a meeting).</p>

<h4>9.4 <code>recordings</code> <span class="badge badge-sql">SQL</span></h4>
<table>
  <tr><th>Column</th><th>Type</th><th>Constraints</th><th>Description</th></tr>
  <tr><td><code>recording_id</code></td><td>UUID</td><td><strong>Primary Key</strong></td><td>Unique recording identifier</td></tr>
  <tr><td><code>meeting_id</code></td><td>UUID</td><td><strong>Foreign Key → meetings.meeting_id</strong>, NOT NULL</td><td>Associated meeting</td></tr>
  <tr><td><code>storage_path</code></td><td>VARCHAR(1024)</td><td></td><td>Object storage path/key</td></tr>
  <tr><td><code>duration_seconds</code></td><td>INTEGER</td><td></td><td>Recording duration</td></tr>
  <tr><td><code>file_size_bytes</code></td><td>BIGINT</td><td></td><td>File size</td></tr>
  <tr><td><code>status</code></td><td>ENUM</td><td>NOT NULL</td><td>'recording', 'processing', 'ready', 'failed'</td></tr>
  <tr><td><code>created_at</code></td><td>TIMESTAMP</td><td>NOT NULL</td><td>When recording started</td></tr>
  <tr><td><code>completed_at</code></td><td>TIMESTAMP</td><td></td><td>When recording finished</td></tr>
</table>
<p><strong>Why SQL:</strong> Recording metadata is relational (linked to meetings), structured, and requires consistent status transitions (recording → processing → ready/failed). The volume of recording records is relatively low compared to chat messages, so SQL handles it well.</p>
<p><strong>Indexes:</strong></p>
<ul>
  <li><code>meeting_id</code> — <strong>B-tree index</strong> for querying recordings by meeting.</li>
  <li><code>(status, created_at)</code> — <strong>Composite B-tree index</strong> for monitoring/admin queries like "all recordings still processing."</li>
</ul>
<p><strong>Read events:</strong> User accessing recording link, admin monitoring recording pipeline.</p>
<p><strong>Write events:</strong> Recording started (insert), recording completed (update status + storage_path + duration + file_size), recording failure (update status).</p>

<h3>NoSQL Tables</h3>

<h4>9.5 <code>chat_messages</code> <span class="badge badge-nosql">NoSQL (Wide-Column)</span></h4>
<table>
  <tr><th>Column</th><th>Type</th><th>Key</th><th>Description</th></tr>
  <tr><td><code>meeting_id</code></td><td>UUID</td><td><strong>Partition Key</strong></td><td>Groups all messages per meeting</td></tr>
  <tr><td><code>timestamp</code></td><td>TIMESTAMP</td><td><strong>Sort Key</strong></td><td>Message time (for ordering)</td></tr>
  <tr><td><code>message_id</code></td><td>UUID</td><td></td><td>Unique message identifier</td></tr>
  <tr><td><code>sender_id</code></td><td>UUID</td><td></td><td>User who sent the message</td></tr>
  <tr><td><code>sender_name</code></td><td>STRING</td><td></td><td>Display name (denormalized)</td></tr>
  <tr><td><code>content</code></td><td>TEXT</td><td></td><td>Message text</td></tr>
  <tr><td><code>message_type</code></td><td>STRING</td><td></td><td>'text', 'file', 'reaction'</td></tr>
</table>
<p><strong>Why NoSQL:</strong> Chat messages during a meeting generate extremely bursty, high-throughput writes (imagine 500 participants all chatting simultaneously). The access pattern is simple and predictable: write a message (append), read all messages for a meeting (range scan by timestamp). No joins are needed. Wide-column NoSQL stores excel at this time-series-like pattern with partition + sort key design.</p>
<p><strong>Denormalization:</strong> <code>sender_name</code> is denormalized from the users table to avoid a join on every message read. Since display names rarely change and chat messages are ephemeral (deleted after some retention period), the cost of stale names is negligible.</p>
<p><strong>Sharding:</strong> Automatic partitioning by <code>meeting_id</code> (the partition key). Each meeting's messages reside on the same partition for efficient retrieval. Since meetings are independent, this provides excellent write distribution across the cluster.</p>
<p><strong>Read events:</strong> Late joiner fetching chat history, user scrolling through past messages.</p>
<p><strong>Write events:</strong> Participant sending a chat message.</p>
<p><strong>Retention:</strong> Messages for ended meetings can be purged after a configurable retention period (e.g., 30 days) using TTL (time-to-live) on the NoSQL records.</p>

<!-- ========================================== -->
<h2 id="cache">10. Caching &amp; CDN Strategy</h2>

<h3>CDN (Content Delivery Network)</h3>
<p>A CDN is <strong>appropriate and essential</strong> for the following:</p>
<ul>
  <li><strong>Static assets:</strong> JavaScript bundles, CSS, images, fonts for the web client. These are served from CDN edge nodes worldwide, reducing page load time.</li>
  <li><strong>Meeting recordings:</strong> After a meeting is recorded and uploaded to Object Storage, the CDN fronts the Object Storage to serve recording downloads with low latency globally. Recordings are large files (hundreds of MB to GB) that benefit from edge caching and geographic distribution.</li>
</ul>
<p>A CDN is <strong>NOT appropriate</strong> for:</p>
<ul>
  <li><strong>Live media streams:</strong> Real-time video/audio goes through the SFU, not a CDN. CDNs add caching latency that is unacceptable for interactive communication (&lt;200ms requirement). CDNs are designed for one-to-many distribution of the same content, while a video meeting has unique per-participant streams.</li>
</ul>

<h3>In-Memory Cache</h3>
<div class="card">
<h4>What is cached</h4>
<table>
  <tr><th>Cache Key</th><th>Cached Data</th><th>TTL</th><th>Rationale</th></tr>
  <tr><td><code>meeting:{meeting_code}</code></td><td>meeting_id, status, settings, sfu_server_id</td><td>2 minutes</td><td>Hot path: every user joining a meeting does a meeting_code lookup. Short TTL because meeting status changes (active → ended).</td></tr>
  <tr><td><code>meeting:{meeting_id}</code></td><td>Full meeting details</td><td>5 minutes</td><td>Read during join flow, participant list updates, etc.</td></tr>
  <tr><td><code>user:{user_id}</code></td><td>User profile (display_name, avatar_url)</td><td>30 minutes</td><td>Profiles are read often (displayed in participant lists) and change rarely. Longer TTL is safe.</td></tr>
</table>

<h4>Caching Strategy: Cache-Aside (Lazy Loading)</h4>
<p><strong>Why cache-aside:</strong> The application first checks the cache. On a cache miss, it reads from the database, writes the result to the cache, and returns it. This strategy is chosen because:</p>
<ul>
  <li>Only data that is actually requested gets cached (no wasted cache space on unused meetings).</li>
  <li>The cache is eventually consistent with the database, which is acceptable for meeting metadata (a 2-minute stale window is fine — the Signaling Server handles real-time state).</li>
  <li>Simpler to implement than write-through for a read-heavy workload.</li>
</ul>
<p><strong>Why NOT write-through:</strong> Write-through caches every write, which is wasteful since many meetings are created but only a subset are actively joined (and thus need to be in cache). Write-through also adds latency to write operations.</p>

<h4>Eviction Policy: LRU (Least Recently Used)</h4>
<p>LRU is chosen because recent meetings are the most likely to be accessed again (active meetings are looked up repeatedly by joining participants). Old/ended meetings naturally fall out of the cache as they stop being accessed.</p>

<h4>Expiration Policy: TTL-Based</h4>
<p>Each cache entry has a time-to-live as specified above. TTLs ensure that stale data doesn't linger after a meeting status changes (e.g., ended meetings are evicted within 2-5 minutes). The TTL values are tuned to balance freshness (shorter TTL) against database load reduction (longer TTL).</p>

<h4>Cache Invalidation</h4>
<p>For critical state changes (e.g., meeting ended), the Meeting Service explicitly invalidates the cache entry in addition to relying on TTL. This prevents users from joining an ended meeting due to stale cache.</p>
</div>

<!-- ========================================== -->
<h2 id="scaling">11. Scaling Considerations</h2>

<h3>Stateless Service Scaling</h3>
<p>The Meeting Service, Chat Service, User Service, Notification Service, and Recording Service are all <strong>stateless</strong>. They can be horizontally scaled by adding more instances behind load balancers. Auto-scaling based on CPU utilization and request rate.</p>

<h3>Signaling Server Scaling</h3>
<p>Signaling Servers hold WebSocket connections (stateful). Scaling strategy:</p>
<ul>
  <li>Multiple Signaling Server instances behind a <strong>Layer 4 load balancer</strong> with sticky sessions (or meeting-ID-based routing via consistent hashing).</li>
  <li>Cross-instance communication via <strong>Pub/Sub</strong>: When a signaling event needs to reach a user on a different Signaling Server instance, the event is published to a Pub/Sub channel (keyed by <code>meeting_id</code>). All Signaling Server instances subscribed to that meeting's channel receive and deliver the event.</li>
  <li>Target: ~10,000–50,000 concurrent WebSocket connections per instance (tunable based on hardware).</li>
</ul>

<h3>SFU (Media Server) Scaling</h3>
<p>The SFU is the most resource-intensive component (handling all real-time media). Scaling strategy:</p>
<ul>
  <li><strong>Horizontal scaling:</strong> Each SFU instance handles a fixed number of concurrent meetings (bounded by CPU, memory, and network bandwidth). New meetings are assigned to the least-loaded SFU.</li>
  <li><strong>Meeting assignment:</strong> The Meeting Service maintains an SFU registry (in-memory or via a service discovery mechanism). When a meeting starts, it assigns an SFU based on geography (closest to participants) and load.</li>
  <li><strong>Cascaded SFU:</strong> For large meetings (100+ participants), multiple SFU nodes handle the same meeting. SFU nodes exchange streams between each other, and each participant connects to one SFU. This distributes the fan-out load.</li>
  <li><strong>Geo-distribution:</strong> SFU clusters are deployed in multiple regions worldwide. Participants connect to the nearest SFU. For cross-region meetings, SFUs in different regions forward streams to each other over dedicated backbone links.</li>
  <li>Target: One SFU instance handles ~50–100 concurrent small meetings (2-10 participants each) or ~5-10 large meetings (50+ participants each).</li>
</ul>

<h3>Database Scaling</h3>
<ul>
  <li><strong>SQL:</strong> Vertical scaling initially, then horizontal sharding as described per table. Read replicas for read-heavy queries (e.g., user profile lookups, meeting history).</li>
  <li><strong>NoSQL (chat_messages):</strong> Horizontally scalable by design via partition key. Auto-scales with meeting volume.</li>
  <li><strong>Object Storage:</strong> Inherently horizontally scalable and managed.</li>
</ul>

<h3>Estimated Load</h3>
<div class="card">
<p>Assumptions: 10M concurrent users, 2M concurrent meetings, average 5 participants per meeting.</p>
<ul>
  <li><strong>Signaling Server:</strong> 10M WebSocket connections. At 50K connections/instance → ~200 instances.</li>
  <li><strong>SFU:</strong> 2M concurrent meetings. At 75 meetings/instance → ~27,000 SFU instances globally.</li>
  <li><strong>Meeting Service:</strong> Join rate of ~100K requests/sec at peak. At 2K rps/instance → 50 instances.</li>
  <li><strong>Chat Service:</strong> ~500K messages/sec at peak. NoSQL handles this with adequate partitions.</li>
</ul>
</div>

<!-- ========================================== -->
<h2 id="lb">12. Load Balancing</h2>

<h3>Where Load Balancers Are Placed</h3>
<ol>
  <li><strong>Before the API Gateway</strong> — Layer 7 (HTTP) load balancer. Distributes REST API traffic across API Gateway / Meeting Service / User Service instances. Uses round-robin or least-connections. Handles TLS termination.</li>
  <li><strong>Before the Signaling Servers</strong> — Layer 4 (TCP) load balancer. Routes WebSocket upgrade requests. Uses consistent hashing on <code>meeting_id</code> (extracted from the initial HTTP upgrade request) so all participants of a meeting hit the same Signaling Server (minimizing cross-server pub/sub overhead). Falls back to any available instance if the target is overloaded.</li>
  <li><strong>Before the SFU pool</strong> — Custom meeting-aware routing (not a traditional LB). The Meeting Service acts as the "load balancer" by assigning meetings to SFU nodes based on geography and load. Participants are given the SFU address directly and connect to it. This is done at the application level rather than a network load balancer because SFU assignment requires domain-specific logic (region, capacity, media type).</li>
</ol>

<h3>Load Balancer Deep Dive</h3>
<div class="card">
<h4>L7 Load Balancer (API Traffic)</h4>
<ul>
  <li><strong>Algorithm:</strong> Least-connections with health checks. Preferred over round-robin because Meeting Service instances may have varying response times depending on database query complexity.</li>
  <li><strong>Health checks:</strong> Periodic HTTP GET to <code>/health</code> endpoint on each backend instance. Unhealthy instances are removed from the pool within 10 seconds.</li>
  <li><strong>TLS termination:</strong> The LB terminates TLS so backend services communicate over plaintext internally, reducing CPU overhead on service instances.</li>
  <li><strong>Rate limiting:</strong> Applied at the LB level — 100 requests/sec per client IP to prevent abuse.</li>
</ul>

<h4>L4 Load Balancer (Signaling / WebSocket)</h4>
<ul>
  <li><strong>Algorithm:</strong> Consistent hashing on <code>meeting_id</code> (from the URL query parameter in the WebSocket upgrade request). This ensures all participants of a meeting are routed to the same Signaling Server, reducing reliance on the Pub/Sub system for intra-meeting communication.</li>
  <li><strong>Sticky sessions:</strong> Once a WebSocket connection is established, it stays on the same Signaling Server for the duration of the connection (inherent to TCP).</li>
  <li><strong>Failover:</strong> If a Signaling Server instance goes down, the LB detects the failure and routes new connections to another instance. Existing connections are lost, but clients auto-reconnect and re-join via the new instance.</li>
</ul>
</div>

<!-- ========================================== -->
<h2 id="rtc">13. Real-Time Communication Deep Dive</h2>

<h3>WebSocket (Signaling)</h3>
<div class="card">
<h4>Why WebSocket</h4>
<p>Signaling (SDP exchange, ICE candidates, control events like mute/participant join/leave) requires <strong>persistent, bidirectional, low-latency</strong> communication. WebSocket provides this:</p>
<ul>
  <li><strong>Persistent:</strong> No overhead of establishing a new connection per message (unlike HTTP).</li>
  <li><strong>Bidirectional:</strong> Server can push events to the client without the client polling (e.g., "new participant joined").</li>
  <li><strong>Low-latency:</strong> Messages are delivered instantly without HTTP request/response overhead.</li>
</ul>

<h4>Why NOT alternatives</h4>
<ul>
  <li><strong>HTTP Long Polling:</strong> Higher latency (connection re-establishment after each response), higher server resource usage (many idle connections waiting), and more complex to manage. Not suitable for the rapid back-and-forth of SDP/ICE exchange.</li>
  <li><strong>Server-Sent Events (SSE):</strong> Unidirectional (server → client only). Signaling requires bidirectional communication (client sends SDP offers, ICE candidates).</li>
  <li><strong>Raw TCP:</strong> WebSocket runs over TCP but adds framing, the HTTP upgrade handshake (firewall-friendly on port 443), and integration with web browsers. Raw TCP is not accessible from browsers.</li>
</ul>

<h4>Connection Lifecycle</h4>
<ol>
  <li>Client sends HTTP GET with <code>Upgrade: websocket</code> header to the Signaling Server (via the L4 load balancer).</li>
  <li>Server responds with <code>101 Switching Protocols</code> — the TCP connection is now upgraded to WebSocket.</li>
  <li>Client sends a <code>join</code> message with <code>meeting_id</code> and auth token.</li>
  <li>Server authenticates and registers the connection in an in-memory map: <code>Map&lt;meeting_id, Set&lt;{user_id, ws_connection}&gt;&gt;</code>.</li>
  <li>All subsequent signaling messages (SDP, ICE, control) flow over this connection.</li>
  <li>When the user leaves or the meeting ends, the server sends a close frame and cleans up the connection from the map.</li>
</ol>

<h4>Cross-Instance Communication (Pub/Sub)</h4>
<p>When the system scales to multiple Signaling Server instances, participants of the same meeting may be on different servers (especially if the LB's consistent hashing is imperfect or a server was restarted). The Pub/Sub system solves this:</p>
<ul>
  <li>Each Signaling Server subscribes to Pub/Sub channels for every meeting it has active connections for.</li>
  <li>When a Signaling Server needs to broadcast a message to a meeting's participants, it publishes to the <code>meeting:{meeting_id}</code> channel.</li>
  <li>All Signaling Server instances subscribed to that channel receive the message and deliver it to their local WebSocket connections.</li>
  <li>The Pub/Sub system is an in-memory pub/sub (not a persistent message queue) — messages are fire-and-forget. Missed messages during brief disconnections are acceptable for signaling (the client will re-request state).</li>
</ul>
</div>

<h3>WebRTC &amp; SFU (Media)</h3>
<div class="card">
<h4>WebRTC Protocol Stack</h4>
<p>WebRTC is the standard for real-time peer-to-peer media communication in browsers and native apps. The protocol stack:</p>
<ol>
  <li><strong>ICE (Interactive Connectivity Establishment):</strong> Discovers the best network path between client and SFU. Gathers candidates (host, server-reflexive via STUN, relay via TURN) and tests connectivity.</li>
  <li><strong>DTLS (Datagram Transport Layer Security):</strong> Runs over UDP. Performs a handshake to establish encryption keys. Similar to TLS but for UDP.</li>
  <li><strong>SRTP (Secure Real-Time Transport Protocol):</strong> Carries the actual audio/video data, encrypted with keys from DTLS. Runs over UDP.</li>
  <li><strong>SCTP (Stream Control Transmission Protocol) over DTLS:</strong> Used for WebRTC Data Channels (could be used for chat, but we use WebSocket for chat in this design for simpler server-side management).</li>
</ol>

<h4>SFU Architecture</h4>
<p><strong>SFU (Selective Forwarding Unit)</strong> is the media server topology chosen for this design. How it works:</p>
<ul>
  <li>Each participant establishes one WebRTC connection to the SFU.</li>
  <li>Each participant <strong>uploads</strong> one audio track + one video track (+ optionally one screen share track) to the SFU.</li>
  <li>The SFU <strong>forwards</strong> each participant's tracks to all other participants. In a meeting of N participants, each participant downloads N-1 video streams and N-1 audio streams (or a mixed audio stream).</li>
  <li>The SFU does NOT transcode or decode — it operates on encrypted packets, simply routing them. This is extremely efficient.</li>
  <li>With <strong>simulcast</strong>, the client sends 3 quality layers (e.g., 720p, 360p, 180p). The SFU selects which layer to forward to each receiver based on their bandwidth and UI layout (pinned vs. thumbnail).</li>
</ul>
</div>

<h3>Message Queue</h3>
<div class="card">
<h4>Why Message Queue</h4>
<p>Used to decouple meeting lifecycle events from downstream processing (notifications, recording):</p>
<ul>
  <li><strong>Notifications:</strong> When a meeting is scheduled, a notification job is enqueued. The Notification Service processes it asynchronously. This prevents the Meeting Service's <code>POST /api/meetings</code> from blocking on email delivery.</li>
  <li><strong>Recording completion:</strong> When a recording is uploaded, a job is enqueued to trigger notification and post-processing (e.g., generating thumbnails, transcripts).</li>
</ul>

<h4>How Messages Flow</h4>
<ul>
  <li><strong>Producer:</strong> Meeting Service (or Recording Service) enqueues messages with a topic/queue name (e.g., <code>meeting.notifications</code>, <code>recording.completed</code>).</li>
  <li><strong>Consumer:</strong> Notification Service subscribes to the relevant queues and processes messages. Uses a competing consumers pattern — multiple Notification Service instances dequeue messages, but each message is consumed by exactly one instance.</li>
  <li><strong>Delivery guarantee:</strong> At-least-once. Consumers acknowledge messages after processing. If a consumer crashes, the message becomes visible again after a visibility timeout and is retried.</li>
  <li><strong>Idempotency:</strong> Consumers are designed to be idempotent — processing the same notification job twice does not send duplicate emails (checked via a deduplication ID in the database).</li>
</ul>

<h4>Why NOT Pub/Sub for This</h4>
<p>Pub/Sub is fire-and-forget (no persistence, no retry). For notifications, we need guaranteed delivery — an email must eventually be sent even if the consumer is temporarily down. A durable message queue provides persistence and retry semantics that Pub/Sub does not.</p>
</div>

<!-- ========================================== -->
<h2 id="protocols">14. Protocol Decisions</h2>

<h3>UDP for Media Streams</h3>
<div class="card">
<p><strong>Why UDP:</strong></p>
<ul>
  <li><strong>Tolerates packet loss:</strong> In real-time video, a lost packet means a dropped frame. It's better to skip it and show the next frame than to wait for retransmission (which causes jitter/delay). Humans tolerate minor video artifacts from lost packets but are very sensitive to delays.</li>
  <li><strong>No head-of-line blocking:</strong> TCP guarantees ordered delivery. If packet #50 is lost, packets #51-#60 are buffered until #50 is retransmitted. This causes latency spikes. UDP has no ordering guarantee, so each packet is processed immediately.</li>
  <li><strong>Lower latency:</strong> No TCP handshake (3-way), no congestion control backoff that can add 100s of ms. WebRTC implements its own congestion control (Google Congestion Control / GCC) that's optimized for real-time media.</li>
</ul>
<p><strong>Why NOT TCP for media:</strong> TCP's retransmission and ordering guarantees add latency that is unacceptable for interactive video (&lt;200ms target). A retransmitted video packet that arrives 200ms later is useless — the moment has passed.</p>
</div>

<h3>TCP for Signaling, Chat, REST</h3>
<div class="card">
<p><strong>Why TCP:</strong></p>
<ul>
  <li><strong>Reliability required:</strong> SDP offers/answers, ICE candidates, and chat messages <em>must</em> arrive intact and in order. A lost SDP answer would prevent the media connection from being established. A lost chat message would mean missing content.</li>
  <li><strong>WebSocket runs over TCP:</strong> The Signaling Server uses WebSocket, which is built on TCP.</li>
  <li><strong>HTTPS runs over TCP:</strong> REST API calls use HTTPS (TLS over TCP).</li>
</ul>
</div>

<h3>Protocol Summary</h3>
<table>
  <tr><th>Communication</th><th>Protocol</th><th>Transport</th><th>Reason</th></tr>
  <tr><td>REST API calls</td><td>HTTPS</td><td>TCP</td><td>Reliability, standard web protocol</td></tr>
  <tr><td>Signaling (SDP, ICE, control)</td><td>WebSocket</td><td>TCP</td><td>Bidirectional, persistent, low-latency, reliable</td></tr>
  <tr><td>Video/audio media</td><td>SRTP (via WebRTC)</td><td>UDP</td><td>Low latency, tolerate packet loss</td></tr>
  <tr><td>Key exchange for media</td><td>DTLS</td><td>UDP</td><td>Encryption setup for SRTP, over same UDP path</td></tr>
  <tr><td>NAT traversal</td><td>STUN/TURN</td><td>UDP (primary), TCP (fallback)</td><td>Discover public IP, relay if needed</td></tr>
  <tr><td>Chat messages</td><td>WebSocket (reuses signaling)</td><td>TCP</td><td>Reliability, real-time delivery</td></tr>
</table>

<!-- ========================================== -->
<h2 id="tradeoffs">15. Tradeoffs &amp; Deep Dives</h2>

<h3>SFU vs. MCU vs. P2P Mesh</h3>
<table>
  <tr><th>Topology</th><th>Pros</th><th>Cons</th><th>Best For</th></tr>
  <tr>
    <td><strong>SFU</strong> (our choice)</td>
    <td>Low server CPU (no transcoding), preserves per-participant streams (clients can render custom layouts), supports simulcast</td>
    <td>Higher client download bandwidth (N-1 streams), client must decode multiple streams</td>
    <td>General-purpose meetings (2-100 participants). Best balance of scalability and quality.</td>
  </tr>
  <tr>
    <td><strong>MCU</strong></td>
    <td>Low client bandwidth (receives one mixed stream), simple client implementation</td>
    <td>Extremely high server CPU (decoding + mixing + re-encoding all streams), loses per-participant control, high latency from mixing</td>
    <td>Very low-bandwidth clients (legacy phones). Not suitable for modern video conferencing at scale.</td>
  </tr>
  <tr>
    <td><strong>P2P Mesh</strong></td>
    <td>No server infrastructure for media, lowest latency for 1:1</td>
    <td>Exponential bandwidth: each participant sends to N-1 peers. Unusable for groups &gt;4. No server-side recording possible.</td>
    <td>1:1 calls only. Could be used as optimization for two-person meetings.</td>
  </tr>
</table>
<p><strong>Decision:</strong> SFU for all meetings (including 1:1). While P2P could optimize 1:1 calls, using SFU universally simplifies architecture, enables server-side recording for all meetings, and allows consistent quality monitoring. The marginal latency increase from routing through an SFU vs. direct P2P (~10-20ms) is negligible.</p>

<h3>Simulcast vs. SVC (Scalable Video Coding)</h3>
<p><strong>Simulcast</strong> (our choice): The client encodes and sends 3 separate streams at different resolutions. The SFU picks the appropriate one per receiver. Simple to implement, widely supported, but uses ~1.5x the upload bandwidth of a single stream.</p>
<p><strong>SVC:</strong> The client encodes a single layered stream where quality layers can be stripped out by the SFU. More bandwidth-efficient than simulcast but less widely supported in browsers and more complex to implement.</p>
<p><strong>Decision:</strong> Simulcast, due to broader client support and simpler SFU implementation. SVC is a future optimization as browser support matures.</p>

<h3>Audio Mixing on SFU</h3>
<p>A key optimization: instead of forwarding N-1 separate audio streams to each participant (which the client must decode individually), the SFU can <strong>mix the top 3 loudest speakers' audio</strong> into a single stream and forward that. This reduces client CPU for audio decoding and reduces download bandwidth. The tradeoff is added server CPU for audio mixing (much cheaper than video mixing) and a slight quality reduction from re-encoding.</p>

<h3>Consistency vs. Availability</h3>
<p>Following the CAP theorem, different parts of the system make different tradeoffs:</p>
<ul>
  <li><strong>Meeting creation/join (SQL):</strong> Favors <strong>consistency</strong>. Meeting codes must be unique; a user should not be able to join a non-existent or ended meeting.</li>
  <li><strong>Chat messages (NoSQL):</strong> Favors <strong>availability</strong>. A brief delay or reordering of chat messages is acceptable; dropped messages are not (at-least-once delivery).</li>
  <li><strong>Real-time media (SFU):</strong> Favors <strong>availability</strong> and <strong>partition tolerance</strong>. If a network partition occurs, the SFU continues forwarding whatever media it can. Brief quality degradation is better than a dropped call.</li>
</ul>

<h3>Security: Transport Encryption vs. End-to-End Encryption</h3>
<p><strong>Transport encryption</strong> (our default): Media is encrypted between client ↔ SFU (via DTLS/SRTP). The SFU has access to unencrypted media. This enables server-side features: recording, audio mixing, active speaker detection, bandwidth adaptation.</p>
<p><strong>End-to-end encryption (E2EE):</strong> Media is encrypted at the sender and only decrypted by receivers. The SFU cannot read the media, only forward encrypted packets. This provides stronger privacy but <strong>disables</strong> server-side recording, server-side audio mixing, and some quality optimization features.</p>
<p><strong>Decision:</strong> Transport encryption by default (enables all features), with E2EE as an opt-in mode for sensitive meetings (at the cost of disabling recording and some features). This mirrors Zoom's actual approach.</p>

<!-- ========================================== -->
<h2 id="alternatives">16. Alternative Approaches</h2>

<h3>Alternative 1: RTMP-Based Architecture</h3>
<p><strong>Approach:</strong> Participants send media via RTMP (Real-Time Messaging Protocol) to a media server, which transcodes and redistributes via HLS/DASH.</p>
<p><strong>Why not chosen:</strong> RTMP → HLS/DASH introduces 3-10 seconds of latency due to segment-based delivery. This is acceptable for live streaming (Twitch, YouTube Live) but completely unacceptable for interactive video conferencing where sub-200ms latency is required. RTMP also runs over TCP, which has head-of-line blocking issues for real-time media.</p>

<h3>Alternative 2: P2P Mesh for Small Meetings + SFU for Large</h3>
<p><strong>Approach:</strong> Use direct P2P WebRTC for meetings with ≤4 participants and switch to SFU for larger meetings.</p>
<p><strong>Why not chosen:</strong> While this optimizes bandwidth cost for small meetings, it complicates the architecture significantly (two different media topologies with different codepaths), makes server-side recording impossible for P2P meetings, and the transition from P2P to SFU when a 5th participant joins causes a disruptive media renegotiation. The simplicity and uniformity of always using SFU outweighs the bandwidth savings.</p>

<h3>Alternative 3: MCU for All Meetings</h3>
<p><strong>Approach:</strong> Use an MCU that decodes all streams, mixes them into a single composite video, and sends one stream to each participant.</p>
<p><strong>Why not chosen:</strong> MCU's CPU cost is orders of magnitude higher than SFU (it must decode, compose, and re-encode all video — essentially doing what each client does, but server-side). This does not scale to millions of concurrent meetings. It also removes per-participant stream control (custom layouts, pinning).</p>

<h3>Alternative 4: Polling Instead of WebSocket for Signaling</h3>
<p><strong>Approach:</strong> Clients poll the server every 500ms for new signaling events.</p>
<p><strong>Why not chosen:</strong> SDP/ICE exchange requires rapid bidirectional message passing. A 500ms polling interval would add unacceptable latency to connection establishment (which requires multiple round-trips of SDP/ICE). It also wastes bandwidth and server resources with empty polls. WebSocket provides instant bidirectional delivery.</p>

<h3>Alternative 5: gRPC Instead of REST for API</h3>
<p><strong>Approach:</strong> Use gRPC (HTTP/2, Protocol Buffers) for all client-server API communication.</p>
<p><strong>Why not chosen:</strong> While gRPC offers better performance (binary serialization, multiplexing), browser support for gRPC is limited (requires grpc-web proxy). REST over HTTPS is universally supported across all client platforms (browser, mobile, desktop) with no additional infrastructure. The API layer is not a bottleneck — the real performance-sensitive path is media delivery, which already uses WebRTC/SRTP. REST's simplicity and ubiquity make it the pragmatic choice for the API layer.</p>

<!-- ========================================== -->
<h2 id="additional">17. Additional Considerations</h2>

<h3>Codec Selection</h3>
<ul>
  <li><strong>Video:</strong> VP8/VP9 (open, widely supported in WebRTC) or H.264 (hardware acceleration on most devices). VP9 offers better compression than VP8 at the same quality. AV1 is the future (even better compression) but CPU-intensive to encode without hardware support.</li>
  <li><strong>Audio:</strong> Opus codec — the standard for WebRTC. Excellent quality at low bitrates (32 kbps for speech), supports both narrowband and wideband, built-in forward error correction.</li>
</ul>

<h3>Bandwidth Estimation &amp; Adaptation</h3>
<p>The system continuously estimates each participant's available bandwidth using:</p>
<ul>
  <li><strong>REMB (Receiver Estimated Maximum Bitrate):</strong> The receiver tells the sender how much bandwidth it can handle.</li>
  <li><strong>Transport-CC (Congestion Control):</strong> The receiver sends feedback on packet arrival times; the sender estimates bandwidth from these.</li>
</ul>
<p>Based on estimates, the SFU dynamically switches between simulcast layers (e.g., drops from 720p to 360p) and can reduce frame rate.</p>

<h3>Active Speaker Detection</h3>
<p>The SFU analyzes audio levels to detect the active speaker. This information is sent to clients via the Signaling Server so the UI can highlight or enlarge the active speaker. Since the SFU has access to audio (transport encryption), it can compute RMS audio levels per track.</p>

<h3>Reconnection &amp; Resilience</h3>
<ul>
  <li><strong>ICE restart:</strong> If a participant's network changes (e.g., switching from Wi-Fi to cellular), WebRTC supports ICE restart to re-establish connectivity without dropping the call.</li>
  <li><strong>Signaling reconnect:</strong> If the WebSocket connection drops, the client automatically reconnects and re-sends a <code>join</code> message. The Signaling Server restores the participant's state.</li>
  <li><strong>SFU failover:</strong> If an SFU node fails, the Meeting Service detects it (heartbeat) and reassigns participants to a new SFU. Clients perform SDP renegotiation with the new SFU. There is a brief (2-5 second) interruption.</li>
</ul>

<h3>Meeting Size Limits &amp; Webinar Mode</h3>
<ul>
  <li><strong>Standard meeting:</strong> Up to ~300 participants. All can send video/audio. SFU handles this via cascading if needed.</li>
  <li><strong>Webinar mode:</strong> A few panelists send video/audio; thousands of viewers are receive-only. The SFU only forwards panelist streams to viewers (no upload from viewers). This dramatically reduces SFU load per viewer. For 10,000+ viewers, the SFU output can be fanned out via a CDN-like tree of relay servers.</li>
</ul>

<h3>Monitoring &amp; Observability</h3>
<ul>
  <li><strong>Media quality metrics:</strong> Packet loss rate, jitter, round-trip time, bitrate per participant — collected by the SFU and clients, sent to a metrics/telemetry system.</li>
  <li><strong>Service health:</strong> Request latency, error rates, connection counts per Signaling Server, SFU CPU/bandwidth utilization.</li>
  <li><strong>Alerting:</strong> Alert on SFU overload (&gt;80% CPU), signaling connection failures, recording failures.</li>
</ul>

<!-- ========================================== -->
<h2 id="vendors">18. Vendor Recommendations</h2>

<table>
  <tr><th>Component</th><th>Vendor Options</th><th>Rationale</th></tr>
  <tr>
    <td>SQL Database</td>
    <td>PostgreSQL, MySQL, CockroachDB (distributed SQL), Amazon Aurora</td>
    <td>PostgreSQL: mature, excellent JSON support for settings column, strong ecosystem. CockroachDB: if global distribution with SQL semantics is needed. Aurora: if on AWS, provides managed scaling with MySQL/PostgreSQL compatibility.</td>
  </tr>
  <tr>
    <td>NoSQL Database (Chat)</td>
    <td>Apache Cassandra, ScyllaDB, Amazon DynamoDB, Google Cloud Bigtable</td>
    <td>Cassandra/ScyllaDB: excellent write throughput, partition + sort key model fits chat perfectly, tunable consistency. DynamoDB: fully managed, auto-scaling, partition + sort key native.</td>
  </tr>
  <tr>
    <td>In-Memory Cache</td>
    <td>Redis, Memcached, Dragonfly</td>
    <td>Redis: supports rich data structures, pub/sub (can double as the Signaling Server's cross-instance pub/sub), persistence for cache warming. Memcached: simpler, slightly faster for pure key-value caching.</td>
  </tr>
  <tr>
    <td>Pub/Sub (Signaling cross-instance)</td>
    <td>Redis Pub/Sub, NATS, RabbitMQ (fanout exchange)</td>
    <td>Redis Pub/Sub: simple, low-latency, already in the stack for caching. NATS: purpose-built for real-time messaging, extremely fast, lightweight.</td>
  </tr>
  <tr>
    <td>Message Queue</td>
    <td>RabbitMQ, Apache Kafka, Amazon SQS, NATS JetStream</td>
    <td>RabbitMQ: mature, supports exactly the competing-consumers pattern needed, dead-letter queues for failed messages. Kafka: if event sourcing or replay is desired. SQS: fully managed, no operational overhead.</td>
  </tr>
  <tr>
    <td>Object Storage</td>
    <td>Amazon S3, Google Cloud Storage, Azure Blob Storage, MinIO (self-hosted)</td>
    <td>S3: industry standard, 11 nines durability, lifecycle policies for cost management (move old recordings to cold storage). MinIO: if self-hosted/multi-cloud portability is needed (S3-compatible API).</td>
  </tr>
  <tr>
    <td>SFU Media Server</td>
    <td>Janus, mediasoup, Jitsi Videobridge, Pion (Go-based), LiveKit</td>
    <td>mediasoup: lightweight, Node.js-based, excellent performance, actively maintained. Janus: C-based, very performant, plugin architecture. LiveKit: modern, open-source, built-in scaling. Pion: Go-based, ideal if the team prefers Go.</td>
  </tr>
  <tr>
    <td>CDN</td>
    <td>Cloudflare, Fastly, Amazon CloudFront, Akamai</td>
    <td>Cloudflare: global network, WebSocket support, DDoS protection built-in. CloudFront: tight integration with S3 for recordings. Akamai: largest CDN, best for enterprise.</td>
  </tr>
  <tr>
    <td>TURN Server</td>
    <td>coturn (open-source), Twilio TURN, Cloudflare TURN</td>
    <td>coturn: open-source, battle-tested, self-hosted for cost control. Cloud-hosted TURN services reduce operational burden.</td>
  </tr>
</table>

<!-- ========================================== -->
<p style="margin-top: 3rem; text-align: center; color: #94a3b8; font-size: 0.85rem;">
  System Design Document — Zoom (Video Conferencing) — Generated for study purposes
</p>

<script>
  mermaid.initialize({ startOnLoad: true, theme: 'default', securityLevel: 'loose' });
</script>

</body>
</html>
