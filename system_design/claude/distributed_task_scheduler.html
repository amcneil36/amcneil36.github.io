<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>System Design: Distributed Task Scheduler</title>
    <script src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script>
    <style>
        :root {
            --bg: #0d1117;
            --surface: #161b22;
            --border: #30363d;
            --text: #e6edf3;
            --text-secondary: #8b949e;
            --accent: #58a6ff;
            --accent-green: #3fb950;
            --accent-orange: #d29922;
            --accent-red: #f85149;
            --accent-purple: #bc8cff;
        }
        * { box-sizing: border-box; margin: 0; padding: 0; }
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
            background: var(--bg);
            color: var(--text);
            line-height: 1.7;
            padding: 2rem;
            max-width: 1200px;
            margin: 0 auto;
        }
        h1 {
            font-size: 2.5rem;
            margin-bottom: 0.5rem;
            background: linear-gradient(135deg, var(--accent), var(--accent-purple));
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
        }
        h2 {
            font-size: 1.8rem;
            margin: 2.5rem 0 1rem;
            padding-bottom: 0.5rem;
            border-bottom: 2px solid var(--accent);
            color: var(--accent);
        }
        h3 {
            font-size: 1.4rem;
            margin: 1.8rem 0 0.8rem;
            color: var(--accent-green);
        }
        h4 {
            font-size: 1.1rem;
            margin: 1.2rem 0 0.5rem;
            color: var(--accent-orange);
        }
        p { margin: 0.6rem 0; }
        ul, ol { margin: 0.6rem 0 0.6rem 1.5rem; }
        li { margin: 0.3rem 0; }
        .card {
            background: var(--surface);
            border: 1px solid var(--border);
            border-radius: 8px;
            padding: 1.5rem;
            margin: 1rem 0;
        }
        .example-card {
            background: #1a2332;
            border-left: 4px solid var(--accent);
            border-radius: 0 8px 8px 0;
            padding: 1.2rem 1.5rem;
            margin: 1rem 0;
        }
        .example-card strong { color: var(--accent); }
        .warn-card {
            background: #2a1f0e;
            border-left: 4px solid var(--accent-orange);
            border-radius: 0 8px 8px 0;
            padding: 1.2rem 1.5rem;
            margin: 1rem 0;
        }
        .warn-card strong { color: var(--accent-orange); }
        .alt-card {
            background: #1a1a2e;
            border-left: 4px solid var(--accent-purple);
            border-radius: 0 8px 8px 0;
            padding: 1.2rem 1.5rem;
            margin: 1rem 0;
        }
        .alt-card strong { color: var(--accent-purple); }
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 1rem 0;
            font-size: 0.95rem;
        }
        th {
            background: var(--accent);
            color: var(--bg);
            padding: 0.7rem 1rem;
            text-align: left;
            font-weight: 600;
        }
        td {
            padding: 0.6rem 1rem;
            border-bottom: 1px solid var(--border);
        }
        tr:nth-child(even) td { background: rgba(255,255,255,0.02); }
        code {
            background: rgba(255,255,255,0.08);
            padding: 0.15rem 0.4rem;
            border-radius: 4px;
            font-size: 0.9em;
            color: var(--accent-green);
        }
        pre {
            background: var(--surface);
            border: 1px solid var(--border);
            border-radius: 8px;
            padding: 1rem;
            overflow-x: auto;
            font-size: 0.9rem;
            margin: 1rem 0;
        }
        .mermaid {
            background: #ffffff;
            border-radius: 8px;
            padding: 1.5rem;
            margin: 1rem 0;
            text-align: center;
        }
        .badge {
            display: inline-block;
            padding: 0.2rem 0.6rem;
            border-radius: 12px;
            font-size: 0.8rem;
            font-weight: 600;
            margin: 0.1rem;
        }
        .badge-sql { background: #1f3a5f; color: #58a6ff; }
        .badge-nosql { background: #3a1f3a; color: #bc8cff; }
        .badge-pk { background: #1f3a1f; color: #3fb950; }
        .badge-fk { background: #3a3a1f; color: #d29922; }
        .badge-idx { background: #3a1f1f; color: #f85149; }
        hr { border: none; border-top: 1px solid var(--border); margin: 2rem 0; }
        .toc { columns: 2; }
        .toc a { color: var(--accent); text-decoration: none; }
        .toc a:hover { text-decoration: underline; }
        .subtitle { color: var(--text-secondary); font-size: 1.1rem; margin-bottom: 2rem; }
    </style>
</head>
<body>

<h1>Distributed Task Scheduler</h1>
<p class="subtitle">A comprehensive system design for scheduling, dispatching, and executing tasks at massive scale with reliability guarantees.</p>

<div class="card">
    <h3 style="margin-top:0">Table of Contents</h3>
    <div class="toc">
        <ol>
            <li><a href="#fr">Functional Requirements</a></li>
            <li><a href="#nfr">Non-Functional Requirements</a></li>
            <li><a href="#flow1">Flow 1: Task Submission</a></li>
            <li><a href="#flow2">Flow 2: Task Execution</a></li>
            <li><a href="#flow3">Flow 3: Task Status Query</a></li>
            <li><a href="#flow4">Flow 4: Task Cancellation / Update</a></li>
            <li><a href="#overall">Overall Combined Diagram</a></li>
            <li><a href="#schema">Database Schema</a></li>
            <li><a href="#cache">CDN &amp; Cache Deep Dive</a></li>
            <li><a href="#mq">Message Queue Deep Dive</a></li>
            <li><a href="#scaling">Scaling Considerations</a></li>
            <li><a href="#tradeoffs">Tradeoffs &amp; Deep Dives</a></li>
            <li><a href="#alternatives">Alternative Approaches</a></li>
            <li><a href="#additional">Additional Information</a></li>
            <li><a href="#vendors">Vendor Suggestions</a></li>
        </ol>
    </div>
</div>

<!-- ============================================================ -->
<h2 id="fr">1. Functional Requirements</h2>
<!-- ============================================================ -->
<div class="card">
    <ol>
        <li><strong>Create a task:</strong> Clients can submit a one-time or recurring task with a payload, a scheduled execution time (or cron expression), a priority level, a callback URL, and retry configuration.</li>
        <li><strong>Execute tasks on schedule:</strong> The system fires tasks at (or very close to) their scheduled time and delivers the payload to the configured handler/callback.</li>
        <li><strong>Cancel / update a task:</strong> Clients can cancel a pending task or update its schedule, payload, or priority before execution.</li>
        <li><strong>Query task status:</strong> Clients can query the current status of a task (SCHEDULED, QUEUED, RUNNING, COMPLETED, FAILED, CANCELLED) and view execution history.</li>
        <li><strong>Retry failed tasks:</strong> The system automatically retries failed tasks up to a configurable maximum with configurable back-off.</li>
        <li><strong>Support recurring tasks:</strong> Tasks can be defined with cron-like expressions and the system automatically re-schedules the next occurrence after each execution.</li>
        <li><strong>Dead-letter handling:</strong> Tasks that exhaust all retries are routed to a Dead Letter Queue for manual inspection or alerting.</li>
        <li><strong>Support task priorities:</strong> Higher-priority tasks are executed before lower-priority ones when multiple tasks are due simultaneously.</li>
    </ol>
</div>

<!-- ============================================================ -->
<h2 id="nfr">2. Non-Functional Requirements</h2>
<!-- ============================================================ -->
<div class="card">
    <table>
        <tr><th>Attribute</th><th>Target</th></tr>
        <tr><td><strong>Reliability</strong></td><td>At-least-once execution guarantee. No scheduled task is silently dropped.</td></tr>
        <tr><td><strong>Availability</strong></td><td>99.99% uptime. The scheduling path has no single point of failure.</td></tr>
        <tr><td><strong>Scalability</strong></td><td>Support 100M+ scheduled tasks with 10K+ tasks/sec throughput for submission and execution.</td></tr>
        <tr><td><strong>Latency (scheduling precision)</strong></td><td>Tasks execute within 1–5 seconds of their scheduled time under normal load.</td></tr>
        <tr><td><strong>Latency (API)</strong></td><td>Task submission and status queries return within 100ms p99.</td></tr>
        <tr><td><strong>Durability</strong></td><td>Task definitions survive node failures. Data is replicated.</td></tr>
        <tr><td><strong>Exactly-once semantics (best effort)</strong></td><td>The system is designed so that duplicate execution is minimized via idempotency tokens and database-level locking, but ultimate exactly-once is the responsibility of the task handler (idempotent handlers).</td></tr>
        <tr><td><strong>Fault tolerance</strong></td><td>Worker or scheduler node failures are detected within seconds and partitions are reassigned.</td></tr>
        <tr><td><strong>Observability</strong></td><td>Metrics on queue depth, execution latency, failure rates, and scheduler lag.</td></tr>
    </table>
</div>

<!-- ============================================================ -->
<h2 id="flow1">3. Flow 1 — Task Submission</h2>
<!-- ============================================================ -->
<h3>3.1 Diagram</h3>
<div class="mermaid">
flowchart LR
    C["Client<br/>(SDK / API Caller)"] -->|"HTTP POST<br/>/tasks"| LB["Load Balancer"]
    LB --> API["Task API<br/>Service"]
    API -->|"validate &amp; persist"| DB[("Task Store<br/>(SQL)")]
    API -->|"return task_id<br/>201 Created"| LB
    LB --> C
</div>

<h3>3.2 Examples</h3>
<div class="example-card">
    <strong>Example 1 — One-time task:</strong> An analytics platform needs to generate a report at 2026-03-01 09:00 UTC. The client sends <code>POST /tasks</code> with body <code>{"type": "one_time", "scheduled_time": "2026-03-01T09:00:00Z", "payload": {"report_id": "rpt_123"}, "callback_url": "https://analytics.internal/hooks/report", "priority": "medium", "max_retries": 3}</code>. The Load Balancer routes the request to one of several Task API Service instances. The API Service validates the payload, generates a unique <code>task_id</code> (e.g., <code>task_abc123</code>), inserts a row into the Task Store with status <code>SCHEDULED</code>, and returns <code>201 Created</code> with the <code>task_id</code>.
</div>
<div class="example-card">
    <strong>Example 2 — Recurring task:</strong> A billing system needs to charge subscribers every day at midnight. The client sends <code>POST /tasks</code> with body <code>{"type": "recurring", "cron_expression": "0 0 * * *", "payload": {"action": "charge_subscribers"}, "callback_url": "https://billing.internal/hooks/charge", "priority": "high", "max_retries": 5}</code>. The API Service computes the next scheduled time from the cron expression (e.g., tonight at midnight), stores the task with the computed <code>next_execution_time</code>, and returns the <code>task_id</code>.
</div>
<div class="example-card">
    <strong>Example 3 — Validation failure:</strong> A client submits a task with <code>"scheduled_time": "2020-01-01T00:00:00Z"</code> (a time in the past). The API Service rejects the request with <code>400 Bad Request</code> and body <code>{"error": "scheduled_time must be in the future"}</code>. No row is written.
</div>

<h3>3.3 Component Deep Dive</h3>

<h4>Client (SDK / API Caller)</h4>
<p>Any internal or external service that needs to schedule work. Clients typically use an SDK that wraps the HTTP API, provides retry logic, and ensures idempotency by attaching an <code>Idempotency-Key</code> header so that network retries do not create duplicate tasks.</p>

<h4>Load Balancer</h4>
<p>A Layer-7 (HTTP-aware) load balancer sitting in front of the Task API Service fleet. It distributes incoming HTTP requests using a <strong>round-robin</strong> or <strong>least-connections</strong> strategy. It also performs health checks on API instances and removes unhealthy ones from the pool. TLS termination happens here.</p>

<h4>Task API Service</h4>
<p>A stateless HTTP service responsible for the CRUD interface of tasks. Protocol: <strong>HTTP/1.1 or HTTP/2</strong>.</p>
<table>
    <tr><th>Endpoint</th><th>Method</th><th>Input</th><th>Output</th></tr>
    <tr>
        <td><code>/tasks</code></td>
        <td><code>POST</code></td>
        <td>JSON body: <code>type</code>, <code>scheduled_time</code> or <code>cron_expression</code>, <code>payload</code>, <code>callback_url</code>, <code>priority</code>, <code>max_retries</code></td>
        <td><code>201 Created</code> with <code>{"task_id": "..."}</code></td>
    </tr>
</table>
<p>On receipt the service: (1) validates all fields, (2) checks idempotency key against the DB to avoid duplicates, (3) computes <code>next_execution_time</code> if recurring, (4) writes to the Task Store within a transaction, and (5) returns the task_id.</p>

<h4>Task Store (SQL Database)</h4>
<p>The persistent source of truth for task definitions. SQL is chosen here because we need <strong>ACID transactions</strong> for atomic status transitions (e.g., claiming a task with <code>SELECT … FOR UPDATE SKIP LOCKED</code>). The schema is described in the <a href="#schema">Database Schema</a> section.</p>

<!-- ============================================================ -->
<h2 id="flow2">4. Flow 2 — Task Execution</h2>
<!-- ============================================================ -->
<h3>4.1 Diagram</h3>
<div class="mermaid">
flowchart TB
    subgraph Scheduling Layer
        SS1["Scheduler Instance 1<br/>(owns partitions 0-3)"]
        SS2["Scheduler Instance 2<br/>(owns partitions 4-7)"]
        CS["Coordination<br/>Service"]
    end

    DB[("Task Store<br/>(SQL)")]
    MQ["Message Queue<br/>(Priority Queues)"]
    DLQ["Dead Letter<br/>Queue"]

    subgraph Worker Pool
        W1["Worker 1"]
        W2["Worker 2"]
        W3["Worker N"]
    end

    CB["Callback<br/>Destination"]

    CS -. "partition<br/>assignment" .-> SS1
    CS -. "partition<br/>assignment" .-> SS2

    SS1 -->|"1. Poll for due tasks<br/>(SELECT ... FOR UPDATE<br/>SKIP LOCKED)"| DB
    SS2 -->|"1. Poll for due tasks"| DB

    SS1 -->|"2. Publish task<br/>messages"| MQ
    SS2 -->|"2. Publish task<br/>messages"| MQ

    SS1 -->|"3. Update status<br/>→ QUEUED"| DB
    SS2 -->|"3. Update status<br/>→ QUEUED"| DB

    MQ -->|"4. Consume<br/>task message"| W1
    MQ -->|"4. Consume<br/>task message"| W2
    MQ -->|"4. Consume<br/>task message"| W3

    W1 -->|"5a. Update status<br/>→ RUNNING / COMPLETED"| DB
    W2 -->|"5a. Update status"| DB
    W3 -->|"5a. Update status"| DB

    W1 -->|"5b. Invoke<br/>callback"| CB
    W2 -->|"5b. Invoke<br/>callback"| CB

    W1 -. "5c. On failure<br/>(retries exhausted)" .-> DLQ
</div>

<h3>4.2 Examples</h3>
<div class="example-card">
    <strong>Example 1 — Successful one-time execution:</strong> It is now 09:00:00 UTC. Scheduler Instance 1 runs its polling loop and executes <code>SELECT task_id, payload, callback_url FROM tasks WHERE status = 'SCHEDULED' AND next_execution_time &lt;= NOW() AND partition_key IN (0,1,2,3) ORDER BY priority DESC, next_execution_time ASC LIMIT 100 FOR UPDATE SKIP LOCKED</code>. It finds <code>task_abc123</code> (the analytics report task from Flow 1). It updates the status to <code>QUEUED</code>, publishes a message <code>{"task_id": "task_abc123", "payload": {"report_id": "rpt_123"}, "callback_url": "...", "attempt": 1}</code> to the high-priority queue, and commits the transaction. Worker 2 picks up the message, updates status to <code>RUNNING</code>, invokes the callback URL with an HTTP POST containing the payload, receives a <code>200 OK</code>, updates status to <code>COMPLETED</code>, writes an execution log entry, and ACKs the message on the queue.
</div>
<div class="example-card">
    <strong>Example 2 — Transient failure with retry:</strong> Worker 1 picks up a payment-processing task and invokes the callback URL but receives a <code>503 Service Unavailable</code>. The worker increments <code>retry_count</code> (now 1 out of max 3), computes the next retry time using exponential back-off (e.g., 30 seconds from now), updates the task status back to <code>SCHEDULED</code> with the new <code>next_execution_time</code>, and NACKs the message. On the next polling cycle after 30 seconds, the scheduler picks up the task again and the cycle repeats. If it eventually succeeds on attempt 2, the status goes to <code>COMPLETED</code>.
</div>
<div class="example-card">
    <strong>Example 3 — Retries exhausted → Dead Letter Queue:</strong> A task has <code>max_retries = 3</code> and has now failed 3 times. On the 3rd failure, the worker updates status to <code>FAILED</code>, writes an execution log with the error, and publishes the task to the Dead Letter Queue. An alerting service monitors the DLQ and notifies the task owner via email or webhook that their task has permanently failed.
</div>
<div class="example-card">
    <strong>Example 4 — Recurring task re-scheduling:</strong> The daily billing task completes successfully at 00:00:15 UTC. The worker recognizes the task type is <code>recurring</code>, computes the next execution from the cron expression <code>0 0 * * *</code> → tomorrow at 00:00 UTC, updates the task's <code>next_execution_time</code> and resets <code>retry_count</code> to 0, and sets status back to <code>SCHEDULED</code>. The task will be picked up by the scheduler the following night.
</div>
<div class="example-card">
    <strong>Example 5 — Scheduler failover:</strong> Scheduler Instance 1 crashes. The Coordination Service detects the failure via a missed heartbeat within 10 seconds. It reassigns partitions 0–3 to Scheduler Instance 2 (or to a newly spawned Instance 3). The new owner begins polling those partitions. Some tasks may be slightly delayed (by the heartbeat interval) but none are lost because the task definitions are durably stored in the Task Store.
</div>

<h3>4.3 Component Deep Dive</h3>

<h4>Scheduler Service (Instances)</h4>
<p>Stateless polling processes that each own a subset of <strong>task partitions</strong>. Each instance runs a tight loop (every 1–2 seconds):</p>
<ol>
    <li>Query the Task Store for tasks where <code>status = 'SCHEDULED'</code>, <code>next_execution_time &lt;= NOW()</code>, and the <code>partition_key</code> falls within its assigned range.</li>
    <li>The query uses <code>FOR UPDATE SKIP LOCKED</code> so that multiple schedulers never claim the same task.</li>
    <li>Batch-update claimed tasks to <code>QUEUED</code>.</li>
    <li>Publish messages to the Message Queue.</li>
    <li>Commit the transaction. If the commit fails, the messages are not ACKed and the tasks revert to <code>SCHEDULED</code>.</li>
</ol>
<p>The polling interval is a tunable parameter that trades off scheduling precision vs. database load.</p>

<h4>Coordination Service</h4>
<p>A distributed coordination system (consensus-based, e.g., Raft or Paxos under the hood) that handles:</p>
<ul>
    <li><strong>Partition assignment:</strong> Divides the task keyspace into N partitions and assigns them to active scheduler instances.</li>
    <li><strong>Leader election:</strong> If needed, elects a leader among schedulers for any global operations.</li>
    <li><strong>Failure detection:</strong> Monitors heartbeats from scheduler instances. If an instance fails to heartbeat within the timeout (e.g., 10 s), its partitions are redistributed.</li>
</ul>

<h4>Message Queue (Priority Queues)</h4>
<p>A durable, partitioned message queue that decouples the scheduling layer from the execution layer. See the <a href="#mq">Message Queue Deep Dive</a> for full details. Key properties:</p>
<ul>
    <li>Supports <strong>priority levels</strong> (high, medium, low) mapped to separate queues or priority ordering within a single queue.</li>
    <li>Guarantees <strong>at-least-once delivery</strong> with visibility timeout (if a worker does not ACK within the timeout, the message becomes visible again).</li>
    <li>Messages carry the full context needed for execution: <code>task_id</code>, <code>payload</code>, <code>callback_url</code>, <code>attempt</code> number.</li>
</ul>

<h4>Worker Pool</h4>
<p>A fleet of stateless worker processes that consume from the Message Queue. Each worker:</p>
<ol>
    <li>Pulls a message from the queue.</li>
    <li>Checks the task status in the DB (to handle cancellation race conditions). If <code>CANCELLED</code>, ACKs and discards.</li>
    <li>Updates status to <code>RUNNING</code>.</li>
    <li>Invokes the <code>callback_url</code> via <strong>HTTP POST</strong> with the payload in the body and metadata (task_id, attempt) in headers.</li>
    <li>If the callback returns <code>2xx</code>: marks <code>COMPLETED</code>, writes execution log, ACKs message.</li>
    <li>If the callback returns an error or times out: implements retry logic (described above).</li>
</ol>
<p>Workers send heartbeats so that long-running tasks are not mistaken for failed ones. The visibility timeout on the queue is extended periodically.</p>

<h4>Dead Letter Queue (DLQ)</h4>
<p>A separate queue that receives tasks which have exhausted all retries. It serves as a safety net ensuring no failed task is silently lost. An alerting/monitoring service can subscribe to the DLQ to notify operators or trigger manual intervention workflows.</p>

<h4>Callback Destination</h4>
<p>The external service or webhook endpoint that the task's <code>callback_url</code> points to. This is owned by the client and must be <strong>idempotent</strong> because at-least-once delivery means the callback could be invoked more than once for the same task execution.</p>

<!-- ============================================================ -->
<h2 id="flow3">5. Flow 3 — Task Status Query</h2>
<!-- ============================================================ -->
<h3>5.1 Diagram</h3>
<div class="mermaid">
flowchart LR
    C["Client"] -->|"HTTP GET<br/>/tasks/{task_id}"| LB["Load Balancer"]
    LB --> API["Task API<br/>Service"]
    API -->|"1. Check cache"| CACHE["In-Memory<br/>Cache"]
    CACHE -. "cache miss" .-> DB[("Task Store<br/>(SQL)")]
    DB -. "populate cache" .-> CACHE
    CACHE -->|"2. Return task"| API
    API -->|"200 OK + task details"| LB
    LB --> C

    API -->|"HTTP GET<br/>/tasks/{task_id}/executions"| LOG[("Execution Log<br/>(NoSQL)")]
</div>

<h3>5.2 Examples</h3>
<div class="example-card">
    <strong>Example 1 — Cache hit:</strong> A client polls <code>GET /tasks/task_abc123</code> to check if its report task is complete. The Load Balancer routes to an API instance. The API checks the in-memory cache for key <code>task:task_abc123</code>, finds a cache hit with <code>{"status": "COMPLETED", "last_execution": "2026-03-01T09:00:03Z"}</code>, and returns <code>200 OK</code> with the task details. Total latency: ~5ms.
</div>
<div class="example-card">
    <strong>Example 2 — Cache miss:</strong> A different client queries <code>GET /tasks/task_xyz789</code> which has not been queried recently. The cache returns a miss. The API Service queries the Task Store by primary key <code>task_id = 'task_xyz789'</code>, gets the row, populates the cache with a 60-second TTL, and returns <code>200 OK</code>. Total latency: ~20ms.
</div>
<div class="example-card">
    <strong>Example 3 — Execution history:</strong> A client calls <code>GET /tasks/task_abc123/executions</code> to see the full execution history (all attempts). The API Service queries the Execution Log (NoSQL) for all records with <code>task_id = 'task_abc123'</code>, returns a list of execution records sorted by <code>started_at DESC</code>, including status, duration, worker_id, and any error messages.
</div>

<h3>5.3 Component Deep Dive</h3>

<h4>Task API Service (Query Path)</h4>
<table>
    <tr><th>Endpoint</th><th>Method</th><th>Input</th><th>Output</th></tr>
    <tr>
        <td><code>/tasks/{task_id}</code></td>
        <td><code>GET</code></td>
        <td>Path param: <code>task_id</code></td>
        <td><code>200 OK</code> with task object (status, schedule, payload, etc.)</td>
    </tr>
    <tr>
        <td><code>/tasks/{task_id}/executions</code></td>
        <td><code>GET</code></td>
        <td>Path param: <code>task_id</code>, Query params: <code>limit</code>, <code>offset</code></td>
        <td><code>200 OK</code> with array of execution log entries</td>
    </tr>
    <tr>
        <td><code>/tasks?created_by={owner}</code></td>
        <td><code>GET</code></td>
        <td>Query params: <code>created_by</code>, <code>status</code>, <code>limit</code>, <code>cursor</code></td>
        <td><code>200 OK</code> with paginated list of tasks</td>
    </tr>
</table>

<h4>In-Memory Cache</h4>
<p>See the <a href="#cache">Cache Deep Dive</a> section for full details. The cache sits between the API layer and the Task Store to absorb repeated reads for hot tasks.</p>

<h4>Execution Log (NoSQL)</h4>
<p>A document/wide-column NoSQL store optimized for append-heavy writes and time-range queries. Each execution attempt creates a new document. The partition key is <code>task_id</code> and the sort key is <code>started_at</code>, enabling efficient retrieval of all executions for a task in chronological order.</p>

<!-- ============================================================ -->
<h2 id="flow4">6. Flow 4 — Task Cancellation / Update</h2>
<!-- ============================================================ -->
<h3>6.1 Diagram</h3>
<div class="mermaid">
flowchart LR
    C["Client"] -->|"HTTP PATCH or DELETE<br/>/tasks/{task_id}"| LB["Load Balancer"]
    LB --> API["Task API<br/>Service"]
    API -->|"1. Update status<br/>→ CANCELLED or<br/>update fields"| DB[("Task Store<br/>(SQL)")]
    API -->|"2. Invalidate<br/>cache entry"| CACHE["In-Memory<br/>Cache"]
    API -->|"200 OK"| LB
    LB --> C
</div>

<h3>6.2 Examples</h3>
<div class="example-card">
    <strong>Example 1 — Cancel a pending task:</strong> The analytics team no longer needs the report. They call <code>DELETE /tasks/task_abc123</code>. The API Service checks the current status. It is <code>SCHEDULED</code>, so it updates the status to <code>CANCELLED</code>, invalidates the cache entry <code>task:task_abc123</code>, and returns <code>200 OK</code>. When the scheduler later encounters this task (if it's within the poll window), it will see <code>CANCELLED</code> and skip it. If the task has already been queued on the Message Queue, the worker will check the DB status before executing and see <code>CANCELLED</code>, so it will ACK and discard the message without invoking the callback.
</div>
<div class="example-card">
    <strong>Example 2 — Cancel a running task:</strong> A client calls <code>DELETE /tasks/task_xyz789</code> but the task is currently <code>RUNNING</code>. The API Service returns <code>409 Conflict</code> with message <code>{"error": "Cannot cancel a task that is currently running. Wait for completion or let it time out."}</code>. Alternatively, the system can mark it as <code>CANCEL_REQUESTED</code> and the worker can check this flag periodically for cooperative cancellation.
</div>
<div class="example-card">
    <strong>Example 3 — Update a task's schedule:</strong> The billing team wants to change the recurring charge from midnight to 02:00 AM. They call <code>PATCH /tasks/task_billing_001</code> with body <code>{"cron_expression": "0 2 * * *"}</code>. The API Service recalculates <code>next_execution_time</code>, updates the row in the Task Store, invalidates the cache, and returns <code>200 OK</code>.
</div>

<h3>6.3 Component Deep Dive</h3>

<h4>Task API Service (Mutation Path)</h4>
<table>
    <tr><th>Endpoint</th><th>Method</th><th>Input</th><th>Output</th></tr>
    <tr>
        <td><code>/tasks/{task_id}</code></td>
        <td><code>DELETE</code></td>
        <td>Path param: <code>task_id</code></td>
        <td><code>200 OK</code> if cancelled; <code>409 Conflict</code> if running; <code>404</code> if not found</td>
    </tr>
    <tr>
        <td><code>/tasks/{task_id}</code></td>
        <td><code>PATCH</code></td>
        <td>Path param: <code>task_id</code>; JSON body with fields to update (<code>scheduled_time</code>, <code>cron_expression</code>, <code>payload</code>, <code>priority</code>, etc.)</td>
        <td><code>200 OK</code> with updated task object</td>
    </tr>
</table>
<p>For both operations the service wraps the database update and cache invalidation in a transaction to maintain consistency.</p>

<!-- ============================================================ -->
<h2 id="overall">7. Overall Combined Diagram</h2>
<!-- ============================================================ -->
<div class="mermaid">
flowchart TB
    C["Client<br/>(SDK / API Caller)"]

    subgraph API Layer
        LB["Load Balancer"]
        API1["Task API Service<br/>(Instance 1)"]
        API2["Task API Service<br/>(Instance N)"]
    end

    subgraph Data Layer
        CACHE["In-Memory Cache"]
        DB[("Task Store<br/>(SQL — Sharded)")]
        LOG[("Execution Log<br/>(NoSQL)")]
    end

    subgraph Scheduling Layer
        CS["Coordination<br/>Service"]
        SS1["Scheduler<br/>Instance 1"]
        SS2["Scheduler<br/>Instance N"]
    end

    subgraph Messaging Layer
        MQ["Message Queue<br/>(Priority Queues)"]
        DLQ["Dead Letter<br/>Queue"]
    end

    subgraph Execution Layer
        W1["Worker 1"]
        W2["Worker 2"]
        W3["Worker N"]
    end

    CB["Callback<br/>Destination"]
    ALERT["Alerting /<br/>Monitoring"]

    C -->|"POST/GET/PATCH/DELETE<br/>/tasks/*"| LB
    LB --> API1
    LB --> API2

    API1 --> CACHE
    API2 --> CACHE
    CACHE --> DB
    API1 --> DB
    API2 --> DB
    API1 --> LOG
    API2 --> LOG

    CS -. "partition<br/>assignment" .-> SS1
    CS -. "partition<br/>assignment" .-> SS2

    SS1 -->|"poll due tasks"| DB
    SS2 -->|"poll due tasks"| DB

    SS1 -->|"enqueue"| MQ
    SS2 -->|"enqueue"| MQ

    MQ --> W1
    MQ --> W2
    MQ --> W3

    W1 --> DB
    W2 --> DB
    W3 --> DB
    W1 --> LOG
    W2 --> LOG
    W3 --> LOG

    W1 -->|"HTTP POST<br/>callback"| CB
    W2 -->|"HTTP POST<br/>callback"| CB
    W3 -->|"HTTP POST<br/>callback"| CB

    W1 -. "retries exhausted" .-> DLQ
    W2 -. "retries exhausted" .-> DLQ
    W3 -. "retries exhausted" .-> DLQ

    DLQ --> ALERT
</div>

<h3>7.1 End-to-End Examples</h3>
<div class="example-card">
    <strong>Example 1 — Full happy-path lifecycle of a one-time task:</strong>
    <ol>
        <li><strong>Submission:</strong> The analytics client sends <code>POST /tasks</code> through the Load Balancer to API Instance 2, which validates the request, writes task <code>task_abc123</code> with status <code>SCHEDULED</code> and <code>next_execution_time = 2026-03-01T09:00:00Z</code> to the Task Store (shard determined by hash of task_id), and returns <code>201 Created</code>.</li>
        <li><strong>Waiting:</strong> The task sits in the Task Store. The scheduler's polling queries do not pick it up because <code>next_execution_time</code> is in the future.</li>
        <li><strong>Scheduling:</strong> At 09:00:01 UTC, Scheduler Instance 1 (which owns the partition containing <code>task_abc123</code>) runs its polling query, claims the task with <code>FOR UPDATE SKIP LOCKED</code>, updates status to <code>QUEUED</code>, and publishes a message to the medium-priority message queue.</li>
        <li><strong>Execution:</strong> Worker 3 consumes the message, verifies the task is not cancelled by checking the DB, updates status to <code>RUNNING</code>, sends <code>HTTP POST</code> to the callback URL with the payload, receives <code>200 OK</code>, updates status to <code>COMPLETED</code>, writes an execution log entry to the NoSQL Execution Log, and ACKs the message.</li>
        <li><strong>Status query:</strong> The analytics client polls <code>GET /tasks/task_abc123</code> → cache miss → queries Task Store → returns <code>{"status": "COMPLETED", ...}</code>. The response is cached for 60s.</li>
    </ol>
</div>
<div class="example-card">
    <strong>Example 2 — Recurring task with failure and recovery:</strong>
    <ol>
        <li><strong>Submission:</strong> Billing system submits a recurring task with <code>cron_expression = "0 0 * * *"</code>. API stores it with <code>next_execution_time = tonight 00:00 UTC</code>.</li>
        <li><strong>First execution attempt:</strong> Scheduler picks it up at midnight, enqueues it. Worker 1 invokes the callback but gets <code>503</code>. Worker sets <code>retry_count = 1</code>, computes back-off delay (30s), sets <code>next_execution_time = 00:00:30</code>, status back to <code>SCHEDULED</code>.</li>
        <li><strong>Second attempt:</strong> 30 seconds later the scheduler picks it up again. Worker 2 invokes the callback, gets <code>200 OK</code>. Status → <code>COMPLETED</code>.</li>
        <li><strong>Re-scheduling:</strong> Worker 2 sees <code>type = recurring</code>, evaluates the cron expression, sets <code>next_execution_time = tomorrow 00:00 UTC</code>, resets <code>retry_count = 0</code>, status → <code>SCHEDULED</code>.</li>
        <li><strong>Cycle continues</strong> every night.</li>
    </ol>
</div>
<div class="example-card">
    <strong>Example 3 — Task cancelled while queued:</strong>
    <ol>
        <li>Task <code>task_cancel_demo</code> is submitted and scheduled for 5 minutes from now.</li>
        <li>The scheduler picks it up and enqueues it. Status is now <code>QUEUED</code>.</li>
        <li>Before a worker picks it up, the client calls <code>DELETE /tasks/task_cancel_demo</code>. The API updates status to <code>CANCELLED</code> in the DB and invalidates the cache.</li>
        <li>Worker 1 picks up the message from the queue, performs the cancellation check (reads status from DB), sees <code>CANCELLED</code>, ACKs the message without invoking the callback, and writes an execution log entry noting the cancellation.</li>
    </ol>
</div>

<!-- ============================================================ -->
<h2 id="schema">8. Database Schema</h2>
<!-- ============================================================ -->

<h3>8.1 SQL — <code>tasks</code> Table</h3>
<div class="card">
    <p><span class="badge badge-sql">SQL</span> <strong>Why SQL?</strong> Task scheduling requires strong consistency and ACID transactions. The scheduler claims tasks using <code>SELECT … FOR UPDATE SKIP LOCKED</code>, which is an SQL-specific feature. We also need atomic status transitions (SCHEDULED → QUEUED → RUNNING → COMPLETED) to prevent duplicate execution. Relational integrity ensures referential consistency.</p>

    <table>
        <tr><th>Column</th><th>Type</th><th>Constraints</th><th>Description</th></tr>
        <tr><td><code>task_id</code></td><td>VARCHAR(64)</td><td><span class="badge badge-pk">PK</span></td><td>Globally unique task identifier (UUID or ULID)</td></tr>
        <tr><td><code>idempotency_key</code></td><td>VARCHAR(128)</td><td>UNIQUE, NULLABLE</td><td>Client-supplied key to prevent duplicate task creation</td></tr>
        <tr><td><code>task_type</code></td><td>ENUM('one_time', 'recurring')</td><td>NOT NULL</td><td>Whether the task runs once or on a schedule</td></tr>
        <tr><td><code>status</code></td><td>ENUM('SCHEDULED', 'QUEUED', 'RUNNING', 'COMPLETED', 'FAILED', 'CANCELLED')</td><td>NOT NULL</td><td>Current lifecycle state</td></tr>
        <tr><td><code>priority</code></td><td>SMALLINT</td><td>NOT NULL, DEFAULT 5</td><td>0 = highest, 10 = lowest</td></tr>
        <tr><td><code>cron_expression</code></td><td>VARCHAR(64)</td><td>NULLABLE</td><td>Cron string for recurring tasks (NULL for one-time)</td></tr>
        <tr><td><code>next_execution_time</code></td><td>TIMESTAMP</td><td>NOT NULL, INDEXED</td><td>When the task should next fire</td></tr>
        <tr><td><code>payload</code></td><td>JSON / TEXT</td><td>NOT NULL</td><td>Opaque payload delivered to the callback</td></tr>
        <tr><td><code>callback_url</code></td><td>VARCHAR(2048)</td><td>NOT NULL</td><td>HTTP endpoint to invoke on execution</td></tr>
        <tr><td><code>max_retries</code></td><td>INT</td><td>NOT NULL, DEFAULT 3</td><td>Maximum retry attempts</td></tr>
        <tr><td><code>retry_count</code></td><td>INT</td><td>NOT NULL, DEFAULT 0</td><td>Current retry count for the current execution cycle</td></tr>
        <tr><td><code>created_by</code></td><td>VARCHAR(128)</td><td>NOT NULL</td><td>Owner/creator identifier (service name or user ID)</td></tr>
        <tr><td><code>partition_key</code></td><td>SMALLINT</td><td>NOT NULL</td><td>Derived from hash(task_id) % num_partitions, used for scheduler partition assignment</td></tr>
        <tr><td><code>created_at</code></td><td>TIMESTAMP</td><td>NOT NULL, DEFAULT NOW()</td><td>Creation time</td></tr>
        <tr><td><code>updated_at</code></td><td>TIMESTAMP</td><td>NOT NULL, DEFAULT NOW()</td><td>Last update time</td></tr>
    </table>

    <h4>Indexes</h4>
    <table>
        <tr><th>Index Name</th><th>Columns</th><th>Type</th><th>Rationale</th></tr>
        <tr>
            <td><span class="badge badge-idx">IDX</span> <code>idx_scheduling</code></td>
            <td><code>(partition_key, status, next_execution_time, priority)</code></td>
            <td><strong>Composite B-Tree</strong></td>
            <td>This is the <em>critical</em> index. The scheduler's polling query filters by <code>partition_key IN (...)</code> and <code>status = 'SCHEDULED'</code>, orders by <code>next_execution_time ASC</code>, and breaks ties by <code>priority DESC</code>. A B-Tree composite index on these four columns means the query can be satisfied entirely from the index without scanning the table. The <code>partition_key</code> leading column ensures each scheduler instance only reads its assigned partitions.</td>
        </tr>
        <tr>
            <td><span class="badge badge-idx">IDX</span> <code>idx_owner_lookup</code></td>
            <td><code>(created_by, status, created_at)</code></td>
            <td><strong>Composite B-Tree</strong></td>
            <td>Supports the <code>GET /tasks?created_by=X&amp;status=Y</code> query for clients listing their own tasks. Filtering by owner first dramatically reduces the scan space.</td>
        </tr>
        <tr>
            <td><span class="badge badge-idx">IDX</span> <code>idx_idempotency</code></td>
            <td><code>(idempotency_key)</code></td>
            <td><strong>Hash Index</strong></td>
            <td>Exact-match lookups for idempotency checks during task creation. Hash indexing provides O(1) lookups for equality checks.</td>
        </tr>
    </table>

    <h4>Sharding Strategy</h4>
    <p><strong>Hash-based sharding on <code>task_id</code></strong>. We hash the <code>task_id</code> to determine which database shard the task lives on. This provides uniform data distribution since task IDs are UUIDs/ULIDs. The <code>partition_key</code> column (derived from <code>hash(task_id) % N</code>) aligns database shards with scheduler partitions, so each scheduler instance reads from a predictable set of shards, minimizing cross-shard queries.</p>
    <p><strong>Why not shard by <code>next_execution_time</code>?</strong> Range-based sharding on time would create severe hot-spots: all tasks scheduled in the near future would concentrate on one shard, making it the bottleneck for both the scheduler's read queries and the workers' status updates.</p>
    <p><strong>Why not shard by <code>created_by</code>?</strong> This would cause data skew — large tenants with millions of tasks would overload a single shard.</p>

    <h4>Read/Write Events</h4>
    <table>
        <tr><th>Event</th><th>Operation</th></tr>
        <tr><td>Client submits a task via <code>POST /tasks</code></td><td><strong>Write:</strong> INSERT new row</td></tr>
        <tr><td>Scheduler polls for due tasks</td><td><strong>Read + Write:</strong> SELECT ... FOR UPDATE SKIP LOCKED → UPDATE status to QUEUED</td></tr>
        <tr><td>Worker starts executing</td><td><strong>Write:</strong> UPDATE status to RUNNING</td></tr>
        <tr><td>Worker completes execution</td><td><strong>Write:</strong> UPDATE status to COMPLETED (or SCHEDULED for recurring + reset retry_count)</td></tr>
        <tr><td>Worker encounters failure</td><td><strong>Write:</strong> UPDATE retry_count, next_execution_time, status</td></tr>
        <tr><td>Client queries task status via <code>GET /tasks/{id}</code></td><td><strong>Read:</strong> SELECT by PK (cache miss path)</td></tr>
        <tr><td>Client cancels via <code>DELETE /tasks/{id}</code></td><td><strong>Write:</strong> UPDATE status to CANCELLED</td></tr>
        <tr><td>Client updates via <code>PATCH /tasks/{id}</code></td><td><strong>Write:</strong> UPDATE specified fields</td></tr>
    </table>
</div>

<h3>8.2 NoSQL — <code>execution_logs</code> Table</h3>
<div class="card">
    <p><span class="badge badge-nosql">NoSQL (Wide-Column / Document)</span> <strong>Why NoSQL?</strong> Execution logs are <em>append-only</em> — we never update an existing log entry. They are write-heavy (every execution attempt creates a new record) and read patterns are simple (get all logs for a task, sorted by time). There are no complex joins. A wide-column or document store excels at this pattern with natural partitioning by <code>task_id</code> and efficient range scans within a partition.</p>

    <table>
        <tr><th>Column</th><th>Type</th><th>Constraints</th><th>Description</th></tr>
        <tr><td><code>task_id</code></td><td>VARCHAR</td><td><span class="badge badge-pk">Partition Key</span></td><td>Links to the parent task</td></tr>
        <tr><td><code>execution_id</code></td><td>VARCHAR</td><td><span class="badge badge-pk">Sort Key</span> (ULID for time-ordering)</td><td>Unique execution attempt identifier</td></tr>
        <tr><td><code>attempt_number</code></td><td>INT</td><td>NOT NULL</td><td>Which attempt this is (1, 2, 3...)</td></tr>
        <tr><td><code>status</code></td><td>STRING</td><td>NOT NULL</td><td>RUNNING, COMPLETED, FAILED, CANCELLED</td></tr>
        <tr><td><code>started_at</code></td><td>TIMESTAMP</td><td>NOT NULL</td><td>When execution began</td></tr>
        <tr><td><code>completed_at</code></td><td>TIMESTAMP</td><td>NULLABLE</td><td>When execution ended (NULL if still running)</td></tr>
        <tr><td><code>duration_ms</code></td><td>BIGINT</td><td>NULLABLE</td><td>Execution duration in milliseconds</td></tr>
        <tr><td><code>worker_id</code></td><td>VARCHAR</td><td>NOT NULL</td><td>Which worker processed this execution</td></tr>
        <tr><td><code>callback_status_code</code></td><td>INT</td><td>NULLABLE</td><td>HTTP status code returned by the callback</td></tr>
        <tr><td><code>error_message</code></td><td>TEXT</td><td>NULLABLE</td><td>Error details if execution failed</td></tr>
    </table>

    <p><strong>No additional indexes are needed</strong> because the partition key (<code>task_id</code>) + sort key (<code>execution_id</code> as ULID, which is time-ordered) inherently supports the primary access pattern: "Get all executions for task X, sorted by time."</p>

    <h4>Read/Write Events</h4>
    <table>
        <tr><th>Event</th><th>Operation</th></tr>
        <tr><td>Worker begins executing a task</td><td><strong>Write:</strong> INSERT with status RUNNING</td></tr>
        <tr><td>Worker finishes (success or failure)</td><td><strong>Write:</strong> UPDATE the record with completed_at, status, duration_ms, etc.</td></tr>
        <tr><td>Client queries <code>GET /tasks/{id}/executions</code></td><td><strong>Read:</strong> Query by partition key task_id, range scan on sort key</td></tr>
    </table>

    <p><strong>Denormalization note:</strong> The <code>execution_logs</code> table stores the <code>task_id</code> directly rather than using a foreign key join to the <code>tasks</code> table. This is intentional denormalization. Since the two tables live in different database systems (SQL vs. NoSQL), joins are impossible. The denormalization trades minimal storage overhead (duplicating the task_id) for massive read performance gains and simpler architecture.</p>
</div>

<!-- ============================================================ -->
<h2 id="cache">9. CDN &amp; Cache Deep Dive</h2>
<!-- ============================================================ -->
<div class="card">
    <h3 style="margin-top:0">CDN — Not Appropriate</h3>
    <p>A CDN is <strong>not appropriate</strong> for this system. A distributed task scheduler is an API-driven backend service. It does not serve static assets (HTML, images, videos) or user-facing web pages. All interactions are programmatic HTTP API calls between services. CDNs are optimized for geographically distributed caching of static or semi-static content — none of which applies here. Adding a CDN would add cost and complexity with zero benefit.</p>

    <hr style="margin: 1.5rem 0">

    <h3>In-Memory Cache — Appropriate</h3>
    <p>An in-memory cache is placed between the Task API Service and the Task Store to accelerate <strong>read queries</strong> for task status.</p>

    <h4>What is Cached</h4>
    <ul>
        <li><strong>Task metadata by task_id:</strong> Key = <code>task:{task_id}</code>, Value = serialized task object (status, priority, schedule, etc.)</li>
        <li>Execution logs are <strong>not cached</strong> because they are less frequently queried and the NoSQL store already has low-latency reads.</li>
    </ul>

    <h4>Caching Strategy: Write-Through with Invalidation</h4>
    <p>When the Task API Service writes to the Task Store (creating, updating, or cancelling a task), it <strong>also writes the updated value to the cache</strong> in the same code path. This ensures the cache always has the latest data immediately after a write.</p>
    <p><strong>Why write-through?</strong></p>
    <ul>
        <li>Task status changes frequently (SCHEDULED → QUEUED → RUNNING → COMPLETED). Clients often poll <code>GET /tasks/{id}</code> shortly after submission to check if their task has run. Write-through ensures they see the latest status without hitting the database.</li>
        <li>Simpler than cache-aside for this use case since every write path goes through the API Service anyway.</li>
    </ul>
    <p><strong>Why not write-behind (write-back)?</strong> Write-behind buffers writes and asynchronously persists them, risking data loss on cache failure. For a task scheduler where durability is critical (NFR), this is unacceptable.</p>
    <p><strong>Why not cache-aside?</strong> Cache-aside would work but introduces a window where reads see stale data (between a DB write and the next cache miss). Since clients actively poll for status, stale reads would be confusing.</p>

    <h4>Cache Population</h4>
    <ul>
        <li><strong>On write:</strong> Any <code>POST</code>, <code>PATCH</code>, <code>DELETE</code>, or worker status update writes through to the cache.</li>
        <li><strong>On read (cache miss):</strong> If a GET request finds no cache entry, the API queries the Task Store, caches the result, and returns it. This handles tasks that were written by the scheduler/worker path which may have bypassed the API Service (e.g., scheduler directly updating DB).</li>
    </ul>

    <h4>Eviction Policy: LRU (Least Recently Used)</h4>
    <p>When the cache reaches its memory limit, the <strong>least recently used</strong> entries are evicted first.</p>
    <p><strong>Why LRU?</strong> Task status queries exhibit temporal locality — clients tend to repeatedly poll tasks that were recently created or are currently running. Older, completed tasks are rarely queried. LRU naturally retains the working set of "active" tasks while evicting stale completed ones.</p>

    <h4>Expiration Policy: TTL of 60 seconds</h4>
    <p>Every cache entry has a <strong>60-second TTL</strong>. Even if the entry is frequently accessed, it expires after 60 seconds and is refreshed from the database on the next read.</p>
    <p><strong>Why 60 seconds?</strong></p>
    <ul>
        <li>Task status can change rapidly (a task might go from QUEUED → RUNNING → COMPLETED in seconds). The write-through strategy handles most updates, but the scheduler and workers may also update the DB directly. The 60s TTL acts as a safety net to bound staleness.</li>
        <li>Short enough to keep data fresh; long enough to meaningfully reduce DB load for polling clients.</li>
    </ul>
</div>

<!-- ============================================================ -->
<h2 id="mq">10. Message Queue Deep Dive</h2>
<!-- ============================================================ -->
<div class="card">
    <h3 style="margin-top:0">Why a Message Queue?</h3>
    <p>The message queue <strong>decouples the scheduling layer from the execution layer</strong>. Without it, the scheduler would need to directly invoke workers, creating tight coupling, difficulty with load balancing across workers, and no buffering for bursts of due tasks. The queue provides:</p>
    <ul>
        <li><strong>Buffering:</strong> If 10,000 tasks are due at the same second (e.g., midnight cron jobs), the queue absorbs the burst and workers drain it at their own pace.</li>
        <li><strong>Reliability:</strong> Messages persist until ACKed. If a worker crashes mid-execution, the message becomes visible again after the visibility timeout.</li>
        <li><strong>Load balancing:</strong> Workers pull messages, naturally distributing load. No centralized dispatch logic.</li>
        <li><strong>Priority support:</strong> Multiple priority queues (or priority ordering) ensure high-priority tasks are consumed first.</li>
    </ul>

    <h4>Why Not Alternatives?</h4>
    <table>
        <tr><th>Alternative</th><th>Why Not?</th></tr>
        <tr>
            <td><strong>Pub/Sub</strong></td>
            <td>Pub/Sub broadcasts messages to all subscribers. We need <em>exactly one</em> worker to handle each task (point-to-point), not all of them. Pub/Sub's fan-out model would cause duplicate execution unless we add complex deduplication, defeating the purpose.</td>
        </tr>
        <tr>
            <td><strong>WebSockets</strong></td>
            <td>WebSockets are bidirectional, persistent connections designed for real-time client-server communication (e.g., chat). Task execution is asynchronous batch processing with no need for persistent connections between schedulers and workers. WebSockets would add unnecessary connection management overhead.</td>
        </tr>
        <tr>
            <td><strong>Polling the DB directly from workers</strong></td>
            <td>This would work but concentrates all load on the database. With thousands of workers polling every second, the DB becomes the bottleneck. The message queue offloads this fan-out.</td>
        </tr>
        <tr>
            <td><strong>Direct RPC from scheduler to workers</strong></td>
            <td>Creates tight coupling. The scheduler would need to know about all workers, handle worker failures, implement retry logic, and manage back-pressure — all things the message queue handles natively.</td>
        </tr>
    </table>

    <h4>How Messages Are Enqueued</h4>
    <ol>
        <li>The Scheduler claims due tasks from the DB via <code>SELECT ... FOR UPDATE SKIP LOCKED</code>.</li>
        <li>For each claimed task, the Scheduler constructs a message: <code>{"task_id": "...", "payload": {...}, "callback_url": "...", "attempt": N, "priority": 2, "enqueued_at": "..."}</code>.</li>
        <li>The Scheduler publishes the message to the appropriate priority queue (determined by the task's <code>priority</code> field).</li>
        <li>The Scheduler then updates the task status to <code>QUEUED</code> and commits the DB transaction.</li>
        <li>These steps are performed in a careful order: enqueue first, then update DB. If the DB update fails, the worker will pick up the message, check the DB, see the task is still <code>SCHEDULED</code>, and re-process it (safe due to idempotency). If the enqueue fails, the transaction is rolled back and the task remains <code>SCHEDULED</code> for the next poll.</li>
    </ol>

    <h4>How Messages Are Consumed (Dequeued)</h4>
    <ol>
        <li>Workers use a <strong>pull-based</strong> model: they long-poll the queue asking for the next available message from the highest-priority non-empty queue.</li>
        <li>When a message is delivered to a worker, it becomes <strong>invisible</strong> to other workers for the <strong>visibility timeout</strong> period (e.g., 5 minutes).</li>
        <li>The worker processes the task. If it succeeds, it sends an <strong>ACK</strong>, and the message is permanently deleted from the queue.</li>
        <li>If the worker fails or crashes without ACKing, the visibility timeout expires and the message becomes visible again for another worker to pick up.</li>
        <li>For long-running tasks, workers periodically <strong>extend the visibility timeout</strong> (heartbeat) to prevent the message from being re-delivered while still processing.</li>
        <li>Messages have a <strong>maximum receive count</strong> (e.g., 3). If a message is received that many times without being ACKed, it is automatically moved to the <strong>Dead Letter Queue</strong>.</li>
    </ol>

    <h4>Priority Queue Implementation</h4>
    <p>Three separate physical queues: <code>tasks-high</code>, <code>tasks-medium</code>, <code>tasks-low</code>. Workers always poll <code>tasks-high</code> first. Only when <code>tasks-high</code> is empty do they check <code>tasks-medium</code>, and so on. This ensures strict priority ordering. To prevent starvation of low-priority tasks, a configurable ratio can be used (e.g., 70% of polls to high, 20% to medium, 10% to low).</p>
</div>

<!-- ============================================================ -->
<h2 id="scaling">11. Scaling Considerations</h2>
<!-- ============================================================ -->
<div class="card">
    <h3 style="margin-top:0">11.1 Load Balancers</h3>
    <h4>Where Load Balancers Are Placed</h4>
    <ul>
        <li><strong>Between Clients and Task API Service:</strong> A Layer-7 (HTTP) load balancer distributes incoming API requests across the fleet of stateless API Service instances. This is the primary LB shown in all diagrams.</li>
        <li><strong>Between Message Queue and Workers (logical):</strong> The message queue itself acts as a natural load balancer — workers pull messages at their own rate, achieving work distribution without a dedicated LB. No additional load balancer is needed here.</li>
    </ul>

    <h4>Load Balancer Deep Dive</h4>
    <ul>
        <li><strong>Protocol:</strong> Layer-7 (HTTP/HTTPS). TLS termination at the LB; internal traffic between LB and API instances can use plain HTTP for reduced latency.</li>
        <li><strong>Algorithm:</strong> <strong>Least-connections</strong> is preferred over round-robin because API request durations vary (some writes require DB transactions; some reads hit the cache). Least-connections ensures that slower-processing instances receive fewer new requests.</li>
        <li><strong>Health checks:</strong> HTTP health check to <code>/healthz</code> on each API instance every 5 seconds. Two consecutive failures remove the instance from the pool.</li>
        <li><strong>Auto-scaling integration:</strong> The LB integrates with the auto-scaler — when new API instances are launched, they register with the LB; when instances are terminated, they are drained gracefully (stop receiving new requests, finish in-flight ones).</li>
        <li><strong>Rate limiting:</strong> The LB enforces per-client rate limits (e.g., 1,000 requests/sec per API key) to protect the system from a single abusive client.</li>
    </ul>

    <h3>11.2 Horizontal Scaling</h3>
    <table>
        <tr><th>Component</th><th>Scaling Strategy</th></tr>
        <tr>
            <td><strong>Task API Service</strong></td>
            <td>Stateless — scale horizontally behind the load balancer. Auto-scale based on CPU utilization or request rate. Target: keep each instance below 70% CPU.</td>
        </tr>
        <tr>
            <td><strong>Scheduler Instances</strong></td>
            <td>Add more scheduler instances and redistribute partitions via the Coordination Service. Each new instance takes ownership of a subset of partitions. Scale based on scheduler lag metric (time between task's scheduled time and when it was enqueued).</td>
        </tr>
        <tr>
            <td><strong>Worker Pool</strong></td>
            <td>Add more workers to increase execution throughput. Scale based on queue depth — if messages are accumulating faster than being processed, auto-scale up. Workers are stateless and can be added/removed freely.</td>
        </tr>
        <tr>
            <td><strong>Task Store (SQL)</strong></td>
            <td>Vertical scaling initially (bigger instances). Horizontal scaling via sharding (hash on task_id). Read replicas can absorb read load from status queries, with writes going to the primary. The scheduling query (<code>FOR UPDATE SKIP LOCKED</code>) must hit the primary.</td>
        </tr>
        <tr>
            <td><strong>Execution Log (NoSQL)</strong></td>
            <td>NoSQL stores scale horizontally natively. The partition key (<code>task_id</code>) ensures even distribution. Add nodes to increase throughput.</td>
        </tr>
        <tr>
            <td><strong>Message Queue</strong></td>
            <td>Add partitions to increase throughput. Most message queue systems allow dynamic partition addition.</td>
        </tr>
        <tr>
            <td><strong>In-Memory Cache</strong></td>
            <td>Scale by adding cache nodes in a distributed cache cluster. Use consistent hashing for key distribution to minimize cache misses during scaling events.</td>
        </tr>
    </table>

    <h3>11.3 Handling Burst Load (Thundering Herd)</h3>
    <p>A common scenario: thousands of cron jobs are scheduled for midnight. At 00:00:00, the scheduler suddenly finds 50,000 due tasks.</p>
    <ul>
        <li><strong>Scheduler:</strong> Uses <code>LIMIT</code> on the polling query to claim tasks in batches (e.g., 100 at a time). Multiple scheduler instances process in parallel.</li>
        <li><strong>Message Queue:</strong> Absorbs the burst. Workers drain at their maximum rate.</li>
        <li><strong>Workers:</strong> Auto-scale up based on queue depth. A pre-warm strategy can spin up additional workers a few minutes before known burst times (e.g., midnight).</li>
        <li><strong>Jitter:</strong> For recurring tasks, the system can add a small random jitter (e.g., 0–30 seconds) to the computed <code>next_execution_time</code> to spread out midnight tasks. This is opt-in per task.</li>
    </ul>
</div>

<!-- ============================================================ -->
<h2 id="tradeoffs">12. Tradeoffs &amp; Deep Dives</h2>
<!-- ============================================================ -->
<div class="card">
    <h3 style="margin-top:0">12.1 At-Least-Once vs. Exactly-Once Execution</h3>
    <p><strong>Chosen: At-least-once.</strong> True exactly-once delivery in a distributed system is extremely difficult (it requires distributed transactions across the queue and the task handler). Instead, we guarantee at-least-once and require task handlers to be <strong>idempotent</strong>. This is standard practice (e.g., payment processing uses idempotency keys). The system minimizes duplicates via:</p>
    <ul>
        <li>Database-level locking (<code>FOR UPDATE SKIP LOCKED</code>) prevents two schedulers from claiming the same task.</li>
        <li>Visibility timeout on the queue prevents two workers from processing the same message simultaneously.</li>
        <li>Status checks before execution (worker verifies task is still QUEUED/SCHEDULED, not RUNNING or COMPLETED).</li>
    </ul>

    <h3>12.2 Polling Interval Tradeoff</h3>
    <p>The scheduler polls the Task Store every <strong>1–2 seconds</strong>.</p>
    <ul>
        <li><strong>Shorter interval (e.g., 100ms):</strong> Better scheduling precision, but puts heavy read load on the database. With many scheduler instances, this can overwhelm the DB.</li>
        <li><strong>Longer interval (e.g., 10s):</strong> Lower DB load, but tasks may be delayed by up to 10 seconds. Acceptable for some use cases but not for time-sensitive tasks.</li>
        <li><strong>Chosen tradeoff (1–2s):</strong> Acceptable precision for most scheduling use cases (within the 1–5 second NFR target) with manageable DB load. The batching (<code>LIMIT 100</code>) further reduces per-query overhead.</li>
    </ul>

    <h3>12.3 SQL vs. NoSQL for the Task Store</h3>
    <p><strong>Chosen: SQL.</strong></p>
    <ul>
        <li><strong>Pro SQL:</strong> ACID transactions are essential for the scheduler's claim logic. <code>SELECT FOR UPDATE SKIP LOCKED</code> is an SQL feature. Strong consistency ensures a task is never double-claimed.</li>
        <li><strong>Pro NoSQL:</strong> Easier horizontal scaling, higher write throughput.</li>
        <li><strong>Decision:</strong> The correctness guarantees of SQL outweigh NoSQL's scaling benefits for the task table. We mitigate SQL's scaling limitations with sharding and read replicas.</li>
    </ul>

    <h3>12.4 Push vs. Pull Model for Workers</h3>
    <p><strong>Chosen: Pull (workers pull from the queue).</strong></p>
    <ul>
        <li><strong>Push:</strong> The scheduler/queue pushes tasks to workers. Simpler in theory, but requires the push coordinator to know about all workers, handle back-pressure, and deal with slow consumers. Risk of overwhelming a worker.</li>
        <li><strong>Pull:</strong> Workers request work when ready. Natural back-pressure — a busy worker simply doesn't request more work. Easier to scale (add workers without reconfiguring the push coordinator).</li>
    </ul>

    <h3>12.5 Callback Model vs. Embedded Execution</h3>
    <p><strong>Chosen: Callback model (workers invoke an HTTP endpoint).</strong></p>
    <ul>
        <li>The task scheduler does not execute arbitrary code — it delivers a payload to a URL. This provides a strong <strong>security boundary</strong> (no code injection risk) and makes the scheduler <strong>language-agnostic</strong> (callbacks can be any HTTP service in any language).</li>
        <li>Alternative: Workers pull a code artifact (e.g., a container image) and execute it. This is more powerful but far more complex (requires container orchestration, resource isolation, security sandboxing). It's effectively rebuilding a job execution platform, which is a different problem.</li>
    </ul>

    <h3>12.6 Single DB vs. Separate Scheduling Index</h3>
    <p><strong>Chosen: Single DB with composite index.</strong> An alternative is to maintain a separate in-memory data structure (e.g., a priority queue or timing wheel) for scheduling, with the DB as the source of truth. This reduces DB polling but adds complexity in keeping the in-memory structure synchronized. The composite B-Tree index on the SQL table is efficient enough for our throughput requirements.</p>
</div>

<!-- ============================================================ -->
<h2 id="alternatives">13. Alternative Approaches</h2>
<!-- ============================================================ -->
<div class="card">
    <div class="alt-card">
        <strong>Alternative 1: Delay Queue Only (No Separate Scheduler Service)</strong>
        <p>Instead of a scheduler service polling the DB, tasks are submitted directly to a delay/scheduled message queue with a delivery timestamp. The queue holds the message until the delivery time, then makes it available to workers.</p>
        <p><strong>Why not chosen:</strong></p>
        <ul>
            <li>Most message queues have limited delay capabilities (e.g., max 15 minutes in some systems). This doesn't support tasks scheduled days or weeks in the future.</li>
            <li>Updating or cancelling a delayed message in the queue is difficult or impossible in many queue systems. Our design allows easy modification via the DB.</li>
            <li>The queue would need to durably hold millions of delayed messages, which is not the sweet spot for most queue implementations.</li>
        </ul>
    </div>

    <div class="alt-card">
        <strong>Alternative 2: Hierarchical Timing Wheel</strong>
        <p>Use an in-memory timing wheel (similar to how OS kernels manage timers) distributed across nodes. Tasks are placed in time slots and "advanced" as the wheel ticks.</p>
        <p><strong>Why not chosen:</strong></p>
        <ul>
            <li>Timing wheels are excellent for in-process timer management but complex to make distributed and durable. Node failure loses the in-memory wheel state.</li>
            <li>Requires a WAL (write-ahead log) or persistent backing store anyway — at that point, we're effectively rebuilding the DB-polling approach with extra complexity.</li>
            <li>Better suited as an optimization within a scheduler node rather than the primary architecture.</li>
        </ul>
    </div>

    <div class="alt-card">
        <strong>Alternative 3: Change Data Capture (CDC) Pipeline</strong>
        <p>Instead of the scheduler polling the DB, use CDC to stream task inserts/updates to a stream processor. The stream processor triggers execution when <code>next_execution_time</code> arrives.</p>
        <p><strong>Why not chosen:</strong></p>
        <ul>
            <li>CDC captures <em>changes</em>, not time-based triggers. You'd still need a timer component to fire tasks at the right time.</li>
            <li>Adds significant infrastructure complexity (CDC pipeline, stream processor, windowing logic) for a problem that polling solves simply.</li>
            <li>CDC lag could add unpredictable scheduling delays.</li>
        </ul>
    </div>

    <div class="alt-card">
        <strong>Alternative 4: Distributed Cron (One Cron Daemon per Node)</strong>
        <p>Run a cron-like daemon on each node, with tasks distributed across nodes. Each node is responsible for its assigned tasks.</p>
        <p><strong>Why not chosen:</strong></p>
        <ul>
            <li>Cron is designed for a single machine. Distributing it requires solving coordination, failover, and rebalancing — which is what our scheduler service already does, but with better abstractions.</li>
            <li>No built-in queue buffering for burst scenarios.</li>
            <li>Limited observability and management capabilities compared to a purpose-built scheduler.</li>
        </ul>
    </div>

    <div class="alt-card">
        <strong>Alternative 5: Database-Triggered Approach (DB Scheduled Events / Stored Procedures)</strong>
        <p>Use the database's built-in scheduled event or trigger capability to fire tasks.</p>
        <p><strong>Why not chosen:</strong></p>
        <ul>
            <li>Database scheduled events are not designed for high-throughput task dispatch. They run in the DB process and can block other operations.</li>
            <li>Not portable across database vendors.</li>
            <li>Extremely difficult to scale — you can't distribute DB triggers across multiple nodes.</li>
            <li>Mixes concerns: the database should store data, not orchestrate task execution.</li>
        </ul>
    </div>
</div>

<!-- ============================================================ -->
<h2 id="additional">14. Additional Information</h2>
<!-- ============================================================ -->
<div class="card">
    <h3 style="margin-top:0">14.1 Idempotency</h3>
    <p>Idempotency is a cornerstone of this design. There are two levels:</p>
    <ul>
        <li><strong>Task creation idempotency:</strong> Clients attach an <code>Idempotency-Key</code> header. If a task with the same key already exists, the API returns the existing <code>task_id</code> instead of creating a duplicate. This handles network retries during submission.</li>
        <li><strong>Execution idempotency:</strong> The task handler (callback endpoint) must be idempotent because at-least-once delivery means the same task could be executed twice (e.g., if a worker crashes after completing but before ACKing). The scheduler passes an <code>execution_id</code> in headers so the handler can deduplicate.</li>
    </ul>

    <h3>14.2 Observability &amp; Monitoring</h3>
    <p>Key metrics to monitor:</p>
    <ul>
        <li><strong>Scheduler lag:</strong> <code>NOW() - next_execution_time</code> at the moment a task is enqueued. Alert if p99 exceeds 5 seconds.</li>
        <li><strong>Queue depth:</strong> Number of messages in each priority queue. Alert if growing consistently (workers can't keep up).</li>
        <li><strong>Execution success rate:</strong> Percentage of tasks completing successfully vs. failing. Alert if failure rate spikes.</li>
        <li><strong>DLQ depth:</strong> Number of messages in the dead letter queue. Any non-zero value should generate an alert.</li>
        <li><strong>Worker utilization:</strong> CPU/memory usage of worker fleet. Used for auto-scaling decisions.</li>
        <li><strong>API latency:</strong> p50, p95, p99 latency of task creation and status queries.</li>
    </ul>

    <h3>14.3 Multi-Tenancy</h3>
    <p>If the scheduler is shared across multiple teams/services, consider:</p>
    <ul>
        <li><strong>Rate limiting per tenant</strong> to prevent noisy neighbors.</li>
        <li><strong>Tenant-specific queues</strong> or queue weights to ensure fair scheduling.</li>
        <li><strong>Quotas</strong> on the number of tasks per tenant.</li>
    </ul>

    <h3>14.4 Security</h3>
    <ul>
        <li><strong>Authentication:</strong> Clients authenticate via API keys or mTLS. Each API key is associated with a tenant/service identity.</li>
        <li><strong>Authorization:</strong> Clients can only view, update, or cancel tasks they own (<code>created_by</code> matches their identity).</li>
        <li><strong>Callback validation:</strong> The scheduler only invokes callback URLs on an allow-list of internal domains to prevent SSRF attacks.</li>
        <li><strong>Payload encryption:</strong> Sensitive payloads can be encrypted at rest in the Task Store.</li>
    </ul>

    <h3>14.5 Graceful Shutdown &amp; Draining</h3>
    <p>When a worker or scheduler instance is being shut down (e.g., for a deployment):</p>
    <ul>
        <li><strong>Workers:</strong> Stop pulling new messages from the queue. Finish processing in-flight tasks (up to a timeout). ACK completed tasks. Any un-ACKed tasks will be redelivered by the queue after the visibility timeout.</li>
        <li><strong>Schedulers:</strong> Notify the Coordination Service to reassign partitions. Stop polling. The reassignment ensures no scheduling gap.</li>
    </ul>

    <h3>14.6 Time Zone and Clock Skew Handling</h3>
    <ul>
        <li>All times are stored and compared in <strong>UTC</strong>.</li>
        <li>Scheduler instances use <strong>NTP</strong> for clock synchronization. A clock skew of a few hundred milliseconds is tolerable given the 1–5 second scheduling precision target.</li>
        <li>Cron expressions are evaluated in UTC. If clients need time-zone-aware scheduling, the API Service converts the cron expression to UTC at task creation time and re-evaluates on DST transitions.</li>
    </ul>
</div>

<!-- ============================================================ -->
<h2 id="vendors">15. Vendor Suggestions</h2>
<!-- ============================================================ -->
<div class="card">
    <p>The design above is vendor-agnostic. Below are potential vendors for each technology category, for reference:</p>
    <table>
        <tr><th>Category</th><th>Vendor Options</th><th>Rationale</th></tr>
        <tr>
            <td><strong>SQL Database (Task Store)</strong></td>
            <td>PostgreSQL, MySQL, CockroachDB, Amazon Aurora, Google Cloud Spanner</td>
            <td>PostgreSQL and MySQL are mature, widely supported, and have excellent support for <code>SELECT FOR UPDATE SKIP LOCKED</code>. CockroachDB and Spanner offer global distribution if the scheduler needs to be multi-region. Aurora provides managed scaling.</td>
        </tr>
        <tr>
            <td><strong>NoSQL Database (Execution Log)</strong></td>
            <td>Apache Cassandra, Amazon DynamoDB, Google Bigtable, ScyllaDB</td>
            <td>All are wide-column stores optimized for high write throughput and partition-key lookups. Cassandra/ScyllaDB offer self-hosted flexibility. DynamoDB and Bigtable provide fully managed options with auto-scaling.</td>
        </tr>
        <tr>
            <td><strong>Message Queue</strong></td>
            <td>Apache Kafka, RabbitMQ, Amazon SQS, Apache Pulsar, NATS JetStream</td>
            <td>RabbitMQ and SQS have native support for priority queues and visibility timeouts, which align well with our design. Kafka offers higher throughput but requires more work for priority semantics. Pulsar combines the best of both worlds.</td>
        </tr>
        <tr>
            <td><strong>In-Memory Cache</strong></td>
            <td>Redis, Memcached, Hazelcast</td>
            <td>Redis is the most popular choice with rich data structures, TTL support, and LRU eviction. Memcached is simpler and faster for pure key-value caching. Hazelcast offers embedded Java caching.</td>
        </tr>
        <tr>
            <td><strong>Coordination Service</strong></td>
            <td>Apache ZooKeeper, etcd, Consul</td>
            <td>ZooKeeper is battle-tested for partition assignment and leader election. etcd is lighter-weight and used by Kubernetes. Consul adds service discovery capabilities. All support the distributed consensus needed for scheduler coordination.</td>
        </tr>
        <tr>
            <td><strong>Dead Letter Queue</strong></td>
            <td>Same as Message Queue (typically a separate queue/topic in the same system)</td>
            <td>DLQ is usually implemented as a dedicated queue within the same message queue infrastructure, so no additional vendor is needed.</td>
        </tr>
    </table>
</div>

<hr>
<p style="text-align:center; color: var(--text-secondary); font-size: 0.9rem; margin-top: 2rem;">
    System Design Document — Distributed Task Scheduler — Generated 2026-02-13
</p>

<script>
    mermaid.initialize({
        startOnLoad: true,
        theme: 'default',
        flowchart: {
            useMaxWidth: true,
            htmlLabels: true,
            curve: 'basis'
        }
    });
</script>
</body>
</html>
