<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>System Design: AWS (Cloud Computing Platform)</title>
<script src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script>
<style>
  :root { --bg: #0f1117; --card: #1a1d2e; --border: #2a2d3e; --text: #e0e0e0; --accent: #6c9bff; --accent2: #ff7eb3; --accent3: #7effc0; --heading: #ffffff; }
  * { box-sizing: border-box; margin: 0; padding: 0; }
  body { font-family: 'Segoe UI', system-ui, -apple-system, sans-serif; background: var(--bg); color: var(--text); line-height: 1.7; padding: 2rem; max-width: 1400px; margin: 0 auto; }
  h1 { color: var(--accent); font-size: 2.4rem; margin-bottom: 0.5rem; border-bottom: 3px solid var(--accent); padding-bottom: 0.5rem; }
  h2 { color: var(--accent2); font-size: 1.8rem; margin-top: 2.5rem; margin-bottom: 1rem; border-left: 4px solid var(--accent2); padding-left: 1rem; }
  h3 { color: var(--accent3); font-size: 1.3rem; margin-top: 1.5rem; margin-bottom: 0.7rem; }
  h4 { color: var(--accent); font-size: 1.1rem; margin-top: 1.2rem; margin-bottom: 0.5rem; }
  p, li { margin-bottom: 0.5rem; }
  ul, ol { padding-left: 1.5rem; margin-bottom: 1rem; }
  .card { background: var(--card); border: 1px solid var(--border); border-radius: 12px; padding: 1.5rem; margin: 1.5rem 0; }
  .diagram-container {
    background: #111827;
    border: 1px solid #2d3555;
    border-radius: 12px;
    padding: 2rem 1.5rem;
    margin: 1.5rem 0;
    overflow-x: auto;
  }
  table { width: 100%; border-collapse: collapse; margin: 1rem 0; }
  th { background: #2a2d4e; color: var(--accent); padding: 0.75rem; text-align: left; border: 1px solid var(--border); }
  td { padding: 0.75rem; border: 1px solid var(--border); }
  tr:nth-child(even) { background: rgba(255,255,255,0.03); }
  code { background: #2a2d4e; padding: 0.15rem 0.4rem; border-radius: 4px; font-family: 'Fira Code', 'Consolas', monospace; font-size: 0.9em; color: var(--accent3); }
  pre { background: #2a2d4e; padding: 1rem; border-radius: 8px; overflow-x: auto; margin: 1rem 0; }
  .example { background: #1e2a1e; border-left: 4px solid var(--accent3); padding: 1rem 1.5rem; margin: 1rem 0; border-radius: 0 8px 8px 0; }
  .warning { background: #2a2a1e; border-left: 4px solid #ffcc00; padding: 1rem 1.5rem; margin: 1rem 0; border-radius: 0 8px 8px 0; }
  .tradeoff { background: #2a1e2a; border-left: 4px solid var(--accent2); padding: 1rem 1.5rem; margin: 1rem 0; border-radius: 0 8px 8px 0; }
  .tag { display: inline-block; padding: 0.15rem 0.6rem; border-radius: 20px; font-size: 0.8em; margin-right: 0.4rem; }
  .tag-sql { background: #1e3a5f; color: #6cb4ff; }
  .tag-nosql { background: #3a1e5f; color: #b46cff; }
  .tag-cache { background: #1e5f3a; color: #6cffb4; }
  .subtitle { color: #888; font-size: 1.1rem; margin-bottom: 2rem; }
  hr { border: none; border-top: 1px solid var(--border); margin: 2rem 0; }
  .toc { column-count: 2; }
  .toc li { font-size: 0.95rem; }
  @media (max-width: 800px) { .toc { column-count: 1; } body { padding: 1rem; } }

  /* Mermaid diagram overrides for dark background readability */
  .diagram-container .mermaid svg {
    font-size: 14px;
  }

  /* Node styling */
  .diagram-container .node rect,
  .diagram-container .node polygon,
  .diagram-container .node circle,
  .diagram-container .node ellipse,
  .diagram-container .node .label-container {
    fill: #1e293b !important;
    stroke: #60a5fa !important;
    stroke-width: 2px !important;
  }

  /* Database cylinder nodes */
  .diagram-container .node path {
    fill: #1e1b4b !important;
    stroke: #a78bfa !important;
    stroke-width: 2px !important;
  }

  /* All node text */
  .diagram-container .node .nodeLabel,
  .diagram-container .node .label,
  .diagram-container .nodeLabel,
  .diagram-container .node text,
  .diagram-container .node foreignObject div {
    color: #e2e8f0 !important;
    fill: #e2e8f0 !important;
  }

  /* Edge lines */
  .diagram-container .flowchart-link,
  .diagram-container .edge-pattern-solid,
  .diagram-container path.path {
    stroke: #60a5fa !important;
    stroke-width: 1.5px !important;
  }

  /* Arrowheads */
  .diagram-container marker path {
    fill: #60a5fa !important;
    stroke: #60a5fa !important;
  }

  /* Edge labels */
  .diagram-container .edgeLabel {
    background-color: #1e293b !important;
    color: #cbd5e1 !important;
    padding: 2px 6px !important;
    border-radius: 4px !important;
  }
  .diagram-container .edgeLabel p,
  .diagram-container .edgeLabel span,
  .diagram-container .edgeLabel text,
  .diagram-container .edgeLabel .edgeLabel,
  .diagram-container .edgeLabel rect {
    color: #cbd5e1 !important;
    fill: #1e293b !important;
  }
  .diagram-container .edgeLabel div {
    color: #cbd5e1 !important;
  }

  /* Subgraph labels and boxes */
  .diagram-container .cluster rect {
    fill: #0f172a !important;
    stroke: #475569 !important;
    stroke-width: 1.5px !important;
    rx: 8 !important;
  }
  .diagram-container .cluster text,
  .diagram-container .cluster span {
    fill: #93c5fd !important;
    color: #93c5fd !important;
    font-weight: 600 !important;
  }
</style>
</head>
<body>

<h1>‚òÅÔ∏è System Design: AWS ‚Äî Cloud Computing Platform</h1>
<p class="subtitle">A vendor-agnostic design for a hyperscale cloud computing platform providing on-demand compute, object storage, identity management, networking, and monitoring.</p>

<!-- ============================================================ -->
<h2>üìã Table of Contents</h2>
<!-- ============================================================ -->
<div class="card">
<ol>
  <li><a href="#fr" style="color:var(--accent)">Functional Requirements</a></li>
  <li><a href="#nfr" style="color:var(--accent)">Non-Functional Requirements</a></li>
  <li><a href="#flow1" style="color:var(--accent)">Flow 1 ‚Äî Compute Instance Provisioning</a></li>
  <li><a href="#flow2" style="color:var(--accent)">Flow 2 ‚Äî Object Storage Upload</a></li>
  <li><a href="#flow3" style="color:var(--accent)">Flow 3 ‚Äî Object Storage Download</a></li>
  <li><a href="#flow4" style="color:var(--accent)">Flow 4 ‚Äî Authentication & Authorization (IAM)</a></li>
  <li><a href="#flow5" style="color:var(--accent)">Flow 5 ‚Äî Auto-Scaling</a></li>
  <li><a href="#flow6" style="color:var(--accent)">Flow 6 ‚Äî Networking (Virtual Private Cloud)</a></li>
  <li><a href="#overall" style="color:var(--accent)">Overall Combined Diagram</a></li>
  <li><a href="#schema" style="color:var(--accent)">Database Schema</a></li>
  <li><a href="#cdn-cache" style="color:var(--accent)">CDN & Caching Deep Dive</a></li>
  <li><a href="#scaling" style="color:var(--accent)">Scaling Considerations</a></li>
  <li><a href="#tradeoffs" style="color:var(--accent)">Tradeoffs & Deep Dives</a></li>
  <li><a href="#alternatives" style="color:var(--accent)">Alternative Approaches</a></li>
  <li><a href="#additional" style="color:var(--accent)">Additional Considerations</a></li>
  <li><a href="#vendors" style="color:var(--accent)">Vendor Recommendations</a></li>
</ol>
</div>

<!-- ============================================================ -->
<h2 id="fr">1. Functional Requirements</h2>
<!-- ============================================================ -->
<div class="card">
<h3>Compute Service</h3>
<ul>
  <li>Users can <strong>create/launch</strong> virtual machine (VM) instances with configurable CPU, memory, disk, and OS image.</li>
  <li>Users can <strong>start, stop, reboot, and terminate</strong> instances.</li>
  <li>Users can <strong>SSH/RDP</strong> into running instances.</li>
  <li>Users can attach <strong>block storage volumes</strong> to instances.</li>
  <li>Users can create <strong>machine images (snapshots)</strong> from running instances.</li>
</ul>

<h3>Object Storage Service</h3>
<ul>
  <li>Users can <strong>create and delete buckets</strong> (named containers for objects).</li>
  <li>Users can <strong>upload objects</strong> (files up to 5 TB) into buckets.</li>
  <li>Users can <strong>download objects</strong> from buckets.</li>
  <li>Users can <strong>list objects</strong> in a bucket with prefix filtering.</li>
  <li>Users can <strong>delete objects</strong>.</li>
  <li>Users can set <strong>access policies</strong> on buckets and objects.</li>
  <li>Users can enable <strong>versioning</strong> on a bucket.</li>
  <li>Objects are <strong>replicated</strong> across at least 3 availability zones for durability.</li>
</ul>

<h3>Identity & Access Management (IAM)</h3>
<ul>
  <li>Users can <strong>create accounts</strong> (root accounts and sub-accounts).</li>
  <li>Users can create <strong>IAM users, groups, and roles</strong>.</li>
  <li>Users can write and attach <strong>permission policies</strong> (JSON-based policy documents).</li>
  <li>Users can generate <strong>API access keys</strong> (access key ID + secret key).</li>
  <li>All API calls are <strong>authenticated and authorized</strong> against policies.</li>
  <li>Users can enable <strong>multi-factor authentication (MFA)</strong>.</li>
</ul>

<h3>Networking (VPC)</h3>
<ul>
  <li>Users can create <strong>virtual private clouds (VPCs)</strong> with custom CIDR blocks.</li>
  <li>Users can create <strong>subnets</strong> (public and private) within VPCs.</li>
  <li>Users can configure <strong>security groups</strong> (firewall rules).</li>
  <li>Users can set up <strong>load balancers</strong> to distribute traffic across instances.</li>
  <li>Users can allocate <strong>elastic (static) IP addresses</strong>.</li>
</ul>

<h3>Monitoring & Auto-Scaling</h3>
<ul>
  <li>System <strong>collects metrics</strong> (CPU, memory, network, disk) from all resources.</li>
  <li>Users can create <strong>alarms</strong> on metrics with thresholds.</li>
  <li>Users can create <strong>auto-scaling groups</strong> that automatically add/remove compute instances based on metric thresholds.</li>
  <li>Users can view <strong>dashboards</strong> with real-time and historical metrics.</li>
</ul>
</div>

<!-- ============================================================ -->
<h2 id="nfr">2. Non-Functional Requirements</h2>
<!-- ============================================================ -->
<div class="card">
<table>
<tr><th>Requirement</th><th>Target</th><th>Rationale</th></tr>
<tr><td><strong>Availability</strong></td><td>99.99% (‚â§ 52.6 min downtime/year) for control plane; 99.95% per-instance SLA</td><td>Cloud customers build mission-critical systems on this platform; any downtime cascades to millions of downstream users.</td></tr>
<tr><td><strong>Durability (Object Storage)</strong></td><td>99.999999999% (11 nines)</td><td>Customers trust the platform to never lose data. This requires cross-AZ replication with error-correcting codes.</td></tr>
<tr><td><strong>Latency (API)</strong></td><td>Control plane API calls &lt; 500ms p99; Object Storage GET &lt; 100ms p99 for small objects</td><td>Programmatic workflows depend on fast API responses.</td></tr>
<tr><td><strong>Scalability</strong></td><td>Millions of concurrent VMs; billions of stored objects; tens of thousands of API calls/second</td><td>Must support the largest enterprises and internet-scale applications.</td></tr>
<tr><td><strong>Security</strong></td><td>Encryption at rest (AES-256) and in transit (TLS 1.2+); tenant isolation; zero-trust between services</td><td>Multi-tenant platform must guarantee total isolation between customers.</td></tr>
<tr><td><strong>Consistency</strong></td><td>Strong consistency for compute/IAM metadata; strong read-after-write consistency for object storage</td><td>Users expect to immediately see newly created resources.</td></tr>
<tr><td><strong>Multi-Region</strong></td><td>20+ geographic regions, each with 3+ availability zones</td><td>Compliance (data sovereignty), disaster recovery, and latency reduction.</td></tr>
<tr><td><strong>Fault Tolerance</strong></td><td>Survive loss of an entire availability zone with no data loss</td><td>Hardware failures are a matter of "when", not "if" at this scale.</td></tr>
<tr><td><strong>Throughput (Object Storage)</strong></td><td>Hundreds of Gbps aggregate throughput per region</td><td>Large data-intensive workloads (ML training, analytics) require high bandwidth.</td></tr>
</table>
</div>

<!-- ============================================================ -->
<h2 id="flow1">3. Flow 1 ‚Äî Compute Instance Provisioning</h2>
<!-- ============================================================ -->
<div class="card">
<p>This flow covers the lifecycle of a user requesting a new virtual machine, from API call to a running instance with network connectivity.</p>

<div class="diagram-container">
<pre class="mermaid">
flowchart LR
    User["üë§ User / SDK / Console"]
    LB["‚öñÔ∏è Load Balancer"]
    API["üåê API Gateway"]
    IAM["üîê IAM Service"]
    CS["üñ•Ô∏è Compute Service<br/>(Control Plane)"]
    PS["üìç Placement Service"]
    MQ["üì¨ Message Queue"]
    HA["ü§ñ Host Agent<br/>(on Physical Server)"]
    HV["üíª Hypervisor<br/>(KVM)"]
    NC["üåê Network Controller"]
    MetaDB[("üóÑÔ∏è Compute<br/>Metadata DB")]
    ImgStore[("üìÄ Image<br/>Store")]
    BlockStore[("üíæ Block<br/>Storage")]

    User -->|"HTTPS POST<br/>/instances"| LB
    LB --> API
    API -->|"Authenticate<br/>+ Authorize"| IAM
    IAM -->|"‚úÖ Allowed"| API
    API -->|"CreateInstance<br/>Request"| CS
    CS -->|"Find optimal<br/>host"| PS
    PS -->|"Query capacity"| MetaDB
    MetaDB -->|"Host list +<br/>capacity"| PS
    PS -->|"Selected<br/>host_id"| CS
    CS -->|"Write instance<br/>record"| MetaDB
    CS -->|"Enqueue<br/>launch task"| MQ
    MQ -->|"Dequeue<br/>launch task"| HA
    HA -->|"Pull OS<br/>image"| ImgStore
    HA -->|"Allocate<br/>root disk"| BlockStore
    HA -->|"Create VM"| HV
    HA -->|"Assign VPC<br/>IP + SG"| NC
    HA -->|"Update status<br/>‚Üí RUNNING"| MetaDB
</pre>
</div>

<h3>Examples</h3>

<div class="example">
<strong>Example 1 ‚Äî Happy Path (Standard Launch):</strong> A developer calls <code>POST /instances</code> with body <code>{"image_id": "ubuntu-22.04", "instance_type": "m5.xlarge", "vpc_id": "vpc-abc123", "subnet_id": "subnet-priv1"}</code>. The request hits the Load Balancer, which routes it to the API Gateway. The API Gateway calls the IAM Service with the user's access key and the action <code>compute:CreateInstance</code>. IAM checks the user's policies and confirms authorization. The Compute Service receives the validated request, calls the Placement Service to find a physical host in the target AZ with at least 4 vCPUs and 16 GB RAM available. The Placement Service queries the Compute Metadata DB for host capacity and returns <code>host-7f2a</code>. The Compute Service writes a new instance record with status <code>PENDING</code> to the Metadata DB and enqueues a launch task on the Message Queue. The Host Agent on <code>host-7f2a</code> dequeues the task, pulls the Ubuntu 22.04 image from the Image Store, allocates a 100 GB root volume from Block Storage, instructs the Hypervisor (KVM) to create the VM, and tells the Network Controller to assign a private IP (<code>10.0.1.47</code>) within the subnet and apply security group rules. The Host Agent updates the instance status to <code>RUNNING</code> in the Metadata DB. The user polls <code>GET /instances/i-9f8e7d</code> and sees the instance is running with its assigned IP.
</div>

<div class="example">
<strong>Example 2 ‚Äî Insufficient Capacity:</strong> A data science team calls <code>POST /instances</code> requesting a GPU instance type <code>p4d.24xlarge</code> in <code>us-east-1a</code>. The Placement Service queries the Metadata DB and finds no available hosts with 8 A100 GPUs free. The Compute Service returns <code>HTTP 500 InsufficientCapacity</code> with message "There is not enough capacity for the requested instance type in the specified availability zone." The user retries with a different AZ (<code>us-east-1b</code>) where capacity exists, and the launch succeeds.
</div>

<div class="example">
<strong>Example 3 ‚Äî Permission Denied:</strong> A junior developer's IAM user has a policy that only allows <code>instance_type: t3.*</code>. They attempt to launch an <code>m5.4xlarge</code>. The IAM Service evaluates the request against the attached policy, finds a condition mismatch on <code>compute:instanceType</code>, and returns <code>HTTP 403 AccessDenied</code>. The API Gateway forwards this to the user. No instance record is created.
</div>

<h3>Component Deep Dive</h3>

<h4>Load Balancer</h4>
<p>A Layer 7 (HTTP/HTTPS) load balancer that terminates TLS connections from users. It performs health checks on API Gateway instances and distributes requests via <strong>least-connections</strong> routing. It also handles rate limiting per-account to protect the control plane from abuse.</p>

<h4>API Gateway</h4>
<p>Stateless HTTP service that accepts RESTful API calls. It performs request validation (schema checks), authentication delegation to IAM, request throttling, and routes to the appropriate backend service. Protocol: <strong>HTTPS</strong>.</p>
<ul>
  <li><code>POST /instances</code> ‚Äî Input: instance_type, image_id, vpc_id, subnet_id, security_group_ids, key_pair_name. Output: <code>{"instance_id": "i-xxx", "status": "PENDING"}</code></li>
  <li><code>GET /instances/{instance_id}</code> ‚Äî Input: instance_id (path param). Output: full instance details including status, IP, type, launch time.</li>
  <li><code>POST /instances/{instance_id}/stop</code> ‚Äî Input: instance_id. Output: <code>{"status": "STOPPING"}</code></li>
  <li><code>DELETE /instances/{instance_id}</code> ‚Äî Input: instance_id. Output: <code>{"status": "SHUTTING_DOWN"}</code></li>
</ul>

<h4>IAM Service</h4>
<p>Stateless gRPC service that validates HMAC-signed request signatures and evaluates JSON policy documents against the requested action, resource, and conditions. Returns allow/deny. Internal protocol: <strong>gRPC</strong> over TLS for low-latency inter-service calls.</p>
<ul>
  <li>Input: access_key_id, request_signature, action (e.g., <code>compute:CreateInstance</code>), resource ARN, conditions.</li>
  <li>Output: <code>{allowed: true/false, reason: "..."}</code></li>
</ul>

<h4>Compute Service (Control Plane)</h4>
<p>Stateless HTTP service that orchestrates instance lifecycle. It coordinates between Placement, networking, and the Host Agent layer. Protocol: <strong>HTTPS</strong> (external), <strong>gRPC</strong> (internal to Placement).</p>

<h4>Placement Service</h4>
<p>Stateless service that implements the bin-packing/scheduling algorithm. It considers: (1) available capacity on each host, (2) spread placement constraints (spread instances across fault domains), (3) affinity/anti-affinity rules, (4) AZ preference. It queries the Compute Metadata DB for real-time host capacity. Protocol: <strong>gRPC</strong>.</p>
<ul>
  <li>Input: instance_type, az, placement_constraints.</li>
  <li>Output: <code>{host_id: "host-7f2a", rack_id: "rack-12"}</code></li>
</ul>

<h4>Message Queue</h4>
<p>An asynchronous, durable message queue that decouples the control plane from the data plane. When the Compute Service decides where to place an instance, it enqueues a <code>LaunchInstance</code> message containing the full specification. The Host Agent polls the queue for tasks assigned to its host. Messages are acknowledged only after the VM is successfully created, ensuring at-least-once delivery. If the Host Agent crashes, the message reappears after a visibility timeout and can be picked up again (idempotency is ensured via the instance_id). See the <a href="#mq-deepdive" style="color:var(--accent)">Message Queue Deep Dive</a> for details.</p>

<h4>Host Agent</h4>
<p>A daemon process running on every physical server. It manages the local hypervisor, pulls machine images, allocates local/network disks, and reports host health + capacity metrics. It communicates with the control plane via the Message Queue (pull model) and periodically heartbeats to the Metadata DB.</p>

<h4>Hypervisor (KVM)</h4>
<p>The hardware virtualization layer that creates and manages virtual machines on the physical host. The Host Agent interacts with it via <code>libvirt</code> API calls. Each VM gets isolated vCPUs, memory, and virtual network interfaces.</p>

<h4>Network Controller</h4>
<p>Manages the software-defined networking (SDN) layer. It programs virtual switches (Open vSwitch) on each host to enforce VPC isolation, security group rules, and routes traffic between subnets. It assigns private IPs from the subnet's CIDR pool and manages NAT gateways for internet access. Protocol: <strong>gRPC</strong>.</p>

<h4>Compute Metadata DB</h4>
<p>A strongly consistent SQL database (leader-follower replication within an AZ) that stores instance records, host capacity, placement history, and reservation state. Queried by the Placement Service for scheduling decisions and updated by Host Agents for status changes.</p>

<h4>Image Store</h4>
<p>A read-optimized object store that holds machine images (AMIs). Images are immutable once created and are replicated across AZs. Cached at the rack level to avoid repeated downloads when many instances launch the same image.</p>

<h4>Block Storage</h4>
<p>A network-attached block storage system that provides persistent disk volumes to VMs. Data is replicated synchronously across 3 nodes in the AZ for durability. Supports snapshot to Object Storage for backups.</p>
</div>

<!-- ============================================================ -->
<h2 id="flow2">4. Flow 2 ‚Äî Object Storage Upload (PUT Object)</h2>
<!-- ============================================================ -->
<div class="card">
<p>This flow covers a user uploading a file (object) to a storage bucket.</p>

<div class="diagram-container">
<pre class="mermaid">
flowchart LR
    User["üë§ User / SDK"]
    LB["‚öñÔ∏è Load Balancer"]
    API["üåê API Gateway"]
    IAM["üîê IAM Service"]
    ObjAPI["üì¶ Object Storage<br/>API Frontend"]
    MetaSvc["üìá Object Metadata<br/>Service"]
    MetaDB[("üóÑÔ∏è Object<br/>Metadata DB")]
    SN1[("üíæ Storage<br/>Node 1<br/>(AZ-a)")]
    SN2[("üíæ Storage<br/>Node 2<br/>(AZ-b)")]
    SN3[("üíæ Storage<br/>Node 3<br/>(AZ-c)")]

    User -->|"HTTPS PUT<br/>/bucket/key"| LB
    LB --> API
    API -->|"Auth"| IAM
    IAM -->|"‚úÖ"| API
    API --> ObjAPI
    ObjAPI -->|"Generate<br/>chunk plan"| MetaSvc
    MetaSvc -->|"Write<br/>metadata"| MetaDB
    ObjAPI -->|"Stream data<br/>(primary)"| SN1
    SN1 -->|"Replicate"| SN2
    SN1 -->|"Replicate"| SN3
    SN2 -->|"ACK"| SN1
    SN3 -->|"ACK"| SN1
    SN1 -->|"All ACKs<br/>received"| ObjAPI
    ObjAPI -->|"HTTP 200<br/>+ ETag"| User
</pre>
</div>

<h3>Examples</h3>

<div class="example">
<strong>Example 1 ‚Äî Small Object Upload (Happy Path):</strong> A web app calls <code>PUT /my-bucket/photos/vacation.jpg</code> with a 5 MB JPEG in the request body and a <code>Content-MD5</code> header. The Load Balancer routes to the API Gateway, which delegates authentication to IAM (checks <code>storage:PutObject</code> permission on <code>arn:cloud:storage:::my-bucket/photos/*</code>). IAM approves. The Object Storage API Frontend asks the Metadata Service for the optimal storage node set. The Metadata Service writes a metadata record <code>{bucket: "my-bucket", key: "photos/vacation.jpg", version: 1, size: 5MB, nodes: [SN1, SN2, SN3], status: "UPLOADING"}</code> to the Object Metadata DB. The API Frontend streams the data to Storage Node 1 (primary), which writes it locally and replicates to SN2 and SN3 in parallel. Once all 3 nodes ACK, the API Frontend updates the metadata status to <code>COMMITTED</code> and returns <code>HTTP 200</code> with an <code>ETag: "d41d8cd98f00b204e9800998ecf8427e"</code> to the user.
</div>

<div class="example">
<strong>Example 2 ‚Äî Large Object Upload (Multipart):</strong> A data engineer uploads a 2 GB dataset. The SDK automatically initiates a multipart upload: (1) <code>POST /my-bucket/data.csv?uploads</code> gets an <code>upload_id</code>. (2) The SDK splits the file into 100 MB chunks and sends each via <code>PUT /my-bucket/data.csv?partNumber=1&uploadId=xxx</code> in parallel (up to 10 concurrent). Each part is replicated across 3 storage nodes. (3) After all 20 parts upload, the SDK calls <code>POST /my-bucket/data.csv?uploadId=xxx</code> with the part list to complete the upload. The Metadata Service creates a manifest linking all parts as a single object. The user gets <code>HTTP 200</code>.
</div>

<div class="example">
<strong>Example 3 ‚Äî Replication Failure (Partial):</strong> During upload, SN3 fails to ACK within the timeout (the server crashed). The API Frontend detects 2 of 3 ACKs (the write quorum is 2 of 3). It still returns success to the user. A background repair process detects that SN3 is missing the object and schedules a re-replication from SN1 to a replacement node SN4. Durability is temporarily reduced to 2 copies but is restored automatically.
</div>

<h3>Component Deep Dive</h3>

<h4>Object Storage API Frontend</h4>
<p>Stateless HTTP service that implements the object storage API (compatible with S3-like semantics). Handles chunked transfer encoding for large uploads, computes checksums, and coordinates the write path. Protocol: <strong>HTTPS</strong>.</p>
<ul>
  <li><code>PUT /{bucket}/{key}</code> ‚Äî Input: object bytes, Content-MD5, metadata headers. Output: <code>HTTP 200</code> + ETag.</li>
  <li><code>POST /{bucket}/{key}?uploads</code> ‚Äî Initiate multipart upload. Output: <code>upload_id</code>.</li>
  <li><code>PUT /{bucket}/{key}?partNumber=N&uploadId=X</code> ‚Äî Upload a part. Output: ETag for the part.</li>
  <li><code>POST /{bucket}/{key}?uploadId=X</code> ‚Äî Complete multipart upload. Input: list of part numbers + ETags. Output: final ETag.</li>
</ul>

<h4>Object Metadata Service</h4>
<p>Manages the mapping from <code>(bucket, key)</code> ‚Üí physical storage locations. It selects which storage nodes to write to (balancing across AZs, preferring nodes with lower utilization). Protocol: <strong>gRPC</strong> (internal).</p>
<ul>
  <li>Input: bucket, key, object_size.</li>
  <li>Output: <code>{node_set: [SN1, SN2, SN3], chunk_plan: [...]}</code></li>
</ul>

<h4>Object Metadata DB</h4>
<p>Stores every object's metadata: bucket, key, version, size, checksum, creation time, storage class, node locations, ACLs. This is a strongly consistent, partitioned NoSQL store with the partition key as <code>(bucket, key)</code>. See <a href="#schema" style="color:var(--accent)">Schema</a> section.</p>

<h4>Storage Nodes</h4>
<p>Physical servers with large disk arrays (HDDs for standard tier, SSDs for performance tier). Each node manages its local file system, handles reads/writes, performs data integrity checks (periodic checksum verification), and participates in replication. Nodes in different AZs ensure that data survives AZ-level failures. Communication between nodes for replication uses <strong>TCP</strong> (custom binary protocol) for maximum throughput.</p>
</div>

<!-- ============================================================ -->
<h2 id="flow3">5. Flow 3 ‚Äî Object Storage Download (GET Object)</h2>
<!-- ============================================================ -->
<div class="card">
<p>This flow covers a user downloading a file from object storage.</p>

<div class="diagram-container">
<pre class="mermaid">
flowchart LR
    User["üë§ User / SDK"]
    CDN["üåç CDN<br/>(Edge Cache)"]
    LB["‚öñÔ∏è Load Balancer"]
    API["üåê API Gateway"]
    IAM["üîê IAM Service"]
    ObjAPI["üì¶ Object Storage<br/>API Frontend"]
    Cache["‚ö° Metadata<br/>Cache"]
    MetaDB[("üóÑÔ∏è Object<br/>Metadata DB")]
    SN["üíæ Storage<br/>Node<br/>(nearest)"]

    User -->|"HTTPS GET<br/>/bucket/key"| CDN
    CDN -->|"Cache MISS"| LB
    LB --> API
    API -->|"Auth"| IAM
    IAM -->|"‚úÖ"| API
    API --> ObjAPI
    ObjAPI -->|"Lookup<br/>metadata"| Cache
    Cache -->|"MISS"| MetaDB
    MetaDB -->|"node_set +<br/>metadata"| Cache
    Cache -->|"SN locations"| ObjAPI
    ObjAPI -->|"Stream bytes"| SN
    SN -->|"Object data"| ObjAPI
    ObjAPI -->|"Stream to<br/>user via CDN"| User
</pre>
</div>

<h3>Examples</h3>

<div class="example">
<strong>Example 1 ‚Äî CDN Hit (Public Object):</strong> A user's browser requests <code>GET /static-assets/logo.png</code> from a public bucket configured with CDN distribution. The CDN edge node in the user's city has the object cached (it was fetched within the TTL). The CDN returns <code>HTTP 200</code> with the cached PNG directly ‚Äî zero round trips to origin. Latency: ~10ms.
</div>

<div class="example">
<strong>Example 2 ‚Äî CDN Miss, Cache Hit (Private Object):</strong> A mobile app calls <code>GET /user-data/config.json</code> with a pre-signed URL. The CDN doesn't have it cached (it's a private, infrequently accessed object). The request flows through to the API Gateway ‚Üí IAM (validates the pre-signed URL signature and expiration) ‚Üí Object Storage API Frontend. The Frontend checks the Metadata Cache for the key's storage location ‚Äî cache HIT. It reads from the nearest Storage Node and streams the response back. The CDN caches the response for subsequent requests (respecting the <code>Cache-Control</code> header from the signed URL).
</div>

<div class="example">
<strong>Example 3 ‚Äî Storage Node Failure During Read:</strong> The API Frontend attempts to read from SN1 (the primary), but SN1 is unreachable (network partition). The Frontend automatically falls back to SN2 (the second replica in the node set from metadata). SN2 returns the data successfully. The user is unaware of the failure ‚Äî they just get their object. A background health check marks SN1 as unhealthy and triggers re-replication.
</div>

<h3>Component Deep Dive</h3>

<h4>CDN (Content Delivery Network)</h4>
<p>A globally distributed network of edge servers that cache frequently accessed objects close to users. Detailed in the <a href="#cdn-cache" style="color:var(--accent)">CDN & Caching Deep Dive</a> section. Reduces latency for public/static content from hundreds of milliseconds to single-digit milliseconds. Also offloads massive read traffic from origin storage.</p>

<h4>Metadata Cache</h4>
<p>An in-memory cache (deployed per-region) that caches the object metadata lookups. Since the read path for any object always requires a metadata lookup to find which storage nodes hold the data, caching this avoids hitting the Metadata DB on every GET. See <a href="#cdn-cache" style="color:var(--accent)">CDN & Caching Deep Dive</a> for caching strategy details.</p>
</div>

<!-- ============================================================ -->
<h2 id="flow4">6. Flow 4 ‚Äî Authentication & Authorization (IAM)</h2>
<!-- ============================================================ -->
<div class="card">
<p>This flow shows how every API request is authenticated and authorized before any action is taken.</p>

<div class="diagram-container">
<pre class="mermaid">
flowchart LR
    User["üë§ User / SDK"]
    API["üåê API Gateway"]
    IAM["üîê IAM Service"]
    PolicyCache["‚ö° Policy<br/>Cache"]
    IAM_DB[("üóÑÔ∏è IAM<br/>Database")]
    TargetSvc["üéØ Target Service<br/>(Compute / Storage / etc)"]

    User -->|"HTTPS request +<br/>Signature V4 Header"| API
    API -->|"Extract access_key_id<br/>+ signature"| IAM
    IAM -->|"Lookup<br/>user + policies"| PolicyCache
    PolicyCache -->|"MISS"| IAM_DB
    IAM_DB -->|"User record +<br/>attached policies"| PolicyCache
    PolicyCache -->|"Policies"| IAM
    IAM -->|"Evaluate:<br/>action + resource +<br/>conditions vs policies"| IAM
    IAM -->|"‚úÖ Allow / ‚ùå Deny"| API
    API -->|"If allowed,<br/>forward request"| TargetSvc
    API -->|"If denied,<br/>HTTP 403"| User
</pre>
</div>

<h3>Examples</h3>

<div class="example">
<strong>Example 1 ‚Äî Successful Authentication + Authorization:</strong> A CI/CD pipeline sends <code>POST /instances</code> with an <code>Authorization</code> header containing a Signature V4 signed request. The API Gateway extracts the access key ID (<code>AKIA...</code>) and forwards it with the string-to-sign and provided signature to the IAM Service. IAM looks up the access key in the Policy Cache (HIT ‚Äî the pipeline runs every hour). It retrieves the secret key associated with the access key, recomputes the expected signature using the same algorithm (HMAC-SHA256), and confirms it matches the provided signature ‚Äî <strong>authentication passes</strong>. IAM then collects all policies attached to the IAM user, its groups, and any session policies. It evaluates each policy's statements against the action <code>compute:CreateInstance</code> and the resource ARN. One policy grants <code>compute:*</code> on <code>*</code> ‚Äî <strong>authorization passes</strong>. IAM returns <code>Allow</code> to the API Gateway, which forwards the request to the Compute Service.
</div>

<div class="example">
<strong>Example 2 ‚Äî Explicit Deny Overrides Allow:</strong> An admin created a policy attached to a user that allows <code>storage:*</code> on all buckets. However, there's also a Service Control Policy (SCP) at the organization level that explicitly denies <code>storage:DeleteBucket</code>. The user calls <code>DELETE /critical-data-bucket</code>. IAM evaluates all policies: the user policy says <code>Allow</code>, but the SCP says <code>Deny</code>. Per the policy evaluation logic, an explicit <code>Deny</code> always overrides any <code>Allow</code>. IAM returns <code>Deny</code>, and the API Gateway returns <code>HTTP 403 AccessDenied</code>.
</div>

<div class="example">
<strong>Example 3 ‚Äî Temporary Credentials (Assume Role):</strong> A compute instance needs to access object storage. Instead of embedding long-lived keys, it calls the local metadata endpoint (<code>GET http://169.254.169.254/latest/meta-data/iam/security-credentials/my-role</code>). The instance's Host Agent proxies this to the IAM Service, which generates temporary credentials (access key, secret key, session token) valid for 1 hour by issuing a <code>AssumeRole</code> call. The instance uses these temporary credentials to call the Object Storage API. IAM validates the session token and evaluates the role's policies.
</div>

<h3>Component Deep Dive</h3>

<h4>IAM Service (Detail)</h4>
<p>Stateless gRPC service that handles two core functions:</p>
<ol>
  <li><strong>Authentication</strong>: Validates request signatures. The user's SDK computes an HMAC-SHA256 signature over the canonical request (method, path, query, headers, payload hash) using the secret access key. IAM recomputes this and compares.</li>
  <li><strong>Authorization</strong>: Evaluates policies. The evaluation order is: (1) Explicit Deny ‚Üí deny immediately. (2) SCPs ‚Üí must have at least one Allow. (3) Resource policies (bucket policies, etc.) ‚Üí may grant cross-account access. (4) Identity policies (user/group/role policies) ‚Üí must have at least one Allow. If no statement applies ‚Üí implicit deny.</li>
</ol>
<p>Protocol: <strong>gRPC</strong> (internal). Response time target: &lt; 5ms p99 (most calls served from Policy Cache).</p>

<h4>Policy Cache</h4>
<p>An in-memory cache that stores frequently accessed IAM user/role records and their attached policies. Keyed by <code>access_key_id</code>. TTL: 60 seconds. Invalidation: when a user updates their policies, a cache invalidation event is published via an internal pub-sub topic that all IAM service instances subscribe to. See <a href="#cdn-cache" style="color:var(--accent)">CDN & Caching Deep Dive</a>.</p>

<h4>IAM Database</h4>
<p>A strongly consistent SQL database that stores all IAM entities: accounts, users, groups, roles, policies, access keys, and MFA configurations. Strong consistency is critical because a permission change must take effect immediately (eventual consistency could create a security window where a revoked permission still works).</p>
</div>

<!-- ============================================================ -->
<h2 id="flow5">7. Flow 5 ‚Äî Auto-Scaling</h2>
<!-- ============================================================ -->
<div class="card">
<p>This flow shows how the platform automatically scales compute instances up or down based on metrics and user-defined policies.</p>

<div class="diagram-container">
<pre class="mermaid">
flowchart LR
    HA["ü§ñ Host Agents<br/>(all hosts)"]
    MC["üìä Metrics<br/>Collector"]
    TSDB[("üìà Time-Series<br/>DB")]
    AE["üîî Alarm<br/>Engine"]
    ASG["üìê Auto-Scaling<br/>Service"]
    CS["üñ•Ô∏è Compute<br/>Service"]
    MQ["üì¨ Message<br/>Queue"]
    MetaDB[("üóÑÔ∏è Scaling<br/>Policy DB")]

    HA -->|"Push metrics<br/>(CPU, mem, net)<br/>every 60s"| MC
    MC -->|"Aggregate +<br/>store"| TSDB
    AE -->|"Poll metrics<br/>every 60s"| TSDB
    AE -->|"Evaluate<br/>alarm rules"| AE
    AE -->|"ALARM state:<br/>trigger action"| MQ
    MQ -->|"ScaleUp/Down<br/>event"| ASG
    ASG -->|"Read scaling<br/>policy"| MetaDB
    ASG -->|"Launch N<br/>instances"| CS
    ASG -->|"Or terminate<br/>N instances"| CS
</pre>
</div>

<h3>Examples</h3>

<div class="example">
<strong>Example 1 ‚Äî Scale-Out (Traffic Spike):</strong> An e-commerce site experiences a flash sale. The auto-scaling group has a policy: "If average CPU across all instances > 70% for 2 consecutive minutes, add 4 instances." Host Agents push CPU metrics every 60s to the Metrics Collector, which writes to the Time-Series DB. The Alarm Engine polls the TSDB and detects that the average CPU has been 85% for 2 minutes. It transitions the alarm to <code>ALARM</code> state and publishes a <code>ScaleUp</code> event to the Message Queue. The Auto-Scaling Service dequeues the event, reads the scaling policy from the Policy DB (min: 2, max: 20, desired: 6, step: 4), calculates the new desired count as <code>min(10, 20) = 10</code>, and calls the Compute Service to launch 4 new instances. The Compute Service goes through the standard provisioning flow (Flow 1). After ~90 seconds, the new instances are RUNNING, registered with the load balancer, and absorbing traffic. CPU drops to 55%.
</div>

<div class="example">
<strong>Example 2 ‚Äî Scale-In (Off-Peak):</strong> At 3 AM, traffic drops. Average CPU falls to 20% for 10 minutes. The Alarm Engine fires a <code>ScaleDown</code> alarm. The Auto-Scaling Service calculates the new desired count as <code>max(4, 2) = 4</code> (removing 6 instances). It selects instances to terminate using the "oldest first" strategy (to keep the freshest instances), waits for in-flight requests to drain (connection draining for 30 seconds), then calls <code>DELETE /instances/{id}</code> for each. The Compute Service terminates them.
</div>

<div class="example">
<strong>Example 3 ‚Äî Cooldown Period Prevents Thrashing:</strong> After a scale-out event adds 4 instances, the CPU temporarily dips to 60% as new instances absorb load, then briefly spikes to 72% again as traffic keeps rising. The Alarm Engine fires again, but the Auto-Scaling Service checks the cooldown timer (300 seconds since last action) and finds only 120 seconds have elapsed. It ignores the event. After the cooldown expires, if CPU is still high, it scales out again. This prevents rapid, wasteful add/remove cycles.
</div>

<h3>Component Deep Dive</h3>

<h4>Metrics Collector</h4>
<p>A fleet of stateless ingest servers that receive metric data points via <strong>UDP</strong> (for high throughput, low overhead ‚Äî losing an occasional metric point is acceptable since metrics are aggregated over time windows). Each Host Agent pushes CPU, memory, disk I/O, and network metrics every 60 seconds. The Metrics Collector aggregates data (averaging per-instance metrics over the auto-scaling group) and writes to the Time-Series DB.</p>

<h4>Time-Series DB</h4>
<p>A specialized time-series database optimized for high write throughput and range queries over time windows. Stores metric data points in the format <code>(metric_name, dimensions, timestamp, value)</code>. Retention: 15 months at 1-minute resolution, then downsampled to 5-minute resolution for 5 years. See <a href="#schema" style="color:var(--accent)">Schema</a> section.</p>

<h4>Alarm Engine</h4>
<p>A stateful service that maintains all user-defined alarms in memory (loaded from the alarm configuration database). Every 60 seconds, it queries the Time-Series DB for each alarm's metric over the evaluation period (e.g., "average CPU over 2 minutes"). It evaluates the threshold condition and transitions the alarm between <code>OK</code> ‚Üí <code>ALARM</code> ‚Üí <code>INSUFFICIENT_DATA</code> states. On state transition to <code>ALARM</code>, it publishes a message to the Message Queue. Protocol: <strong>gRPC</strong> for internal communication with TSDB.</p>

<h4>Auto-Scaling Service</h4>
<p>Stateless service that consumes scaling events from the Message Queue. It reads the auto-scaling group's configuration (min, max, desired, cooldown, scaling policy type ‚Äî step/target-tracking) from the Scaling Policy DB. It calculates the new desired instance count and orchestrates the launch or termination of instances via the Compute Service. Protocol: <strong>gRPC</strong> (to Compute Service).</p>
</div>

<!-- ============================================================ -->
<h2 id="flow6">8. Flow 6 ‚Äî Networking (Virtual Private Cloud)</h2>
<!-- ============================================================ -->
<div class="card">
<p>This flow shows how a user creates a VPC, configures networking, and how traffic is routed between instances.</p>

<div class="diagram-container">
<pre class="mermaid">
flowchart LR
    User["üë§ User"]
    API["üåê API Gateway"]
    IAM["üîê IAM Service"]
    NS["üåê Network<br/>Service"]
    NC["üéõÔ∏è Network<br/>Controller"]
    NetDB[("üóÑÔ∏è Network<br/>Metadata DB")]
    OVS1["üîÄ Virtual Switch<br/>(Host A)"]
    OVS2["üîÄ Virtual Switch<br/>(Host B)"]
    VM1["üíª VM 1<br/>(10.0.1.10)"]
    VM2["üíª VM 2<br/>(10.0.1.20)"]
    IGW["üåç Internet<br/>Gateway"]

    User -->|"HTTPS POST<br/>/vpcs"| API
    API -->|"Auth"| IAM
    API -->|"Create VPC"| NS
    NS -->|"Store VPC<br/>+ subnet config"| NetDB
    NS -->|"Program<br/>overlay network"| NC
    NC -->|"Install flow<br/>rules"| OVS1
    NC -->|"Install flow<br/>rules"| OVS2
    VM1 -->|"Packet to<br/>10.0.1.20"| OVS1
    OVS1 -->|"VXLAN tunnel<br/>encapsulation"| OVS2
    OVS2 -->|"Decapsulate +<br/>deliver"| VM2
    VM1 -->|"Packet to<br/>internet"| OVS1
    OVS1 -->|"Route via"| IGW
</pre>
</div>

<h3>Examples</h3>

<div class="example">
<strong>Example 1 ‚Äî VPC Creation + Instance Communication:</strong> A user calls <code>POST /vpcs</code> with <code>{"cidr_block": "10.0.0.0/16"}</code>. The Network Service creates a VPC record in the Network Metadata DB and configures the Network Controller with the overlay ID. The user then creates a subnet: <code>POST /subnets</code> with <code>{"vpc_id": "vpc-abc", "cidr_block": "10.0.1.0/24", "az": "us-east-1a"}</code>. Two instances are launched in this subnet (via Flow 1), getting IPs 10.0.1.10 and 10.0.1.20 on different physical hosts. When VM1 sends a packet to 10.0.1.20, the virtual switch on Host A looks up the flow rules installed by the Network Controller, encapsulates the packet in a VXLAN tunnel header (adding the overlay network ID), and sends it over the physical network to Host B. The virtual switch on Host B decapsulates the packet, checks security group rules (e.g., "allow TCP:8080 from 10.0.1.0/24"), and delivers it to VM2.
</div>

<div class="example">
<strong>Example 2 ‚Äî Internet Access via Internet Gateway:</strong> VM1 needs to reach the public internet. The user creates an Internet Gateway and attaches it to the VPC: <code>POST /internet-gateways</code> ‚Üí <code>PUT /vpcs/vpc-abc/internet-gateway/igw-xyz</code>. The user adds a route: <code>POST /route-tables/rt-main/routes</code> with <code>{"destination": "0.0.0.0/0", "target": "igw-xyz"}</code>. Now when VM1 sends a packet to 8.8.8.8, the virtual switch on Host A matches the default route, forwards the packet to the Internet Gateway, which performs NAT (translating 10.0.1.10 ‚Üí public Elastic IP) and routes it to the internet.
</div>

<div class="example">
<strong>Example 3 ‚Äî Security Group Blocks Traffic:</strong> VM1 (in subnet A) tries to SSH (TCP:22) to VM2 (in subnet B). However, VM2's security group only allows inbound TCP:443 from 10.0.0.0/16. The virtual switch on Host B evaluates the security group rules, finds no match for TCP:22, and drops the packet. VM1's SSH connection times out. The user updates the security group to allow TCP:22, the Network Controller pushes new flow rules to the virtual switch, and subsequent SSH connections succeed.
</div>

<h3>Component Deep Dive</h3>

<h4>Network Service</h4>
<p>Control plane service for networking. Manages VPCs, subnets, route tables, security groups, NAT gateways, and internet gateways. Protocol: <strong>HTTPS</strong> (external API), <strong>gRPC</strong> (internal to Network Controller).</p>
<ul>
  <li><code>POST /vpcs</code> ‚Äî Input: cidr_block, tags. Output: <code>{"vpc_id": "vpc-xxx"}</code></li>
  <li><code>POST /subnets</code> ‚Äî Input: vpc_id, cidr_block, az. Output: <code>{"subnet_id": "subnet-xxx"}</code></li>
  <li><code>PUT /security-groups/{sg_id}/rules</code> ‚Äî Input: direction, protocol, port_range, cidr. Output: <code>{"rule_id": "sgr-xxx"}</code></li>
</ul>

<h4>Network Controller</h4>
<p>A centralized SDN controller that translates high-level networking intent (VPCs, subnets, security groups) into low-level flow rules installed on virtual switches. It uses the <strong>OpenFlow</strong> protocol to program Open vSwitch (OVS) instances on each physical host. It maintains a real-time view of all VMs and their physical locations to set up VXLAN tunnels between hosts.</p>

<h4>Virtual Switches (OVS)</h4>
<p>Open vSwitch instances running on each physical host. They handle packet forwarding, VXLAN encapsulation/decapsulation, security group enforcement, and routing. All packet processing happens in kernel space for performance. Configuration is pushed from the Network Controller.</p>

<h4>Internet Gateway</h4>
<p>A horizontally scalable, redundant component that provides bidirectional internet connectivity for VPCs. It performs 1:1 NAT between private VPC IPs and public Elastic IPs. It is implemented as a cluster of gateway nodes with traffic distributed by ECMP (Equal-Cost Multi-Path) routing.</p>

<h4>Network Metadata DB</h4>
<p>Stores VPC definitions, subnet allocations, route tables, security group rules, Elastic IP mappings, and network interface records. This is a strongly consistent SQL database because networking configuration must be immediately consistent (a security group change must take effect within seconds). Partitioned by region.</p>
</div>

<!-- ============================================================ -->
<h2 id="overall">9. Overall Combined Diagram</h2>
<!-- ============================================================ -->
<div class="card">
<p>This diagram combines all flows into one unified view of the cloud platform architecture.</p>

<div class="diagram-container">
<pre class="mermaid">
flowchart TB
    subgraph Users["External Users"]
        Console["üñ•Ô∏è Web Console"]
        SDK["üì± SDK / CLI"]
    end

    subgraph Edge["Edge Layer"]
        CDN["üåç CDN"]
        LB["‚öñÔ∏è Load Balancer"]
    end

    subgraph ControlPlane["Control Plane"]
        API["üåê API Gateway"]
        IAM["üîê IAM Service"]
        PolicyCache["‚ö° Policy Cache"]
        IAM_DB[("üóÑÔ∏è IAM DB")]
    end

    subgraph ComputeCtrl["Compute Control Plane"]
        CS["üñ•Ô∏è Compute Service"]
        PS["üìç Placement Service"]
        ComputeDB[("üóÑÔ∏è Compute Metadata DB")]
        MQ_C["üì¨ Compute Task Queue"]
    end

    subgraph StorageCtrl["Object Storage Control Plane"]
        ObjAPI["üì¶ Object Storage API"]
        MetaSvc["üìá Metadata Service"]
        MetaCache["‚ö° Metadata Cache"]
        ObjMetaDB[("üóÑÔ∏è Object Metadata DB")]
    end

    subgraph NetworkCtrl["Network Control Plane"]
        NS["üåê Network Service"]
        NC["üéõÔ∏è Network Controller"]
        NetDB[("üóÑÔ∏è Network DB")]
    end

    subgraph MonitorCtrl["Monitoring & Auto-Scaling"]
        MC["üìä Metrics Collector"]
        TSDB[("üìà Time-Series DB")]
        AE["üîî Alarm Engine"]
        ASG["üìê Auto-Scaling<br/>Service"]
        MQ_A["üì¨ Alarm Queue"]
    end

    subgraph DataPlane["Data Plane (Physical Hosts)"]
        HA1["ü§ñ Host Agent 1"]
        HV1["üíª Hypervisor 1"]
        OVS1["üîÄ vSwitch 1"]
        HA2["ü§ñ Host Agent 2"]
        HV2["üíª Hypervisor 2"]
        OVS2["üîÄ vSwitch 2"]
    end

    subgraph StorageLayer["Storage Data Plane"]
        SN1[("üíæ Storage<br/>Node AZ-a")]
        SN2[("üíæ Storage<br/>Node AZ-b")]
        SN3[("üíæ Storage<br/>Node AZ-c")]
        ImgStore[("üìÄ Image Store")]
        BlockStore[("üíæ Block Storage")]
    end

    IGW["üåç Internet<br/>Gateway"]

    Console --> LB
    SDK --> LB
    SDK --> CDN
    LB --> API
    CDN --> LB

    API --> IAM
    IAM --> PolicyCache
    PolicyCache --> IAM_DB

    API --> CS
    API --> ObjAPI
    API --> NS

    CS --> PS
    PS --> ComputeDB
    CS --> ComputeDB
    CS --> MQ_C
    MQ_C --> HA1
    MQ_C --> HA2

    ObjAPI --> MetaSvc
    ObjAPI --> MetaCache
    MetaCache --> ObjMetaDB
    MetaSvc --> ObjMetaDB
    ObjAPI --> SN1
    SN1 --> SN2
    SN1 --> SN3

    NS --> NC
    NS --> NetDB
    NC --> OVS1
    NC --> OVS2
    OVS1 <--> OVS2
    OVS1 --> IGW

    HA1 --> HV1
    HA1 --> ImgStore
    HA1 --> BlockStore
    HA1 --> MC
    HA2 --> HV2
    HA2 --> MC

    MC --> TSDB
    AE --> TSDB
    AE --> MQ_A
    MQ_A --> ASG
    ASG --> CS
</pre>
</div>

<h3>Examples (End-to-End)</h3>

<div class="example">
<strong>Example 1 ‚Äî Full Stack Deployment:</strong> A startup deploys their web application on the cloud platform. (1) The DevOps engineer logs into the <strong>Web Console</strong> and creates a VPC with public and private subnets via the <strong>Network Service</strong>. (2) They create a load balancer in the public subnet. (3) They launch 3 compute instances in the private subnet via <strong>Compute Service</strong> ‚Äî the <strong>Placement Service</strong> distributes them across 3 AZs, the <strong>Host Agents</strong> provision VMs via <strong>Hypervisors</strong>, and the <strong>Network Controller</strong> programs <strong>vSwitches</strong> so the VMs can communicate. (4) They create an object storage bucket for static assets and upload images via the <strong>Object Storage API</strong>, which replicates to <strong>3 Storage Nodes</strong>. (5) They create a CDN distribution pointing to the bucket for fast global delivery. (6) They set up an auto-scaling group: <strong>Metrics Collector</strong> ingests CPU data from <strong>Host Agents</strong> into the <strong>Time-Series DB</strong>, the <strong>Alarm Engine</strong> watches for high CPU, and the <strong>Auto-Scaling Service</strong> adds instances via the <strong>Compute Service</strong> during traffic spikes. (7) All API calls are authenticated by the <strong>IAM Service</strong> using the engineer's access keys and policies. The application is now live, auto-scaling, and globally distributed.
</div>

<div class="example">
<strong>Example 2 ‚Äî Machine Learning Pipeline:</strong> A data scientist: (1) Uploads a 500 GB training dataset to object storage (multipart upload ‚Üí 3-way replication across AZs). (2) Launches a GPU instance (<code>p4d.24xlarge</code>) via the CLI SDK ‚Üí Compute Service ‚Üí Placement Service finds a host with available GPUs ‚Üí Host Agent pulls a deep learning OS image from Image Store and attaches a 2 TB block storage volume. (3) The instance's IAM role (temporary credentials from IAM Service) grants read access to the training data bucket. (4) The training job reads data from object storage (GET requests hit the Metadata Cache ‚Üí Storage Nodes stream data). (5) After training, the model artifact is uploaded back to object storage. (6) The instance is terminated to stop billing. Throughout, the Monitoring system tracks GPU utilization and the user views dashboards of training progress.
</div>

<div class="example">
<strong>Example 3 ‚Äî Security Incident Response:</strong> A security team detects suspicious activity. (1) They use IAM to immediately revoke an access key: <code>DELETE /iam/access-keys/AKIA...</code>. The IAM Service deletes the key from the IAM DB and publishes a cache invalidation event to the pub-sub topic. All IAM Service instances evict the cached entry within seconds. (2) Any subsequent API call with the revoked key fails authentication at the API Gateway ‚Üí IAM Service step. (3) They review the audit trail (all API calls logged by the API Gateway) in the Time-Series DB to understand the blast radius. (4) They update security group rules via the Network Service to block the attacker's IP range ‚Äî the Network Controller pushes new flow rules to all vSwitches within seconds.
</div>
</div>

<!-- ============================================================ -->
<h2 id="schema">10. Database Schema</h2>
<!-- ============================================================ -->
<div class="card">

<h3>SQL Tables</h3>
<p><span class="tag tag-sql">SQL</span> Used for strongly consistent data that requires ACID transactions, complex joins, and relational integrity ‚Äî critical for control plane metadata.</p>

<h4>Table: <code>accounts</code></h4>
<table>
<tr><th>Column</th><th>Type</th><th>Key</th><th>Description</th></tr>
<tr><td>account_id</td><td>VARCHAR(12)</td><td>üîë PK</td><td>Unique 12-digit account identifier</td></tr>
<tr><td>email</td><td>VARCHAR(255)</td><td>UNIQUE</td><td>Root account email</td></tr>
<tr><td>name</td><td>VARCHAR(255)</td><td></td><td>Account name</td></tr>
<tr><td>status</td><td>ENUM</td><td></td><td>ACTIVE, SUSPENDED, CLOSED</td></tr>
<tr><td>created_at</td><td>TIMESTAMP</td><td></td><td>Account creation time</td></tr>
<tr><td>mfa_enabled</td><td>BOOLEAN</td><td></td><td>Whether MFA is enabled for root</td></tr>
</table>
<p><strong>Why SQL:</strong> Account data requires strong consistency and ACID transactions (e.g., account creation must be atomic). The data is relational (accounts ‚Üí users ‚Üí policies). Accounts are rarely created but frequently read for auth ‚Äî SQL handles this well.</p>
<p><strong>Read:</strong> On every API call (IAM authentication lookup). <strong>Write:</strong> On account creation, status change, or settings update.</p>
<p><strong>Indexes:</strong> Hash index on <code>account_id</code> (primary key, point lookups). Unique B-tree index on <code>email</code> for login lookups.</p>
<p><strong>Sharding:</strong> Sharded by <code>account_id</code> (hash-based). Each account and all its children (users, roles, policies) live on the same shard, ensuring single-shard transactions for policy evaluation. This avoids cross-shard joins during the latency-critical auth path.</p>

<hr>

<h4>Table: <code>iam_users</code></h4>
<table>
<tr><th>Column</th><th>Type</th><th>Key</th><th>Description</th></tr>
<tr><td>user_id</td><td>VARCHAR(64)</td><td>üîë PK</td><td>Unique user identifier</td></tr>
<tr><td>account_id</td><td>VARCHAR(12)</td><td>üîó FK ‚Üí accounts</td><td>Parent account</td></tr>
<tr><td>username</td><td>VARCHAR(64)</td><td></td><td>Human-readable name</td></tr>
<tr><td>path</td><td>VARCHAR(512)</td><td></td><td>IAM path for organization (e.g., /dev/team-a/)</td></tr>
<tr><td>created_at</td><td>TIMESTAMP</td><td></td><td></td></tr>
</table>
<p><strong>Read:</strong> On every API call (as part of IAM auth chain). <strong>Write:</strong> When creating/deleting IAM users.</p>
<p><strong>Indexes:</strong> B-tree index on <code>(account_id, username)</code> for looking up users within an account. Hash index on <code>user_id</code> (PK).</p>
<p><strong>Sharding:</strong> Co-located with <code>accounts</code> on the same shard (sharded by <code>account_id</code>), so that looking up an account's users doesn't require cross-shard queries.</p>

<hr>

<h4>Table: <code>access_keys</code></h4>
<table>
<tr><th>Column</th><th>Type</th><th>Key</th><th>Description</th></tr>
<tr><td>access_key_id</td><td>VARCHAR(20)</td><td>üîë PK</td><td>The public access key ID (e.g., AKIA...)</td></tr>
<tr><td>user_id</td><td>VARCHAR(64)</td><td>üîó FK ‚Üí iam_users</td><td>Owning IAM user</td></tr>
<tr><td>secret_key_hash</td><td>VARCHAR(64)</td><td></td><td>HMAC of the secret key (never stored in plaintext)</td></tr>
<tr><td>status</td><td>ENUM</td><td></td><td>ACTIVE, INACTIVE</td></tr>
<tr><td>created_at</td><td>TIMESTAMP</td><td></td><td></td></tr>
</table>
<p><strong>Read:</strong> On every API call (used to verify signature). <strong>Write:</strong> On key creation, rotation, or deletion.</p>
<p><strong>Indexes:</strong> Hash index on <code>access_key_id</code> (PK) ‚Äî this is the most latency-critical lookup in the system (happens on every single API call). B-tree index on <code>user_id</code> to list all keys for a user.</p>
<p><strong>Sharding:</strong> Co-located with <code>iam_users</code> by <code>account_id</code>. The <code>access_key_id</code> also needs a global lookup index so that any API Gateway instance can find the right shard from just the access key ‚Äî this is done via a lightweight global hash map that maps <code>access_key_id</code> ‚Üí <code>account_id</code> (shard key).</p>

<hr>

<h4>Table: <code>policies</code></h4>
<table>
<tr><th>Column</th><th>Type</th><th>Key</th><th>Description</th></tr>
<tr><td>policy_id</td><td>VARCHAR(64)</td><td>üîë PK</td><td>Unique policy identifier</td></tr>
<tr><td>account_id</td><td>VARCHAR(12)</td><td>üîó FK ‚Üí accounts</td><td>Parent account</td></tr>
<tr><td>policy_name</td><td>VARCHAR(128)</td><td></td><td>Human-readable name</td></tr>
<tr><td>policy_document</td><td>JSON</td><td></td><td>The full JSON policy (statements, actions, resources, conditions)</td></tr>
<tr><td>version</td><td>INT</td><td></td><td>Policy version number</td></tr>
<tr><td>created_at</td><td>TIMESTAMP</td><td></td><td></td></tr>
</table>
<p><strong>Read:</strong> On every API call (policy evaluation). <strong>Write:</strong> When policies are created or updated (infrequent).</p>
<p><strong>Indexes:</strong> Hash index on <code>policy_id</code> (PK). B-tree index on <code>(account_id, policy_name)</code>.</p>

<hr>

<h4>Table: <code>instances</code></h4>
<table>
<tr><th>Column</th><th>Type</th><th>Key</th><th>Description</th></tr>
<tr><td>instance_id</td><td>VARCHAR(19)</td><td>üîë PK</td><td>e.g., i-0123456789abcdef0</td></tr>
<tr><td>account_id</td><td>VARCHAR(12)</td><td>üîó FK ‚Üí accounts</td><td>Owning account</td></tr>
<tr><td>instance_type</td><td>VARCHAR(32)</td><td></td><td>e.g., m5.xlarge</td></tr>
<tr><td>image_id</td><td>VARCHAR(21)</td><td></td><td>OS image identifier</td></tr>
<tr><td>host_id</td><td>VARCHAR(32)</td><td></td><td>Physical host where the VM runs</td></tr>
<tr><td>vpc_id</td><td>VARCHAR(21)</td><td>üîó FK ‚Üí vpcs</td><td></td></tr>
<tr><td>subnet_id</td><td>VARCHAR(24)</td><td>üîó FK ‚Üí subnets</td><td></td></tr>
<tr><td>private_ip</td><td>VARCHAR(15)</td><td></td><td>VPC-internal IP address</td></tr>
<tr><td>public_ip</td><td>VARCHAR(15)</td><td></td><td>Elastic IP if attached (nullable)</td></tr>
<tr><td>status</td><td>ENUM</td><td></td><td>PENDING, RUNNING, STOPPING, STOPPED, SHUTTING_DOWN, TERMINATED</td></tr>
<tr><td>launched_at</td><td>TIMESTAMP</td><td></td><td></td></tr>
<tr><td>az</td><td>VARCHAR(16)</td><td></td><td>Availability zone (e.g., us-east-1a)</td></tr>
</table>
<p><strong>Why SQL:</strong> Instance records require strong consistency (a user must immediately see the correct state after a stop/start). They reference VPCs and subnets (relational). Complex queries are needed (e.g., "find all running instances of type m5.xlarge in us-east-1a for account X").</p>
<p><strong>Read:</strong> Frequently ‚Äî by user (DescribeInstances API), by Placement Service (capacity queries), by Auto-Scaling Service (count instances in a group). <strong>Write:</strong> On launch, status change, termination.</p>
<p><strong>Indexes:</strong> Hash index on <code>instance_id</code> (PK). B-tree index on <code>(account_id, status)</code> for listing a user's running instances. B-tree index on <code>(host_id, status)</code> for the Placement Service to find available capacity on a host. B-tree index on <code>(az, instance_type, status)</code> for capacity queries.</p>
<p><strong>Sharding:</strong> Sharded by <code>region</code> (each region has its own instances table partition). Within a region, further partitioned by <code>account_id</code> hash. This allows the Placement Service to query within a single region shard efficiently.</p>

<hr>

<h4>Table: <code>hosts</code></h4>
<table>
<tr><th>Column</th><th>Type</th><th>Key</th><th>Description</th></tr>
<tr><td>host_id</td><td>VARCHAR(32)</td><td>üîë PK</td><td>Physical server identifier</td></tr>
<tr><td>region</td><td>VARCHAR(16)</td><td></td><td></td></tr>
<tr><td>az</td><td>VARCHAR(16)</td><td></td><td></td></tr>
<tr><td>rack_id</td><td>VARCHAR(32)</td><td></td><td>Physical rack for failure domain isolation</td></tr>
<tr><td>total_vcpus</td><td>INT</td><td></td><td>Total vCPUs on the host</td></tr>
<tr><td>available_vcpus</td><td>INT</td><td></td><td>Currently free vCPUs</td></tr>
<tr><td>total_memory_gb</td><td>INT</td><td></td><td></td></tr>
<tr><td>available_memory_gb</td><td>INT</td><td></td><td></td></tr>
<tr><td>gpu_type</td><td>VARCHAR(32)</td><td></td><td>Nullable ‚Äî GPU model if present</td></tr>
<tr><td>total_gpus</td><td>INT</td><td></td><td></td></tr>
<tr><td>available_gpus</td><td>INT</td><td></td><td></td></tr>
<tr><td>status</td><td>ENUM</td><td></td><td>HEALTHY, DEGRADED, OFFLINE</td></tr>
<tr><td>last_heartbeat</td><td>TIMESTAMP</td><td></td><td></td></tr>
</table>
<p><strong>Read:</strong> By Placement Service on every instance launch. <strong>Write:</strong> On heartbeat (every 30s), on instance launch/termination (capacity update).</p>
<p><strong>Indexes:</strong> B-tree index on <code>(az, status, available_vcpus)</code> for the Placement Service to quickly find healthy hosts with sufficient capacity. Hash index on <code>host_id</code> (PK).</p>

<hr>

<h4>Table: <code>vpcs</code></h4>
<table>
<tr><th>Column</th><th>Type</th><th>Key</th><th>Description</th></tr>
<tr><td>vpc_id</td><td>VARCHAR(21)</td><td>üîë PK</td><td></td></tr>
<tr><td>account_id</td><td>VARCHAR(12)</td><td>üîó FK ‚Üí accounts</td><td></td></tr>
<tr><td>cidr_block</td><td>VARCHAR(18)</td><td></td><td>e.g., 10.0.0.0/16</td></tr>
<tr><td>region</td><td>VARCHAR(16)</td><td></td><td></td></tr>
<tr><td>status</td><td>ENUM</td><td></td><td>AVAILABLE, DELETING</td></tr>
<tr><td>created_at</td><td>TIMESTAMP</td><td></td><td></td></tr>
</table>

<h4>Table: <code>subnets</code></h4>
<table>
<tr><th>Column</th><th>Type</th><th>Key</th><th>Description</th></tr>
<tr><td>subnet_id</td><td>VARCHAR(24)</td><td>üîë PK</td><td></td></tr>
<tr><td>vpc_id</td><td>VARCHAR(21)</td><td>üîó FK ‚Üí vpcs</td><td></td></tr>
<tr><td>cidr_block</td><td>VARCHAR(18)</td><td></td><td></td></tr>
<tr><td>az</td><td>VARCHAR(16)</td><td></td><td></td></tr>
<tr><td>type</td><td>ENUM</td><td></td><td>PUBLIC, PRIVATE</td></tr>
<tr><td>available_ips</td><td>INT</td><td></td><td>Remaining allocatable IPs</td></tr>
</table>

<h4>Table: <code>security_groups</code></h4>
<table>
<tr><th>Column</th><th>Type</th><th>Key</th><th>Description</th></tr>
<tr><td>sg_id</td><td>VARCHAR(20)</td><td>üîë PK</td><td></td></tr>
<tr><td>vpc_id</td><td>VARCHAR(21)</td><td>üîó FK ‚Üí vpcs</td><td></td></tr>
<tr><td>name</td><td>VARCHAR(255)</td><td></td><td></td></tr>
<tr><td>description</td><td>VARCHAR(1024)</td><td></td><td></td></tr>
</table>

<h4>Table: <code>security_group_rules</code></h4>
<table>
<tr><th>Column</th><th>Type</th><th>Key</th><th>Description</th></tr>
<tr><td>rule_id</td><td>VARCHAR(24)</td><td>üîë PK</td><td></td></tr>
<tr><td>sg_id</td><td>VARCHAR(20)</td><td>üîó FK ‚Üí security_groups</td><td></td></tr>
<tr><td>direction</td><td>ENUM</td><td></td><td>INBOUND, OUTBOUND</td></tr>
<tr><td>protocol</td><td>ENUM</td><td></td><td>TCP, UDP, ICMP, ALL</td></tr>
<tr><td>from_port</td><td>INT</td><td></td><td></td></tr>
<tr><td>to_port</td><td>INT</td><td></td><td></td></tr>
<tr><td>cidr</td><td>VARCHAR(18)</td><td></td><td>Allowed source/dest CIDR</td></tr>
<tr><td>source_sg_id</td><td>VARCHAR(20)</td><td></td><td>Alternative: allow from another SG</td></tr>
</table>
<p><strong>Why SQL for all networking tables:</strong> Networking configuration is inherently relational (VPCs ‚Üí subnets ‚Üí instances, security groups ‚Üí rules). Strong consistency is critical ‚Äî a security group rule change must propagate immediately for security reasons. These tables are written infrequently but read frequently by the Network Controller.</p>
<p><strong>Indexes (security_group_rules):</strong> B-tree index on <code>sg_id</code> for looking up all rules in a security group (this is the primary query pattern when the Network Controller compiles flow rules).</p>

<hr>

<h3>NoSQL Tables</h3>
<p><span class="tag tag-nosql">NoSQL</span> Used for high-throughput, key-value or wide-column data that benefits from horizontal scalability and flexible schemas.</p>

<h4>Table: <code>object_metadata</code> (NoSQL ‚Äî Key-Value / Wide Column)</h4>
<table>
<tr><th>Column</th><th>Type</th><th>Key</th><th>Description</th></tr>
<tr><td>bucket_name + object_key</td><td>STRING</td><td>üîë Partition Key</td><td>Composite key: "my-bucket/photos/vacation.jpg"</td></tr>
<tr><td>version_id</td><td>STRING</td><td>üîë Sort Key</td><td>Version identifier (if versioning enabled)</td></tr>
<tr><td>size_bytes</td><td>BIGINT</td><td></td><td>Object size</td></tr>
<tr><td>content_type</td><td>STRING</td><td></td><td>MIME type</td></tr>
<tr><td>etag</td><td>STRING</td><td></td><td>MD5 hash or multipart hash</td></tr>
<tr><td>storage_nodes</td><td>LIST&lt;STRING&gt;</td><td></td><td>Node IDs holding replicas: [SN1, SN2, SN3]</td></tr>
<tr><td>storage_class</td><td>STRING</td><td></td><td>STANDARD, INFREQUENT_ACCESS, ARCHIVE</td></tr>
<tr><td>owner_account_id</td><td>STRING</td><td></td><td></td></tr>
<tr><td>acl</td><td>JSON</td><td></td><td>Access control list</td></tr>
<tr><td>user_metadata</td><td>MAP&lt;STRING,STRING&gt;</td><td></td><td>User-defined key-value metadata</td></tr>
<tr><td>created_at</td><td>TIMESTAMP</td><td></td><td></td></tr>
<tr><td>status</td><td>STRING</td><td></td><td>UPLOADING, COMMITTED, DELETING</td></tr>
</table>
<p><strong>Why NoSQL:</strong> Object storage metadata must scale to billions/trillions of records. Access patterns are simple key-value lookups (<code>GET (bucket+key)</code>) and prefix scans (<code>LIST objects with prefix</code>). No complex joins. The flexible schema accommodates user-defined metadata. NoSQL's horizontal partitioning handles the massive scale naturally.</p>
<p><strong>Read:</strong> On every <code>GET /bucket/key</code> (the most frequent operation ‚Äî billions/day). On <code>LIST /bucket?prefix=...</code>. <strong>Write:</strong> On <code>PUT</code> (upload), <code>DELETE</code>, metadata updates.</p>
<p><strong>Indexes:</strong> The partition key <code>(bucket_name + object_key)</code> provides O(1) lookups via consistent hashing. For the LIST operation (prefix scan), the sort key within a partition enables lexicographic ordering. A <strong>secondary index on <code>(owner_account_id, bucket_name)</code></strong> supports listing all buckets for an account.</p>
<p><strong>Sharding:</strong> Consistent hashing on <code>(bucket_name + object_key)</code>. This distributes objects evenly across nodes. Hot buckets (e.g., a bucket with millions of requests/sec) are split by auto-sharding: the system detects high throughput on a partition range and splits it into smaller ranges distributed across more nodes.</p>

<div class="warning">
<strong>Denormalization:</strong> The <code>storage_nodes</code> field is denormalized into the object metadata rather than being a separate join table. This is intentional because the read path (GET object) must be as fast as possible ‚Äî adding a join to find storage locations would add latency to every single read. The tradeoff is that when a storage node is decommissioned, a background process must update all affected object records (eventual consistency is acceptable here since the old node ID is still valid during migration).
</div>

<hr>

<h4>Table: <code>buckets</code> (NoSQL ‚Äî Key-Value)</h4>
<table>
<tr><th>Column</th><th>Type</th><th>Key</th><th>Description</th></tr>
<tr><td>bucket_name</td><td>STRING</td><td>üîë Partition Key</td><td>Globally unique bucket name</td></tr>
<tr><td>owner_account_id</td><td>STRING</td><td></td><td></td></tr>
<tr><td>region</td><td>STRING</td><td></td><td></td></tr>
<tr><td>versioning_enabled</td><td>BOOLEAN</td><td></td><td></td></tr>
<tr><td>lifecycle_rules</td><td>JSON</td><td></td><td>Transition/expiration rules</td></tr>
<tr><td>bucket_policy</td><td>JSON</td><td></td><td>Resource-based access policy</td></tr>
<tr><td>cors_config</td><td>JSON</td><td></td><td></td></tr>
<tr><td>created_at</td><td>TIMESTAMP</td><td></td><td></td></tr>
</table>
<p><strong>Why NoSQL:</strong> Simple key-value access by bucket name. Bucket names are globally unique, making them natural partition keys. The flexible schema holds complex nested configurations (lifecycle rules, CORS).</p>
<p><strong>Read:</strong> On every object API call (to verify bucket exists, check policies). <strong>Write:</strong> On bucket creation or configuration updates (rare).</p>

<hr>

<h4>Table: <code>metrics</code> (Time-Series DB)</h4>
<table>
<tr><th>Column</th><th>Type</th><th>Key</th><th>Description</th></tr>
<tr><td>metric_name</td><td>STRING</td><td>üîë Partition Key</td><td>e.g., "compute/cpu_utilization"</td></tr>
<tr><td>dimensions</td><td>MAP&lt;STRING,STRING&gt;</td><td>üîë Part of Partition Key</td><td>e.g., {instance_id: "i-xxx", az: "us-east-1a"}</td></tr>
<tr><td>timestamp</td><td>TIMESTAMP</td><td>üîë Sort Key</td><td>Millisecond precision</td></tr>
<tr><td>value</td><td>DOUBLE</td><td></td><td>Metric value</td></tr>
<tr><td>unit</td><td>STRING</td><td></td><td>Percent, Bytes, Count, etc.</td></tr>
<tr><td>statistic_type</td><td>STRING</td><td></td><td>Average, Sum, Min, Max, Count</td></tr>
</table>
<p><strong>Why Time-Series DB:</strong> Metrics are append-only, time-stamped data with very high write throughput (every host sends metrics every 60s, and there are millions of hosts). Time-series databases are purpose-built for this: columnar storage, time-based partitioning, automatic downsampling, and efficient range queries ("give me CPU for the last 2 hours").</p>
<p><strong>Read:</strong> By Alarm Engine every 60s (for each alarm). By users viewing dashboards. <strong>Write:</strong> By Metrics Collector continuously (millions of writes/sec aggregate).</p>
<p><strong>Sharding:</strong> Sharded by <code>(metric_name, dimensions)</code> hash for write distribution. Partitioned by time (e.g., daily partitions) for efficient time-range queries and automatic expiration of old data.</p>

<hr>

<h4>Table: <code>scaling_policies</code> (NoSQL ‚Äî Key-Value)</h4>
<table>
<tr><th>Column</th><th>Type</th><th>Key</th><th>Description</th></tr>
<tr><td>asg_id</td><td>STRING</td><td>üîë PK</td><td>Auto-scaling group ID</td></tr>
<tr><td>account_id</td><td>STRING</td><td></td><td></td></tr>
<tr><td>min_size</td><td>INT</td><td></td><td>Minimum instances</td></tr>
<tr><td>max_size</td><td>INT</td><td></td><td>Maximum instances</td></tr>
<tr><td>desired_capacity</td><td>INT</td><td></td><td>Current desired count</td></tr>
<tr><td>scaling_policy</td><td>JSON</td><td></td><td>Step scaling or target tracking config</td></tr>
<tr><td>cooldown_seconds</td><td>INT</td><td></td><td></td></tr>
<tr><td>instance_template</td><td>JSON</td><td></td><td>Instance type, image, networking config for new instances</td></tr>
<tr><td>last_scaled_at</td><td>TIMESTAMP</td><td></td><td></td></tr>
</table>
</div>

<!-- ============================================================ -->
<h2 id="cdn-cache">11. CDN & Caching Deep Dive</h2>
<!-- ============================================================ -->
<div class="card">

<h3>CDN (Content Delivery Network)</h3>

<h4>When CDN is Appropriate</h4>
<p>The CDN is used specifically for the <strong>Object Storage GET path</strong> for public or pre-signed objects. It is <strong>not used</strong> for control plane API calls (instance creation, IAM operations, etc.) because those are dynamic, authenticated requests that are not cacheable.</p>

<table>
<tr><th>Aspect</th><th>Details</th></tr>
<tr><td><strong>What it caches</strong></td><td>Object storage content (images, videos, static assets, JS/CSS files). Only objects that are either publicly accessible or accessed via pre-signed URLs with appropriate <code>Cache-Control</code> headers.</td></tr>
<tr><td><strong>Cache Strategy</strong></td><td><strong>Pull-based (Lazy Loading)</strong>. The CDN does not proactively fetch objects. On a cache miss, the edge node fetches from origin (Object Storage), caches it locally, and serves it. Subsequent requests for the same object are served from the edge cache.</td></tr>
<tr><td><strong>Expiration Policy</strong></td><td>Respects the <code>Cache-Control</code> and <code>Expires</code> headers set by the bucket owner. Default TTL: 24 hours for public objects if no header is set. Pre-signed URLs have their own expiration which the CDN respects.</td></tr>
<tr><td><strong>Eviction Policy</strong></td><td><strong>LRU (Least Recently Used)</strong>. When edge node disk is full, the least recently accessed objects are evicted first. This naturally keeps hot objects cached and evicts cold/infrequent ones. LRU was chosen over LFU because access patterns for object storage are often bursty (a new viral asset gets millions of hits, then cools off) ‚Äî LRU adapts faster to changing popularity.</td></tr>
<tr><td><strong>Invalidation</strong></td><td>Bucket owners can create cache invalidation requests (<code>POST /cdn/distributions/{id}/invalidations</code>) specifying path patterns (e.g., <code>/images/*</code>). The CDN propagates invalidation to all edge nodes within 60 seconds.</td></tr>
<tr><td><strong>Population</strong></td><td>Cache is populated on the first GET request for an object at a given edge node (cache-on-read). There is no push/pre-warming by default, but users can optionally warm specific paths.</td></tr>
</table>

<h3>Metadata Cache (In-Memory Cache)</h3>
<p>This cache sits in the Object Storage API Frontend layer and caches object metadata lookups.</p>

<table>
<tr><th>Aspect</th><th>Details</th></tr>
<tr><td><strong>What it caches</strong></td><td>Object metadata records: the mapping from <code>(bucket, key)</code> ‚Üí <code>{storage_nodes, size, etag, status}</code>. This is the information needed to know <em>where</em> to read the object data from.</td></tr>
<tr><td><strong>Cache Strategy</strong></td><td><strong>Read-through cache (Cache-Aside)</strong>. On a GET, the API Frontend first checks the cache. On a miss, it queries the Object Metadata DB, stores the result in the cache, and returns it. On PUT/DELETE, the cache entry is <strong>invalidated</strong> (deleted) so the next read fetches fresh data. This is cache-aside rather than write-through because object metadata writes (PUTs) are less frequent than reads (GETs), and we want to avoid caching objects that are only written once and never read.</td></tr>
<tr><td><strong>Expiration Policy</strong></td><td><strong>TTL: 5 minutes</strong>. Even without explicit invalidation, entries expire after 5 minutes. This provides a safety net for consistency ‚Äî if an invalidation event is lost, the stale entry will expire naturally. 5 minutes was chosen as a balance between cache hit rate and staleness risk.</td></tr>
<tr><td><strong>Eviction Policy</strong></td><td><strong>LRU (Least Recently Used)</strong>. Memory-constrained environment; LRU keeps the most frequently accessed object metadata in cache (hot objects like viral content or frequently-accessed config files).</td></tr>
<tr><td><strong>Population</strong></td><td>Populated on read (cache-on-read). When a GET request misses the cache, the metadata is fetched from the DB and cached. No pre-warming.</td></tr>
<tr><td><strong>Invalidation</strong></td><td>On PUT (new object or overwrite), the API Frontend deletes the cache entry for that key. On DELETE, the cache entry is deleted. This ensures strong read-after-write consistency: after a PUT, the next GET will miss the cache, fetch the latest metadata from the DB, and see the new object.</td></tr>
</table>

<h3>Policy Cache (In-Memory Cache for IAM)</h3>
<table>
<tr><th>Aspect</th><th>Details</th></tr>
<tr><td><strong>What it caches</strong></td><td>IAM user/role records, attached policies, and access key ‚Üí user mappings. Keyed by <code>access_key_id</code>.</td></tr>
<tr><td><strong>Cache Strategy</strong></td><td><strong>Read-through cache</strong>. IAM Service checks cache first; on miss, queries IAM DB. On policy/key updates, an <strong>active invalidation via pub-sub</strong> is used: the IAM write path publishes an invalidation event to a pub-sub topic, and all IAM Service instances subscribe and evict the affected entries. This ensures that permission revocations take effect within seconds, not minutes.</td></tr>
<tr><td><strong>Expiration Policy</strong></td><td><strong>TTL: 60 seconds</strong>. Short TTL because security is critical ‚Äî a revoked access key must stop working quickly. Even with pub-sub invalidation, the TTL acts as a safety net in case an invalidation message is lost.</td></tr>
<tr><td><strong>Eviction Policy</strong></td><td><strong>LRU</strong>. Most API calls come from a relatively small set of active access keys (CI/CD pipelines, application service accounts). LRU keeps these hot keys in cache.</td></tr>
<tr><td><strong>Population</strong></td><td>On first API call with a given access key (cache-on-read).</td></tr>
</table>

<h3>Why CDN is NOT appropriate for Control Plane APIs</h3>
<p>Control plane operations (create instance, update security group, etc.) are <strong>dynamic, stateful, mutating operations</strong> that cannot be cached. Each request has a unique signature, and responses depend on the current state of the system. Caching would serve stale/incorrect data and create security vulnerabilities. Only <strong>read-heavy, static or semi-static content</strong> (object storage data) benefits from CDN caching.</p>
</div>

<!-- ============================================================ -->
<h2 id="scaling">12. Scaling Considerations</h2>
<!-- ============================================================ -->
<div class="card">

<h3>Load Balancers</h3>
<p>Load balancers are essential at multiple levels of the architecture:</p>

<h4>1. External Load Balancer (User ‚Üí API Gateway)</h4>
<ul>
  <li><strong>Position:</strong> Between users/SDKs and the API Gateway fleet.</li>
  <li><strong>Type:</strong> Layer 7 (HTTP/HTTPS) load balancer.</li>
  <li><strong>Protocol:</strong> Terminates TLS (HTTPS). Users connect over TLS 1.2+.</li>
  <li><strong>Algorithm:</strong> Least connections ‚Äî routes to the API Gateway instance with the fewest active connections. This handles varying request durations (a PUT of a 5 GB object takes much longer than a GET metadata call).</li>
  <li><strong>Health Checks:</strong> HTTP health check endpoint (<code>GET /health</code>) every 10 seconds. Instances failing 3 consecutive checks are removed from the pool.</li>
  <li><strong>Scale:</strong> Multiple LB instances per region with DNS-based distribution (GeoDNS for global routing, then Anycast or multiple A records within a region). The LB fleet itself auto-scales based on connection count.</li>
  <li><strong>Rate Limiting:</strong> Enforces per-account API rate limits (e.g., 10,000 requests/sec per account) to protect the control plane from abuse.</li>
  <li><strong>SSL Offloading:</strong> Terminates TLS at the LB, allowing internal communication to use plaintext HTTP (within the trusted internal network) or mutual TLS (for zero-trust environments).</li>
</ul>

<h4>2. Internal Load Balancers (Service-to-Service)</h4>
<ul>
  <li><strong>Position:</strong> Between the API Gateway and each backend service (Compute Service, Object Storage API, Network Service, IAM Service).</li>
  <li><strong>Type:</strong> Layer 4 (TCP) or Layer 7 (gRPC) load balancer depending on the protocol.</li>
  <li><strong>Algorithm:</strong> Round-robin with health checks for stateless services. Consistent hashing for requests that benefit from affinity (e.g., routing all requests for the same bucket to the same Object Storage API node for cache locality).</li>
  <li><strong>Implementation:</strong> Client-side load balancing via service mesh (each service has a sidecar proxy that discovers healthy endpoints via a service registry and load balances locally). This avoids the single-point-of-failure of a central load balancer for internal traffic.</li>
</ul>

<h4>3. Storage Node Load Balancer</h4>
<ul>
  <li><strong>Position:</strong> Between Object Storage API Frontend and Storage Nodes.</li>
  <li><strong>Approach:</strong> Not a traditional LB ‚Äî the Metadata Service determines which nodes to read from/write to based on the consistent hash ring. The API Frontend connects directly to the designated storage nodes. Load distribution is inherent in the consistent hashing scheme.</li>
</ul>

<h3>Horizontal Scaling by Component</h3>
<table>
<tr><th>Component</th><th>Scaling Strategy</th><th>Bottleneck & Mitigation</th></tr>
<tr><td>API Gateway</td><td>Stateless ‚Äî add more instances behind the LB. Auto-scale based on request rate.</td><td>CPU-bound (TLS termination, request parsing). Scale by adding instances.</td></tr>
<tr><td>IAM Service</td><td>Stateless ‚Äî scale horizontally. Each instance has its own Policy Cache.</td><td>Memory-bound (Policy Cache size). Scale vertically per instance (more RAM) and horizontally (more instances with distributed cache).</td></tr>
<tr><td>Compute Service</td><td>Stateless ‚Äî scale horizontally behind LB.</td><td>Dependency on Metadata DB for writes. DB sharding alleviates this.</td></tr>
<tr><td>Placement Service</td><td>Stateless ‚Äî scale horizontally. Can partition by AZ (each instance handles placement for a subset of AZs).</td><td>Query complexity on hosts table. Solved by efficient indexes and caching host capacity in memory.</td></tr>
<tr><td>Object Storage API</td><td>Stateless ‚Äî scale horizontally. Can partition by bucket prefix for cache locality.</td><td>Network I/O bound (streaming large objects). Scale by adding instances and provisioning more network bandwidth.</td></tr>
<tr><td>Storage Nodes</td><td>Add more nodes to the consistent hash ring. Existing data rebalances automatically.</td><td>Disk I/O and capacity. Solved by adding nodes (linear scale-out).</td></tr>
<tr><td>Metadata DB (SQL)</td><td>Leader-follower replication for read scaling. Sharding for write scaling.</td><td>Write bottleneck on hot shards. Mitigated by proper shard key selection and shard splitting.</td></tr>
<tr><td>Object Metadata DB (NoSQL)</td><td>Consistent hash ring with auto-sharding of hot partitions.</td><td>Hot key problem (viral object). Solved by request-level caching in the Metadata Cache and read replicas.</td></tr>
<tr><td>Time-Series DB</td><td>Partition by metric name + time window. Add more nodes for more throughput.</td><td>Write throughput at ingest. Solved by batching at the Metrics Collector and partitioning across many TSDB nodes.</td></tr>
<tr><td>Message Queues</td><td>Partition by queue topic. Add partitions for throughput.</td><td>Consumer lag during spikes. Solved by adding consumer instances (Host Agents, Auto-Scaling Service).</td></tr>
</table>

<h3>Multi-Region Architecture</h3>
<ul>
  <li>Each region is a fully independent deployment of the entire stack (control plane + data plane).</li>
  <li>Regions do NOT share databases ‚Äî this ensures isolation and compliance with data sovereignty laws.</li>
  <li>Global services (IAM, account management, bucket namespace) are replicated across regions with a single-leader or consensus-based approach. IAM has a primary region where writes go, with asynchronous replication to read replicas in other regions.</li>
  <li>Bucket names are globally unique ‚Äî enforced by a global namespace service that coordinates across regions.</li>
  <li>Users can optionally enable <strong>cross-region replication</strong> for object storage, where a background process asynchronously copies objects from one region's storage nodes to another's.</li>
</ul>

<h3>Availability Zone Architecture</h3>
<ul>
  <li>Each region has 3+ availability zones (physically separate data centers with independent power, cooling, networking).</li>
  <li>Control plane services are deployed across all AZs with leader election per AZ for stateful components.</li>
  <li>Object storage replicates across 3 AZs by default.</li>
  <li>Compute instances can be spread across AZs via placement groups for HA.</li>
  <li>Database leaders and followers are distributed across AZs. If the leader's AZ fails, automatic failover promotes a follower in a surviving AZ.</li>
</ul>
</div>

<!-- ============================================================ -->
<h2 id="tradeoffs">13. Tradeoffs & Deep Dives</h2>
<!-- ============================================================ -->
<div class="card">

<h3 id="mq-deepdive">Message Queue Deep Dive</h3>
<p>Message queues are used in two places: (1) Compute Task Queue (between Compute Service and Host Agents), and (2) Alarm Queue (between Alarm Engine and Auto-Scaling Service).</p>

<h4>Why Message Queue (not direct RPC)?</h4>
<ul>
  <li><strong>Decoupling:</strong> The control plane doesn't need to know which Host Agent is available. It just enqueues a task; the appropriate agent picks it up.</li>
  <li><strong>Reliability:</strong> If a Host Agent crashes mid-provisioning, the message becomes visible again after the visibility timeout and another agent (or the same agent after restart) can retry. With direct RPC, the request would be lost.</li>
  <li><strong>Backpressure:</strong> During a launch storm (e.g., auto-scaling event triggering 100 launches simultaneously), the queue absorbs the burst. Host Agents consume at their own pace without being overwhelmed.</li>
</ul>

<h4>Why not Pub-Sub?</h4>
<p>Pub-sub delivers messages to all subscribers. For compute tasks, we need <strong>exactly one</strong> Host Agent to process each launch task ‚Äî this is a work queue pattern, not a broadcast pattern. Pub-sub would cause duplicate processing.</p>

<h4>Why not WebSockets / Long Polling?</h4>
<p>We <em>could</em> use WebSockets for push-based task delivery to Host Agents. However, this adds complexity: the control plane must maintain persistent connections to thousands of Host Agents, track which agents are connected, and handle reconnection logic. The pull-based queue model is simpler, more robust, and naturally handles agent failures (messages reappear in the queue).</p>

<h4>How Messages Flow</h4>
<ol>
  <li><strong>Enqueue:</strong> Compute Service sends a <code>LaunchInstance</code> message containing <code>{instance_id, host_id, instance_type, image_id, network_config, block_storage_config}</code>. The queue durably stores the message (replicated to disk on multiple nodes).</li>
  <li><strong>Dequeue:</strong> The Host Agent on the target <code>host_id</code> polls the queue (filtered by host_id partition). It receives the message and the message becomes invisible to other consumers (visibility timeout = 10 minutes).</li>
  <li><strong>Processing:</strong> The Host Agent executes the provisioning steps (pull image, create disk, create VM, configure networking).</li>
  <li><strong>Acknowledge:</strong> Upon successful VM creation, the Host Agent sends an ACK to the queue, permanently removing the message.</li>
  <li><strong>Failure:</strong> If the Host Agent doesn't ACK within 10 minutes, the message becomes visible again. A retry counter is incremented. After 3 retries, the message moves to a dead-letter queue for investigation.</li>
</ol>

<h3>Pub-Sub Deep Dive (IAM Cache Invalidation)</h3>
<p>Pub-sub is used specifically for IAM cache invalidation ‚Äî broadcasting policy/key changes to all IAM Service instances.</p>

<h4>Why Pub-Sub (not Message Queue)?</h4>
<p>When a policy is updated, <strong>every</strong> IAM Service instance needs to evict the affected cache entry. This is a fan-out/broadcast pattern ‚Äî exactly what pub-sub is designed for. A message queue would deliver to only one consumer.</p>

<h4>How It Works</h4>
<ol>
  <li>IAM write path (e.g., <code>PUT /iam/policies/{id}</code>) updates the IAM DB and publishes an invalidation event to a pub-sub topic: <code>{event: "INVALIDATE", entity_type: "POLICY", entity_id: "policy-abc", account_id: "123456789012"}</code>.</li>
  <li>All IAM Service instances subscribe to this topic. Each instance receives the event and evicts the affected entries from its Policy Cache.</li>
  <li>The pub-sub system stores messages for a configurable retention period (e.g., 1 hour) so that IAM instances that were temporarily disconnected can catch up on missed invalidation events.</li>
</ol>

<h3>UDP for Metrics Collection</h3>
<p><strong>Why UDP over TCP:</strong> Host Agents push metrics every 60 seconds. Metrics are small payloads (~200 bytes each) sent at high frequency from millions of hosts. TCP's overhead (3-way handshake, connection maintenance, guaranteed delivery retransmission) is unnecessary because:</p>
<ul>
  <li>Losing an occasional metric data point is acceptable ‚Äî the Alarm Engine evaluates metrics over windows (2+ minutes), so a single missed point doesn't affect alarm accuracy.</li>
  <li>UDP eliminates connection overhead, reducing the load on both the Host Agents and the Metrics Collector by ~40%.</li>
  <li>At the scale of millions of hosts, maintaining millions of persistent TCP connections to the Metrics Collector would be resource-prohibitive.</li>
</ul>
<p><strong>Tradeoff:</strong> In a network congestion scenario, more metric points could be dropped with UDP. This is mitigated by the Alarm Engine requiring multiple consecutive evaluation periods to trigger (reducing false positives from missing data).</p>

<h3>TCP for Storage Node Replication</h3>
<p><strong>Why TCP over UDP:</strong> Object data replication between storage nodes uses TCP because data integrity is paramount ‚Äî a single corrupted byte in a replicated object would violate the 11-nines durability guarantee. TCP's guaranteed delivery, ordering, and checksum ensure that replicated data is identical. The overhead of TCP is acceptable here because replication is a background process with tolerance for slightly higher latency.</p>

<h3>Strong Consistency vs. Eventual Consistency Tradeoff</h3>
<div class="tradeoff">
<p><strong>Control Plane (SQL databases for IAM, Compute, Networking):</strong> Strong consistency is chosen because correctness is more important than availability for these operations. If a user changes a security group rule, it <em>must</em> take effect immediately ‚Äî eventual consistency could leave a security window. Similarly, if a user terminates an instance, the next <code>GET</code> must show it as terminated, not still running. The cost is slightly higher write latency (synchronous replication to followers) and reduced availability during network partitions (if the leader is unreachable, writes are blocked). This is an acceptable tradeoff ‚Äî control plane operations are relatively low volume.</p>
</div>
<div class="tradeoff">
<p><strong>Object Metadata (NoSQL):</strong> Strong read-after-write consistency is chosen (similar to how AWS S3 achieves this). After a PUT, the next GET returns the new object. This requires the metadata write to be acknowledged by a quorum of replicas before returning success. The cost is higher write latency (~10-20ms extra), but this is worth it to avoid subtle bugs where applications upload a config file and immediately read back stale data.</p>
</div>

<h3>Bin-Packing vs. Spread Placement</h3>
<div class="tradeoff">
<p>The Placement Service supports two strategies: <strong>Bin-packing</strong> (fill up hosts to maximize utilization, reducing infrastructure cost) and <strong>Spread</strong> (distribute instances across hosts/racks for fault tolerance). By default, a hybrid approach is used: bin-pack within a rack for cost efficiency, but spread across racks and AZs for fault tolerance. Users can override this with placement groups (e.g., "cluster" for low-latency co-location or "spread" for maximum HA).</p>
</div>

<h3>VXLAN Overlay Networking</h3>
<div class="tradeoff">
<p><strong>Why VXLAN:</strong> Virtual extensible LAN (VXLAN) encapsulates Layer 2 Ethernet frames in UDP packets. This allows virtual networks (VPCs) to have overlapping IP spaces (multiple customers can use 10.0.0.0/8) because the VXLAN network identifier (VNI) differentiates traffic. VXLAN runs over the existing physical IP network, so no special hardware is needed.</p>
<p><strong>Tradeoff:</strong> VXLAN adds ~50 bytes of overhead per packet (outer UDP/IP header + VXLAN header), reducing MTU. This is mitigated by using jumbo frames (9000 byte MTU) on the physical network, leaving ample room for the overhead. Latency added by encapsulation/decapsulation is &lt; 1 microsecond (done in kernel/hardware offload).</p>
</div>
</div>

<!-- ============================================================ -->
<h2 id="alternatives">14. Alternative Approaches</h2>
<!-- ============================================================ -->
<div class="card">

<h3>Alternative 1: Container-Based Compute Instead of VMs</h3>
<p><strong>Approach:</strong> Instead of full VMs with hypervisors, use containers (like Docker) for compute instances.</p>
<p><strong>Why not chosen:</strong> While containers offer faster startup (~seconds vs ~minutes for VMs) and better resource density, they provide weaker isolation than hardware-assisted virtualization. A cloud platform serving untrusted multi-tenant workloads requires the strong security boundary that a hypervisor provides (each VM has a separate kernel). Containers share the host kernel, making kernel exploits a risk. VMs are the correct primitive for IaaS. (Note: containers can be offered as a separate, higher-level service built on top of VMs ‚Äî similar to ECS/EKS ‚Äî but the foundational compute layer should use VMs.)</p>

<h3>Alternative 2: Erasure Coding Instead of 3-Way Replication for Object Storage</h3>
<p><strong>Approach:</strong> Instead of storing 3 full copies of each object, use Reed-Solomon erasure coding (e.g., 6+3 scheme: split into 6 data chunks + 3 parity chunks, needing only any 6 of 9 to reconstruct).</p>
<p><strong>Why partially chosen / hybrid:</strong> Erasure coding reduces storage overhead from 3x to 1.5x ‚Äî a massive cost saving at exabyte scale. However, it adds computational overhead for encoding/decoding and increases read latency (reading from 6 nodes instead of 1). The design uses a <strong>hybrid approach</strong>: hot data in STANDARD storage class uses 3-way replication for maximum read performance; data transitioned to INFREQUENT_ACCESS or ARCHIVE storage classes uses erasure coding for cost efficiency. This is why the <code>storage_class</code> field exists in the object metadata.</p>

<h3>Alternative 3: Push-Based Metrics Instead of Push + Pull</h3>
<p><strong>Approach:</strong> Have the Alarm Engine subscribe to a real-time metrics stream (pub-sub) instead of polling the Time-Series DB every 60 seconds.</p>
<p><strong>Why not chosen:</strong> A push-based approach would give faster alarm reaction times but would require the Alarm Engine to maintain state for every alarm's evaluation window in memory (millions of alarms √ó sliding windows). Polling the TSDB offloads the windowed aggregation to the database, which is purpose-built for it. The 60-second polling interval matches the metric resolution (60s), so there's no added delay. The tradeoff is acceptable for a monitoring system where minute-level granularity is sufficient.</p>

<h3>Alternative 4: Centralized Load Balancer for Internal Traffic</h3>
<p><strong>Approach:</strong> Use a central hardware/software load balancer for all service-to-service communication.</p>
<p><strong>Why not chosen:</strong> A central LB becomes a single point of failure and a throughput bottleneck for internal traffic, which is orders of magnitude higher than external traffic (every external API call generates multiple internal calls). Client-side load balancing via a service mesh (sidecar proxy + service discovery) eliminates this bottleneck, distributes the load balancing compute to each service instance, and provides better failure isolation.</p>

<h3>Alternative 5: Graph Database for IAM Policies</h3>
<p><strong>Approach:</strong> Model IAM relationships (users ‚Üí groups ‚Üí roles ‚Üí policies ‚Üí resources) as a graph and use a graph database for policy evaluation.</p>
<p><strong>Why not chosen:</strong> While the IAM data model is graph-like, the query patterns are simple: "given an access key, find all attached policies." This doesn't require graph traversals ‚Äî it's a series of lookups (key ‚Üí user ‚Üí groups ‚Üí policies). SQL handles this efficiently with indexed foreign keys. A graph database would add operational complexity without meaningful query performance improvement. The Policy Cache further eliminates any performance concern since the hot path is a cache hit.</p>

<h3>Alternative 6: WebSockets for Instance Status Updates</h3>
<p><strong>Approach:</strong> Instead of users polling <code>GET /instances/{id}</code> to check if an instance is running, push status updates via WebSockets.</p>
<p><strong>Why not chosen:</strong> The audience for instance status updates is typically a CLI or automation script that runs ephemerally, not a long-lived browser session. Maintaining WebSocket connections for millions of short-lived clients is wasteful. Polling with exponential backoff (poll every 2s, then 4s, then 8s) is simpler, stateless, and sufficient. For the Web Console, long polling (HTTP long poll) can be used to reduce unnecessary requests while still being simpler than WebSockets.</p>
</div>

<!-- ============================================================ -->
<h2 id="additional">15. Additional Considerations</h2>
<!-- ============================================================ -->
<div class="card">

<h3>Audit Logging</h3>
<p>Every API call must be logged for compliance and security. The API Gateway logs each request (action, resource, user, timestamp, source IP, response code) to a durable append-only log. This log is stored in object storage (immutable, long-term retention: 7 years). Users can access their logs via a dedicated audit trail API. Logs are also indexed in a search engine for real-time investigation.</p>

<h3>Encryption</h3>
<ul>
  <li><strong>In transit:</strong> All external connections use TLS 1.2+ (negotiated via ALPN). Internal service-to-service communication uses mutual TLS (mTLS) with automatically rotated certificates.</li>
  <li><strong>At rest:</strong> All data stored on disk (object storage, block storage, databases) is encrypted with AES-256. Encryption keys are managed by a dedicated Key Management Service (KMS) that stores master keys in hardware security modules (HSMs). Customers can bring their own keys (BYOK) or use the platform-managed keys.</li>
</ul>

<h3>Billing & Metering</h3>
<p>A separate metering system tracks resource usage (compute hours, storage bytes, network egress, API calls) per account. Host Agents report instance uptime. Storage Nodes report object sizes. The Metrics Collector feeds a billing pipeline that aggregates usage hourly and generates invoices. This system uses an <strong>event-sourcing</strong> pattern: every resource lifecycle event (instance launched, instance terminated, object stored, object deleted) is published to a message queue and consumed by the billing service, ensuring no usage is missed.</p>

<h3>Quotas & Limits</h3>
<p>Each account has default quotas (e.g., max 20 instances per region, max 100 buckets, max 5 VPCs) to prevent abuse and protect shared infrastructure. Quotas are checked by the API Gateway before forwarding requests. Users can request quota increases via a support process. Quotas are stored in the IAM DB (co-located with account data for single-shard reads).</p>

<h3>Health Checking & Self-Healing</h3>
<ul>
  <li><strong>Host health:</strong> Host Agents send heartbeats every 30 seconds. If 3 consecutive heartbeats are missed, the host is marked DEGRADED. A recovery controller attempts to restart the agent. If recovery fails, the host is marked OFFLINE, and all instances on it are migrated to healthy hosts (live migration for running instances where possible, or re-launch for terminated VMs).</li>
  <li><strong>Storage node health:</strong> Periodic checksum verification (background scrub) detects bit rot. Corrupted replicas are re-replicated from healthy replicas. A dedicated <strong>repair scheduler</strong> continuously scans for under-replicated objects and triggers re-replication.</li>
</ul>

<h3>API Versioning</h3>
<p>The API Gateway supports versioned endpoints (<code>/v1/instances</code>, <code>/v2/instances</code>). Older versions are maintained for backward compatibility with a deprecation timeline (12+ months notice). This is critical because thousands of applications and SDKs depend on stable API contracts.</p>

<h3>Rate Limiting & Throttling</h3>
<p>The API Gateway implements token-bucket rate limiting per account per API. Defaults: 10,000 requests/sec for reads, 1,000/sec for writes. Exceeding the limit returns <code>HTTP 429 Too Many Requests</code> with a <code>Retry-After</code> header. Burst capacity (up to 5x the limit for 1 second) absorbs short spikes. Rate limit counters are stored in the in-memory cache (distributed counter with per-instance partitioning).</p>

<h3>Idempotency</h3>
<p>All mutating API calls support an optional <code>ClientToken</code> (idempotency key). If a client retries a request with the same token (e.g., due to a network timeout), the system detects it and returns the original response without creating a duplicate resource. Tokens are stored in a short-lived cache (TTL: 24 hours) for deduplication.</p>

<h3>Disaster Recovery</h3>
<ul>
  <li><strong>RPO (Recovery Point Objective):</strong> Zero for object storage (synchronous 3-AZ replication). Near-zero for databases (synchronous leader-follower within-region).</li>
  <li><strong>RTO (Recovery Time Objective):</strong> &lt; 5 minutes for AZ failover (automatic). &lt; 1 hour for region failover (requires manual or automated regional failover).</li>
  <li>Regular disaster recovery drills (chaos engineering) test AZ failure, network partition, and database failover scenarios.</li>
</ul>
</div>

<!-- ============================================================ -->
<h2 id="vendors">16. Vendor Recommendations</h2>
<!-- ============================================================ -->
<div class="card">
<p>While the design above is vendor-agnostic, the following vendors would be strong choices for implementing each component:</p>

<table>
<tr><th>Component</th><th>Vendor(s)</th><th>Rationale</th></tr>
<tr><td><strong>SQL Database</strong> (IAM, Compute Metadata, Network Metadata)</td><td>CockroachDB, YugabyteDB, TiDB</td><td>All three are distributed SQL databases that provide strong consistency, automatic sharding, and multi-AZ replication ‚Äî critical for control plane metadata. CockroachDB's geo-partitioning is ideal for multi-region IAM. TiDB's MySQL compatibility eases migration.</td></tr>
<tr><td><strong>NoSQL Database</strong> (Object Metadata, Buckets)</td><td>Apache Cassandra, ScyllaDB, FoundationDB</td><td>Cassandra/ScyllaDB offer proven scale to billions of keys with tunable consistency (QUORUM for strong read-after-write). ScyllaDB's C++ implementation provides better p99 latency than Cassandra's JVM. FoundationDB provides ACID transactions at scale for scenarios requiring transactional metadata operations.</td></tr>
<tr><td><strong>Time-Series Database</strong> (Metrics)</td><td>InfluxDB, TimescaleDB, VictoriaMetrics</td><td>VictoriaMetrics handles massive ingest rates (millions of metrics/sec) with excellent compression. TimescaleDB provides SQL compatibility for complex queries on metrics (useful for dashboards). InfluxDB has a purpose-built query language (Flux) for time-series analysis.</td></tr>
<tr><td><strong>Message Queue</strong></td><td>Apache Pulsar, NATS JetStream</td><td>Pulsar provides durable, partitioned queues with dead-letter queues and delayed delivery ‚Äî all needed for the compute task queue. NATS JetStream is a lighter alternative with excellent performance for simpler queue patterns. Both support at-least-once delivery with acknowledgments.</td></tr>
<tr><td><strong>Pub-Sub</strong></td><td>Apache Pulsar (topics), NATS</td><td>Pulsar can serve as both message queue and pub-sub (unified messaging). NATS provides extremely low-latency pub-sub for the IAM cache invalidation use case.</td></tr>
<tr><td><strong>In-Memory Cache</strong></td><td>Redis, Memcached, Dragonfly</td><td>Redis provides versatile data structures (useful for rate limiting counters, policy caching). Dragonfly is a newer, multi-threaded drop-in Redis replacement with better throughput on modern hardware. Memcached is simpler and sufficient for pure key-value caching (Metadata Cache).</td></tr>
<tr><td><strong>Object Storage Backend</strong></td><td>MinIO, Ceph (RADOS)</td><td>MinIO is S3-compatible and designed for high-performance object storage. Ceph provides a unified storage layer (block + object + file) with built-in replication and erasure coding ‚Äî ideal for the storage data plane.</td></tr>
<tr><td><strong>Block Storage Backend</strong></td><td>Ceph (RBD), LVM + custom replication</td><td>Ceph RBD provides thin-provisioned, replicated block devices with snapshot support. For maximum performance and control, a custom solution using LVM on local NVMe drives with synchronous replication over RDMA is common at hyperscale.</td></tr>
<tr><td><strong>CDN</strong></td><td>Self-built (anycast + edge caching), or Cloudflare, Fastly</td><td>At cloud-provider scale, a self-built CDN using anycast IP routing and globally distributed edge PoPs is most cost-effective and customizable. Cloudflare/Fastly are excellent if outsourcing the edge layer.</td></tr>
<tr><td><strong>Service Mesh / Load Balancing</strong></td><td>Envoy Proxy, Istio, Linkerd</td><td>Envoy is the de-facto sidecar proxy for service mesh, providing client-side load balancing, circuit breaking, retries, and observability. Istio and Linkerd build on Envoy for higher-level service mesh features (mTLS, traffic policies).</td></tr>
<tr><td><strong>SDN Controller</strong></td><td>Open vSwitch (OVS), Tungsten Fabric, Cilium</td><td>OVS is the industry standard for virtual switching with VXLAN support. Cilium uses eBPF for high-performance packet processing with lower overhead than OVS. Tungsten Fabric (formerly OpenContrail) provides a full SDN stack including routing and security.</td></tr>
<tr><td><strong>Hypervisor</strong></td><td>KVM/QEMU, Firecracker</td><td>KVM is the de-facto Linux hypervisor with mature tooling (libvirt). Firecracker (created by AWS for Lambda/Fargate) provides microVMs with ~125ms boot time and minimal memory overhead ‚Äî ideal for rapid auto-scaling scenarios.</td></tr>
<tr><td><strong>Search / Audit Log Indexing</strong></td><td>Elasticsearch, OpenSearch, Quickwit</td><td>Elasticsearch/OpenSearch provide full-text search and analytics on audit logs. Quickwit is a newer, cloud-native alternative with better cost efficiency for append-only log data.</td></tr>
</table>
</div>

<!-- ============================================================ -->
<h2>17. Summary</h2>
<!-- ============================================================ -->
<div class="card">
<p>This design covers the core building blocks of a hyperscale cloud computing platform:</p>
<ul>
  <li><strong>Compute:</strong> VM provisioning via a Placement Service + Host Agent architecture with message-queue-based task distribution.</li>
  <li><strong>Object Storage:</strong> A 3-way replicated, strongly consistent object store with a NoSQL metadata layer and CDN for read acceleration.</li>
  <li><strong>IAM:</strong> Signature-based authentication and JSON policy-based authorization with aggressive caching and pub-sub invalidation.</li>
  <li><strong>Networking:</strong> VXLAN-based SDN with centralized Network Controller programming distributed virtual switches.</li>
  <li><strong>Monitoring & Auto-Scaling:</strong> UDP-based metric ingestion ‚Üí Time-Series DB ‚Üí Alarm Engine ‚Üí Auto-Scaling Service ‚Üí Compute Service feedback loop.</li>
</ul>
<p>The architecture prioritizes <strong>strong consistency</strong> for control plane operations, <strong>11-nines durability</strong> for stored data, <strong>horizontal scalability</strong> at every layer, and <strong>multi-AZ fault tolerance</strong> as the baseline. Each component is independently scalable, and the use of message queues, caches, and CDNs ensures the system handles both steady-state and burst traffic patterns efficiently.</p>
</div>

<script>
  mermaid.initialize({ 
    startOnLoad: true, 
    theme: 'dark',
    flowchart: { 
      useMaxWidth: true, 
      htmlLabels: true,
      curve: 'basis'
    }
  });
</script>

</body>
</html>
