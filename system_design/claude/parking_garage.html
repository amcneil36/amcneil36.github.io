<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>System Design: Parking Garage</title>
    <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
    <style>
        :root {
            --bg: #0d1117;
            --card: #161b22;
            --border: #30363d;
            --text: #c9d1d9;
            --heading: #e6edf3;
            --accent: #58a6ff;
            --accent2: #3fb950;
            --accent3: #d2a8ff;
            --accent4: #f0883e;
            --code-bg: #1c2129;
        }
        * { box-sizing: border-box; margin: 0; padding: 0; }
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
            background: var(--bg);
            color: var(--text);
            line-height: 1.7;
            padding: 40px 20px;
        }
        .container { max-width: 1100px; margin: 0 auto; }
        h1 {
            font-size: 2.4em;
            color: var(--heading);
            border-bottom: 2px solid var(--accent);
            padding-bottom: 12px;
            margin-bottom: 30px;
        }
        h2 {
            font-size: 1.7em;
            color: var(--accent);
            margin-top: 50px;
            margin-bottom: 16px;
            border-left: 4px solid var(--accent);
            padding-left: 14px;
        }
        h3 {
            font-size: 1.3em;
            color: var(--accent3);
            margin-top: 30px;
            margin-bottom: 10px;
        }
        h4 {
            font-size: 1.1em;
            color: var(--accent4);
            margin-top: 22px;
            margin-bottom: 8px;
        }
        p { margin-bottom: 14px; }
        ul, ol {
            margin-left: 24px;
            margin-bottom: 14px;
        }
        li { margin-bottom: 6px; }
        .card {
            background: var(--card);
            border: 1px solid var(--border);
            border-radius: 10px;
            padding: 24px;
            margin-bottom: 24px;
        }
        .example {
            background: #1a2332;
            border-left: 4px solid var(--accent2);
            padding: 16px 20px;
            border-radius: 0 8px 8px 0;
            margin: 14px 0;
            font-style: italic;
        }
        .example strong { color: var(--accent2); font-style: normal; }
        .diagram-wrapper {
            background: #fff;
            border-radius: 10px;
            padding: 20px;
            margin: 20px 0;
            overflow-x: auto;
        }
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 16px 0;
        }
        th {
            background: #21262d;
            color: var(--accent);
            padding: 10px 14px;
            text-align: left;
            border: 1px solid var(--border);
        }
        td {
            padding: 10px 14px;
            border: 1px solid var(--border);
        }
        tr:nth-child(even) { background: #161b22; }
        tr:nth-child(odd) { background: #0d1117; }
        code {
            background: var(--code-bg);
            padding: 2px 6px;
            border-radius: 4px;
            font-size: 0.92em;
            color: var(--accent2);
        }
        .tag {
            display: inline-block;
            padding: 3px 10px;
            border-radius: 12px;
            font-size: 0.8em;
            font-weight: 600;
            margin-right: 6px;
        }
        .tag-sql { background: #1f3a5f; color: #79c0ff; }
        .tag-nosql { background: #2a1f3f; color: #d2a8ff; }
        .tag-cache { background: #1f3f2a; color: #56d364; }
        .highlight { color: var(--accent); font-weight: 600; }
        .warn { color: var(--accent4); }
        hr { border: none; border-top: 1px solid var(--border); margin: 40px 0; }
    </style>
</head>
<body>
<div class="container">

<h1>üÖøÔ∏è System Design: Parking Garage</h1>

<!-- ============================= -->
<!-- FUNCTIONAL REQUIREMENTS       -->
<!-- ============================= -->
<h2>1. Functional Requirements</h2>
<div class="card">
<ol>
    <li><strong>Vehicle Entry:</strong> When a vehicle arrives at the entry gate, the system detects the vehicle (via sensor/camera), issues a ticket, assigns an available parking spot, and opens the gate.</li>
    <li><strong>Vehicle Exit &amp; Payment:</strong> When a vehicle arrives at the exit gate, the driver scans their ticket, the system calculates the parking fee based on duration (and vehicle type), processes payment, opens the gate, and frees the spot.</li>
    <li><strong>Real-Time Availability Display:</strong> Digital signage at each floor and at the garage entrance displays the current number of available spots per floor and per vehicle type. A mobile app also shows this info.</li>
    <li><strong>Multiple Vehicle Types:</strong> Support motorcycle, compact, regular, and large/handicapped spots. Vehicles can only be assigned to compatible spot types.</li>
    <li><strong>Multiple Floors/Zones:</strong> The garage has multiple floors each with zones, and the system tracks availability per floor and zone.</li>
    <li><strong>Reservation (Pre-Booking):</strong> Users can reserve a spot in advance via a mobile app, specifying arrival time and duration. Reserved spots are held until the reservation window expires.</li>
    <li><strong>Admin Dashboard:</strong> Garage operators can view occupancy analytics, revenue reports, configure pricing, and manage the garage.</li>
</ol>
</div>

<!-- ============================= -->
<!-- NON-FUNCTIONAL REQUIREMENTS   -->
<!-- ============================= -->
<h2>2. Non-Functional Requirements</h2>
<div class="card">
<ol>
    <li><strong>High Availability:</strong> Entry/exit gates must operate 24/7. A gate failure means vehicles are trapped. Target 99.99% uptime for gate operations.</li>
    <li><strong>Low Latency:</strong> Gate open/close operations must complete in &lt;500ms. Drivers should not wait more than a second at the gate.</li>
    <li><strong>Strong Consistency for Spot Assignment:</strong> No two vehicles should ever be assigned the same spot. This requires strict transactional guarantees.</li>
    <li><strong>Durability:</strong> Payment and ticket records must never be lost. Financial data requires durable storage.</li>
    <li><strong>Fault Tolerance:</strong> If the central server goes down, gates should have a local fallback mode (issue tickets locally, reconcile later).</li>
    <li><strong>Scalability:</strong> The system should support a chain of garages (hundreds of garages, thousands of spots each).</li>
    <li><strong>Security:</strong> Payment data must be encrypted. Access to admin features must be authenticated and authorized.</li>
</ol>
</div>

<hr>

<!-- ============================= -->
<!-- FLOW 1: VEHICLE ENTRY         -->
<!-- ============================= -->
<h2>3. Flow 1 ‚Äî Vehicle Entry</h2>

<div class="diagram-wrapper">
<pre class="mermaid">
flowchart LR
    V["üöó Vehicle"]
    S["Sensor / Camera\n(Entry Gate)"]
    GC["Gate Controller\n(IoT Edge Device)"]
    AG["API Gateway\n+ Load Balancer"]
    ES["Entry Service\n(HTTP)"]
    SAS["Spot Assignment\nService"]
    DB[("SQL Database\n(Spots, Tickets)")]
    CACHE[("In-Memory Cache\n(Availability)")]
    PUB["Pub/Sub\n(Availability Events)"]
    SIGN["Digital Signs\n(Per Floor)"]
    GATE["Physical Gate\nActuator"]

    V -->|arrives| S
    S -->|"vehicle detected"| GC
    GC -->|"HTTP POST /entry"| AG
    AG --> ES
    ES -->|"assign spot"| SAS
    SAS -->|"BEGIN TRANSACTION\nSELECT + UPDATE"| DB
    SAS -->|"update availability"| CACHE
    ES -->|"INSERT ticket"| DB
    ES -->|"publish: spot_occupied"| PUB
    PUB -->|"push update"| SIGN
    ES -->|"return ticket + spot info"| GC
    GC -->|"open gate"| GATE
    GC -->|"print ticket"| V
</pre>
</div>

<h3>Flow 1 ‚Äî Examples</h3>

<div class="example">
    <strong>Example 1 (Happy Path ‚Äî Regular Vehicle):</strong> A sedan arrives at the Level 1 entry gate at 9:02 AM. The infrared sensor detects the vehicle and triggers the Gate Controller. The Gate Controller sends an <code>HTTP POST /api/v1/entry</code> request to the API Gateway with <code>{garage_id: "G1", vehicle_type: "regular"}</code>. The Entry Service calls the Spot Assignment Service, which runs a SQL transaction: <code>SELECT id FROM spots WHERE garage_id='G1' AND status='available' AND type='regular' ORDER BY floor ASC, zone ASC LIMIT 1 FOR UPDATE</code>, then <code>UPDATE spots SET status='occupied'</code>. It finds Spot #B-204 on Floor 2. A ticket record is inserted: <code>{ticket_id: "T-88291", spot_id: "B-204", entry_time: "09:02:00"}</code>. A <code>spot_occupied</code> event is published to Pub/Sub. The digital sign on Floor 2 decrements its available count from 45 to 44. The gate opens and a physical ticket with barcode is printed for the driver.
</div>

<div class="example">
    <strong>Example 2 (Garage Full):</strong> A motorcycle arrives at 6:45 PM. The Gate Controller sends the entry request, but the Spot Assignment Service finds zero available motorcycle spots (<code>SELECT</code> returns empty). The Entry Service returns <code>HTTP 409 Conflict</code> with <code>{error: "no_spots_available", vehicle_type: "motorcycle"}</code>. The Gate Controller displays "FULL" on its LED screen and does NOT open the gate. No ticket is issued.
</div>

<div class="example">
    <strong>Example 3 (Reserved Spot ‚Äî Arrival):</strong> A user who reserved Spot #A-112 at 10:00 AM arrives at 9:58 AM. The Gate Controller sends the entry request with <code>{garage_id: "G1", reservation_code: "R-55123"}</code>. The Entry Service looks up the reservation, confirms it's valid and within the arrival window (10:00 AM ¬± 15 min), assigns the pre-reserved spot, issues a ticket linked to the reservation, and opens the gate.
</div>

<h3>Flow 1 ‚Äî Component Deep Dive</h3>
<div class="card">
<h4>Sensor / Camera (Entry Gate)</h4>
<p>An infrared beam-break sensor or an ANPR (Automatic Number Plate Recognition) camera mounted at the entry lane. Detects vehicle presence and optionally reads the license plate. Communicates with the Gate Controller over a local wired connection (serial/GPIO).</p>

<h4>Gate Controller (IoT Edge Device)</h4>
<p>A small embedded computer (e.g., ARM-based) running at each entry lane. It interfaces with the sensor, the physical gate arm, the ticket printer, and the LED display. It communicates with the backend over a <strong>local network (LAN/Wi-Fi) using HTTP</strong>. In offline fallback mode, it can issue tickets locally with a sequential offline ticket ID and reconcile with the server once connectivity is restored.</p>

<h4>API Gateway + Load Balancer</h4>
<p>A reverse proxy that routes incoming requests from gate controllers and mobile apps to the appropriate microservice. Handles rate limiting, authentication (API keys for gate controllers, JWT for mobile app), and TLS termination. Uses round-robin or least-connections load balancing to distribute traffic across Entry Service instances.</p>

<h4>Entry Service</h4>
<ul>
    <li><strong>Protocol:</strong> HTTP REST</li>
    <li><strong>Endpoint:</strong> <code>POST /api/v1/entry</code></li>
    <li><strong>Input:</strong> <code>{garage_id: string, vehicle_type: enum("motorcycle"|"compact"|"regular"|"large"), reservation_code?: string}</code></li>
    <li><strong>Output (success):</strong> <code>HTTP 201 Created ‚Äî {ticket_id: string, spot_id: string, floor: int, zone: string, entry_time: ISO8601}</code></li>
    <li><strong>Output (full):</strong> <code>HTTP 409 Conflict ‚Äî {error: "no_spots_available", vehicle_type: string}</code></li>
    <li>Orchestrates the spot assignment and ticket creation. Publishes availability change events to Pub/Sub.</li>
</ul>

<h4>Spot Assignment Service</h4>
<p>Internal service (called by Entry Service, not exposed externally). Responsible for selecting the best available spot using a strategy (nearest to entrance, lowest floor first, etc.). Uses a <strong>SQL transaction with SELECT ... FOR UPDATE</strong> to guarantee no double-booking. This is the critical consistency point of the system.</p>

<h4>SQL Database (Spots, Tickets)</h4>
<p>Relational database storing spot definitions, ticket records, and reservation records. Chosen for ACID transactions which are critical for spot assignment consistency. More detail in the Schema section.</p>

<h4>In-Memory Cache (Availability)</h4>
<p>Stores aggregated availability counts per garage/floor/vehicle_type for fast reads. Populated on startup from DB and updated on every entry/exit event. Serves the digital signs and mobile app without hitting the database. More detail in the Cache section.</p>

<h4>Pub/Sub (Availability Events)</h4>
<p>A publish-subscribe messaging system. When a spot status changes, an event is published to the <code>spot-status-changes</code> topic. Subscribers include: the Availability Service (which updates the cache and pushes to digital signs) and the Analytics Service. More detail in the Pub/Sub Deep Dive section.</p>

<h4>Digital Signs (Per Floor)</h4>
<p>LED/LCD displays at each floor entrance and at the garage entrance. Connected to a lightweight sign controller that maintains a <strong>WebSocket connection</strong> to the Availability Service. When an availability update is received, the display refreshes. Shows: <code>"Floor 2: 44 spots available (Regular: 30, Compact: 10, Motorcycle: 4)"</code>.</p>

<h4>Physical Gate Actuator</h4>
<p>The barrier arm mechanism. Controlled by the Gate Controller via a relay/GPIO signal. Opens on successful entry, closes automatically after a timeout or when the vehicle passes through.</p>
</div>

<hr>

<!-- ============================= -->
<!-- FLOW 2: VEHICLE EXIT/PAYMENT  -->
<!-- ============================= -->
<h2>4. Flow 2 ‚Äî Vehicle Exit &amp; Payment</h2>

<div class="diagram-wrapper">
<pre class="mermaid">
flowchart LR
    V["üöó Vehicle"]
    SC["Ticket Scanner\n(Exit Gate)"]
    GC["Gate Controller\n(IoT Edge Device)"]
    AG["API Gateway\n+ Load Balancer"]
    XS["Exit Service\n(HTTP)"]
    FEE["Fee Calculation\nService"]
    PAY["Payment Service\n(HTTP)"]
    DB[("SQL Database\n(Tickets, Payments,\nSpots)")]
    CACHE[("In-Memory Cache\n(Availability)")]
    PUB["Pub/Sub\n(Availability Events)"]
    SIGN["Digital Signs"]
    GATE["Physical Gate\nActuator"]
    PMT["Payment Terminal\n(Card/NFC)"]

    V -->|"insert/scan ticket"| SC
    SC -->|"ticket_id"| GC
    GC -->|"HTTP POST /exit/initiate"| AG
    AG --> XS
    XS -->|"lookup ticket"| DB
    XS -->|"calculate fee"| FEE
    FEE -->|"return amount"| XS
    XS -->|"return fee"| GC
    GC -->|"display fee"| PMT
    V -->|"tap card / insert cash"| PMT
    PMT -->|"payment info"| GC
    GC -->|"HTTP POST /exit/pay"| AG
    AG --> PAY
    PAY -->|"charge + record"| DB
    PAY -->|"payment success"| XS
    XS -->|"UPDATE spot = available\nUPDATE ticket = completed"| DB
    XS -->|"update availability"| CACHE
    XS -->|"publish: spot_freed"| PUB
    PUB -->|"push update"| SIGN
    XS -->|"gate open command"| GC
    GC -->|"open gate"| GATE
</pre>
</div>

<h3>Flow 2 ‚Äî Examples</h3>

<div class="example">
    <strong>Example 1 (Happy Path ‚Äî Card Payment):</strong> At 5:30 PM, the driver of the sedan parked since 9:02 AM inserts their ticket at the exit scanner. The Gate Controller reads ticket ID <code>T-88291</code> and sends <code>HTTP POST /api/v1/exit/initiate {ticket_id: "T-88291"}</code>. The Exit Service looks up the ticket, finds <code>entry_time: 09:02, spot_id: B-204</code>. The Fee Calculation Service computes: 8 hours 28 minutes √ó $3.00/hour = $25.50 (rounded to the next half hour = 8.5 hours √ó $3.00 = $25.50). The fee is returned to the Gate Controller, which displays "$25.50" on the payment terminal screen. The driver taps their credit card. The Gate Controller sends <code>HTTP POST /api/v1/exit/pay {ticket_id: "T-88291", amount: 25.50, method: "card", card_token: "tok_xxx"}</code>. The Payment Service charges the card, records the payment, the Exit Service updates the spot to <code>available</code> and the ticket to <code>completed</code>. A <code>spot_freed</code> event is published. Floor 2's sign increments from 44 to 45. The gate opens.
</div>

<div class="example">
    <strong>Example 2 (Payment Failure):</strong> The driver's card is declined. The Payment Service returns <code>HTTP 402 Payment Required ‚Äî {error: "card_declined"}</code>. The Gate Controller displays "Payment Failed ‚Äî Try Another Method" on the terminal. The gate remains closed. The driver inserts cash instead. The Gate Controller re-sends the payment request with <code>method: "cash"</code>. This time it succeeds, and the gate opens.
</div>

<div class="example">
    <strong>Example 3 (Lost Ticket):</strong> A driver arrives at the exit without a ticket. They press the "Lost Ticket" button on the terminal. The Gate Controller sends <code>HTTP POST /api/v1/exit/lost-ticket {garage_id: "G1", license_plate: "ABC-1234"}</code>. The Exit Service searches for an active (uncompleted) ticket matching the license plate (captured by ANPR on entry). If found, the fee is calculated from the original entry time with a surcharge (e.g., +$10 lost ticket fee). If not found, the system pages a human attendant.
</div>

<h3>Flow 2 ‚Äî Component Deep Dive</h3>
<div class="card">
<h4>Ticket Scanner (Exit Gate)</h4>
<p>A barcode/QR scanner or magnetic strip reader mounted at the exit lane. Reads the ticket ID from the physical ticket. Communicates with the Gate Controller locally.</p>

<h4>Exit Service</h4>
<ul>
    <li><strong>Protocol:</strong> HTTP REST</li>
    <li><strong>Endpoint 1:</strong> <code>POST /api/v1/exit/initiate</code></li>
    <li><strong>Input:</strong> <code>{ticket_id: string}</code></li>
    <li><strong>Output:</strong> <code>HTTP 200 ‚Äî {ticket_id, entry_time, duration_minutes, fee_amount, currency}</code></li>
    <li><strong>Endpoint 2:</strong> <code>POST /api/v1/exit/pay</code></li>
    <li><strong>Input:</strong> <code>{ticket_id: string, amount: float, method: enum("card"|"cash"|"mobile"), card_token?: string}</code></li>
    <li><strong>Output (success):</strong> <code>HTTP 200 ‚Äî {payment_id, receipt_url, gate_command: "open"}</code></li>
    <li><strong>Output (failure):</strong> <code>HTTP 402 ‚Äî {error: "card_declined" | "insufficient_funds"}</code></li>
</ul>

<h4>Fee Calculation Service</h4>
<p>A stateless internal service that computes the parking fee. Takes <code>{entry_time, exit_time, vehicle_type, garage_id, reservation_id?}</code> and returns the fee. Supports configurable pricing tiers (per-hour, daily max cap, weekend rates, reservation discounts). Reads pricing configuration from the database (cached in memory). Pure function ‚Äî no side effects.</p>

<h4>Payment Service</h4>
<ul>
    <li><strong>Protocol:</strong> HTTP REST</li>
    <li><strong>Responsibility:</strong> Processes payments via card, cash, or mobile. For card payments, communicates with an external payment gateway. Records all transactions in the Payments table.</li>
    <li>Idempotency is enforced via <code>ticket_id + attempt_number</code> to prevent double charges on retries.</li>
</ul>

<h4>Payment Terminal (Card/NFC)</h4>
<p>Hardware device at the exit lane. Supports chip, tap (NFC), magnetic stripe, and cash. Communicates with the Gate Controller locally. Card data is encrypted at the terminal (PCI-DSS compliant) and a token is passed to the backend ‚Äî raw card numbers never leave the terminal.</p>
</div>

<hr>

<!-- ============================= -->
<!-- FLOW 3: REAL-TIME AVAILABILITY -->
<!-- ============================= -->
<h2>5. Flow 3 ‚Äî Real-Time Availability Display</h2>

<div class="diagram-wrapper">
<pre class="mermaid">
flowchart LR
    PUB["Pub/Sub\n(spot-status-changes)"]
    AS["Availability\nService"]
    CACHE[("In-Memory Cache\n(Availability Counts)")]
    WS["WebSocket\nServer"]
    SIGN1["Digital Sign\nFloor 1"]
    SIGN2["Digital Sign\nFloor 2"]
    SIGNN["Digital Sign\n... Floor N"]
    ENT_SIGN["Entrance\nSign"]
    MOB["üì± Mobile App"]
    API["API Gateway"]

    PUB -->|"subscribe:\nspot_occupied / spot_freed"| AS
    AS -->|"UPDATE aggregated\ncounts"| CACHE
    AS -->|"push via WebSocket"| WS
    WS -->|"ws://push"| SIGN1
    WS -->|"ws://push"| SIGN2
    WS -->|"ws://push"| SIGNN
    WS -->|"ws://push"| ENT_SIGN
    MOB -->|"HTTP GET\n/availability"| API
    API -->|"read"| CACHE
    API -->|"return counts"| MOB
</pre>
</div>

<h3>Flow 3 ‚Äî Examples</h3>

<div class="example">
    <strong>Example 1 (Spot Freed ‚Äî Sign Update):</strong> A vehicle exits from Floor 3, Spot #C-015 (compact). The Exit Service publishes <code>{event: "spot_freed", garage_id: "G1", floor: 3, spot_type: "compact", spot_id: "C-015"}</code> to the Pub/Sub <code>spot-status-changes</code> topic. The Availability Service receives this event, increments the availability count in the cache for <code>G1:Floor3:compact</code> from 12 to 13. It then pushes the updated counts over the WebSocket connection to the Floor 3 sign controller, which refreshes the display to show "Compact: 13 available". The entrance sign also updates its total garage availability.
</div>

<div class="example">
    <strong>Example 2 (Mobile App Check):</strong> A user opens the "ParkHere" mobile app and views Garage G1. The app sends <code>HTTP GET /api/v1/garages/G1/availability</code>. The API Gateway routes this to the Availability Service, which reads from the in-memory cache (not the database) and returns: <code>{garage_id: "G1", total_available: 320, floors: [{floor: 1, available: {regular: 40, compact: 12, motorcycle: 5, large: 3}}, {floor: 2, available: {regular: 30, compact: 10, motorcycle: 4, large: 2}}, ...]}</code>. Latency: ~15ms. No database hit.
</div>

<div class="example">
    <strong>Example 3 (WebSocket Reconnection):</strong> The digital sign on Floor 2 loses its WebSocket connection due to a brief network hiccup. The sign controller detects the disconnection and automatically reconnects within 5 seconds using an exponential backoff strategy. Upon reconnection, it sends a <code>"sync"</code> message to the WebSocket server, which responds with the current availability snapshot for Floor 2, ensuring the sign displays accurate data even after the disconnect.
</div>

<h3>Flow 3 ‚Äî Component Deep Dive</h3>
<div class="card">
<h4>Availability Service</h4>
<ul>
    <li><strong>Protocol:</strong> Subscribes to Pub/Sub (internal), exposes HTTP REST for mobile app queries.</li>
    <li><strong>Endpoint:</strong> <code>GET /api/v1/garages/{garage_id}/availability</code></li>
    <li><strong>Input:</strong> <code>garage_id</code> (path parameter)</li>
    <li><strong>Output:</strong> <code>HTTP 200 ‚Äî {garage_id, total_available, floors: [{floor, available: {regular, compact, motorcycle, large}}]}</code></li>
    <li>Maintains the in-memory cache as the source of truth for availability counts. On startup, initializes from a full DB scan: <code>SELECT floor, type, COUNT(*) FROM spots WHERE status='available' GROUP BY floor, type</code>.</li>
</ul>

<h4>WebSocket Server</h4>
<p>Maintains persistent WebSocket connections with all digital sign controllers in the garage. When the Availability Service updates counts, it pushes the delta to all connected signs for the affected garage/floor. Each sign controller registers with <code>{garage_id, floor}</code> on connection, and only receives updates relevant to its floor (plus the entrance sign which receives all updates).</p>
<p><strong>Connection lifecycle:</strong></p>
<ol>
    <li>Sign controller boots up and opens a WebSocket connection to <code>ws://availability-service:8080/ws/signs?garage=G1&floor=2</code>.</li>
    <li>Server registers the connection in a local <strong>connection registry</strong> (in-memory map: <code>garage_id:floor ‚Üí [WebSocket connections]</code>).</li>
    <li>Server sends an initial snapshot of current availability.</li>
    <li>On each spot change event, server looks up all connections for the affected garage/floor and sends the update.</li>
    <li>Heartbeat pings every 30 seconds to detect dead connections. Dead connections are removed from the registry.</li>
</ol>

<h4>Why WebSocket (and not alternatives)?</h4>
<ul>
    <li><strong>Why not HTTP polling?</strong> Digital signs need real-time updates (&lt;1 second). Polling every second from dozens of signs creates unnecessary load and still has up to 1 second latency. WebSocket gives instant push with a single persistent connection per sign.</li>
    <li><strong>Why not Server-Sent Events (SSE)?</strong> SSE would also work well here (unidirectional server‚Üíclient push). WebSocket was chosen because the sign controllers occasionally send messages back (sync requests, health status), making bidirectional communication useful. SSE is a valid alternative.</li>
    <li><strong>Why not Pub/Sub directly to signs?</strong> The signs are thin IoT devices. Having them directly subscribe to a Pub/Sub system adds complexity (client libraries, credentials). A WebSocket connection to the Availability Service is simpler for the embedded device.</li>
</ul>
</div>

<hr>

<!-- ============================= -->
<!-- FLOW 4: RESERVATION           -->
<!-- ============================= -->
<h2>6. Flow 4 ‚Äî Reservation (Pre-Booking)</h2>

<div class="diagram-wrapper">
<pre class="mermaid">
flowchart LR
    MOB["üì± Mobile App"]
    AG["API Gateway\n+ Load Balancer"]
    RS["Reservation\nService (HTTP)"]
    DB[("SQL Database\n(Reservations,\nSpots)")]
    CACHE[("In-Memory Cache\n(Availability)")]
    NS["Notification\nService"]
    MQ["Message Queue\n(Notifications)"]
    PUSH["Push Notification\nProvider"]
    PHONE["üì± User's Phone"]

    MOB -->|"HTTP POST\n/reservations"| AG
    AG --> RS
    RS -->|"BEGIN TRANSACTION\ncheck availability\nreserve spot"| DB
    RS -->|"decrement available"| CACHE
    RS -->|"return confirmation"| MOB
    RS -->|"enqueue reminder"| MQ
    MQ -->|"dequeue"| NS
    NS -->|"send push"| PUSH
    PUSH -->|"reminder:\n'Reservation in 30 min'"| PHONE
</pre>
</div>

<h3>Flow 4 ‚Äî Examples</h3>

<div class="example">
    <strong>Example 1 (Successful Reservation):</strong> A user opens the mobile app at 7:00 AM and wants to reserve a regular spot at Garage G1 for 9:00 AM‚Äì5:00 PM. The app sends <code>HTTP POST /api/v1/reservations {garage_id: "G1", vehicle_type: "regular", start_time: "09:00", end_time: "17:00", user_id: "U-4412"}</code>. The Reservation Service begins a transaction: checks for available regular spots that are not already reserved for the overlapping time window, picks Spot #A-105, inserts a reservation record <code>{reservation_id: "R-55123", spot_id: "A-105", user_id: "U-4412", status: "confirmed"}</code>, and marks the spot as <code>reserved</code>. The cache decrements the available count. At 8:30 AM, a reminder notification is dequeued from the message queue and sent: "Your parking reservation at G1, Spot A-105 is in 30 minutes."
</div>

<div class="example">
    <strong>Example 2 (No-Show ‚Äî Reservation Expiry):</strong> The user from Example 1 does not arrive by 9:15 AM (15-minute grace period). A scheduled job detects the expired reservation, updates it to <code>status: "expired"</code>, frees the spot back to <code>available</code>, publishes a <code>spot_freed</code> event, and sends a notification: "Your reservation R-55123 has expired. You were charged a $5 no-show fee." The availability cache is updated.
</div>

<div class="example">
    <strong>Example 3 (Cancellation):</strong> At 8:00 AM, the user decides to cancel. The app sends <code>HTTP DELETE /api/v1/reservations/R-55123</code>. The Reservation Service updates the reservation to <code>status: "cancelled"</code>, frees the spot, and the availability count is incremented. If cancellation is before 8:30 AM (30-min policy), no fee is charged. Otherwise, a cancellation fee applies.
</div>

<h3>Flow 4 ‚Äî Component Deep Dive</h3>
<div class="card">
<h4>Reservation Service</h4>
<ul>
    <li><strong>Protocol:</strong> HTTP REST</li>
    <li><strong>Endpoint (create):</strong> <code>POST /api/v1/reservations</code></li>
    <li><strong>Input:</strong> <code>{garage_id, vehicle_type, start_time, end_time, user_id}</code></li>
    <li><strong>Output:</strong> <code>HTTP 201 ‚Äî {reservation_id, spot_id, floor, zone, start_time, end_time, status: "confirmed"}</code></li>
    <li><strong>Endpoint (cancel):</strong> <code>DELETE /api/v1/reservations/{reservation_id}</code></li>
    <li><strong>Output:</strong> <code>HTTP 200 ‚Äî {status: "cancelled", cancellation_fee: float}</code></li>
    <li>Uses SQL transaction with time-range overlap checks to prevent double-booking of spots for overlapping time windows.</li>
</ul>

<h4>Message Queue (Notifications)</h4>
<p>Used for asynchronous, reliable delivery of notifications. When a reservation is created, a message is enqueued: <code>{type: "reservation_reminder", user_id, reservation_id, send_at: start_time - 30min}</code>. The Notification Service dequeues messages at the appropriate time and sends push notifications.</p>
<p><strong>Why a message queue for notifications?</strong></p>
<ul>
    <li>Notifications are not time-critical to the millisecond ‚Äî a few seconds of delay is acceptable.</li>
    <li>The message queue provides <strong>guaranteed delivery</strong> (at-least-once semantics), so reminders are never lost even if the Notification Service temporarily goes down.</li>
    <li>It <strong>decouples</strong> the Reservation Service from the Notification Service ‚Äî they can scale independently.</li>
    <li>Supports <strong>delayed messages</strong> (deliver 30 min before reservation start).</li>
</ul>

<h4>Notification Service</h4>
<p>Consumes messages from the queue and dispatches push notifications (via APNs for iOS, FCM for Android) or SMS. Stateless and horizontally scalable.</p>
</div>

<hr>

<!-- ============================= -->
<!-- COMBINED OVERALL DIAGRAM      -->
<!-- ============================= -->
<h2>7. Combined Overall System Diagram</h2>

<div class="diagram-wrapper">
<pre class="mermaid">
flowchart TB
    subgraph Clients
        CAR_IN["üöó Vehicle\n(Entry)"]
        CAR_OUT["üöó Vehicle\n(Exit)"]
        MOB["üì± Mobile App"]
        SIGN["üñ•Ô∏è Digital Signs"]
        ADMIN["üë§ Admin\nDashboard"]
    end

    subgraph Edge["Edge Devices (Per Gate)"]
        GC_IN["Entry Gate\nController"]
        GC_OUT["Exit Gate\nController"]
        SCANNER["Ticket Scanner"]
        PRINTER["Ticket Printer"]
        PMTTERM["Payment Terminal"]
    end

    subgraph Gateway["API Layer"]
        LB["Load Balancer"]
        AG["API Gateway\n(Auth, Rate Limit, TLS)"]
    end

    subgraph Services["Microservices"]
        ES["Entry Service"]
        XS["Exit Service"]
        SAS["Spot Assignment\nService"]
        FEE["Fee Calculation\nService"]
        PAY["Payment Service"]
        RS["Reservation\nService"]
        AVAIL["Availability\nService"]
        NS["Notification\nService"]
        ANALYTICS["Analytics\nService"]
    end

    subgraph Messaging["Async Messaging"]
        PUB["Pub/Sub\n(spot-status-changes)"]
        MQ["Message Queue\n(notifications,\npayment receipts)"]
    end

    subgraph DataLayer["Data Layer"]
        DB[("SQL Database\n(Primary)")]
        DB_R[("SQL Database\n(Read Replica)")]
        CACHE[("In-Memory\nCache")]
    end

    subgraph External["External"]
        PUSH["Push Notification\nProvider"]
        PG["Payment\nGateway"]
    end

    CAR_IN --> GC_IN
    GC_IN --> PRINTER
    CAR_OUT --> SCANNER --> GC_OUT
    GC_OUT --> PMTTERM
    GC_IN --> LB
    GC_OUT --> LB
    MOB --> LB
    ADMIN --> LB
    LB --> AG

    AG --> ES
    AG --> XS
    AG --> RS
    AG --> AVAIL
    AG --> ANALYTICS

    ES --> SAS
    SAS --> DB
    ES --> DB
    ES --> PUB

    XS --> FEE
    XS --> PAY
    PAY --> PG
    PAY --> DB
    XS --> DB
    XS --> PUB

    RS --> DB
    RS --> MQ

    PUB --> AVAIL
    AVAIL --> CACHE
    AVAIL --> SIGN

    MQ --> NS
    NS --> PUSH

    ANALYTICS --> DB_R

    DB --> DB_R
</pre>
</div>

<h3>Combined Diagram ‚Äî End-to-End Examples</h3>

<div class="example">
    <strong>Full Lifecycle Example:</strong>
    <br><br>
    <strong>1. Reservation (7:00 AM):</strong> User opens the mobile app ‚Üí <code>POST /reservations</code> ‚Üí API Gateway ‚Üí Reservation Service ‚Üí SQL transaction reserves Spot A-105 ‚Üí Cache decremented ‚Üí Reminder enqueued to Message Queue.
    <br><br>
    <strong>2. Reminder (8:30 AM):</strong> Message Queue delivers the delayed message to Notification Service ‚Üí Push notification sent to user's phone: "Your reservation is in 30 minutes."
    <br><br>
    <strong>3. Entry (8:55 AM):</strong> User arrives ‚Üí Sensor triggers ‚Üí Gate Controller sends <code>POST /entry</code> with reservation code ‚Üí Entry Service validates reservation ‚Üí Spot A-105 confirmed ‚Üí Ticket T-88291 issued ‚Üí Pub/Sub event published ‚Üí Digital signs updated ‚Üí Gate opens ‚Üí Ticket printed.
    <br><br>
    <strong>4. During the day:</strong> Mobile app users and digital signs continuously reflect real-time availability. The admin dashboard shows occupancy trends via the Analytics Service reading from the read replica.
    <br><br>
    <strong>5. Exit (5:15 PM):</strong> User drives to exit ‚Üí Inserts ticket ‚Üí <code>POST /exit/initiate</code> ‚Üí Fee calculated (8 hours, reserved rate: $20.00) ‚Üí Fee displayed on terminal ‚Üí User taps card ‚Üí <code>POST /exit/pay</code> ‚Üí Payment Service charges card via external payment gateway ‚Üí Payment recorded ‚Üí Spot A-105 freed ‚Üí Ticket completed ‚Üí Pub/Sub event ‚Üí Digital signs updated ‚Üí Gate opens.
    <br><br>
    <strong>6. Analytics (ongoing):</strong> Analytics Service reads from the read replica to generate dashboards: peak hours, average duration, revenue per floor, etc.
</div>

<hr>

<!-- ============================= -->
<!-- SCHEMA                        -->
<!-- ============================= -->
<h2>8. Database Schema</h2>

<h3>SQL Tables</h3>
<p>All core tables use a relational (SQL) database because the data is highly relational (garages have floors, floors have spots, spots have tickets) and requires <strong>ACID transactions</strong> for spot assignment and payment processing. Financial data (payments) demands strong consistency and durability guarantees that SQL databases provide.</p>

<h4>Table: <code>garages</code></h4>
<table>
    <tr><th>Column</th><th>Type</th><th>Constraints</th><th>Description</th></tr>
    <tr><td><code>garage_id</code></td><td>UUID</td><td><strong>PRIMARY KEY</strong></td><td>Unique garage identifier</td></tr>
    <tr><td><code>name</code></td><td>VARCHAR(255)</td><td>NOT NULL</td><td>Garage name (e.g., "Downtown Central Garage")</td></tr>
    <tr><td><code>address</code></td><td>TEXT</td><td>NOT NULL</td><td>Physical address</td></tr>
    <tr><td><code>total_floors</code></td><td>INT</td><td>NOT NULL</td><td>Number of floors</td></tr>
    <tr><td><code>total_spots</code></td><td>INT</td><td>NOT NULL</td><td>Total parking spots</td></tr>
    <tr><td><code>latitude</code></td><td>DECIMAL(9,6)</td><td></td><td>GPS latitude</td></tr>
    <tr><td><code>longitude</code></td><td>DECIMAL(9,6)</td><td></td><td>GPS longitude</td></tr>
    <tr><td><code>created_at</code></td><td>TIMESTAMP</td><td>NOT NULL</td><td>Record creation time</td></tr>
</table>
<p><strong>Read:</strong> On app startup (list of garages), on admin dashboard load.<br>
<strong>Write:</strong> When a new garage is added or configuration is changed (rare ‚Äî admin action).</p>
<p><strong>Index:</strong> <code>(latitude, longitude)</code> ‚Äî <strong>R-tree / spatial index</strong> ‚Äî enables the mobile app to query "garages near me" efficiently using spatial proximity search. Without this index, finding nearby garages would require a full table scan with distance calculations.</p>

<h4>Table: <code>floors</code></h4>
<table>
    <tr><th>Column</th><th>Type</th><th>Constraints</th><th>Description</th></tr>
    <tr><td><code>floor_id</code></td><td>UUID</td><td><strong>PRIMARY KEY</strong></td><td>Unique floor identifier</td></tr>
    <tr><td><code>garage_id</code></td><td>UUID</td><td><strong>FOREIGN KEY ‚Üí garages</strong></td><td>Parent garage</td></tr>
    <tr><td><code>floor_number</code></td><td>INT</td><td>NOT NULL</td><td>Floor level (1, 2, 3...)</td></tr>
    <tr><td><code>total_spots</code></td><td>INT</td><td>NOT NULL</td><td>Total spots on this floor</td></tr>
</table>
<p><strong>Read:</strong> When querying availability per floor, when assigning spots (to determine which floor).<br>
<strong>Write:</strong> When a new floor is configured (rare ‚Äî admin action).</p>
<p><strong>Index:</strong> <code>(garage_id, floor_number)</code> ‚Äî <strong>B-tree composite index</strong> ‚Äî supports fast lookup of floors within a specific garage, ordered by floor number.</p>

<h4>Table: <code>spots</code></h4>
<table>
    <tr><th>Column</th><th>Type</th><th>Constraints</th><th>Description</th></tr>
    <tr><td><code>spot_id</code></td><td>UUID</td><td><strong>PRIMARY KEY</strong></td><td>Unique spot identifier</td></tr>
    <tr><td><code>floor_id</code></td><td>UUID</td><td><strong>FOREIGN KEY ‚Üí floors</strong></td><td>Parent floor</td></tr>
    <tr><td><code>garage_id</code></td><td>UUID</td><td><strong>FOREIGN KEY ‚Üí garages</strong></td><td>Parent garage (denormalized)</td></tr>
    <tr><td><code>spot_number</code></td><td>VARCHAR(10)</td><td>NOT NULL</td><td>Human-readable label (e.g., "B-204")</td></tr>
    <tr><td><code>spot_type</code></td><td>ENUM</td><td>NOT NULL</td><td>motorcycle, compact, regular, large</td></tr>
    <tr><td><code>status</code></td><td>ENUM</td><td>NOT NULL</td><td>available, occupied, reserved, out_of_service</td></tr>
    <tr><td><code>zone</code></td><td>VARCHAR(10)</td><td></td><td>Zone within floor (A, B, C...)</td></tr>
</table>
<p><strong>Read:</strong> On every vehicle entry (find available spot), on availability queries.<br>
<strong>Write:</strong> On every vehicle entry (mark occupied), on every vehicle exit (mark available), on reservation creation (mark reserved).</p>
<p><strong>Denormalization note:</strong> <code>garage_id</code> is stored directly on the <code>spots</code> table even though it can be derived from <code>floors.garage_id</code>. This is intentional denormalization to <strong>avoid a JOIN</strong> during the hot-path spot assignment query. The query <code>SELECT ... FROM spots WHERE garage_id = ? AND status = 'available' AND spot_type = ?</code> can now be answered from a single table scan with an index, instead of joining <code>spots</code> with <code>floors</code>. Since garage-floor relationships rarely change, the denormalization cost (maintaining consistency on the rare floor reconfiguration) is negligible compared to the performance benefit on every entry.</p>
<p><strong>Index:</strong> <code>(garage_id, status, spot_type, floor_id)</code> ‚Äî <strong>B-tree composite index</strong> ‚Äî this is the critical index for the spot assignment query. It allows the database to efficiently find available spots of the correct type in a specific garage, ordered by floor. The column order matches the query's WHERE clause and ORDER BY, enabling an index-only scan for the most frequent operation in the system.</p>

<h4>Table: <code>tickets</code></h4>
<table>
    <tr><th>Column</th><th>Type</th><th>Constraints</th><th>Description</th></tr>
    <tr><td><code>ticket_id</code></td><td>UUID</td><td><strong>PRIMARY KEY</strong></td><td>Unique ticket identifier</td></tr>
    <tr><td><code>ticket_number</code></td><td>VARCHAR(20)</td><td>UNIQUE, NOT NULL</td><td>Human-readable ticket number printed on ticket</td></tr>
    <tr><td><code>garage_id</code></td><td>UUID</td><td><strong>FOREIGN KEY ‚Üí garages</strong></td><td>Garage where ticket was issued</td></tr>
    <tr><td><code>spot_id</code></td><td>UUID</td><td><strong>FOREIGN KEY ‚Üí spots</strong></td><td>Assigned spot</td></tr>
    <tr><td><code>reservation_id</code></td><td>UUID</td><td>FOREIGN KEY ‚Üí reservations (nullable)</td><td>Linked reservation, if any</td></tr>
    <tr><td><code>license_plate</code></td><td>VARCHAR(20)</td><td></td><td>License plate (if ANPR available)</td></tr>
    <tr><td><code>vehicle_type</code></td><td>ENUM</td><td>NOT NULL</td><td>motorcycle, compact, regular, large</td></tr>
    <tr><td><code>entry_time</code></td><td>TIMESTAMP</td><td>NOT NULL</td><td>When the vehicle entered</td></tr>
    <tr><td><code>exit_time</code></td><td>TIMESTAMP</td><td></td><td>When the vehicle exited (null if still parked)</td></tr>
    <tr><td><code>status</code></td><td>ENUM</td><td>NOT NULL</td><td>active, completed, lost</td></tr>
</table>
<p><strong>Read:</strong> On vehicle exit (lookup by ticket_number), on lost ticket (lookup by license_plate and garage_id).<br>
<strong>Write:</strong> On vehicle entry (INSERT), on vehicle exit (UPDATE exit_time and status).</p>
<p><strong>Index 1:</strong> <code>(ticket_number)</code> ‚Äî <strong>Hash index</strong> ‚Äî ticket lookup on exit is an exact match by ticket number. Hash index provides O(1) lookup, which is ideal for this use case. This is the most critical exit-path query.</p>
<p><strong>Index 2:</strong> <code>(garage_id, license_plate, status)</code> ‚Äî <strong>B-tree composite index</strong> ‚Äî used for the "lost ticket" flow to find active tickets by license plate within a garage.</p>
<p><strong>Index 3:</strong> <code>(entry_time)</code> ‚Äî <strong>B-tree index</strong> ‚Äî supports time-range queries for analytics (e.g., "all entries between 8 AM and 10 AM today").</p>

<h4>Table: <code>payments</code></h4>
<table>
    <tr><th>Column</th><th>Type</th><th>Constraints</th><th>Description</th></tr>
    <tr><td><code>payment_id</code></td><td>UUID</td><td><strong>PRIMARY KEY</strong></td><td>Unique payment identifier</td></tr>
    <tr><td><code>ticket_id</code></td><td>UUID</td><td><strong>FOREIGN KEY ‚Üí tickets</strong></td><td>Associated ticket</td></tr>
    <tr><td><code>amount</code></td><td>DECIMAL(10,2)</td><td>NOT NULL</td><td>Amount charged</td></tr>
    <tr><td><code>currency</code></td><td>VARCHAR(3)</td><td>NOT NULL</td><td>Currency code (USD, EUR, etc.)</td></tr>
    <tr><td><code>method</code></td><td>ENUM</td><td>NOT NULL</td><td>card, cash, mobile</td></tr>
    <tr><td><code>status</code></td><td>ENUM</td><td>NOT NULL</td><td>pending, completed, failed, refunded</td></tr>
    <tr><td><code>external_ref</code></td><td>VARCHAR(100)</td><td></td><td>External payment gateway reference ID</td></tr>
    <tr><td><code>paid_at</code></td><td>TIMESTAMP</td><td></td><td>When payment was completed</td></tr>
    <tr><td><code>created_at</code></td><td>TIMESTAMP</td><td>NOT NULL</td><td>Record creation time</td></tr>
</table>
<p><strong>Read:</strong> On payment status check, on receipt generation, on admin revenue reports.<br>
<strong>Write:</strong> On payment processing (INSERT), on payment status update (UPDATE status).</p>
<p><strong>Index 1:</strong> <code>(ticket_id)</code> ‚Äî <strong>Hash index</strong> ‚Äî fast lookup of payment by ticket (used during exit flow to check if a ticket was already paid).</p>
<p><strong>Index 2:</strong> <code>(paid_at)</code> ‚Äî <strong>B-tree index</strong> ‚Äî supports time-range queries for revenue reporting.</p>

<h4>Table: <code>reservations</code></h4>
<table>
    <tr><th>Column</th><th>Type</th><th>Constraints</th><th>Description</th></tr>
    <tr><td><code>reservation_id</code></td><td>UUID</td><td><strong>PRIMARY KEY</strong></td><td>Unique reservation identifier</td></tr>
    <tr><td><code>user_id</code></td><td>UUID</td><td><strong>FOREIGN KEY ‚Üí users</strong></td><td>User who made the reservation</td></tr>
    <tr><td><code>garage_id</code></td><td>UUID</td><td><strong>FOREIGN KEY ‚Üí garages</strong></td><td>Target garage</td></tr>
    <tr><td><code>spot_id</code></td><td>UUID</td><td><strong>FOREIGN KEY ‚Üí spots</strong></td><td>Reserved spot</td></tr>
    <tr><td><code>vehicle_type</code></td><td>ENUM</td><td>NOT NULL</td><td>Vehicle type</td></tr>
    <tr><td><code>start_time</code></td><td>TIMESTAMP</td><td>NOT NULL</td><td>Reservation start time</td></tr>
    <tr><td><code>end_time</code></td><td>TIMESTAMP</td><td>NOT NULL</td><td>Reservation end time</td></tr>
    <tr><td><code>status</code></td><td>ENUM</td><td>NOT NULL</td><td>confirmed, cancelled, expired, fulfilled</td></tr>
    <tr><td><code>created_at</code></td><td>TIMESTAMP</td><td>NOT NULL</td><td>When reservation was created</td></tr>
</table>
<p><strong>Read:</strong> On vehicle entry with reservation code, on reservation cancellation, by the expiration scheduler.<br>
<strong>Write:</strong> On reservation creation (INSERT), on cancellation/expiry/fulfillment (UPDATE status).</p>
<p><strong>Index:</strong> <code>(garage_id, spot_id, start_time, end_time)</code> ‚Äî <strong>B-tree composite index</strong> ‚Äî supports the overlap check query: <code>SELECT ... WHERE garage_id = ? AND spot_id = ? AND status = 'confirmed' AND start_time < ? AND end_time > ?</code>. This prevents double-booking a spot for overlapping time windows.</p>

<h4>Table: <code>users</code></h4>
<table>
    <tr><th>Column</th><th>Type</th><th>Constraints</th><th>Description</th></tr>
    <tr><td><code>user_id</code></td><td>UUID</td><td><strong>PRIMARY KEY</strong></td><td>Unique user identifier</td></tr>
    <tr><td><code>email</code></td><td>VARCHAR(255)</td><td>UNIQUE, NOT NULL</td><td>User's email</td></tr>
    <tr><td><code>name</code></td><td>VARCHAR(255)</td><td>NOT NULL</td><td>User's name</td></tr>
    <tr><td><code>phone</code></td><td>VARCHAR(20)</td><td></td><td>Phone number (for SMS)</td></tr>
    <tr><td><code>payment_method_token</code></td><td>VARCHAR(255)</td><td></td><td>Stored payment method token</td></tr>
    <tr><td><code>created_at</code></td><td>TIMESTAMP</td><td>NOT NULL</td><td>Account creation time</td></tr>
</table>
<p><strong>Read:</strong> On login, on reservation creation (validate user), on notification sending.<br>
<strong>Write:</strong> On user registration (INSERT), on profile update (UPDATE).</p>
<p><strong>Index:</strong> <code>(email)</code> ‚Äî <strong>Hash index</strong> ‚Äî fast exact-match lookup for login.</p>

<h4>Table: <code>pricing_rules</code></h4>
<table>
    <tr><th>Column</th><th>Type</th><th>Constraints</th><th>Description</th></tr>
    <tr><td><code>rule_id</code></td><td>UUID</td><td><strong>PRIMARY KEY</strong></td><td>Unique rule identifier</td></tr>
    <tr><td><code>garage_id</code></td><td>UUID</td><td><strong>FOREIGN KEY ‚Üí garages</strong></td><td>Garage this rule applies to</td></tr>
    <tr><td><code>vehicle_type</code></td><td>ENUM</td><td>NOT NULL</td><td>Which vehicle type</td></tr>
    <tr><td><code>rate_per_hour</code></td><td>DECIMAL(10,2)</td><td>NOT NULL</td><td>Hourly rate</td></tr>
    <tr><td><code>daily_max</code></td><td>DECIMAL(10,2)</td><td></td><td>Max daily charge</td></tr>
    <tr><td><code>day_of_week</code></td><td>VARCHAR(10)</td><td></td><td>Applicable day (null = all days)</td></tr>
    <tr><td><code>effective_from</code></td><td>TIMESTAMP</td><td>NOT NULL</td><td>When this rule takes effect</td></tr>
    <tr><td><code>effective_to</code></td><td>TIMESTAMP</td><td></td><td>When this rule expires (null = indefinite)</td></tr>
</table>
<p><strong>Read:</strong> On fee calculation (every exit), cached in-memory by the Fee Calculation Service.<br>
<strong>Write:</strong> When admin updates pricing (rare).</p>
<p><strong>Index:</strong> <code>(garage_id, vehicle_type, effective_from)</code> ‚Äî <strong>B-tree composite index</strong> ‚Äî supports finding the current applicable rate for a given garage and vehicle type.</p>

<h3>Sharding Strategy</h3>
<div class="card">
<p>For a multi-garage chain operating at scale (hundreds of garages, thousands of spots each), sharding by <code>garage_id</code> is the recommended strategy for the <code>spots</code>, <code>tickets</code>, <code>payments</code>, and <code>reservations</code> tables.</p>
<p><strong>Why shard by <code>garage_id</code>?</strong></p>
<ul>
    <li><strong>Data locality:</strong> All operations (entry, exit, reservation, payment) happen within a single garage. A vehicle entering Garage G1 will never need data from Garage G2's spots or tickets. This means all queries within a single transaction are served by a single shard ‚Äî no cross-shard joins or transactions needed.</li>
    <li><strong>Even distribution:</strong> Each garage has a roughly similar number of spots (hundreds to low thousands), so data is distributed relatively evenly across shards.</li>
    <li><strong>Independent scaling:</strong> A busy downtown garage can be placed on a more powerful shard, while a quiet suburban garage shares a shard with others.</li>
    <li><strong>No hotspot risk:</strong> Unlike sharding by time or user_id, garage_id-based sharding avoids hotspots because the traffic to each garage is bounded by its physical capacity (a 500-spot garage can only have ~500 entries/day).</li>
</ul>
<p><strong>Cross-shard queries</strong> (e.g., "total revenue across all garages") are handled by the Analytics Service reading from read replicas or a separate analytics data warehouse that aggregates data from all shards.</p>
</div>

<hr>

<!-- ============================= -->
<!-- CDN & CACHE DEEP DIVE         -->
<!-- ============================= -->
<h2>9. CDN &amp; Cache Deep Dive</h2>

<h3>CDN</h3>
<div class="card">
<p><strong>A CDN is NOT a core requirement for a parking garage system.</strong></p>
<p>Unlike social media or video streaming, a parking garage system does not serve large static assets to millions of geographically distributed users. The primary consumers are:</p>
<ul>
    <li><strong>Gate controllers:</strong> On a local network ‚Äî CDN is irrelevant.</li>
    <li><strong>Digital signs:</strong> On a local network ‚Äî CDN is irrelevant.</li>
    <li><strong>Mobile app:</strong> The app binary is distributed via the app store, not our infrastructure. API responses are small JSON payloads (&lt;1KB) that change frequently (availability counts), making them poor candidates for CDN caching.</li>
    <li><strong>Admin dashboard:</strong> Used by a small number of operators ‚Äî CDN unnecessary.</li>
</ul>
<p><strong>Exception:</strong> If the mobile app has a web-based version (SPA), a CDN could serve the static JavaScript/CSS/image assets. But this is standard web deployment, not specific to the parking domain.</p>
</div>

<h3>In-Memory Cache</h3>
<div class="card">
<p>An in-memory cache is <strong>critical</strong> for this system. Here's the deep dive:</p>

<h4>What is cached?</h4>
<p>Aggregated availability counts per garage, per floor, per vehicle type. Example cache entry:</p>
<pre style="background: var(--code-bg); padding: 12px; border-radius: 6px; overflow-x: auto; margin: 10px 0;">
Key:   "availability:G1:floor2"
Value: {"regular": 30, "compact": 10, "motorcycle": 4, "large": 2, "total": 46}
</pre>

<h4>Why cache?</h4>
<p>Availability is the most frequently read data in the system. Digital signs poll or receive pushes every second. Mobile app users check availability frequently. Without caching, every availability check would require: <code>SELECT spot_type, COUNT(*) FROM spots WHERE garage_id = ? AND floor_id = ? AND status = 'available' GROUP BY spot_type</code> ‚Äî an aggregation query on a table with thousands of rows, executed potentially hundreds of times per minute. The cache reduces this to a single O(1) key-value lookup.</p>

<h4>Caching Strategy: Write-Through</h4>
<p><strong>Why write-through?</strong> When a spot status changes (entry/exit), the Entry or Exit Service updates both the database AND the cache in the same operation flow. This ensures the cache is always consistent with the database. The sequence is:</p>
<ol>
    <li>SQL transaction: UPDATE spot status in DB ‚Üí COMMIT</li>
    <li>Immediately after commit: UPDATE cache (decrement/increment the availability count)</li>
    <li>Publish event to Pub/Sub</li>
</ol>
<p>If the cache update fails (rare), the Pub/Sub event will trigger the Availability Service to re-read from DB and correct the cache.</p>
<p><strong>Why not write-behind (write-back)?</strong> Write-behind would write to cache first and asynchronously to DB. This risks data loss if the cache crashes before the DB write, which is unacceptable for spot assignment (could cause double-booking). The database transaction MUST be the source of truth.</p>
<p><strong>Why not cache-aside (lazy loading)?</strong> Cache-aside only populates the cache on cache misses. Since availability is read constantly (digital signs), we want the cache pre-populated and always up-to-date, not populated lazily on the first miss.</p>

<h4>Eviction Policy: LRU (Least Recently Used)</h4>
<p>While the working set is small enough to fit entirely in memory for most deployments (a few hundred keys for a multi-garage system), LRU eviction is configured as a safety net. If memory is constrained, the least accessed garage/floor combinations (e.g., a garage that's been closed for maintenance) are evicted first.</p>

<h4>Expiration (TTL) Policy: 5-minute TTL</h4>
<p>Each cache entry has a 5-minute TTL as a <strong>staleness safety net</strong>. In normal operation, the cache is updated on every entry/exit event and should never become stale. However, if an event is lost (network partition, service restart), the 5-minute TTL ensures the cache self-heals by expiring and being re-populated from the database on the next read. The 5-minute window is acceptable because a slightly stale availability count (off by 1-2 spots) is not dangerous ‚Äî it just means a sign might briefly show 44 instead of 45 available spots.</p>

<h4>Cache Warm-Up</h4>
<p>On service startup, the Availability Service executes a full aggregation query on the DB and populates the cache for all garages/floors. This ensures the cache is immediately ready to serve requests without waiting for entry/exit events to gradually populate it.</p>
</div>

<hr>

<!-- ============================= -->
<!-- PUB/SUB DEEP DIVE             -->
<!-- ============================= -->
<h2>10. Pub/Sub Deep Dive</h2>
<div class="card">
<h4>Purpose</h4>
<p>The Pub/Sub system decouples spot status changes (produced by Entry/Exit Services) from consumers that need to react to those changes (Availability Service, Analytics Service, potential future consumers).</p>

<h4>Topics</h4>
<table>
    <tr><th>Topic</th><th>Publisher</th><th>Subscribers</th><th>Message Format</th></tr>
    <tr>
        <td><code>spot-status-changes</code></td>
        <td>Entry Service, Exit Service, Reservation Service</td>
        <td>Availability Service, Analytics Service</td>
        <td><code>{event_type: "spot_occupied"|"spot_freed"|"spot_reserved", garage_id, floor_id, spot_id, spot_type, timestamp}</code></td>
    </tr>
</table>

<h4>How it works</h4>
<ol>
    <li><strong>Publish:</strong> After the Entry Service commits the SQL transaction (spot marked as occupied), it publishes a message to the <code>spot-status-changes</code> topic. The publish call is made asynchronously (fire-and-forget with at-least-once guarantee from the Pub/Sub system).</li>
    <li><strong>Subscribe:</strong> The Availability Service has a long-running subscription to the topic. It receives every message, updates the in-memory cache, and pushes updates to digital signs via WebSocket.</li>
    <li><strong>Delivery guarantee:</strong> At-least-once delivery. If a message is delivered twice, the Availability Service is idempotent (incrementing/decrementing a counter based on the event type and spot_id ‚Äî replaying the same event for the same spot_id is a no-op if the spot status hasn't changed).</li>
</ol>

<h4>Why Pub/Sub (and not alternatives)?</h4>
<ul>
    <li><strong>Why not direct HTTP calls from Entry Service ‚Üí Availability Service?</strong> Tight coupling. If the Availability Service is down, the Entry Service would fail or need retry logic. Pub/Sub provides buffering and decoupling.</li>
    <li><strong>Why not a Message Queue instead?</strong> A message queue delivers each message to exactly one consumer. We need the same event to be consumed by multiple subscribers (Availability Service AND Analytics Service). Pub/Sub's fan-out model is the right fit. A message queue is used separately for notifications (single consumer).</li>
    <li><strong>Why not database polling (Availability Service periodically queries the spots table)?</strong> Polling adds latency (up to the polling interval) and places continuous load on the database. Pub/Sub delivers events in near real-time with no polling overhead.</li>
</ul>
</div>

<hr>

<!-- ============================= -->
<!-- MESSAGE QUEUE DEEP DIVE       -->
<!-- ============================= -->
<h2>11. Message Queue Deep Dive</h2>
<div class="card">
<h4>Purpose</h4>
<p>The message queue is used for <strong>asynchronous, reliable, single-consumer</strong> tasks ‚Äî specifically notification delivery (push notifications, SMS) and payment receipt generation.</p>

<h4>Queue: <code>notifications</code></h4>
<table>
    <tr><th>Aspect</th><th>Detail</th></tr>
    <tr><td>Producer</td><td>Reservation Service (enqueues reminders, confirmations, cancellations)</td></tr>
    <tr><td>Consumer</td><td>Notification Service</td></tr>
    <tr><td>Message Format</td><td><code>{type: "reservation_reminder"|"reservation_confirmed"|"reservation_cancelled"|"no_show", user_id, reservation_id, send_at: ISO8601, payload: {...}}</code></td></tr>
    <tr><td>Delivery</td><td>At-least-once, FIFO within a single user</td></tr>
</table>

<h4>How messages are produced and consumed</h4>
<ol>
    <li><strong>Enqueue:</strong> The Reservation Service calls <code>queue.send({...message, delay_until: send_at})</code>. The message is persisted in the queue with a visibility delay (the message won't be delivered to consumers until <code>send_at</code> time).</li>
    <li><strong>Dequeue:</strong> The Notification Service runs a consumer loop that calls <code>queue.receive(batch_size=10)</code>. It picks up messages whose <code>send_at</code> time has passed. For each message, it sends the notification via the appropriate channel (push/SMS).</li>
    <li><strong>Acknowledgment:</strong> After successfully sending the notification, the consumer calls <code>queue.ack(message_id)</code> to remove the message from the queue. If the consumer crashes before acknowledging, the message becomes visible again after a visibility timeout and is redelivered.</li>
</ol>

<h4>Why a Message Queue for notifications (and not Pub/Sub)?</h4>
<ul>
    <li>Notifications have a <strong>single consumer</strong> (the Notification Service). Pub/Sub's fan-out model is unnecessary and would waste resources.</li>
    <li>The message queue supports <strong>delayed delivery</strong>, which is needed for "send reminder 30 minutes before reservation" ‚Äî Pub/Sub delivers immediately.</li>
    <li>The queue provides <strong>backpressure</strong> ‚Äî if the Notification Service is slow, messages accumulate safely in the queue without putting pressure on the Reservation Service.</li>
</ul>
</div>

<hr>

<!-- ============================= -->
<!-- SCALING CONSIDERATIONS        -->
<!-- ============================= -->
<h2>12. Scaling Considerations</h2>
<div class="card">

<h4>Traffic Estimates</h4>
<p>For a chain of 100 garages, each with 1,000 spots, average 1.5 turnovers/day per spot:</p>
<ul>
    <li><strong>Entry/Exit events:</strong> 100 √ó 1,000 √ó 1.5 = 150,000 events/day ‚âà ~2 events/second (very manageable)</li>
    <li><strong>Availability reads:</strong> 100 garages √ó 5 floors √ó digital signs polling = ~500 WebSocket connections (tiny). Mobile app: assuming 10,000 daily users checking availability 3 times = 30,000 reads/day ‚âà 0.3/sec</li>
    <li><strong>Peak:</strong> Rush hours (8-9 AM, 5-6 PM) may see 10√ó the average = ~20 events/second ‚Äî still very manageable.</li>
</ul>
<p>A parking garage system is <strong>NOT</strong> a high-throughput system compared to social media or e-commerce. The bottleneck is the physical world (vehicles can only move so fast). However, we design for scale to support a large chain and to handle spikes gracefully.</p>

<h4>Load Balancers</h4>
<p>Load balancers are placed at two points:</p>
<ol>
    <li><strong>In front of the API Gateway:</strong> A Layer 7 (HTTP) load balancer distributes incoming requests from gate controllers and mobile apps across multiple API Gateway instances. Uses <strong>least-connections</strong> algorithm (since exit/pay requests take longer than entry requests, even distribution by request count would overload some instances). Health checks every 10 seconds to remove unhealthy instances. TLS termination happens at the load balancer.</li>
    <li><strong>Between API Gateway and microservices:</strong> The API Gateway itself acts as a load balancer for the backend services, routing requests to service instances registered in a service registry. Uses <strong>round-robin</strong> with health checks. This is implemented as client-side load balancing or via a service mesh.</li>
</ol>

<h4>Horizontal Scaling</h4>
<table>
    <tr><th>Component</th><th>Scaling Strategy</th></tr>
    <tr><td>Entry/Exit Service</td><td>Stateless ‚Äî scale horizontally behind the load balancer. 2-3 instances per garage cluster is sufficient.</td></tr>
    <tr><td>Spot Assignment Service</td><td>Stateless ‚Äî scale horizontally. The bottleneck is the DB transaction, not the service itself.</td></tr>
    <tr><td>Fee Calculation Service</td><td>Stateless, pure computation ‚Äî scale horizontally. Pricing rules cached in memory.</td></tr>
    <tr><td>Payment Service</td><td>Stateless ‚Äî scale horizontally. External payment gateway is the bottleneck.</td></tr>
    <tr><td>Availability Service</td><td>Scale horizontally, but cache coordination needed. Can partition by garage_id (each instance owns a subset of garages).</td></tr>
    <tr><td>SQL Database</td><td>Vertical scaling first (a single modern DB server can handle the load). Add read replicas for analytics. Shard by garage_id if needed at extreme scale.</td></tr>
    <tr><td>In-Memory Cache</td><td>The dataset is small (few hundred keys). A single cache instance is sufficient. Add replication for high availability.</td></tr>
    <tr><td>Pub/Sub</td><td>Partition the topic by garage_id for parallel consumption.</td></tr>
    <tr><td>WebSocket Server</td><td>Partition connections by garage. Each WebSocket server handles all signs for a subset of garages.</td></tr>
</table>

<h4>Read Replica for Analytics</h4>
<p>The Analytics Service and Admin Dashboard should read from a <strong>SQL read replica</strong>, not the primary database. This prevents heavy analytical queries (e.g., "revenue by hour for the last 30 days") from competing with the latency-sensitive spot assignment transactions on the primary.</p>

<h4>Fault Tolerance ‚Äî Offline Mode</h4>
<p>The Gate Controller edge devices have a <strong>local fallback mode</strong>. If connectivity to the backend is lost:</p>
<ul>
    <li>The gate controller issues tickets locally with offline ticket IDs (prefixed with "OFFLINE-").</li>
    <li>It opens the gate based on a local spot count estimate.</li>
    <li>When connectivity is restored, offline tickets are reconciled with the backend via a sync mechanism.</li>
    <li>This ensures the garage remains operational even during server outages.</li>
</ul>
</div>

<hr>

<!-- ============================= -->
<!-- TRADEOFFS & DEEP DIVES        -->
<!-- ============================= -->
<h2>13. Tradeoffs &amp; Deep Dives</h2>
<div class="card">

<h4>Tradeoff 1: Strong Consistency vs. Performance for Spot Assignment</h4>
<p><strong>Chosen:</strong> Strong consistency (SQL transaction with <code>SELECT ... FOR UPDATE</code>).</p>
<p><strong>Tradeoff:</strong> This introduces a lock on the rows being read during spot assignment, which limits concurrent spot assignments within the same garage. If two vehicles enter simultaneously, one must wait for the other's transaction to commit.</p>
<p><strong>Why acceptable:</strong> The transaction is extremely fast (&lt;10ms for a single SELECT + UPDATE). Even at peak (20 entries/second across 100 garages), each garage sees only 0.2 entries/second ‚Äî virtually no contention. The cost of a double-booking (two vehicles directed to the same spot) is far worse than the minor latency of serialized transactions.</p>

<h4>Tradeoff 2: Denormalized <code>garage_id</code> on <code>spots</code> table</h4>
<p><strong>Chosen:</strong> Denormalize (store <code>garage_id</code> on both <code>floors</code> and <code>spots</code>).</p>
<p><strong>Tradeoff:</strong> Data duplication ‚Äî if a spot's floor is moved to a different garage (extremely rare), both tables must be updated.</p>
<p><strong>Why acceptable:</strong> The spot assignment query is the hottest query in the system and runs on every vehicle entry. Eliminating a JOIN saves significant query time and allows an efficient composite index scan. Garage-floor relationships essentially never change after initial setup.</p>

<h4>Tradeoff 3: In-Memory Cache vs. Direct DB for Availability</h4>
<p><strong>Chosen:</strong> In-memory cache with write-through.</p>
<p><strong>Tradeoff:</strong> Added complexity (cache invalidation, warm-up, TTL management). Small risk of stale data if an event is lost.</p>
<p><strong>Why acceptable:</strong> Availability is read orders of magnitude more often than it's written. The cache turns an aggregation query into a O(1) lookup. The 5-minute TTL safety net bounds staleness. A slightly stale availability count is not a safety issue ‚Äî it's a minor UX annoyance at worst.</p>

<h4>Tradeoff 4: Microservices vs. Monolith</h4>
<p><strong>Chosen:</strong> Microservices (Entry, Exit, Reservation, Payment, Availability, Notification as separate services).</p>
<p><strong>Tradeoff:</strong> More operational complexity (deployment, monitoring, distributed tracing). Network overhead for inter-service communication.</p>
<p><strong>Why acceptable:</strong> Each service has a clear bounded context and different scaling needs (Payment Service may need fewer instances than Entry Service). Independent deployment allows faster iteration. For a smaller deployment (single garage), a monolith is perfectly fine ‚Äî the architecture can start monolithic and split later.</p>

<h4>Tradeoff 5: WebSocket vs. SSE for Digital Signs</h4>
<p><strong>Chosen:</strong> WebSocket.</p>
<p><strong>Tradeoff:</strong> WebSocket is bidirectional and more complex than SSE (Server-Sent Events), which would also work for the primarily server‚Üíclient push use case.</p>
<p><strong>Why chosen over SSE:</strong> The sign controllers occasionally need to send messages back (sync requests after reconnection, heartbeat acknowledgments, diagnostic data). WebSocket's bidirectional nature supports this without needing a separate HTTP channel.</p>
</div>

<hr>

<!-- ============================= -->
<!-- ALTERNATIVE APPROACHES        -->
<!-- ============================= -->
<h2>14. Alternative Approaches</h2>
<div class="card">

<h4>Alternative 1: Optimistic Locking Instead of Pessimistic Locking for Spot Assignment</h4>
<p><strong>Approach:</strong> Instead of <code>SELECT ... FOR UPDATE</code> (pessimistic), use a version column on the <code>spots</code> table and <code>UPDATE spots SET status='occupied', version=version+1 WHERE spot_id=? AND version=? AND status='available'</code>. If the update affects 0 rows (someone else grabbed the spot), retry with a different spot.</p>
<p><strong>Why not chosen:</strong> While optimistic locking avoids holding row locks, it introduces retry loops. In a parking garage, the user is physically waiting at a gate ‚Äî adding retry latency (even 50ms per retry) degrades the experience. Pessimistic locking with the low contention in this system (&lt;0.2 entries/sec/garage) gives deterministic, fast behavior without retries.</p>

<h4>Alternative 2: NoSQL Database for Spots/Tickets</h4>
<p><strong>Approach:</strong> Use a NoSQL (document or wide-column) database for faster writes and easier horizontal scaling.</p>
<p><strong>Why not chosen:</strong> Spot assignment requires transactional guarantees (atomically check availability AND assign the spot). NoSQL databases generally lack multi-row/multi-document ACID transactions. The volume of data is small enough that SQL handles it efficiently, and the strong consistency guarantees are essential to prevent double-booking. The relational nature of the data (garages ‚Üí floors ‚Üí spots ‚Üí tickets ‚Üí payments) maps naturally to SQL.</p>

<h4>Alternative 3: Event Sourcing for Spot State</h4>
<p><strong>Approach:</strong> Instead of storing the current spot status in a mutable row, store an append-only log of events (spot_created, spot_occupied, spot_freed). Derive current state by replaying events.</p>
<p><strong>Why not chosen:</strong> Event sourcing adds significant complexity (event store, projections, snapshots) for a domain where the current state is simple (a spot is either available or occupied). The audit trail benefit can be achieved more simply with a separate <code>spot_status_history</code> table or log. Event sourcing would be overkill for this scale.</p>

<h4>Alternative 4: Central Spot Counter Instead of Per-Spot Status</h4>
<p><strong>Approach:</strong> Instead of tracking every spot individually, just maintain a counter of available spots per type per floor. Increment on exit, decrement on entry.</p>
<p><strong>Why not chosen:</strong> A counter alone doesn't allow assigning a specific spot to a vehicle. Drivers need to be directed to a specific spot (digital signs or ticket printouts say "Go to Spot B-204"). Reservations require reserving specific spots. The counter approach also makes reconciliation (auditing, detecting ghost vehicles) impossible.</p>

<h4>Alternative 5: HTTP Long Polling Instead of WebSocket for Signs</h4>
<p><strong>Approach:</strong> Digital signs make HTTP GET requests that the server holds open until there's an update, then responds.</p>
<p><strong>Why not chosen:</strong> Long polling requires re-establishing the connection after every update, adding overhead. With dozens of signs updating every few seconds, this creates many short-lived connections. WebSocket's persistent connection is more efficient for the continuous, frequent updates in this use case.</p>

<h4>Alternative 6: gRPC Instead of HTTP REST for Gate Controllers</h4>
<p><strong>Approach:</strong> Use gRPC for communication between gate controllers and backend services for lower latency (binary protocol, HTTP/2 multiplexing).</p>
<p><strong>Why not chosen (as default):</strong> HTTP REST is simpler to implement on embedded gate controllers, easier to debug (human-readable JSON), and the latency difference (~5-10ms) is negligible compared to the physical gate arm opening time (~500ms). gRPC would be a good optimization for very high-throughput garages but adds implementation complexity for the edge devices. HTTP REST is the pragmatic starting point.</p>
</div>

<hr>

<!-- ============================= -->
<!-- ADDITIONAL INFO               -->
<!-- ============================= -->
<h2>15. Additional Information</h2>
<div class="card">

<h4>Protocols Used</h4>
<table>
    <tr><th>Protocol</th><th>Where</th><th>Why</th></tr>
    <tr><td><strong>HTTP/1.1 (REST)</strong></td><td>Gate Controller ‚Üî API Gateway, Mobile App ‚Üî API Gateway</td><td>Simple, widely supported, request/response pattern fits the entry/exit workflow. JSON payloads are small. TLS for encryption.</td></tr>
    <tr><td><strong>WebSocket (over TCP)</strong></td><td>Availability Service ‚Üî Digital Signs</td><td>Persistent bidirectional connection for real-time availability push. Lower overhead than repeated HTTP connections.</td></tr>
    <tr><td><strong>TCP</strong></td><td>All network communication</td><td>Reliability is essential ‚Äî we can't afford lost packets for spot assignments or payments. UDP's lower latency is unnecessary (the bottleneck is physical, not network).</td></tr>
</table>

<h4>Reservation Expiry Scheduler</h4>
<p>A background scheduled job runs every minute, querying: <code>SELECT * FROM reservations WHERE status='confirmed' AND start_time + grace_period &lt; NOW()</code>. For each expired reservation, it updates the status, frees the spot, and publishes a <code>spot_freed</code> event. This is a lightweight cron-style process, not a user-facing service.</p>

<h4>Monitoring &amp; Observability</h4>
<ul>
    <li><strong>Metrics:</strong> Track entry/exit latency (P50, P95, P99), spot assignment transaction time, payment success/failure rates, WebSocket connection count, cache hit rate, queue depth.</li>
    <li><strong>Alerts:</strong> Gate controller offline, availability cache miss rate &gt; 5%, payment failure rate &gt; 2%, DB transaction latency &gt; 100ms.</li>
    <li><strong>Logging:</strong> Structured logs for every entry/exit event with correlation IDs for end-to-end tracing.</li>
</ul>

<h4>Security Considerations</h4>
<ul>
    <li>Gate controllers authenticate with the API Gateway using API keys (rotated monthly).</li>
    <li>Mobile app users authenticate via JWT tokens (short-lived, 15-minute expiry with refresh tokens).</li>
    <li>Payment data is tokenized at the terminal ‚Äî PCI-DSS compliance ensures raw card numbers never reach our servers.</li>
    <li>Admin dashboard requires multi-factor authentication.</li>
    <li>All communication over TLS 1.3.</li>
</ul>

<h4>Idempotency</h4>
<p>Exit/payment operations use idempotency keys (<code>ticket_id + attempt_number</code>) to prevent double charges if a request is retried (e.g., network timeout). The Payment Service checks if a payment was already processed for the given ticket before charging again.</p>

<h4>Data Retention</h4>
<p>Completed tickets and payment records are retained for 7 years (financial compliance). After 90 days, they are moved to cold storage (archival database) to keep the primary database lean. Active queries only hit the primary, while historical reports query the archive.</p>
</div>

<hr>

<!-- ============================= -->
<!-- VENDOR SECTION                -->
<!-- ============================= -->
<h2>16. Vendor Suggestions</h2>
<div class="card">
<p>The architecture above is vendor-agnostic. Below are potential vendor choices for each component, with rationale:</p>

<table>
    <tr><th>Component</th><th>Vendor Options</th><th>Rationale</th></tr>
    <tr>
        <td>SQL Database</td>
        <td>PostgreSQL, MySQL, Amazon Aurora, Google Cloud SQL</td>
        <td>PostgreSQL is preferred for its excellent support for complex queries, spatial indexes (PostGIS for garage location queries), ACID compliance, and row-level locking (SELECT ... FOR UPDATE). Aurora provides managed PostgreSQL with automatic failover.</td>
    </tr>
    <tr>
        <td>In-Memory Cache</td>
        <td>Redis, Memcached, Hazelcast</td>
        <td>Redis is the top choice due to its data structure support (hashes for availability counts), pub/sub capability (potential dual-use), persistence options, and TTL support. Memcached is simpler if only key-value caching is needed.</td>
    </tr>
    <tr>
        <td>Pub/Sub</td>
        <td>Apache Kafka, Google Cloud Pub/Sub, Amazon SNS + SQS, NATS</td>
        <td>Kafka provides durable, ordered event streaming with partition-based parallelism (partition by garage_id). Good for the spot-status-changes topic. Google Cloud Pub/Sub or Amazon SNS are managed alternatives with less operational overhead.</td>
    </tr>
    <tr>
        <td>Message Queue</td>
        <td>RabbitMQ, Amazon SQS, Apache ActiveMQ</td>
        <td>RabbitMQ supports delayed message delivery (needed for reservation reminders), message acknowledgment, and dead-letter queues. Amazon SQS is a managed alternative with built-in delay queue support.</td>
    </tr>
    <tr>
        <td>API Gateway</td>
        <td>Kong, NGINX, AWS API Gateway, Envoy</td>
        <td>Kong or NGINX provide flexible routing, rate limiting, authentication plugins, and load balancing. Envoy is preferred in a service mesh architecture.</td>
    </tr>
    <tr>
        <td>Object Storage (for receipts/reports)</td>
        <td>Amazon S3, Google Cloud Storage, MinIO</td>
        <td>S3 or GCS for storing generated PDF receipts and analytics reports. MinIO for self-hosted environments.</td>
    </tr>
    <tr>
        <td>Gate Controller Hardware</td>
        <td>Raspberry Pi (with industrial enclosure), Arduino-based boards, Custom ARM SoC</td>
        <td>Raspberry Pi offers WiFi, GPIO for gate/sensor control, and can run a lightweight Linux OS with HTTP client capabilities. For production, a ruggedized industrial single-board computer is preferred.</td>
    </tr>
    <tr>
        <td>Push Notifications</td>
        <td>Firebase Cloud Messaging (FCM), Apple Push Notification Service (APNs), OneSignal</td>
        <td>FCM for Android, APNs for iOS ‚Äî these are the platform-mandated services. OneSignal provides a unified abstraction over both.</td>
    </tr>
</table>
</div>

</div>

<script>
    mermaid.initialize({
        startOnLoad: true,
        theme: 'default',
        flowchart: {
            useMaxWidth: true,
            htmlLabels: true,
            curve: 'basis'
        }
    });
</script>
</body>
</html>
