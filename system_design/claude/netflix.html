<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>System Design: Netflix</title>
<script src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script>
<script>mermaid.initialize({startOnLoad:true, theme:'neutral', securityLevel:'loose'});</script>
<style>
  body { font-family: 'Segoe UI', Arial, sans-serif; line-height:1.7; max-width:1100px; margin:40px auto; padding:0 30px; color:#1a1a2e; background:#f8f9fa; }
  h1 { font-size:2.4em; border-bottom:4px solid #e50914; padding-bottom:12px; color:#141414; }
  h2 { font-size:1.7em; margin-top:48px; border-left:5px solid #e50914; padding-left:14px; color:#141414; }
  h3 { font-size:1.3em; margin-top:32px; color:#333; }
  h4 { font-size:1.1em; margin-top:24px; color:#444; }
  ul, ol { margin:10px 0; padding-left:28px; }
  li { margin:6px 0; }
  table { border-collapse:collapse; width:100%; margin:16px 0; }
  th, td { border:1px solid #ccc; padding:10px 14px; text-align:left; }
  th { background:#e50914; color:#fff; }
  tr:nth-child(even) { background:#f2f2f2; }
  code { background:#eee; padding:2px 6px; border-radius:4px; font-size:0.95em; }
  .mermaid { background:#fff; padding:20px; border-radius:10px; box-shadow:0 2px 8px rgba(0,0,0,0.08); margin:20px 0; }
  .example-box { background:#fff8e1; border-left:4px solid #ffc107; padding:16px 20px; margin:16px 0; border-radius:6px; }
  .deep-dive { background:#e8f5e9; border-left:4px solid #4caf50; padding:16px 20px; margin:16px 0; border-radius:6px; }
  .callout { background:#e3f2fd; border-left:4px solid #2196f3; padding:16px 20px; margin:16px 0; border-radius:6px; }
  .warn { background:#fce4ec; border-left:4px solid #e50914; padding:16px 20px; margin:16px 0; border-radius:6px; }
  .toc { background:#fff; padding:24px 30px; border-radius:10px; box-shadow:0 2px 8px rgba(0,0,0,0.06); margin:30px 0; }
  .toc a { text-decoration:none; color:#e50914; }
  .toc a:hover { text-decoration:underline; }
  .toc ul { list-style:none; padding-left:18px; }
  .toc > ul { padding-left:0; }
</style>
</head>
<body>

<h1>üé¨ System Design: Netflix</h1>

<div class="toc">
<h3>Table of Contents</h3>
<ul>
  <li><a href="#fr">1. Functional Requirements</a></li>
  <li><a href="#nfr">2. Non-Functional Requirements</a></li>
  <li><a href="#flow1">3. Flow 1 ‚Äî Content Upload &amp; Processing</a></li>
  <li><a href="#flow2">4. Flow 2 ‚Äî Content Browsing &amp; Search</a></li>
  <li><a href="#flow3">5. Flow 3 ‚Äî Video Streaming</a></li>
  <li><a href="#flow4">6. Flow 4 ‚Äî Recommendation Generation (Offline)</a></li>
  <li><a href="#overall">7. Overall Combined Diagram</a></li>
  <li><a href="#schema">8. Database Schema</a></li>
  <li><a href="#cdn-cache">9. CDN &amp; Caching Deep Dive</a></li>
  <li><a href="#scaling">10. Scaling Considerations</a></li>
  <li><a href="#tradeoffs">11. Tradeoffs &amp; Deep Dives</a></li>
  <li><a href="#alternatives">12. Alternative Approaches</a></li>
  <li><a href="#additional">13. Additional Information</a></li>
  <li><a href="#vendors">14. Vendor Recommendations</a></li>
</ul>
</div>

<!-- ============================= SECTION 1 ============================= -->
<h2 id="fr">1. Functional Requirements</h2>
<ol>
  <li><strong>Content Upload &amp; Processing</strong> ‚Äî Content providers / studio admins can upload raw video files. The system transcodes them into multiple resolutions and bitrates.</li>
  <li><strong>Content Browsing</strong> ‚Äî Users can browse a personalized homepage of content organized by genre, trending, "continue watching," etc.</li>
  <li><strong>Content Search</strong> ‚Äî Users can search for movies and TV shows by title, genre, actor, director, etc.</li>
  <li><strong>Video Streaming</strong> ‚Äî Users can play video with adaptive bitrate streaming. Playback should seamlessly adjust quality based on network conditions.</li>
  <li><strong>Resume Playback ("Continue Watching")</strong> ‚Äî Users can resume watching from where they previously stopped.</li>
  <li><strong>User Profiles</strong> ‚Äî A single account can have multiple profiles (e.g., family members), each with their own watch history and preferences.</li>
  <li><strong>Ratings &amp; Reviews</strong> ‚Äî Users can rate content (thumbs up / thumbs down or star rating).</li>
  <li><strong>Personalized Recommendations</strong> ‚Äî The system recommends content based on viewing history, ratings, and behavior.</li>
</ol>

<!-- ============================= SECTION 2 ============================= -->
<h2 id="nfr">2. Non-Functional Requirements</h2>
<ol>
  <li><strong>High Availability</strong> ‚Äî 99.99% uptime. Users should be able to stream content at all times.</li>
  <li><strong>Low Latency</strong> ‚Äî Video playback should begin within 2 seconds of pressing "Play." Homepage and search should respond in &lt;200 ms.</li>
  <li><strong>Massive Scale</strong> ‚Äî Support 200M+ subscribers, with 10M+ concurrent streams at peak.</li>
  <li><strong>Global Reach</strong> ‚Äî Content should be available worldwide with low latency via edge infrastructure.</li>
  <li><strong>Adaptive Quality</strong> ‚Äî Streaming must dynamically adapt to varying network conditions without buffering.</li>
  <li><strong>Eventual Consistency (acceptable)</strong> ‚Äî Watch history, recommendations, and like counts can be eventually consistent. Billing and account data must be strongly consistent.</li>
  <li><strong>Fault Tolerance</strong> ‚Äî No single point of failure. The system should gracefully degrade (e.g., serve generic recommendations if the ML pipeline is down).</li>
  <li><strong>Data Durability</strong> ‚Äî Video content and user data must be durable and replicated across regions.</li>
  <li><strong>Security</strong> ‚Äî Content must be protected with DRM. User credentials and payment data must be encrypted.</li>
</ol>

<!-- ============================= SECTION 3 ‚Äî FLOW 1 ============================= -->
<h2 id="flow1">3. Flow 1 ‚Äî Content Upload &amp; Processing</h2>

<div class="mermaid">
graph LR
    CP["üé¨ Content Provider"] -->|"HTTP POST /upload\n(multipart)"| AG["API Gateway\n+ Load Balancer"]
    AG --> US["Upload Service"]
    US -->|"Store raw file"| OS_RAW[("Object Storage\n(Raw Video)")]
    US -->|"Enqueue transcode job"| MQ["Message Queue"]
    MQ --> TS["Transcoding Service\n(Worker Pool)"]
    TS -->|"Read raw file"| OS_RAW
    TS -->|"Write transcoded\nchunks + manifests"| OS_PROC[("Object Storage\n(Processed Video)")]
    TS -->|"Update metadata"| MS["Metadata Service"]
    MS --> CDB[("Content DB\n(SQL)")]
    OS_PROC -->|"Push to edge"| CDN["CDN\n(Edge Servers)"]
</div>

<h3>Examples</h3>

<div class="example-box">
<strong>Example 1 ‚Äî New Movie Upload:</strong><br>
A studio admin at "Lionsgate" uploads a 4K raw movie file (40 GB) via the admin portal. This triggers an <code>HTTP POST /upload</code> to the API Gateway, which routes it to the <strong>Upload Service</strong>. The Upload Service stores the raw file in <strong>Object Storage (Raw)</strong> and enqueues a transcode job message on the <strong>Message Queue</strong> with the content ID and storage path. A <strong>Transcoding Service</strong> worker picks up the message, reads the raw file, and produces multiple output renditions: 240p, 360p, 480p, 720p, 1080p, 4K ‚Äî each at multiple bitrates. Each rendition is split into 4-second HLS chunks and an HLS manifest (<code>.m3u8</code>) is generated. All chunks and manifests are written to <strong>Object Storage (Processed)</strong>. The Transcoding Service then calls the <strong>Metadata Service</strong> via <code>HTTP POST /metadata</code> to store the content's title, genre, duration, available resolutions, and manifest URLs in the <strong>Content DB</strong>. Finally, the processed video chunks are pushed to <strong>CDN</strong> edge servers worldwide.
</div>

<div class="example-box">
<strong>Example 2 ‚Äî Transcoding Failure &amp; Retry:</strong><br>
A content provider uploads a corrupted video file. The <strong>Upload Service</strong> successfully stores the raw file and enqueues the job. When the <strong>Transcoding Service</strong> worker picks up the message and attempts to transcode, it fails. The message is <em>not</em> acknowledged, so the <strong>Message Queue</strong> re-delivers it after the visibility timeout. After 3 retries, the message is moved to a <strong>Dead Letter Queue</strong>. An alert is generated for the operations team, and the content provider is notified that the upload failed validation.
</div>

<h3>Component Deep Dive</h3>

<div class="deep-dive">
<h4>API Gateway + Load Balancer</h4>
<ul>
  <li><strong>Protocol:</strong> HTTPS (TLS terminated here)</li>
  <li><strong>Responsibilities:</strong> Authentication, rate limiting, request routing, SSL termination</li>
  <li><strong>Load balancing strategy:</strong> Round-robin or least-connections across service instances</li>
  <li><strong>Input:</strong> Client HTTP requests</li>
  <li><strong>Output:</strong> Routed request to the appropriate downstream microservice</li>
</ul>
</div>

<div class="deep-dive">
<h4>Upload Service</h4>
<ul>
  <li><strong>Protocol:</strong> <code>HTTP POST /upload</code> (multipart/form-data for large file uploads; supports resumable uploads via chunked upload protocol for files &gt;1 GB)</li>
  <li><strong>Input:</strong> Raw video file + metadata (title, description, genre, cast, etc.)</li>
  <li><strong>Output:</strong> <code>202 Accepted</code> with a <code>content_id</code> and upload status URL for polling</li>
  <li><strong>Responsibilities:</strong> Validates the upload, stores the raw file in Object Storage, and enqueues a transcode job to the Message Queue.</li>
</ul>
</div>

<div class="deep-dive">
<h4>Object Storage (Raw Video)</h4>
<ul>
  <li><strong>Type:</strong> Blob / Object Storage</li>
  <li><strong>Usage:</strong> Stores the original unprocessed video files. Retained for potential re-transcoding if new codecs emerge.</li>
  <li><strong>Durability:</strong> 99.999999999% (11 nines) via replication across multiple availability zones.</li>
</ul>
</div>

<div class="deep-dive">
<h4>Message Queue</h4>
<ul>
  <li><strong>Purpose:</strong> Decouples the upload path from the CPU-intensive transcoding path. Enables asynchronous processing, backpressure handling, and retry on failure.</li>
  <li><strong>How messages are enqueued:</strong> The Upload Service publishes a message containing <code>{content_id, raw_storage_path, desired_resolutions, codec_preferences}</code>.</li>
  <li><strong>How messages are dequeued:</strong> Transcoding Service workers pull messages. Upon successful transcoding, the worker acknowledges (ACKs) the message, removing it from the queue. On failure, the message becomes visible again after a visibility timeout for retry.</li>
  <li><strong>Dead Letter Queue (DLQ):</strong> Messages that fail after N retries are moved to a DLQ for manual inspection.</li>
  <li><strong>Why Message Queue over Pub/Sub:</strong> We need guaranteed at-least-once delivery with backpressure control. Each transcode job should be processed by exactly one worker ‚Äî this is a point-to-point pattern, not fan-out. Pub/Sub would deliver to all subscribers, which is wasteful here.</li>
  <li><strong>Why Message Queue over synchronous processing:</strong> Transcoding is extremely CPU-intensive (can take 30 min ‚Äì several hours for a 4K film). Synchronous processing would block the upload API and timeout.</li>
</ul>
</div>

<div class="deep-dive">
<h4>Transcoding Service (Worker Pool)</h4>
<ul>
  <li><strong>Type:</strong> Stateless workers that pull jobs from the Message Queue</li>
  <li><strong>Responsibilities:</strong>
    <ul>
      <li>Read raw video from Object Storage</li>
      <li>Transcode into multiple resolutions (240p, 360p, 480p, 720p, 1080p, 4K) and bitrates</li>
      <li>Encode with video codecs (H.264/AVC for broad compatibility; H.265/HEVC or AV1 for higher efficiency)</li>
      <li>Segment the video into small chunks (2‚Äì6 seconds each) for HLS/DASH streaming</li>
      <li>Generate HLS manifests (<code>.m3u8</code>) or DASH manifests (<code>.mpd</code>) listing all chunks and quality levels</li>
      <li>Write processed output to Object Storage (Processed)</li>
      <li>Call Metadata Service to register the content</li>
    </ul>
  </li>
  <li><strong>Scaling:</strong> Horizontally scaled ‚Äî more workers are added during peak upload times. Workers are stateless and disposable.</li>
</ul>
</div>

<div class="deep-dive">
<h4>Object Storage (Processed Video)</h4>
<ul>
  <li><strong>Type:</strong> Blob / Object Storage</li>
  <li><strong>Contents:</strong> HLS/DASH video chunks and manifest files, organized by <code>content_id/resolution/chunk_number</code></li>
  <li><strong>This storage serves as the origin for the CDN.</strong></li>
</ul>
</div>

<div class="deep-dive">
<h4>Metadata Service</h4>
<ul>
  <li><strong>Protocol:</strong> <code>HTTP POST /metadata</code> (internal service-to-service call)</li>
  <li><strong>Input:</strong> <code>{content_id, title, genre, duration, resolutions[], manifest_urls, thumbnail_url, ...}</code></li>
  <li><strong>Output:</strong> <code>201 Created</code></li>
  <li><strong>Writes to:</strong> Content DB (SQL) ‚Äî inserts a new row in the <code>content</code> table and rows in <code>content_resolution</code></li>
</ul>
</div>

<div class="deep-dive">
<h4>CDN (Edge Servers)</h4>
<ul>
  <li>Processed video chunks are pushed proactively to CDN edge servers worldwide (push-based for new releases; pull-based for long-tail content).</li>
  <li>Detailed CDN deep dive is in <a href="#cdn-cache">Section 9</a>.</li>
</ul>
</div>

<!-- ============================= SECTION 4 ‚Äî FLOW 2 ============================= -->
<h2 id="flow2">4. Flow 2 ‚Äî Content Browsing &amp; Search</h2>

<div class="mermaid">
graph LR
    C["üì± Client App"] -->|"HTTP GET /home"| AG["API Gateway\n+ Load Balancer"]
    C -->|"HTTP GET /search?q=..."| AG
    AG --> CS["Content Service"]
    AG --> SS["Search Service"]
    AG --> RS["Recommendation\nService"]
    CS --> CC["Content Cache\n(In-Memory)"]
    CC -->|"Cache Miss"| CDB[("Content DB\n(SQL)")]
    SS --> SI[("Search Index\n(Inverted Index)")]
    RS --> RC["Recommendation\nCache (In-Memory)"]
    RC -->|"Cache Miss"| RDB[("Recommendation\nStore (NoSQL)")]
</div>

<h3>Examples</h3>

<div class="example-box">
<strong>Example 1 ‚Äî User Opens Homepage (Browse):</strong><br>
User "Alice" opens the Netflix app on her iPhone. The client sends <code>HTTP GET /home?profile_id=abc123</code> to the <strong>API Gateway</strong>. The gateway routes parallel requests to: (1) the <strong>Recommendation Service</strong> to fetch Alice's personalized rows (e.g., "Because You Watched Stranger Things", "Trending Now"), (2) the <strong>Content Service</strong> to fetch metadata for each recommended <code>content_id</code> (title, thumbnail URL, rating). The Recommendation Service checks the <strong>Recommendation Cache</strong>; since Alice's recommendations were pre-computed in last night's batch run, they are found in cache. It returns a list of <code>[{row_title, [content_ids]}]</code>. The Content Service checks the <strong>Content Cache</strong> for each content_id's metadata ‚Äî most are cache hits. The API Gateway aggregates the results and returns the homepage payload to Alice's client. The client fetches thumbnails directly from the <strong>CDN</strong>.
</div>

<div class="example-box">
<strong>Example 2 ‚Äî User Searches for Content:</strong><br>
User "Bob" types "dark knight" into the search bar. The client sends <code>HTTP GET /search?q=dark+knight&profile_id=xyz789</code>. The API Gateway routes this to the <strong>Search Service</strong>, which queries the <strong>Search Index</strong> (an inverted index over content title, description, actors, directors, genres). The index returns ranked content IDs matching the query: "The Dark Knight," "The Dark Knight Rises," "Dark," etc. The Search Service then calls the <strong>Content Service</strong> to hydrate these content IDs with metadata (title, thumbnail, rating, year). Results are returned to the client.
</div>

<div class="example-box">
<strong>Example 3 ‚Äî Cache Miss on Content Metadata:</strong><br>
A newly uploaded indie film has just been published. When a user's homepage includes this film in "New Releases," the <strong>Content Service</strong> checks the <strong>Content Cache</strong> ‚Äî it's a cache miss. The service falls back to the <strong>Content DB (SQL)</strong>, fetches the metadata, returns it, and populates the cache for subsequent requests.
</div>

<h3>Component Deep Dive</h3>

<div class="deep-dive">
<h4>Content Service</h4>
<ul>
  <li><strong>Protocol:</strong> <code>HTTP GET /content/{content_id}</code> ‚Äî returns metadata for a single piece of content; <code>HTTP GET /content?ids=a,b,c</code> ‚Äî batch fetch</li>
  <li><strong>Input:</strong> <code>content_id</code> (or list of IDs)</li>
  <li><strong>Output:</strong> JSON with title, description, genre, cast, rating, thumbnail_url, duration, release_year, available_resolutions</li>
  <li><strong>Reads from:</strong> Content Cache ‚Üí Content DB (SQL) on miss</li>
</ul>
</div>

<div class="deep-dive">
<h4>Search Service</h4>
<ul>
  <li><strong>Protocol:</strong> <code>HTTP GET /search?q={query}&page={n}&size={m}</code></li>
  <li><strong>Input:</strong> Search query string, pagination parameters, optional filters (genre, year, rating)</li>
  <li><strong>Output:</strong> Ranked list of <code>{content_id, title, thumbnail_url, relevance_score}</code></li>
  <li><strong>Reads from:</strong> Search Index (inverted index). The index is built and kept up-to-date by an indexing pipeline that reads from the Content DB whenever content is added/updated.</li>
</ul>
</div>

<div class="deep-dive">
<h4>Search Index (Inverted Index)</h4>
<ul>
  <li><strong>Type:</strong> Inverted index (full-text search engine)</li>
  <li><strong>Indexed fields:</strong> title, description, genre, actors, directors, keywords/tags</li>
  <li><strong>Features:</strong> Tokenization, stemming, fuzzy matching (to handle typos), relevance scoring (TF-IDF or BM25)</li>
  <li><strong>Why inverted index:</strong> It is the gold standard for full-text search. It maps each term to the list of documents (content) containing that term, enabling sub-second lookups even across millions of titles.</li>
  <li><strong>Kept in sync:</strong> An async indexing pipeline reads change events from the Content DB (via change data capture or periodic polling) and updates the index.</li>
</ul>
</div>

<div class="deep-dive">
<h4>Recommendation Service</h4>
<ul>
  <li><strong>Protocol:</strong> <code>HTTP GET /recommendations?profile_id={id}</code></li>
  <li><strong>Input:</strong> <code>profile_id</code></li>
  <li><strong>Output:</strong> Ordered list of recommendation rows, each containing a title and a list of content_ids: <code>[{row_title: "Action Movies For You", content_ids: [...]}]</code></li>
  <li><strong>Reads from:</strong> Recommendation Cache ‚Üí Recommendation Store (NoSQL) on miss</li>
  <li><strong>Note:</strong> Recommendations are pre-computed offline (see Flow 4). This service simply reads them.</li>
</ul>
</div>

<div class="deep-dive">
<h4>Content Cache (In-Memory)</h4>
<ul>
  <li>Caches content metadata (title, genre, thumbnail_url, etc.) keyed by <code>content_id</code></li>
  <li><strong>Strategy:</strong> Read-through cache (on miss, fetch from Content DB and populate cache)</li>
  <li><strong>Eviction policy:</strong> LRU (Least Recently Used) ‚Äî popular content stays in cache; rarely-accessed long-tail content is evicted</li>
  <li><strong>Expiration (TTL):</strong> 1 hour ‚Äî balances freshness (metadata updates like a corrected description) with cache hit rate</li>
  <li><strong>Populated by:</strong> Cache misses during read operations. Optionally, a cache-warming job pre-loads top 1,000 most popular titles on startup.</li>
  <li>See <a href="#cdn-cache">Section 9</a> for full caching deep dive.</li>
</ul>
</div>

<div class="deep-dive">
<h4>Recommendation Cache (In-Memory)</h4>
<ul>
  <li>Caches pre-computed recommendation lists keyed by <code>profile_id</code></li>
  <li><strong>Strategy:</strong> Write-through cache ‚Äî when the offline batch pipeline generates new recommendations, it writes to both the Recommendation Store and the cache simultaneously</li>
  <li><strong>Eviction policy:</strong> LRU ‚Äî active users' recommendations stay in cache; inactive users are evicted</li>
  <li><strong>Expiration (TTL):</strong> 24 hours ‚Äî recommendations are refreshed daily by the batch pipeline</li>
  <li><strong>Populated by:</strong> The offline recommendation batch pipeline (Flow 4), and on cache miss during reads</li>
  <li>See <a href="#cdn-cache">Section 9</a> for full caching deep dive.</li>
</ul>
</div>

<!-- ============================= SECTION 5 ‚Äî FLOW 3 ============================= -->
<h2 id="flow3">5. Flow 3 ‚Äî Video Streaming</h2>

<div class="mermaid">
graph LR
    C["üì± Client App\n(Video Player)"] -->|"1. HTTP GET /stream/{content_id}"| AG["API Gateway\n+ Load Balancer"]
    AG --> StS["Streaming Service"]
    StS -->|"Read manifest URL\n+ DRM license"| CDB[("Content DB\n(SQL)")]
    StS -->|"2. Return manifest URL\n+ DRM token"| C
    C -->|"3. HTTP GET manifest\n(.m3u8 / .mpd)"| CDN["CDN\n(Edge Servers)"]
    CDN -->|"4. Return manifest"| C
    C -->|"5. HTTP GET video chunks\n(Adaptive Bitrate)"| CDN
    CDN -->|"Cache Miss"| OS[("Object Storage\n(Processed Video)")]
    C -->|"6. HTTP PUT /watch-history\n{progress, timestamp}"| AG
    AG --> WHS["Watch History\nService"]
    WHS --> WHDB[("Watch History DB\n(NoSQL)")]
</div>

<h3>Examples</h3>

<div class="example-box">
<strong>Example 1 ‚Äî Normal Playback:</strong><br>
User "Alice" clicks "Play" on "Stranger Things S4E1." The client sends <code>HTTP GET /stream/content_abc?profile_id=abc123</code> to the <strong>API Gateway</strong>, which routes to the <strong>Streaming Service</strong>. The Streaming Service looks up the content in the <strong>Content DB</strong> to find the HLS manifest URL and generates a time-limited DRM license token. It returns <code>{manifest_url: "https://cdn.example.com/.../master.m3u8", drm_token: "...", resume_position_seconds: 0}</code>. The client fetches the HLS manifest from the <strong>CDN</strong>. The manifest lists available quality levels (240p‚Äì4K) and the URLs for each chunk. The client's ABR (Adaptive Bitrate) algorithm measures current bandwidth (~25 Mbps on WiFi), selects 1080p, and begins fetching 4-second video chunks from the CDN via <code>HTTP GET</code>. As Alice watches, the client periodically (every 30 seconds) sends <code>HTTP PUT /watch-history</code> with <code>{profile_id, content_id, progress_seconds: 120}</code> to the <strong>Watch History Service</strong>, which writes to the <strong>Watch History DB (NoSQL)</strong>.
</div>

<div class="example-box">
<strong>Example 2 ‚Äî Resume Playback:</strong><br>
User "Alice" stopped watching "Stranger Things S4E1" at the 23-minute mark yesterday. Today she opens the app and clicks "Continue Watching." The client sends <code>HTTP GET /stream/content_abc?profile_id=abc123</code>. The <strong>Streaming Service</strong> queries the <strong>Watch History Service</strong> (or the Watch History DB directly) and finds <code>progress_seconds: 1380</code>. It returns <code>{manifest_url: "...", resume_position_seconds: 1380}</code>. The client seeks to the appropriate chunk in the manifest and begins playback from minute 23.
</div>

<div class="example-box">
<strong>Example 3 ‚Äî Network Degradation (Adaptive Bitrate):</strong><br>
User "Bob" is streaming "The Witcher" at 1080p on a train. As the train enters a tunnel, bandwidth drops from 15 Mbps to 2 Mbps. The client's ABR algorithm detects increased chunk download time and buffer depletion. It switches the next chunk request from 1080p (5 Mbps bitrate) to 480p (1.5 Mbps bitrate). The user sees a momentary quality reduction but no buffering interruption. When the train exits the tunnel and bandwidth recovers, the ABR algorithm gradually steps back up to 1080p.
</div>

<div class="example-box">
<strong>Example 4 ‚Äî CDN Cache Miss:</strong><br>
A user in Iceland requests a rarely-watched documentary. The <strong>CDN</strong> edge server in Reykjavik does not have the video chunks cached (cache miss). The edge server fetches the chunks from the <strong>Object Storage (Processed Video)</strong> origin, caches them locally, and serves them to the user. Subsequent users in Iceland requesting the same documentary will get a cache hit.
</div>

<h3>Component Deep Dive</h3>

<div class="deep-dive">
<h4>Streaming Service</h4>
<ul>
  <li><strong>Protocol:</strong> <code>HTTP GET /stream/{content_id}?profile_id={id}</code></li>
  <li><strong>Input:</strong> <code>content_id</code>, <code>profile_id</code></li>
  <li><strong>Output:</strong> <code>{manifest_url, drm_token, resume_position_seconds, available_subtitles[], available_audio_tracks[]}</code></li>
  <li><strong>Reads from:</strong> Content DB (for manifest URL), Watch History DB (for resume position)</li>
  <li><strong>Responsibilities:</strong> Determine which manifest to serve, generate DRM license tokens, determine resume position, return streaming metadata to the client</li>
</ul>
</div>

<div class="deep-dive">
<h4>HLS / DASH Streaming Protocol</h4>
<ul>
  <li><strong>Protocol:</strong> HLS (HTTP Live Streaming) or DASH (Dynamic Adaptive Streaming over HTTP), both over <strong>HTTP/TCP</strong></li>
  <li><strong>Why HLS/DASH:</strong>
    <ul>
      <li>Designed for Video-on-Demand (VoD) of pre-recorded content ‚Äî perfect for Netflix</li>
      <li>Works natively over HTTP, meaning video chunks can be served from standard CDNs, proxies, and web infrastructure</li>
      <li>Supports Adaptive Bitrate (ABR): the client selects the appropriate quality level for each chunk based on measured bandwidth and buffer health</li>
      <li>Industry standard supported by all major platforms (iOS/Safari natively support HLS; Android/Chrome/Smart TVs support DASH)</li>
    </ul>
  </li>
  <li><strong>Why NOT UDP:</strong> TCP's reliability is critical ‚Äî a dropped video chunk would cause visual glitches. TCP retransmission ensures every byte arrives. The latency penalty of TCP vs. UDP is irrelevant for VoD (we aren't doing real-time communication).</li>
  <li><strong>Why NOT WebRTC:</strong> WebRTC is designed for real-time peer-to-peer communication (video calls). It adds complexity (STUN/TURN servers, ICE negotiation) that is unnecessary for one-directional VoD streaming from a CDN.</li>
  <li><strong>Why NOT WebSockets:</strong> Video streaming is a request-response pattern (client requests a chunk, server responds). There is no need for bidirectional server-push. HLS/DASH over HTTP are simpler, more cacheable (by CDN), and more widely supported.</li>
  <li><strong>How it works:</strong>
    <ol>
      <li>The client fetches the <strong>master manifest</strong> (e.g., <code>master.m3u8</code>) which lists available quality levels (e.g., 480p@1.5Mbps, 720p@3Mbps, 1080p@5Mbps)</li>
      <li>The client's ABR algorithm selects a quality level based on current bandwidth</li>
      <li>The client fetches the <strong>media playlist</strong> for that quality level, which lists the URLs of all chunks</li>
      <li>The client sequentially fetches chunks (each 2‚Äì6 seconds of video) and feeds them to the video decoder</li>
      <li>Before each chunk request, the ABR algorithm re-evaluates bandwidth and may switch quality levels</li>
    </ol>
  </li>
</ul>
</div>

<div class="deep-dive">
<h4>Watch History Service</h4>
<ul>
  <li><strong>Protocol:</strong> <code>HTTP PUT /watch-history</code></li>
  <li><strong>Input:</strong> <code>{profile_id, content_id, progress_seconds, timestamp}</code></li>
  <li><strong>Output:</strong> <code>200 OK</code></li>
  <li><strong>Protocol (read):</strong> <code>HTTP GET /watch-history?profile_id={id}</code></li>
  <li><strong>Output (read):</strong> <code>[{content_id, progress_seconds, last_watched_at}]</code></li>
  <li><strong>Writes to:</strong> Watch History DB (NoSQL) ‚Äî high write throughput (millions of concurrent watchers updating progress every 30 seconds)</li>
  <li><strong>Write pattern:</strong> Upsert (insert or update) ‚Äî keyed by <code>(profile_id, content_id)</code></li>
</ul>
</div>

<div class="deep-dive">
<h4>CDN for Video Chunks</h4>
<ul>
  <li>Video chunks are served entirely from CDN edge servers close to the user.</li>
  <li>On cache miss, the edge server pulls from Object Storage (origin) and caches locally.</li>
  <li>Popular content has near-100% cache hit rate; long-tail content has lower hit rates.</li>
  <li>Full deep dive in <a href="#cdn-cache">Section 9</a>.</li>
</ul>
</div>

<!-- ============================= SECTION 6 ‚Äî FLOW 4 ============================= -->
<h2 id="flow4">6. Flow 4 ‚Äî Recommendation Generation (Offline Batch)</h2>

<div class="mermaid">
graph LR
    WHDB[("Watch History DB\n(NoSQL)")] -->|"Export watch data"| ETL["ETL / Batch\nProcessing Pipeline"]
    CDB[("Content DB\n(SQL)")] -->|"Export content features"| ETL
    RATINGS[("Ratings DB\n(NoSQL)")] -->|"Export ratings"| ETL
    ETL -->|"Feature engineering"| ML["ML Training\nPipeline"]
    ML -->|"Generate per-profile\nrecommendations"| RDB[("Recommendation\nStore (NoSQL)")]
    RDB -->|"Pre-warm cache"| RC["Recommendation\nCache (In-Memory)"]
</div>

<h3>Examples</h3>

<div class="example-box">
<strong>Example 1 ‚Äî Nightly Batch Recommendation Run:</strong><br>
Every night at 2:00 AM UTC, the <strong>ETL / Batch Processing Pipeline</strong> kicks off. It exports the last 30 days of watch history from the <strong>Watch History DB</strong>, content metadata and features (genre, cast, tags) from the <strong>Content DB</strong>, and user ratings from the <strong>Ratings DB</strong>. The ETL pipeline performs feature engineering ‚Äî e.g., computing "Alice watched 15 hours of sci-fi in the last month" and "Alice rated 'Interstellar' 5 stars." These features are fed to the <strong>ML Training Pipeline</strong>, which uses collaborative filtering and content-based filtering models to generate personalized recommendation lists for each of the 200M+ profiles. The output ‚Äî e.g., <code>{profile_id: "abc123", rows: [{title: "Sci-Fi Picks For You", content_ids: [...]}, ...]}</code> ‚Äî is written to the <strong>Recommendation Store (NoSQL)</strong>. The pipeline then pre-warms the <strong>Recommendation Cache</strong> for the most active profiles (top 20M daily active users).
</div>

<div class="example-box">
<strong>Example 2 ‚Äî Cold Start (New User):</strong><br>
User "Charlie" just signed up and has no watch history. The ML pipeline has no data for him. The <strong>Recommendation Service</strong> falls back to a <strong>default recommendation set</strong>: globally trending content, top-rated by genre, and popular in Charlie's geographic region. As Charlie watches content and rates titles, the next nightly batch run will produce personalized recommendations.
</div>

<h3>Component Deep Dive</h3>

<div class="deep-dive">
<h4>ETL / Batch Processing Pipeline</h4>
<ul>
  <li><strong>Type:</strong> Distributed batch processing (MapReduce-style)</li>
  <li><strong>Schedule:</strong> Nightly (or every few hours for near-real-time refresh)</li>
  <li><strong>Input:</strong> Watch history, content metadata, user ratings, user demographics</li>
  <li><strong>Output:</strong> Feature vectors for the ML pipeline</li>
  <li><strong>Why batch over real-time:</strong> Recommendation quality benefits from considering the full history, not just the last action. Batch is more cost-efficient for processing 200M+ profiles. Real-time recommendations can supplement batch (e.g., "because you just watched X") but the core homepage is batch-generated.</li>
</ul>
</div>

<div class="deep-dive">
<h4>ML Training Pipeline</h4>
<ul>
  <li><strong>Models:</strong> Collaborative filtering (user-to-user and item-to-item similarity), content-based filtering (genre/cast/director features), deep learning models for engagement prediction</li>
  <li><strong>Output:</strong> For each profile, an ordered list of recommendation "rows" (e.g., "Action Movies For You," "Trending Now," "Because You Watched X"), each containing a ranked list of content IDs</li>
</ul>
</div>

<div class="deep-dive">
<h4>Recommendation Store (NoSQL)</h4>
<ul>
  <li><strong>Type:</strong> NoSQL (key-value or wide-column)</li>
  <li><strong>Key:</strong> <code>profile_id</code></li>
  <li><strong>Value:</strong> JSON document containing the recommendation rows and content IDs</li>
  <li><strong>Why NoSQL:</strong> The data is denormalized (one record per profile), read-heavy, and doesn't require joins. NoSQL provides high read throughput and horizontal scalability.</li>
</ul>
</div>

<!-- ============================= SECTION 7 ‚Äî OVERALL ============================= -->
<h2 id="overall">7. Overall Combined Diagram</h2>

<div class="mermaid">
graph TB
    CP["üé¨ Content\nProvider"] -->|"HTTP POST\n/upload"| AG["API Gateway + Load Balancer"]
    CLIENT["üì± Client App\n(iOS / Android / Smart TV / Browser)"] -->|"HTTP GET/PUT"| AG

    subgraph Upload & Processing
        AG --> US["Upload\nService"]
        US --> OS_RAW[("Object Storage\n(Raw)")]
        US --> MQ["Message Queue"]
        MQ --> TS["Transcoding\nService"]
        TS --> OS_RAW
        TS --> OS_PROC[("Object Storage\n(Processed)")]
        TS --> MS["Metadata\nService"]
    end

    subgraph Browsing & Search
        AG --> CS["Content\nService"]
        AG --> SS["Search\nService"]
        AG --> RS["Recommendation\nService"]
        CS --> CC["Content\nCache"]
        CC --> CDB[("Content DB\n(SQL)")]
        SS --> SI[("Search Index\n(Inverted Index)")]
        RS --> RC["Recommendation\nCache"]
        RC --> RDB[("Recommendation\nStore (NoSQL)")]
    end

    subgraph Streaming
        AG --> StS["Streaming\nService"]
        StS --> CDB
        CLIENT -->|"HLS/DASH\nchunk requests"| CDN["CDN\n(Edge Servers)"]
        CDN --> OS_PROC
    end

    subgraph Watch History & Ratings
        AG --> WHS["Watch History\nService"]
        WHS --> WHDB[("Watch History DB\n(NoSQL)")]
        AG --> RATS["Rating\nService"]
        RATS --> RATDB[("Ratings DB\n(NoSQL)")]
    end

    subgraph Offline Recommendation Pipeline
        WHDB --> ETL["ETL / Batch\nProcessing"]
        CDB --> ETL
        RATDB --> ETL
        ETL --> ML["ML Pipeline"]
        ML --> RDB
        ML --> RC
    end

    MS --> CDB
    OS_PROC --> CDN
</div>

<h3>Examples (End-to-End)</h3>

<div class="example-box">
<strong>Example 1 ‚Äî Full Content Lifecycle:</strong><br>
(1) A studio admin uploads a new film via <code>HTTP POST /upload</code> ‚Üí API Gateway ‚Üí Upload Service ‚Üí Object Storage (Raw) + Message Queue. (2) A Transcoding worker processes the raw video into HLS chunks at multiple quality levels ‚Üí Object Storage (Processed) ‚Üí CDN push. The Metadata Service registers the film in the Content DB. (3) That night, the ETL pipeline picks up the new content's metadata and refreshes the recommendation model. (4) The next morning, user "Alice" opens the app. Her homepage (<code>HTTP GET /home</code>) is assembled from the Recommendation Service (cache hit: pre-computed rows including the new film in "New Releases") and the Content Service (metadata for each title). (5) Alice clicks "Play" ‚Üí Streaming Service returns the HLS manifest URL ‚Üí Client fetches chunks from the CDN via HLS. Watch History Service records her progress. (6) After watching, Alice rates the film üëç ‚Üí Rating Service writes to Ratings DB. This feeds into tomorrow's recommendation pipeline.
</div>

<div class="example-box">
<strong>Example 2 ‚Äî Search to Stream:</strong><br>
User "Bob" searches "breaking bad" ‚Üí API Gateway ‚Üí Search Service queries the inverted Search Index ‚Üí returns ranked results ‚Üí Content Service hydrates metadata from Content Cache ‚Üí Client displays results with thumbnails from CDN ‚Üí Bob clicks "Play" on S1E1 ‚Üí Streaming Service returns manifest ‚Üí Client streams HLS chunks from CDN ‚Üí Watch History Service tracks progress.
</div>

<!-- ============================= SECTION 8 ‚Äî SCHEMA ============================= -->
<h2 id="schema">8. Database Schema</h2>

<h3>8.1 SQL Tables</h3>

<h4><code>users</code> Table</h4>
<table>
  <tr><th>Column</th><th>Type</th><th>Key</th><th>Description</th></tr>
  <tr><td>user_id</td><td>UUID</td><td>PK</td><td>Unique user identifier</td></tr>
  <tr><td>email</td><td>VARCHAR(255)</td><td>UNIQUE INDEX</td><td>User's email (login credential)</td></tr>
  <tr><td>password_hash</td><td>VARCHAR(255)</td><td></td><td>Hashed password</td></tr>
  <tr><td>subscription_tier</td><td>ENUM('basic','standard','premium')</td><td></td><td>Current subscription level</td></tr>
  <tr><td>country</td><td>VARCHAR(2)</td><td></td><td>ISO country code (for content licensing)</td></tr>
  <tr><td>created_at</td><td>TIMESTAMP</td><td></td><td>Account creation timestamp</td></tr>
  <tr><td>updated_at</td><td>TIMESTAMP</td><td></td><td>Last update timestamp</td></tr>
</table>
<div class="callout">
<strong>Why SQL:</strong> User account data requires strong consistency (especially for billing and authentication). ACID transactions ensure that subscription changes, password updates, and billing events are atomic. The dataset is relatively small (~200M rows) and fits well in a relational model.<br>
<strong>Index:</strong> <code>email</code> ‚Äî B-tree index for fast login lookups by email. Hash index would also work but B-tree supports range queries and is the default for most SQL databases.<br>
<strong>Read:</strong> On every login/authentication; when the Streaming Service checks subscription tier for DRM quality limits.<br>
<strong>Write:</strong> On user registration; on subscription changes; on profile updates.<br>
<strong>Sharding:</strong> Shard by <code>user_id</code> (hash-based). This distributes users uniformly across shards. Since most queries are by <code>user_id</code> or <code>email</code> (with a global secondary index on email), lookups are efficient.
</div>

<h4><code>profiles</code> Table</h4>
<table>
  <tr><th>Column</th><th>Type</th><th>Key</th><th>Description</th></tr>
  <tr><td>profile_id</td><td>UUID</td><td>PK</td><td>Unique profile identifier</td></tr>
  <tr><td>user_id</td><td>UUID</td><td>FK ‚Üí users.user_id</td><td>Parent account</td></tr>
  <tr><td>name</td><td>VARCHAR(100)</td><td></td><td>Profile display name</td></tr>
  <tr><td>avatar_url</td><td>VARCHAR(500)</td><td></td><td>URL to avatar image on CDN</td></tr>
  <tr><td>maturity_rating</td><td>ENUM('kids','teen','adult')</td><td></td><td>Content restriction level</td></tr>
  <tr><td>language</td><td>VARCHAR(5)</td><td></td><td>Preferred language (e.g., "en-US")</td></tr>
  <tr><td>created_at</td><td>TIMESTAMP</td><td></td><td>Profile creation timestamp</td></tr>
</table>
<div class="callout">
<strong>Why SQL:</strong> Profiles are tightly coupled to users (FK relationship). Strong consistency is needed ‚Äî if a profile is deleted, related watch history cleanup should be coordinated. Low cardinality (~5 profiles per user, ~1B rows total).<br>
<strong>Index:</strong> <code>user_id</code> ‚Äî B-tree index for fetching all profiles belonging to a user (common query: "show me my profiles on login").<br>
<strong>Read:</strong> On app launch (profile selection screen).<br>
<strong>Write:</strong> On profile creation/update/deletion.<br>
<strong>Sharding:</strong> Co-located with <code>users</code> table on the same shard by <code>user_id</code> to enable efficient joins.
</div>

<h4><code>content</code> Table</h4>
<table>
  <tr><th>Column</th><th>Type</th><th>Key</th><th>Description</th></tr>
  <tr><td>content_id</td><td>UUID</td><td>PK</td><td>Unique content identifier</td></tr>
  <tr><td>title</td><td>VARCHAR(500)</td><td></td><td>Title of movie/show</td></tr>
  <tr><td>description</td><td>TEXT</td><td></td><td>Synopsis</td></tr>
  <tr><td>type</td><td>ENUM('movie','series')</td><td></td><td>Content type</td></tr>
  <tr><td>genre</td><td>VARCHAR(100)</td><td></td><td>Primary genre</td></tr>
  <tr><td>release_year</td><td>INT</td><td></td><td>Year of release</td></tr>
  <tr><td>duration_seconds</td><td>INT</td><td></td><td>Total duration (for movies; NULL for series)</td></tr>
  <tr><td>maturity_rating</td><td>VARCHAR(10)</td><td></td><td>PG, PG-13, R, etc.</td></tr>
  <tr><td>thumbnail_url</td><td>VARCHAR(500)</td><td></td><td>URL to thumbnail on CDN</td></tr>
  <tr><td>manifest_url</td><td>VARCHAR(500)</td><td></td><td>URL to HLS/DASH master manifest</td></tr>
  <tr><td>avg_rating</td><td>DECIMAL(3,2)</td><td></td><td>Denormalized average rating</td></tr>
  <tr><td>total_views</td><td>BIGINT</td><td></td><td>Denormalized total view count</td></tr>
  <tr><td>created_at</td><td>TIMESTAMP</td><td></td><td>When content was added</td></tr>
  <tr><td>updated_at</td><td>TIMESTAMP</td><td></td><td>Last metadata update</td></tr>
</table>
<div class="callout">
<strong>Why SQL:</strong> Content metadata is structured, relational (joins with episodes, genres, actors tables), and relatively small (~500K‚Äì1M titles). Strong consistency ensures metadata accuracy. SQL's schema enforcement prevents bad data.<br>
<strong>Denormalization:</strong> <code>avg_rating</code> and <code>total_views</code> are denormalized (pre-computed aggregates stored directly on the content row). This avoids expensive COUNT/AVG queries across millions of ratings/views at read time. These denormalized fields are updated asynchronously by background jobs that periodically aggregate data from the Ratings DB and Watch History DB.<br>
<strong>Index:</strong> <code>genre</code> ‚Äî B-tree index for filtering by genre on the browse page. <code>(type, release_year)</code> ‚Äî composite B-tree index for queries like "show me movies from 2024."<br>
<strong>Read:</strong> On every homepage load, search result hydration, and stream initiation.<br>
<strong>Write:</strong> On content upload (by Metadata Service); periodic updates to avg_rating and total_views by background jobs.<br>
<strong>Sharding:</strong> Generally not needed (small dataset). If needed, shard by <code>content_id</code>.
</div>

<h4><code>episodes</code> Table (for series)</h4>
<table>
  <tr><th>Column</th><th>Type</th><th>Key</th><th>Description</th></tr>
  <tr><td>episode_id</td><td>UUID</td><td>PK</td><td>Unique episode identifier</td></tr>
  <tr><td>content_id</td><td>UUID</td><td>FK ‚Üí content.content_id</td><td>Parent series</td></tr>
  <tr><td>season_number</td><td>INT</td><td></td><td>Season number</td></tr>
  <tr><td>episode_number</td><td>INT</td><td></td><td>Episode number within season</td></tr>
  <tr><td>title</td><td>VARCHAR(500)</td><td></td><td>Episode title</td></tr>
  <tr><td>duration_seconds</td><td>INT</td><td></td><td>Episode duration</td></tr>
  <tr><td>manifest_url</td><td>VARCHAR(500)</td><td></td><td>HLS/DASH manifest URL for this episode</td></tr>
  <tr><td>thumbnail_url</td><td>VARCHAR(500)</td><td></td><td>Episode-specific thumbnail</td></tr>
</table>
<div class="callout">
<strong>Why SQL:</strong> Tightly coupled to the <code>content</code> table via FK. Requires ordered retrieval (by season and episode number). Small dataset.<br>
<strong>Index:</strong> <code>(content_id, season_number, episode_number)</code> ‚Äî composite B-tree index for fetching episodes in order for a given series.<br>
<strong>Read:</strong> When a user selects a series and browses episodes.<br>
<strong>Write:</strong> When new episodes are uploaded/published.
</div>

<h3>8.2 NoSQL Tables</h3>

<h4><code>watch_history</code> Table</h4>
<table>
  <tr><th>Column</th><th>Type</th><th>Key</th><th>Description</th></tr>
  <tr><td>profile_id</td><td>UUID</td><td>Partition Key</td><td>The profile watching</td></tr>
  <tr><td>content_id</td><td>UUID</td><td>Sort Key</td><td>The content being watched (or episode_id for series)</td></tr>
  <tr><td>progress_seconds</td><td>INT</td><td></td><td>Current playback position</td></tr>
  <tr><td>duration_seconds</td><td>INT</td><td></td><td>Total content duration</td></tr>
  <tr><td>completed</td><td>BOOLEAN</td><td></td><td>Whether the user finished watching</td></tr>
  <tr><td>last_watched_at</td><td>TIMESTAMP</td><td></td><td>When the user last watched this content</td></tr>
</table>
<div class="callout">
<strong>Why NoSQL:</strong> Extremely write-heavy ‚Äî millions of concurrent viewers each updating their progress every ~30 seconds. This requires massive write throughput that scales horizontally. The access pattern is simple: read/write by <code>(profile_id, content_id)</code> ‚Äî no joins needed. Eventual consistency is acceptable (a slight delay in resume position is fine).<br>
<strong>Sharding:</strong> Shard (partition) by <code>profile_id</code> using consistent hashing. This ensures all watch history for a single profile resides on the same partition, enabling efficient reads like "get all continue-watching items for Alice." Consistent hashing minimizes data redistribution when nodes are added/removed.<br>
<strong>Read:</strong> When user opens "Continue Watching" row on homepage; when Streaming Service determines resume position.<br>
<strong>Write:</strong> Every ~30 seconds during active playback (upsert by <code>(profile_id, content_id)</code>).
</div>

<h4><code>ratings</code> Table</h4>
<table>
  <tr><th>Column</th><th>Type</th><th>Key</th><th>Description</th></tr>
  <tr><td>profile_id</td><td>UUID</td><td>Partition Key</td><td>The profile that rated</td></tr>
  <tr><td>content_id</td><td>UUID</td><td>Sort Key</td><td>The content being rated</td></tr>
  <tr><td>rating</td><td>TINYINT</td><td></td><td>1 (üëé) or 2 (üëç) or 1‚Äì5 stars</td></tr>
  <tr><td>rated_at</td><td>TIMESTAMP</td><td></td><td>When the rating was submitted</td></tr>
</table>
<div class="callout">
<strong>Why NoSQL:</strong> High write volume (millions of ratings), simple key-value access pattern (<code>profile_id + content_id ‚Üí rating</code>), no complex queries. Eventual consistency is acceptable for ratings.<br>
<strong>Sharding:</strong> Shard by <code>profile_id</code> (consistent hashing) ‚Äî same rationale as watch_history.<br>
<strong>Read:</strong> By recommendation pipeline (batch export); when showing a user their rating on a content detail page.<br>
<strong>Write:</strong> When a user clicks üëç/üëé or sets a star rating.
</div>

<h4><code>recommendations</code> Table</h4>
<table>
  <tr><th>Column</th><th>Type</th><th>Key</th><th>Description</th></tr>
  <tr><td>profile_id</td><td>UUID</td><td>Partition Key (only key)</td><td>The profile these recommendations are for</td></tr>
  <tr><td>rows</td><td>JSON / LIST</td><td></td><td>Ordered list of recommendation rows, each with a title and list of content_ids</td></tr>
  <tr><td>generated_at</td><td>TIMESTAMP</td><td></td><td>When these recommendations were generated</td></tr>
</table>
<div class="callout">
<strong>Why NoSQL:</strong> Pure key-value lookup by <code>profile_id</code>. The value is a large JSON blob of recommendations ‚Äî no relational structure. Read-heavy (every homepage load). NoSQL provides high throughput for this simple access pattern.<br>
<strong>Sharding:</strong> Shard by <code>profile_id</code> (consistent hashing).<br>
<strong>Read:</strong> On every homepage load (via Recommendation Cache ‚Üí Recommendation Store on miss).<br>
<strong>Write:</strong> By the nightly ML batch pipeline.
</div>

<!-- ============================= SECTION 9 ‚Äî CDN & CACHE ============================= -->
<h2 id="cdn-cache">9. CDN &amp; Caching Deep Dive</h2>

<h3>9.1 CDN (Content Delivery Network)</h3>

<div class="deep-dive">
<h4>Why a CDN is Appropriate (Critical)</h4>
<p>A CDN is <strong>absolutely essential</strong> for Netflix. Video streaming is extremely bandwidth-intensive, and users are distributed globally. Without a CDN:</p>
<ul>
  <li>Every video chunk request would travel to a centralized origin, introducing high latency (100s of ms cross-continent)</li>
  <li>The origin would be crushed under the load of 10M+ concurrent streams</li>
  <li>User experience would degrade massively (buffering, slow start times)</li>
</ul>

<h4>What the CDN Caches</h4>
<ul>
  <li><strong>Video chunks</strong> (HLS .ts / DASH .m4s segments) ‚Äî the primary payload</li>
  <li><strong>HLS/DASH manifests</strong> (.m3u8 / .mpd files)</li>
  <li><strong>Thumbnail images</strong> ‚Äî movie/show poster art</li>
  <li><strong>Static assets</strong> ‚Äî app JavaScript, CSS, fonts</li>
</ul>

<h4>CDN Strategy</h4>
<ul>
  <li><strong>Push-based for popular/new content:</strong> When a new blockbuster is released, the processed video chunks are proactively <em>pushed</em> to CDN edge servers worldwide before the content goes live. This ensures zero cache misses on launch.</li>
  <li><strong>Pull-based for long-tail content:</strong> Rarely watched content is loaded into CDN edge servers on-demand (pull on first request). If nobody in a region requests it, it never occupies edge cache space.</li>
  <li><strong>Tiered architecture:</strong> Edge servers (closest to users) ‚Üí Regional mid-tier caches ‚Üí Origin (Object Storage). A miss at the edge tries the regional cache before going all the way to origin.</li>
</ul>

<h4>CDN Eviction Policy</h4>
<ul>
  <li><strong>LRU (Least Recently Used)</strong> at the edge ‚Äî popular content stays cached; old/unpopular content is evicted to make room.</li>
  <li><strong>Frequency-weighted LRU:</strong> Content that was accessed many times recently is prioritized over content accessed only once recently.</li>
</ul>

<h4>CDN Expiration (TTL)</h4>
<ul>
  <li><strong>Video chunks:</strong> Long TTL (30 days+). Video chunks are immutable once transcoded ‚Äî they never change. If a video needs re-encoding, a new manifest with new chunk URLs is generated.</li>
  <li><strong>Manifests:</strong> Medium TTL (1 hour). Manifests may change if a new audio track or subtitle is added.</li>
  <li><strong>Thumbnails:</strong> Medium TTL (6 hours). Thumbnails can be A/B tested (Netflix personalizes artwork).</li>
</ul>
</div>

<h3>9.2 In-Memory Caches</h3>

<div class="deep-dive">
<h4>Content Cache</h4>
<table>
  <tr><th>Property</th><th>Value</th><th>Rationale</th></tr>
  <tr><td>What it caches</td><td>Content metadata (title, description, genre, thumbnail_url, rating, etc.) keyed by <code>content_id</code></td><td>Content metadata is read on every homepage and search result. Hot data that benefits hugely from caching.</td></tr>
  <tr><td>Caching Strategy</td><td>Read-through</td><td>On cache miss, the Content Service reads from the Content DB, returns the result, and populates the cache. Simple and ensures eventual consistency.</td></tr>
  <tr><td>Eviction Policy</td><td>LRU (Least Recently Used)</td><td>Popular titles (top 10K) are accessed millions of times/day and should stay cached. Long-tail titles are evicted when space is needed.</td></tr>
  <tr><td>Expiration (TTL)</td><td>1 hour</td><td>Content metadata changes infrequently (occasional description edits, rating updates). 1-hour TTL balances freshness with hit rate. For critical updates, cache invalidation can be triggered explicitly.</td></tr>
  <tr><td>Populated by</td><td>Cache misses; optional cache-warming job on service startup for top 1,000 titles</td><td></td></tr>
</table>
</div>

<div class="deep-dive">
<h4>Recommendation Cache</h4>
<table>
  <tr><th>Property</th><th>Value</th><th>Rationale</th></tr>
  <tr><td>What it caches</td><td>Pre-computed recommendation lists keyed by <code>profile_id</code></td><td>Every homepage load needs recommendations. Caching avoids hitting the Recommendation Store (NoSQL) for every request.</td></tr>
  <tr><td>Caching Strategy</td><td>Write-through</td><td>When the nightly batch ML pipeline generates new recommendations, it writes to both the Recommendation Store and the cache simultaneously. This ensures the cache is always fresh after a pipeline run.</td></tr>
  <tr><td>Eviction Policy</td><td>LRU</td><td>Active daily users' recommendations stay in cache. Inactive users (who haven't logged in for weeks) are evicted. Active users represent a small fraction of total profiles.</td></tr>
  <tr><td>Expiration (TTL)</td><td>24 hours</td><td>Recommendations are regenerated nightly. A 24-hour TTL ensures stale recommendations are evicted even if the pipeline doesn't explicitly overwrite them.</td></tr>
  <tr><td>Populated by</td><td>Nightly ML batch pipeline (write-through); cache misses on read</td><td></td></tr>
</table>
</div>

<div class="deep-dive">
<h4>Session / Auth Cache</h4>
<table>
  <tr><th>Property</th><th>Value</th><th>Rationale</th></tr>
  <tr><td>What it caches</td><td>User session tokens and auth data keyed by session token</td><td>Every API request requires authentication. Caching sessions avoids a DB lookup on every request.</td></tr>
  <tr><td>Caching Strategy</td><td>Write-through</td><td>On login, the session is written to both the cache and the session store.</td></tr>
  <tr><td>Eviction Policy</td><td>TTL-based (no LRU needed)</td><td>Sessions have a natural expiry.</td></tr>
  <tr><td>Expiration (TTL)</td><td>30 minutes (sliding window)</td><td>Each API request resets the TTL. Sessions expire after 30 minutes of inactivity.</td></tr>
  <tr><td>Populated by</td><td>Login events</td><td></td></tr>
</table>
</div>

<!-- ============================= SECTION 10 ‚Äî SCALING ============================= -->
<h2 id="scaling">10. Scaling Considerations</h2>

<h3>10.1 Load Balancers</h3>
<div class="deep-dive">
<p>Load balancers are placed at the following points:</p>
<ol>
  <li><strong>In front of the API Gateway:</strong> Distributes incoming client traffic (HTTP/HTTPS) across multiple API Gateway instances. Uses <strong>Layer 7 (application-level)</strong> load balancing to inspect HTTP headers and route based on path (e.g., <code>/upload</code> ‚Üí Upload Service cluster, <code>/stream</code> ‚Üí Streaming Service cluster). Algorithm: <strong>Least connections</strong> ‚Äî ensures no single gateway instance is overloaded.</li>
  <li><strong>Between API Gateway and each microservice:</strong> Each service (Content Service, Search Service, Streaming Service, etc.) sits behind its own internal load balancer. Algorithm: <strong>Round-robin</strong> for stateless services. <strong>Consistent hashing</strong> for cache-affinity (e.g., requests for the same <code>profile_id</code> route to the same Recommendation Service instance to improve local cache hit rates).</li>
  <li><strong>In front of the Transcoding Service workers:</strong> The Message Queue inherently distributes work, so a traditional load balancer isn't needed here. Instead, workers pull from the queue (consumer-based distribution).</li>
  <li><strong>CDN edge servers:</strong> DNS-based (GeoDNS / Anycast) load balancing routes users to the nearest edge server based on geographic location and server health.</li>
</ol>

<h4>Health Checks</h4>
<p>All load balancers perform periodic health checks (HTTP <code>GET /health</code>) on downstream instances. Unhealthy instances are removed from the pool automatically.</p>

<h4>SSL/TLS Termination</h4>
<p>TLS is terminated at the outermost load balancer (in front of API Gateway). Internal service-to-service traffic uses mTLS or plaintext within a trusted VPC.</p>
</div>

<h3>10.2 Horizontal Scaling</h3>
<ul>
  <li><strong>All microservices</strong> (Content Service, Search Service, Streaming Service, Watch History Service, Recommendation Service, Upload Service, Metadata Service, Rating Service) are <strong>stateless</strong> and can be scaled horizontally by adding more instances behind load balancers.</li>
  <li><strong>Transcoding Service:</strong> Scaled by adding more workers. Auto-scale based on Message Queue depth ‚Äî when queue depth exceeds a threshold, spin up more workers.</li>
  <li><strong>CDN:</strong> Scaled by adding more edge server locations globally.</li>
</ul>

<h3>10.3 Database Scaling</h3>
<ul>
  <li><strong>SQL (users, profiles, content):</strong> Primary-replica replication. Reads are distributed across replicas. Writes go to the primary. For users table, shard by <code>user_id</code> (hash-based) when the dataset exceeds a single node's capacity.</li>
  <li><strong>NoSQL (watch_history, ratings, recommendations):</strong> Natively supports horizontal sharding. Add nodes to the cluster as data grows. Consistent hashing distributes partitions across nodes.</li>
  <li><strong>Search Index:</strong> Partitioned by content_id range or hash. Replicated across nodes for read throughput.</li>
</ul>

<h3>10.4 Back-of-the-Envelope Estimates</h3>
<table>
  <tr><th>Metric</th><th>Estimate</th></tr>
  <tr><td>Total users</td><td>~200M subscribers, ~1B profiles</td></tr>
  <tr><td>Daily active users</td><td>~100M</td></tr>
  <tr><td>Peak concurrent streams</td><td>~10M</td></tr>
  <tr><td>Average stream bitrate</td><td>~5 Mbps</td></tr>
  <tr><td>Peak egress bandwidth</td><td>~10M √ó 5 Mbps = <strong>50 Tbps</strong></td></tr>
  <tr><td>Watch history writes/sec (peak)</td><td>10M streams √∑ 30s interval = <strong>~333K writes/sec</strong></td></tr>
  <tr><td>Homepage requests/sec (peak)</td><td>~500K RPS (accounting for browse, scroll, etc.)</td></tr>
  <tr><td>Content catalog size</td><td>~500K titles</td></tr>
  <tr><td>Storage (processed video)</td><td>~100 PB+ (each title √ó multiple resolutions √ó multiple bitrates)</td></tr>
</table>

<h3>10.5 Geographic Distribution</h3>
<ul>
  <li>Deploy services in multiple regions (US, EU, Asia-Pacific, South America) with independent database clusters per region.</li>
  <li>CDN edge servers in 50+ countries.</li>
  <li>Use GeoDNS to route users to the nearest API Gateway region.</li>
</ul>

<!-- ============================= SECTION 11 ‚Äî TRADEOFFS ============================= -->
<h2 id="tradeoffs">11. Tradeoffs &amp; Deep Dives</h2>

<h3>11.1 Consistency vs. Availability</h3>
<ul>
  <li><strong>Watch history:</strong> Eventual consistency is acceptable. If a user watches on their TV and immediately opens their phone, a 1‚Äì2 second delay in resume position sync is tolerable. We favor availability and partition tolerance (AP in CAP theorem).</li>
  <li><strong>User accounts / billing:</strong> Strong consistency required. A subscription downgrade should immediately affect streaming quality limits. We use SQL with ACID transactions.</li>
  <li><strong>Recommendations:</strong> Eventual consistency is fine. Stale recommendations (showing yesterday's list) is acceptable.</li>
</ul>

<h3>11.2 Batch vs. Real-Time Recommendations</h3>
<ul>
  <li><strong>Chosen: Batch (nightly).</strong> Pros: Cost-effective, can use full user history, simpler architecture. Cons: Recommendations are up to 24 hours stale.</li>
  <li><strong>Enhancement:</strong> A lightweight real-time layer can supplement batch recommendations. E.g., "Because you just watched X" can be computed in real-time by the Recommendation Service using simple rules (same genre, same director) without a full ML pipeline.</li>
</ul>

<h3>11.3 Denormalization Tradeoffs</h3>
<ul>
  <li><code>avg_rating</code> and <code>total_views</code> on the <code>content</code> table are denormalized to avoid expensive aggregation queries. The tradeoff is potential staleness (updated by background jobs every few minutes) and write amplification (each rating/view triggers an eventual update to the content row). This is acceptable because exact real-time counts are not user-facing critical.</li>
</ul>

<h3>11.4 Push vs. Pull CDN Strategy</h3>
<ul>
  <li><strong>Push (proactive):</strong> Used for new releases and popular content. Guarantees zero cache misses on launch day. Costs more in edge storage.</li>
  <li><strong>Pull (reactive):</strong> Used for long-tail content. Saves edge storage but causes a cold-start latency for the first user in each region.</li>
  <li><strong>Chosen: Hybrid.</strong> Best of both worlds.</li>
</ul>

<h3>11.5 HLS vs. DASH</h3>
<ul>
  <li><strong>HLS:</strong> Created by Apple, natively supported on iOS/Safari. Widely supported. Uses <code>.m3u8</code> manifests and <code>.ts</code> segments.</li>
  <li><strong>DASH:</strong> Open standard (ISO), supported on Android/Chrome. Uses <code>.mpd</code> manifests and <code>.m4s</code> segments.</li>
  <li><strong>Chosen: Support both.</strong> The Transcoding Service generates both HLS and DASH outputs. The Streaming Service returns the appropriate manifest URL based on the client's platform (iOS ‚Üí HLS, Android/Browser ‚Üí DASH).</li>
</ul>

<h3>11.6 Codec Tradeoffs</h3>
<ul>
  <li><strong>H.264/AVC:</strong> Universal compatibility. Higher bitrate for same quality. Used as baseline for all devices.</li>
  <li><strong>H.265/HEVC:</strong> ~50% bitrate reduction at same quality. Supported on newer devices. Patent licensing complexity.</li>
  <li><strong>AV1:</strong> Open-source, ~30% better than HEVC. Growing device support. Higher encoding cost (slower transcoding).</li>
  <li><strong>Chosen:</strong> Encode in all three. Serve the most efficient codec that the client device supports.</li>
</ul>

<!-- ============================= SECTION 12 ‚Äî ALTERNATIVES ============================= -->
<h2 id="alternatives">12. Alternative Approaches</h2>

<h3>12.1 Peer-to-Peer (P2P) Streaming</h3>
<p><strong>Approach:</strong> Use P2P (like BitTorrent) to distribute video chunks between viewers, reducing CDN load.</p>
<p><strong>Why not chosen:</strong></p>
<ul>
  <li>Quality control is difficult ‚Äî peers may have different bandwidth, causing inconsistent playback</li>
  <li>DRM enforcement is nearly impossible in a P2P model</li>
  <li>Legal and copyright concerns with content being distributed through user devices</li>
  <li>Latency and reliability are unpredictable</li>
  <li>CDN infrastructure is proven and reliable at Netflix's scale</li>
</ul>

<h3>12.2 Monolithic Architecture</h3>
<p><strong>Approach:</strong> Single application handling all functionality (upload, browse, stream, recommend).</p>
<p><strong>Why not chosen:</strong></p>
<ul>
  <li>Cannot scale components independently. Streaming needs far more resources than content upload.</li>
  <li>A bug in the recommendation engine could crash the streaming service</li>
  <li>Deployment becomes risky ‚Äî every change requires redeploying the entire application</li>
  <li>Team velocity decreases as the codebase grows</li>
  <li>Microservices enable independent scaling, deployment, and technology choices per service</li>
</ul>

<h3>12.3 Real-Time Recommendations Only</h3>
<p><strong>Approach:</strong> Compute recommendations on-the-fly for every homepage request.</p>
<p><strong>Why not chosen:</strong></p>
<ul>
  <li>Real-time ML inference for 200M+ profiles at 500K RPS would require enormous compute resources</li>
  <li>Latency: running a recommendation model per request would add 100s of ms to homepage load</li>
  <li>The quality of recommendations improves when considering full user history (batch approach), not just the last few actions</li>
  <li>A hybrid approach (batch + lightweight real-time supplement) provides the best balance of quality, cost, and latency</li>
</ul>

<h3>12.4 WebSocket-Based Streaming</h3>
<p><strong>Approach:</strong> Use WebSockets for video delivery instead of HLS/DASH.</p>
<p><strong>Why not chosen:</strong></p>
<ul>
  <li>WebSockets are designed for bidirectional, low-latency messaging ‚Äî overkill for one-directional video delivery</li>
  <li>Video chunks served over WebSocket cannot be cached by CDNs (CDNs cache HTTP responses, not WebSocket frames)</li>
  <li>HLS/DASH are industry standards with mature ABR algorithms, DRM integration, and device support</li>
  <li>WebSocket connections are stateful and harder to load-balance</li>
</ul>

<h3>12.5 Server-Side Adaptive Bitrate (Instead of Client-Side ABR)</h3>
<p><strong>Approach:</strong> Have the server decide which quality level to send to the client.</p>
<p><strong>Why not chosen:</strong></p>
<ul>
  <li>The client has the most accurate, real-time information about its network conditions (measured download speed, buffer level)</li>
  <li>Server-side ABR introduces a feedback loop delay: client reports bandwidth ‚Üí server decides ‚Üí server sends ‚Äî this lag causes more buffering</li>
  <li>Client-side ABR is the industry standard for VoD streaming (HLS/DASH are designed around it)</li>
</ul>

<h3>12.6 Pub/Sub Instead of Message Queue for Transcoding</h3>
<p><strong>Approach:</strong> Use a pub/sub system to broadcast transcode jobs.</p>
<p><strong>Why not chosen:</strong></p>
<ul>
  <li>Transcoding is a point-to-point job ‚Äî each video should be transcoded by exactly one worker, not fanned out to all workers</li>
  <li>Pub/Sub would deliver the same job to multiple subscribers, causing redundant work unless additional deduplication logic is added</li>
  <li>A Message Queue with competing consumers is the natural fit for task distribution with at-least-once delivery</li>
</ul>

<!-- ============================= SECTION 13 ‚Äî ADDITIONAL ============================= -->
<h2 id="additional">13. Additional Information</h2>

<h3>13.1 DRM (Digital Rights Management)</h3>
<ul>
  <li>Netflix must protect content from piracy using DRM. Industry standards include:
    <ul>
      <li><strong>Widevine</strong> (Google) ‚Äî Android, Chrome</li>
      <li><strong>FairPlay</strong> (Apple) ‚Äî iOS, Safari, Apple TV</li>
      <li><strong>PlayReady</strong> (Microsoft) ‚Äî Windows, Xbox, Smart TVs</li>
    </ul>
  </li>
  <li>The <strong>Streaming Service</strong> generates DRM license tokens. The client's video player decrypts video chunks using the DRM license before rendering.</li>
  <li>Video chunks are encrypted at rest (during transcoding) and the decryption key is only provided via the DRM license server.</li>
</ul>

<h3>13.2 Content Licensing &amp; Geo-Restrictions</h3>
<ul>
  <li>Different content is licensed for different regions. The Content Service must filter available titles based on the user's <code>country</code> field.</li>
  <li>This can be implemented as a <code>content_availability</code> table mapping <code>(content_id, country_code)</code> or as a list of allowed/blocked countries on each content record.</li>
</ul>

<h3>13.3 A/B Testing for Thumbnails</h3>
<ul>
  <li>Netflix famously A/B tests thumbnail artwork. Different users see different thumbnails for the same title to optimize click-through rate.</li>
  <li>The Content Service can return a personalized <code>thumbnail_url</code> based on the user's A/B test group assignment.</li>
</ul>

<h3>13.4 Subtitle &amp; Audio Track Management</h3>
<ul>
  <li>Subtitles (SRT/WebVTT) and multiple audio tracks (e.g., English, Spanish, Japanese) are stored alongside video chunks in Object Storage and served via CDN.</li>
  <li>The HLS/DASH manifest includes references to available subtitle and audio track URLs.</li>
</ul>

<h3>13.5 Monitoring &amp; Observability</h3>
<ul>
  <li><strong>Metrics:</strong> Stream start time, buffering ratio, error rate, ABR quality distribution, CDN cache hit ratio</li>
  <li><strong>Distributed tracing:</strong> Trace requests across microservices for debugging latency issues</li>
  <li><strong>Alerting:</strong> Alerts on anomalies (spike in error rates, CDN miss rate increase, transcoding queue depth)</li>
</ul>

<h3>13.6 Offline Downloads</h3>
<ul>
  <li>Netflix allows users to download content for offline viewing on mobile devices.</li>
  <li>The Streaming Service provides a download manifest with DRM-protected chunks. The client downloads and stores them locally with a time-limited DRM license (e.g., 48-hour offline playback window).</li>
</ul>

<h3>13.7 Rate Limiting &amp; Abuse Prevention</h3>
<ul>
  <li>The API Gateway enforces rate limiting per user/IP to prevent abuse.</li>
  <li>Concurrent stream limits are enforced based on subscription tier (Basic: 1 stream, Standard: 2, Premium: 4).</li>
</ul>

<!-- ============================= SECTION 14 ‚Äî VENDORS ============================= -->
<h2 id="vendors">14. Vendor Recommendations</h2>

<div class="callout">
<p>The design above is vendor-agnostic. Below are potential vendors for key infrastructure components:</p>
</div>

<table>
  <tr><th>Component</th><th>Potential Vendors</th><th>Rationale</th></tr>
  <tr>
    <td>SQL Database</td>
    <td>PostgreSQL, MySQL, Amazon Aurora, Google Cloud Spanner</td>
    <td>PostgreSQL offers rich feature set (JSONB, full-text search, advanced indexing). Aurora provides managed MySQL/PostgreSQL with auto-scaling replicas. Spanner provides global strong consistency if needed.</td>
  </tr>
  <tr>
    <td>NoSQL Database</td>
    <td>Apache Cassandra, Amazon DynamoDB, ScyllaDB</td>
    <td>Cassandra excels at high-write workloads with tunable consistency ‚Äî ideal for watch history (333K writes/sec). DynamoDB offers fully managed scaling with single-digit ms latency. ScyllaDB is a high-performance Cassandra-compatible option.</td>
  </tr>
  <tr>
    <td>In-Memory Cache</td>
    <td>Redis, Memcached, Hazelcast</td>
    <td>Redis offers rich data structures (sorted sets for ranked recommendations), persistence, and replication. Memcached is simpler and slightly faster for pure key-value caching.</td>
  </tr>
  <tr>
    <td>Message Queue</td>
    <td>Apache Kafka, RabbitMQ, Amazon SQS</td>
    <td>Kafka provides high throughput, durability, and replay capability ‚Äî useful for the transcoding pipeline and change data capture. RabbitMQ is simpler for traditional task queues. SQS is fully managed with no operational overhead.</td>
  </tr>
  <tr>
    <td>Object Storage</td>
    <td>Amazon S3, Google Cloud Storage, Azure Blob Storage</td>
    <td>S3 is the industry standard for durable object storage (11 nines durability). All three major clouds offer equivalent services with cross-region replication.</td>
  </tr>
  <tr>
    <td>CDN</td>
    <td>Netflix Open Connect (custom), Akamai, Cloudflare, Amazon CloudFront</td>
    <td>Netflix actually built their own CDN (Open Connect) to control the last mile. For a greenfield design, Akamai or Cloudflare offer global edge networks with ~50Tbps+ capacity.</td>
  </tr>
  <tr>
    <td>Search Index</td>
    <td>Elasticsearch, Apache Solr, Meilisearch</td>
    <td>Elasticsearch is the most popular inverted index engine with excellent full-text search, fuzzy matching, and scalability. Solr is a mature alternative.</td>
  </tr>
  <tr>
    <td>Batch Processing</td>
    <td>Apache Spark, Apache Flink, Google Dataflow</td>
    <td>Spark is the de facto standard for large-scale batch ML pipelines. Flink offers true streaming with batch capability. Dataflow is fully managed.</td>
  </tr>
  <tr>
    <td>Transcoding</td>
    <td>FFmpeg (open source), AWS Elemental MediaConvert, Bitmovin</td>
    <td>FFmpeg is the universal video processing tool. Cloud transcoding services offer managed scaling.</td>
  </tr>
</table>

<hr>
<p style="text-align:center; color:#888; margin-top:40px;">System Design Document ‚Äî Netflix ‚Äî Generated 2026-02-13</p>

</body>
</html>
