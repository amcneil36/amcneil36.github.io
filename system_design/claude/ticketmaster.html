<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>System Design: Ticketmaster</title>
    <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
    <style>
        :root {
            --bg: #ffffff;
            --text: #1a1a2e;
            --accent: #0f3460;
            --secondary: #16213e;
            --border: #e0e0e0;
            --code-bg: #f4f4f8;
            --highlight: #e94560;
            --card-bg: #fafafa;
        }
        * { box-sizing: border-box; margin: 0; padding: 0; }
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background: var(--bg);
            color: var(--text);
            line-height: 1.7;
            max-width: 1100px;
            margin: 0 auto;
            padding: 40px 30px;
        }
        h1 { font-size: 2.2rem; color: var(--accent); border-bottom: 3px solid var(--highlight); padding-bottom: 10px; margin-bottom: 30px; }
        h2 { font-size: 1.6rem; color: var(--secondary); margin-top: 50px; margin-bottom: 18px; border-left: 4px solid var(--highlight); padding-left: 14px; }
        h3 { font-size: 1.25rem; color: var(--accent); margin-top: 30px; margin-bottom: 12px; }
        h4 { font-size: 1.05rem; color: var(--secondary); margin-top: 20px; margin-bottom: 8px; }
        p { margin-bottom: 14px; }
        ul, ol { margin-left: 24px; margin-bottom: 14px; }
        li { margin-bottom: 6px; }
        code {
            background: var(--code-bg);
            padding: 2px 6px;
            border-radius: 4px;
            font-size: 0.92em;
            font-family: 'Courier New', monospace;
        }
        pre {
            background: var(--code-bg);
            padding: 16px 20px;
            border-radius: 8px;
            overflow-x: auto;
            margin-bottom: 18px;
            border: 1px solid var(--border);
            font-size: 0.9em;
            line-height: 1.5;
        }
        .card {
            background: var(--card-bg);
            border: 1px solid var(--border);
            border-radius: 10px;
            padding: 20px 24px;
            margin-bottom: 20px;
        }
        .example {
            background: #f0f7ff;
            border-left: 4px solid #3498db;
            padding: 16px 20px;
            margin-bottom: 16px;
            border-radius: 0 8px 8px 0;
        }
        .example strong { color: #2980b9; }
        .warning {
            background: #fff8e1;
            border-left: 4px solid #f39c12;
            padding: 16px 20px;
            margin-bottom: 16px;
            border-radius: 0 8px 8px 0;
        }
        .mermaid {
            text-align: center;
            margin: 24px 0;
            padding: 20px;
            background: #fdfdfe;
            border: 1px solid var(--border);
            border-radius: 10px;
        }
        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: 20px;
            font-size: 0.95em;
        }
        th, td {
            border: 1px solid var(--border);
            padding: 10px 14px;
            text-align: left;
        }
        th {
            background: var(--accent);
            color: #fff;
            font-weight: 600;
        }
        tr:nth-child(even) { background: var(--card-bg); }
        .toc { background: var(--card-bg); border: 1px solid var(--border); border-radius: 10px; padding: 24px 30px; margin-bottom: 40px; }
        .toc a { color: var(--accent); text-decoration: none; }
        .toc a:hover { text-decoration: underline; }
        .toc ol { margin-left: 20px; }
        .toc li { margin-bottom: 4px; }
        .tag { display: inline-block; background: var(--accent); color: #fff; padding: 2px 10px; border-radius: 12px; font-size: 0.82em; margin-right: 6px; }
        .tag-warn { background: var(--highlight); }
    </style>
</head>
<body>

<h1>System Design: Ticketmaster</h1>
<p>A distributed, scalable online event ticketing platform supporting event discovery, seat selection, ticket purchasing with flash-sale resilience, and event management for organizers.</p>

<!-- TABLE OF CONTENTS -->
<div class="toc">
    <strong>Table of Contents</strong>
    <ol>
        <li><a href="#fr">Functional Requirements</a></li>
        <li><a href="#nfr">Non-Functional Requirements</a></li>
        <li><a href="#capacity">Capacity Estimation</a></li>
        <li><a href="#flow1">Flow 1 &mdash; Event Discovery</a></li>
        <li><a href="#flow2">Flow 2 &mdash; Ticket Booking (Seat Selection, Reservation, Payment)</a></li>
        <li><a href="#flow3">Flow 3 &mdash; Event Management (Organizer)</a></li>
        <li><a href="#combined">Combined System Diagram</a></li>
        <li><a href="#schema">Database Schema</a></li>
        <li><a href="#cdn-cache">CDN &amp; Caching Strategy</a></li>
        <li><a href="#scaling">Scaling Considerations</a></li>
        <li><a href="#tradeoffs">Tradeoffs &amp; Deep Dives</a></li>
        <li><a href="#alternatives">Alternative Approaches</a></li>
        <li><a href="#additional">Additional Considerations</a></li>
        <li><a href="#vendors">Vendor Recommendations</a></li>
    </ol>
</div>

<!-- ================================================================== -->
<h2 id="fr">1. Functional Requirements</h2>
<ul>
    <li><strong>Event Discovery:</strong> Users can search and browse events by keyword, category (concerts, sports, theater), location, date range, and artist/team.</li>
    <li><strong>Event Detail View:</strong> Users can view event details including venue information, date/time, pricing tiers, and an interactive seating chart.</li>
    <li><strong>Seat Selection:</strong> Users can view a real-time seating map showing available, reserved, and sold seats, and select one or more seats.</li>
    <li><strong>Temporary Seat Reservation:</strong> When a user selects seats, they are temporarily held (e.g., 10 minutes) while the user completes checkout.</li>
    <li><strong>Ticket Purchase:</strong> Users can complete payment for reserved seats to confirm the booking.</li>
    <li><strong>Booking Confirmation:</strong> Users receive a confirmation with a digital ticket (QR code / barcode) via email and/or push notification.</li>
    <li><strong>View Purchased Tickets:</strong> Users can view their booking history and upcoming tickets.</li>
    <li><strong>Virtual Waiting Queue:</strong> For high-demand events, users are placed into a fair waiting queue before being allowed to select seats.</li>
    <li><strong>Event Management (Organizer):</strong> Organizers can create events, configure venue/seating, set pricing tiers, and manage event status.</li>
</ul>

<!-- ================================================================== -->
<h2 id="nfr">2. Non-Functional Requirements</h2>
<ul>
    <li><strong>Strong Consistency for Bookings:</strong> No double-selling of the same seat. This is the single most critical invariant.</li>
    <li><strong>High Availability:</strong> The system should target 99.99% uptime; browsing/search may degrade gracefully, but booking must remain consistent.</li>
    <li><strong>Low Latency:</strong> Event search and browsing should return results within 200ms. Seat availability rendering within 500ms.</li>
    <li><strong>High Concurrency:</strong> Must handle flash-sale scenarios where millions of users attempt to buy tickets for the same event simultaneously.</li>
    <li><strong>Scalability:</strong> Horizontal scaling to support ~100M registered users and ~500M tickets sold per year.</li>
    <li><strong>Fault Tolerance:</strong> Graceful degradation; if the payment gateway is temporarily down, reservations should persist and users can retry.</li>
    <li><strong>Idempotency:</strong> Payment and booking APIs must be idempotent to handle retries safely.</li>
    <li><strong>Fairness:</strong> The waiting queue should use randomized position assignment to prevent bots from gaining advantage by connecting first.</li>
</ul>

<!-- ================================================================== -->
<h2 id="capacity">3. Capacity Estimation</h2>
<div class="card">
    <table>
        <tr><th>Metric</th><th>Estimate</th></tr>
        <tr><td>Registered users</td><td>~100M</td></tr>
        <tr><td>Active events at any time</td><td>~100K</td></tr>
        <tr><td>Tickets sold per year</td><td>~500M</td></tr>
        <tr><td>Peak concurrent users (flash sale)</td><td>~10M</td></tr>
        <tr><td>Read:Write ratio (event browsing)</td><td>~100:1</td></tr>
        <tr><td>Read:Write ratio (booking during flash sale)</td><td>~1:1</td></tr>
        <tr><td>Average event seats</td><td>~20K &ndash; 80K for large venues</td></tr>
    </table>
</div>

<!-- ================================================================== -->
<!-- FLOW 1: EVENT DISCOVERY -->
<!-- ================================================================== -->
<h2 id="flow1">4. Flow 1 &mdash; Event Discovery</h2>
<p>This flow covers how a user searches for, browses, and views event details.</p>

<div class="mermaid">
flowchart LR
    Client["ðŸ‘¤ Client<br/>(Web / Mobile)"]
    LB["Load Balancer<br/>(L7)"]
    GW["API Gateway"]
    ES["Event Service"]
    Cache[("Cache")]
    EDB[("Event DB<br/>(NoSQL)")]
    SS["Search Service"]
    SI[("Search Index<br/>(Inverted Index)")]
    OS[("Object Storage")]
    CDN["CDN"]

    Client --> LB --> GW --> ES
    ES --> Cache
    Cache -.->|cache miss| EDB
    ES --> SS
    SS --> SI
    ES -->|images, seating SVG| OS
    OS --> CDN
    CDN -->|static assets| Client
</div>

<h3>Examples</h3>

<div class="example">
    <strong>Example 1 &mdash; Keyword Search:</strong> A user types "Taylor Swift Los Angeles" into the search bar. The client sends <code>GET /api/v1/events?q=taylor+swift&location=los+angeles</code> to the Load Balancer, which forwards it through the API Gateway to the Event Service. The Event Service delegates the query to the Search Service, which executes a full-text search on the inverted Search Index. Matching events are returned ranked by relevance and date. The Event Service enriches results with cached event metadata (venue name, thumbnail URL, price range) from the Cache, falling back to the Event DB (NoSQL) on a cache miss. Thumbnail images are served from the CDN. The user sees a paginated list of matching events.
</div>

<div class="example">
    <strong>Example 2 &mdash; Browse by Category:</strong> A user navigates to "Sports â†’ NBA â†’ Nearby" on the home screen. The client sends <code>GET /api/v1/events?category=sports&subcategory=nba&lat=34.05&lng=-118.24&radius=50mi</code>. The Event Service checks the Cache for this popular query. On a cache hit, results return in &lt;50ms. On a cache miss, it queries the Event DB with a geospatial filter and caches the result with a 2-minute TTL.
</div>

<div class="example">
    <strong>Example 3 &mdash; View Event Details:</strong> A user clicks on "Lakers vs. Celtics â€“ Dec 25." The client sends <code>GET /api/v1/events/{event_id}</code>. The Event Service fetches the event record from Cache (or Event DB on miss) and returns full details: venue, date/time, pricing tiers, description, and a URL to the seating chart SVG hosted on the CDN. The venue map/seating SVG is a static asset loaded directly from the CDN by the client browser.
</div>

<h3>Component Deep Dive</h3>

<h4>Client (Web / Mobile)</h4>
<p>A web application (React, etc.) or native mobile app (iOS / Android). Communicates with the backend exclusively through the API Gateway via HTTPS. Static assets (images, seating chart SVGs, JS bundles) are loaded from the CDN.</p>

<h4>Load Balancer (L7)</h4>
<p>An application-layer (Layer 7) load balancer sitting in front of the API Gateway. It distributes incoming HTTP/HTTPS requests using round-robin or least-connections strategies. It also performs TLS termination, health checking of downstream API Gateway instances, and connection draining during deployments. For the Event Discovery flow, it simply routes read traffic evenly.</p>

<h4>API Gateway</h4>
<p>A single entry point for all client requests. Responsibilities include: request routing to the appropriate microservice, authentication/authorization (JWT validation), rate limiting, request/response transformation, and logging. For high-demand events, the gateway enforces stricter rate limits to prevent abuse.</p>

<h4>Event Service</h4>
<p>Owns the event catalog. Handles CRUD operations for events and serves event detail queries.</p>
<table>
    <tr><th>Endpoint</th><th>Method</th><th>Input</th><th>Output</th></tr>
    <tr>
        <td><code>/api/v1/events</code></td>
        <td><span class="tag">GET</span></td>
        <td>Query params: <code>q</code>, <code>category</code>, <code>location</code>, <code>date_from</code>, <code>date_to</code>, <code>page</code>, <code>page_size</code></td>
        <td>Paginated list of event summaries (id, name, date, venue, thumbnail_url, price_range)</td>
    </tr>
    <tr>
        <td><code>/api/v1/events/{event_id}</code></td>
        <td><span class="tag">GET</span></td>
        <td>Path param: <code>event_id</code></td>
        <td>Full event detail (name, description, venue, seating_chart_url, pricing_tiers, date, status)</td>
    </tr>
</table>
<p>Protocol: HTTP/HTTPS (REST). The service first checks the in-memory Cache for event data. On a cache miss, it reads from the Event DB (NoSQL) and populates the cache. For search queries, it delegates to the Search Service.</p>

<h4>Search Service</h4>
<p>Maintains an inverted index of event data (name, description, artist/team, venue, location) for full-text search with relevance ranking. Also supports faceted search (filter by category, date range, price range, location). The index is updated asynchronously when events are created or modified (via an internal event/message from the Event Service).</p>
<table>
    <tr><th>Endpoint</th><th>Method</th><th>Input</th><th>Output</th></tr>
    <tr>
        <td><code>/internal/search</code></td>
        <td><span class="tag">POST</span></td>
        <td>JSON body: <code>{ query, filters, page, page_size }</code></td>
        <td>Ranked list of event IDs with relevance scores</td>
    </tr>
</table>
<p>Protocol: HTTP (internal RPC). Not directly exposed to clients; only called by the Event Service.</p>

<h4>Cache (In-Memory)</h4>
<p>Stores frequently accessed event details and popular search results. See the <a href="#cdn-cache">CDN &amp; Caching Strategy</a> section for full details on strategies, eviction, and expiration.</p>

<h4>Event DB (NoSQL)</h4>
<p>A document-oriented NoSQL database storing the event catalog. Chosen for: flexible schema (different event types have different attributes), horizontal scalability, and high read throughput. See <a href="#schema">Database Schema</a> for details.</p>

<h4>Search Index (Inverted Index)</h4>
<p>A purpose-built full-text search index supporting tokenization, stemming, fuzzy matching, and geospatial queries. Updated asynchronously when events change. See <a href="#schema">Database Schema</a> for indexed fields.</p>

<h4>Object Storage</h4>
<p>Stores binary assets: event poster images, venue photos, and seating chart SVG/PNG files. Objects are immutable and addressed by content hash. The CDN pulls from object storage as its origin.</p>

<h4>CDN (Content Delivery Network)</h4>
<p>Caches and serves static assets (images, seating chart SVGs, JS/CSS bundles) from edge locations close to users. Reduces latency and offloads traffic from the origin (Object Storage). Assets are cache-busted via versioned URLs.</p>

<!-- ================================================================== -->
<!-- FLOW 2: TICKET BOOKING -->
<!-- ================================================================== -->
<h2 id="flow2">5. Flow 2 &mdash; Ticket Booking</h2>
<p>This flow covers the complete ticket purchase journey: joining the waiting queue (for high-demand events), selecting seats, temporary reservation, payment, and booking confirmation.</p>

<div class="mermaid">
flowchart TD
    Client["ðŸ‘¤ Client<br/>(Web / Mobile)"]
    LB["Load Balancer<br/>(L7)"]
    GW["API Gateway"]
    QS["Queue Service"]
    BS["Booking Service"]
    BDB[("Booking DB<br/>(SQL)")]
    PS["Payment Service"]
    PGW["Payment Gateway<br/>(External)"]
    MQ["Message Queue"]
    NS["Notification Service"]
    ResCleaner["Reservation Cleanup<br/>(Scheduled Job)"]

    Client -->|"1. POST /queue/join"| LB
    LB --> GW --> QS
    QS -.->|"2. SSE: position updates<br/>& booking token"| Client
    Client -->|"3. GET /seats"| LB
    LB --> GW --> BS
    BS --> BDB
    Client -->|"4. POST /reserve"| LB
    LB --> GW --> BS
    BS -->|"SELECT FOR UPDATE<br/>+ INSERT reservation"| BDB
    Client -->|"5. POST /pay"| LB
    LB --> GW --> PS
    PS --> PGW
    PS -->|"payment result"| BS
    BS -->|"UPDATE booking confirmed"| BDB
    BS -->|"6. enqueue confirmation"| MQ
    MQ -->|"7. dequeue"| NS
    NS -.->|"email / push"| Client
    ResCleaner -->|"expire stale reservations"| BDB
</div>

<h3>Examples</h3>

<div class="example">
    <strong>Example 1 &mdash; Happy Path (High-Demand Event with Queue):</strong> User1 clicks "Buy Tickets" for "BeyoncÃ© â€“ World Tour" which is a high-demand event. The client sends <code>POST /api/v1/events/{event_id}/queue/join</code>. The Queue Service assigns User1 a randomized position (#4,823 of 150,000) and opens an SSE (Server-Sent Events) connection to push real-time position updates. After ~8 minutes, User1 reaches the front and receives a time-limited <em>booking token</em> (valid for 10 minutes). The client now calls <code>GET /api/v1/events/{event_id}/seats?token={booking_token}</code> which the Booking Service validates and returns the live seat availability map. User1 selects 2 seats in Section 102. The client sends <code>POST /api/v1/events/{event_id}/reserve</code> with <code>{ token, seat_ids: [S102-R5-14, S102-R5-15] }</code>. The Booking Service begins a database transaction: it executes <code>SELECT ... FOR UPDATE</code> on those seat rows, verifies status is AVAILABLE, sets status to RESERVED with a 10-minute expiry, inserts a reservation record, and commits. The client receives a reservation confirmation with a 10-minute countdown. User1 enters payment info and the client sends <code>POST /api/v1/payments</code> with <code>{ reservation_id, payment_method }</code>. The Payment Service authorizes the charge via the external Payment Gateway and notifies the Booking Service. The Booking Service updates the seat status to BOOKED, converts the reservation to a confirmed booking, and enqueues a confirmation message on the Message Queue. The Notification Service dequeues the message and sends User1 an email and push notification with a QR-code ticket.
</div>

<div class="example">
    <strong>Example 2 &mdash; Reservation Timeout:</strong> User2 selects seats and gets a reservation but gets distracted and does not complete payment within 10 minutes. The Reservation Cleanup Job (runs every 30 seconds) finds the expired reservation, sets the seat status back to AVAILABLE, and marks the reservation as EXPIRED. The seats become available for other users. If User2 tries to pay after timeout, the Payment Service finds no valid reservation and returns <code>409 Conflict</code> with an error message "Reservation expired."
</div>

<div class="example">
    <strong>Example 3 &mdash; Seat Already Taken (Race Condition):</strong> User3 and User4 both attempt to reserve seat S205-R1-8 at nearly the same instant. User3's request arrives first; the Booking Service acquires a row-level lock via <code>SELECT FOR UPDATE</code>. User4's transaction blocks waiting for the lock. User3's transaction commits (seat â†’ RESERVED). User4's transaction now reads the row, sees status is RESERVED, and returns <code>409 Conflict: Seat no longer available</code>. No double-booking occurs.
</div>

<div class="example">
    <strong>Example 4 &mdash; Payment Failure:</strong> User5 reserves 2 seats successfully but their credit card is declined. The Payment Service returns a failure to the client. The reservation remains active until its 10-minute TTL expires (User5 can retry with a different card). If User5 does not retry in time, the Reservation Cleanup Job releases the seats. The Booking Service never transitions the booking to CONFIRMED.
</div>

<div class="example">
    <strong>Example 5 &mdash; Low-Demand Event (No Queue):</strong> User6 wants tickets for a local comedy show that is not high-demand. The event is configured with <code>queue_enabled = false</code>. The client skips the queue step entirely and goes straight to <code>GET /api/v1/events/{event_id}/seats</code>, then follows the same reserve â†’ pay â†’ confirm flow as above. No booking token is required.
</div>

<h3>Component Deep Dive</h3>

<h4>Queue Service (Virtual Waiting Room)</h4>
<p>Activated only for high-demand events (configurable per event). When a user joins the queue, the service:</p>
<ol>
    <li><strong>Assigns a randomized position</strong> &mdash; users joining within the same time window (e.g., 30 seconds before on-sale) are shuffled randomly. This prevents bots from gaining advantage by connecting microseconds earlier.</li>
    <li><strong>Opens an SSE connection</strong> &mdash; the server pushes periodic updates: <code>{ position, estimated_wait_seconds }</code>.</li>
    <li><strong>Batch release</strong> &mdash; the Queue Service releases users in batches (e.g., 500 users every 5 seconds) based on the Booking Service's capacity. This acts as a flow-control valve to prevent overwhelming the Booking Service.</li>
    <li><strong>Issues a booking token</strong> &mdash; when the user reaches the front, the SSE stream sends a JWT booking token containing <code>{ user_id, event_id, exp (10 min), nonce }</code>. The Booking Service validates this token before allowing seat selection or reservation.</li>
</ol>
<table>
    <tr><th>Endpoint</th><th>Method</th><th>Input</th><th>Output</th></tr>
    <tr>
        <td><code>/api/v1/events/{event_id}/queue/join</code></td>
        <td><span class="tag">POST</span></td>
        <td>Path: <code>event_id</code>. Header: Auth JWT.</td>
        <td>SSE stream with position updates + eventual booking token</td>
    </tr>
</table>
<p><strong>Why SSE over WebSockets?</strong> The communication is unidirectional (server â†’ client). SSE is simpler, auto-reconnects, works over standard HTTP, requires no protocol upgrade, and is natively supported by browsers. WebSockets would be overkill since the client does not need to send data back through this channel.</p>
<p><strong>Why SSE over Long Polling?</strong> Long polling requires repeated HTTP request/response cycles, each with TCP handshake and header overhead. SSE uses a single persistent connection, is more efficient for frequent updates, and provides lower latency.</p>

<h4>Booking Service</h4>
<p>The core transactional service. Owns the seats, reservations, and bookings data. Enforces the no-double-booking invariant via database-level pessimistic locking.</p>
<table>
    <tr><th>Endpoint</th><th>Method</th><th>Input</th><th>Output</th></tr>
    <tr>
        <td><code>/api/v1/events/{event_id}/seats</code></td>
        <td><span class="tag">GET</span></td>
        <td>Path: <code>event_id</code>. Optional query: <code>section</code>, <code>price_tier</code>. Header: booking token (if queue-enabled).</td>
        <td>List of seats with status (AVAILABLE / RESERVED / BOOKED) and price</td>
    </tr>
    <tr>
        <td><code>/api/v1/events/{event_id}/reserve</code></td>
        <td><span class="tag">POST</span></td>
        <td>Body: <code>{ seat_ids[], token? }</code></td>
        <td><code>{ reservation_id, expires_at, total_price }</code></td>
    </tr>
    <tr>
        <td><code>/api/v1/events/{event_id}/bookings</code></td>
        <td><span class="tag">GET</span></td>
        <td>Header: Auth JWT (user_id extracted).</td>
        <td>List of confirmed bookings for the user</td>
    </tr>
</table>
<p>Protocol: HTTP/HTTPS (REST). The reserve endpoint begins a SQL transaction with <code>SELECT FOR UPDATE</code> on the requested seat rows, which acquires row-level exclusive locks. If any seat is not AVAILABLE, the transaction rolls back and a <code>409 Conflict</code> is returned. Otherwise, the seats are set to RESERVED, a reservation record is inserted with a 10-minute expiry, and the transaction commits.</p>

<h4>Payment Service</h4>
<table>
    <tr><th>Endpoint</th><th>Method</th><th>Input</th><th>Output</th></tr>
    <tr>
        <td><code>/api/v1/payments</code></td>
        <td><span class="tag">POST</span></td>
        <td>Body: <code>{ reservation_id, payment_method, idempotency_key }</code></td>
        <td><code>{ payment_id, status (SUCCESS/FAILED), booking_id? }</code></td>
    </tr>
</table>
<p>Protocol: HTTP/HTTPS (REST). The service validates the reservation is still active (not expired), calculates the total, and charges the user via the external Payment Gateway. The <code>idempotency_key</code> ensures that retried requests don't double-charge. On success, it calls the Booking Service internally to confirm the booking.</p>

<h4>Payment Gateway (External)</h4>
<p>A third-party payment processor (e.g., Stripe, Adyen). The Payment Service communicates with it via HTTPS REST APIs. This is outside our system boundary.</p>

<h4>Booking DB (SQL)</h4>
<p>A relational SQL database chosen for ACID transactions, which are essential for preventing double-selling. Stores seats, reservations, bookings, and payments. See <a href="#schema">Database Schema</a> for full details.</p>

<h4>Message Queue</h4>
<p>An asynchronous message queue that decouples the booking confirmation from downstream processing (notifications, analytics, ticket generation). When a booking is confirmed, the Booking Service enqueues a message: <code>{ type: "BOOKING_CONFIRMED", booking_id, user_id, event_id, seats }</code>.</p>
<p><strong>Why a Message Queue?</strong> Sending email/push notifications and generating PDF/QR tickets are slow, non-critical-path operations. Making them synchronous would increase booking latency and create tight coupling. The queue provides:</p>
<ul>
    <li><strong>Decoupling:</strong> Booking Service doesn't need to know about notification logic.</li>
    <li><strong>Reliability:</strong> If the Notification Service is temporarily down, messages are retained and processed later (at-least-once delivery).</li>
    <li><strong>Backpressure:</strong> During flash sales, thousands of confirmations can be enqueued and processed at a sustainable rate.</li>
</ul>
<p><strong>Why not Pub/Sub?</strong> A message queue with point-to-point semantics is sufficient here because there is a single consumer group (Notification Service). If we needed multiple independent consumers (e.g., analytics + notification + ticket-gen each processing independently), Pub/Sub with topics would be more appropriate. In practice, we could use a Pub/Sub system with multiple subscriber groups if needed in the future.</p>
<p><strong>How messages are produced/consumed:</strong> The Booking Service publishes a message after committing the booking transaction. The Notification Service has a pool of consumers that poll/pull messages from the queue, process them (send email/push), and acknowledge. Failed processing triggers a retry with exponential backoff; after max retries, the message moves to a dead-letter queue for manual inspection.</p>

<h4>Notification Service</h4>
<p>Consumes messages from the Message Queue. Sends booking confirmations via email (SMTP) and push notifications (APNs for iOS, FCM for Android). Also generates the digital ticket with a QR code/barcode and attaches it to the email.</p>

<h4>Reservation Cleanup Job</h4>
<p>A scheduled background job that runs every 30 seconds. Queries the reservations table for records where <code>status = RESERVED AND expires_at < NOW()</code>. For each expired reservation, it sets the seat status back to AVAILABLE and marks the reservation as EXPIRED within a single transaction. This ensures seats aren't held indefinitely by abandoned sessions.</p>

<!-- ================================================================== -->
<!-- FLOW 3: EVENT MANAGEMENT -->
<!-- ================================================================== -->
<h2 id="flow3">6. Flow 3 &mdash; Event Management (Organizer)</h2>
<p>This flow covers how an event organizer creates and manages events.</p>

<div class="mermaid">
flowchart LR
    Org["ðŸŽ¤ Organizer<br/>(Web Dashboard)"]
    LB["Load Balancer<br/>(L7)"]
    GW["API Gateway"]
    ES["Event Service"]
    EDB[("Event DB<br/>(NoSQL)")]
    OS[("Object Storage")]
    CDN["CDN"]
    BS["Booking Service"]
    BDB[("Booking DB<br/>(SQL)")]
    SS["Search Service"]
    SI[("Search Index")]
    Cache[("Cache")]

    Org -->|"1. POST /events"| LB --> GW --> ES
    ES -->|"2. store event"| EDB
    ES -->|"3. upload images & seating chart"| OS --> CDN
    ES -->|"4. create seat inventory"| BS --> BDB
    ES -->|"5. index event"| SS --> SI
    ES -->|"6. invalidate / warm"| Cache
</div>

<h3>Examples</h3>

<div class="example">
    <strong>Example 1 &mdash; Create a New Concert:</strong> An organizer logs into the web dashboard and fills out the event creation form: "BeyoncÃ© â€“ World Tour, Venue: SoFi Stadium, Date: Aug 15, 2026, Pricing: Floor $350, Lower Bowl $200, Upper Deck $85." They upload a poster image and the venue seating chart SVG. The dashboard sends <code>POST /api/v1/events</code> with the event metadata + multipart file uploads. The Event Service: (a) stores the event document in the Event DB (NoSQL), (b) uploads the image and seating SVG to Object Storage (which is fronted by the CDN), (c) calls the Booking Service to create the seat inventory â€” the Booking Service generates rows in the <code>seats</code> SQL table for every seat in the venue based on the venue template (section, row, seat number, price tier, status=AVAILABLE), (d) sends the event data to the Search Service for indexing, and (e) warms the cache with the new event details.
</div>

<div class="example">
    <strong>Example 2 &mdash; Update Event Details:</strong> The organizer changes the event date from Aug 15 to Aug 22 and updates the poster image. The dashboard sends <code>PUT /api/v1/events/{event_id}</code> with the updated fields. The Event Service updates the Event DB, uploads the new image to Object Storage (new versioned URL), invalidates the old cache entry and writes the new version, and re-indexes the event in the Search Service.
</div>

<div class="example">
    <strong>Example 3 &mdash; Cancel an Event:</strong> The organizer cancels an event. <code>PATCH /api/v1/events/{event_id}</code> with <code>{ status: "CANCELLED" }</code>. The Event Service updates the event status, triggers the Booking Service to process refunds for all confirmed bookings (enqueued via Message Queue for async processing), removes the event from the Search Index, and invalidates the cache entry.
</div>

<h3>Component Deep Dive</h3>

<h4>Event Service (Organizer Endpoints)</h4>
<table>
    <tr><th>Endpoint</th><th>Method</th><th>Input</th><th>Output</th></tr>
    <tr>
        <td><code>/api/v1/events</code></td>
        <td><span class="tag">POST</span></td>
        <td>Body: <code>{ name, description, venue_id, date, pricing_tiers[], images (multipart), seating_chart (multipart), queue_enabled }</code></td>
        <td><code>{ event_id, status: "CREATED" }</code></td>
    </tr>
    <tr>
        <td><code>/api/v1/events/{event_id}</code></td>
        <td><span class="tag">PUT</span></td>
        <td>Body: Updated event fields</td>
        <td><code>{ event_id, status: "UPDATED" }</code></td>
    </tr>
    <tr>
        <td><code>/api/v1/events/{event_id}</code></td>
        <td><span class="tag">PATCH</span></td>
        <td>Body: <code>{ status: "CANCELLED" }</code> or partial fields</td>
        <td><code>{ event_id, status }</code></td>
    </tr>
</table>
<p>Protocol: HTTP/HTTPS (REST). The POST and PUT endpoints are authenticated for organizer roles only. When seat inventory is created, the Event Service makes an internal synchronous call to the Booking Service <code>POST /internal/events/{event_id}/seats/initialize</code> with the venue template and pricing tiers.</p>

<p>All other components in this flow (Load Balancer, API Gateway, Cache, Object Storage, CDN, Search Service, Booking Service, Booking DB) behave identically to their descriptions in Flows 1 and 2.</p>

<!-- ================================================================== -->
<!-- COMBINED DIAGRAM -->
<!-- ================================================================== -->
<h2 id="combined">7. Combined System Diagram</h2>
<p>This diagram unifies all three flows into a single architecture view.</p>

<div class="mermaid">
flowchart TD
    subgraph Clients
        User["ðŸ‘¤ User<br/>(Web / Mobile)"]
        Org["ðŸŽ¤ Organizer<br/>(Web Dashboard)"]
    end

    LB["Load Balancer (L7)"]
    GW["API Gateway<br/>(Auth, Rate Limit, Routing)"]

    subgraph Core Services
        ES["Event Service"]
        BS["Booking Service"]
        QS["Queue Service"]
        PS["Payment Service"]
        NS["Notification Service"]
        SS["Search Service"]
    end

    subgraph Data Stores
        EDB[("Event DB<br/>(NoSQL)")]
        BDB[("Booking DB<br/>(SQL)")]
        SI[("Search Index<br/>(Inverted Index)")]
        Cache[("Cache<br/>(In-Memory)")]
    end

    subgraph External / Infra
        PGW["Payment Gateway<br/>(External)"]
        MQ[["Message Queue"]]
        OS[("Object Storage")]
        CDN["CDN"]
        ResCleaner["Reservation<br/>Cleanup Job"]
    end

    User --> LB
    Org --> LB
    LB --> GW
    GW --> ES
    GW --> BS
    GW --> QS
    GW --> PS

    ES --> EDB
    ES --> Cache
    ES --> SS
    ES --> OS
    SS --> SI
    OS --> CDN
    CDN --> User

    QS -.->|SSE| User
    QS -->|token validated by| BS

    BS --> BDB
    PS --> PGW
    PS --> BS

    BS --> MQ
    MQ --> NS
    NS -.->|email / push| User

    ResCleaner --> BDB

    ES -->|create seats| BS
</div>

<h3>Combined Flow Examples</h3>

<div class="example">
    <strong>End-to-End Example &mdash; Organizer Creates Event, User Buys Ticket:</strong><br/><br/>
    <strong>Phase 1 (Organizer):</strong> The organizer creates "Adele â€“ Las Vegas Residency" via <code>POST /api/v1/events</code>. The Event Service stores the event in the Event DB (NoSQL), uploads the poster to Object Storage (served via CDN), instructs the Booking Service to generate the seat inventory in the Booking DB (SQL), indexes the event in the Search Service, and warms the cache.<br/><br/>
    <strong>Phase 2 (Discovery):</strong> Three days later, User7 searches "Adele Las Vegas" via <code>GET /api/v1/events?q=adele+las+vegas</code>. The Event Service delegates to the Search Service, which returns the event. User7 clicks into the event detail page, loading metadata from cache and the venue image from CDN.<br/><br/>
    <strong>Phase 3 (Booking):</strong> User7 clicks "Buy Tickets." Since this is a high-demand event, the Queue Service places them in a virtual waiting room. After waiting, User7 receives a booking token via SSE. They view available seats via the Booking Service, select Section A Row 3 Seat 7, and the Booking Service creates a 10-minute reservation using <code>SELECT FOR UPDATE</code>. User7 completes payment via the Payment Service â†’ Payment Gateway. The Booking Service confirms the booking, enqueues a notification message, and the Notification Service sends an email with a QR-code ticket.
</div>

<div class="example">
    <strong>End-to-End Example &mdash; Flash Sale Surge:</strong><br/><br/>
    2 million users simultaneously attempt to buy tickets for "FIFA World Cup Final." The Load Balancer distributes traffic across API Gateway instances. The API Gateway applies rate limiting (e.g., 10K req/sec per event). The Queue Service absorbs all 2M users, assigning randomized positions. It releases batches of 500 users every 3 seconds to the Booking Service (controlled flow rate of ~167 users/sec). Each batch of users views seats, selects, and reserves. The Booking Service handles concurrent reservations via row-level locking on the SQL database. The Message Queue buffers thousands of confirmation messages and the Notification Service processes them at its own pace. Total sell-out occurs within ~40 minutes without system degradation.
</div>

<!-- ================================================================== -->
<!-- DATABASE SCHEMA -->
<!-- ================================================================== -->
<h2 id="schema">8. Database Schema</h2>

<h3>SQL Tables (Booking DB)</h3>
<p>SQL was chosen for all booking-related tables because <strong>ACID transactions</strong> are essential to prevent double-selling seats. The core invariant â€” a seat can only be sold once â€” requires serializable transaction isolation, which SQL databases provide natively with row-level locking.</p>

<h4>Table: <code>seats</code></h4>
<p><strong>Purpose:</strong> Represents every bookable seat for every event. Pre-populated when an event is created based on the venue template.</p>
<table>
    <tr><th>Column</th><th>Type</th><th>Key</th><th>Description</th></tr>
    <tr><td><code>seat_id</code></td><td>UUID</td><td>PK</td><td>Unique identifier for the seat-event combination</td></tr>
    <tr><td><code>event_id</code></td><td>UUID</td><td>FK (logical, to NoSQL event)</td><td>The event this seat belongs to</td></tr>
    <tr><td><code>section</code></td><td>VARCHAR</td><td></td><td>Section name (e.g., "Floor", "Section 102")</td></tr>
    <tr><td><code>row</code></td><td>VARCHAR</td><td></td><td>Row identifier</td></tr>
    <tr><td><code>seat_number</code></td><td>VARCHAR</td><td></td><td>Seat number within the row</td></tr>
    <tr><td><code>price_cents</code></td><td>BIGINT</td><td></td><td>Price in cents to avoid floating-point issues</td></tr>
    <tr><td><code>price_tier</code></td><td>VARCHAR</td><td></td><td>Pricing tier label</td></tr>
    <tr><td><code>status</code></td><td>ENUM</td><td></td><td>AVAILABLE / RESERVED / BOOKED</td></tr>
    <tr><td><code>version</code></td><td>INT</td><td></td><td>Optimistic concurrency version (backup mechanism)</td></tr>
    <tr><td><code>created_at</code></td><td>TIMESTAMP</td><td></td><td>When the seat record was created</td></tr>
    <tr><td><code>updated_at</code></td><td>TIMESTAMP</td><td></td><td>Last update timestamp</td></tr>
</table>
<p><strong>Indexes:</strong></p>
<ul>
    <li><strong>B-tree index on <code>(event_id, status)</code>:</strong> The most frequent query is "get all available seats for event X" â†’ <code>WHERE event_id = ? AND status = 'AVAILABLE'</code>. A composite B-tree index makes this an efficient range scan. B-tree is chosen over hash because we need range queries (filtering by status) and the index supports ordered retrieval by section.</li>
    <li><strong>B-tree index on <code>(event_id, section, row, seat_number)</code>:</strong> For looking up a specific seat. Enforces uniqueness (each seat is unique within an event).</li>
</ul>
<p><strong>Sharding:</strong> Sharded by <code>event_id</code>. All seats for a single event reside on the same shard. This is critical because the booking transaction needs to lock multiple seats atomically for a single event &mdash; cross-shard transactions would be prohibitively expensive. This also aligns with the query pattern (all queries are scoped to a single event). The downside is potential hot shards for popular events, mitigated by the Queue Service throttling traffic.</p>
<p><strong>Read/Write triggers:</strong></p>
<ul>
    <li><em>Written when:</em> Event is created (bulk insert all seats), user reserves seats (status â†’ RESERVED), payment succeeds (status â†’ BOOKED), reservation expires (status â†’ AVAILABLE).</li>
    <li><em>Read when:</em> User views the seating chart, Booking Service checks seat availability before reserving.</li>
</ul>

<h4>Table: <code>reservations</code></h4>
<p><strong>Purpose:</strong> Tracks temporary seat holds with an expiration time.</p>
<table>
    <tr><th>Column</th><th>Type</th><th>Key</th><th>Description</th></tr>
    <tr><td><code>reservation_id</code></td><td>UUID</td><td>PK</td><td>Unique reservation identifier</td></tr>
    <tr><td><code>event_id</code></td><td>UUID</td><td>Index</td><td>The event</td></tr>
    <tr><td><code>user_id</code></td><td>UUID</td><td>FK â†’ users</td><td>The user who made the reservation</td></tr>
    <tr><td><code>seat_ids</code></td><td>UUID[]</td><td></td><td>Array of reserved seat IDs</td></tr>
    <tr><td><code>status</code></td><td>ENUM</td><td></td><td>PENDING / CONFIRMED / EXPIRED / CANCELLED</td></tr>
    <tr><td><code>total_price_cents</code></td><td>BIGINT</td><td></td><td>Total price for all seats</td></tr>
    <tr><td><code>expires_at</code></td><td>TIMESTAMP</td><td>Index</td><td>When the reservation expires</td></tr>
    <tr><td><code>created_at</code></td><td>TIMESTAMP</td><td></td><td>When reservation was created</td></tr>
</table>
<p><strong>Indexes:</strong></p>
<ul>
    <li><strong>B-tree index on <code>(status, expires_at)</code>:</strong> The Reservation Cleanup Job queries <code>WHERE status = 'PENDING' AND expires_at < NOW()</code>. A composite B-tree index allows efficient scanning of expired reservations. B-tree is chosen because the query involves a range condition on <code>expires_at</code>.</li>
    <li><strong>B-tree index on <code>(user_id, event_id)</code>:</strong> To look up a user's reservation for a specific event and enforce a limit (e.g., max 6 seats per user per event).</li>
</ul>
<p><strong>Sharding:</strong> Sharded by <code>event_id</code> (same shard as the seats table for the same event, ensuring transactional co-location).</p>
<p><strong>Read/Write triggers:</strong></p>
<ul>
    <li><em>Written when:</em> User reserves seats (INSERT), payment succeeds (status â†’ CONFIRMED), reservation expires (status â†’ EXPIRED).</li>
    <li><em>Read when:</em> Payment Service validates reservation is active, Cleanup Job scans for expired reservations.</li>
</ul>

<h4>Table: <code>bookings</code></h4>
<p><strong>Purpose:</strong> Stores confirmed ticket purchases. This is the permanent record of a completed transaction.</p>
<table>
    <tr><th>Column</th><th>Type</th><th>Key</th><th>Description</th></tr>
    <tr><td><code>booking_id</code></td><td>UUID</td><td>PK</td><td>Unique booking identifier</td></tr>
    <tr><td><code>reservation_id</code></td><td>UUID</td><td>FK â†’ reservations</td><td>Source reservation</td></tr>
    <tr><td><code>event_id</code></td><td>UUID</td><td>Index</td><td>The event</td></tr>
    <tr><td><code>user_id</code></td><td>UUID</td><td>FK â†’ users, Index</td><td>The buyer</td></tr>
    <tr><td><code>seat_ids</code></td><td>UUID[]</td><td></td><td>Booked seats</td></tr>
    <tr><td><code>total_price_cents</code></td><td>BIGINT</td><td></td><td>Total amount charged</td></tr>
    <tr><td><code>payment_id</code></td><td>UUID</td><td>FK â†’ payments</td><td>Associated payment record</td></tr>
    <tr><td><code>status</code></td><td>ENUM</td><td></td><td>CONFIRMED / REFUNDED</td></tr>
    <tr><td><code>qr_code_url</code></td><td>VARCHAR</td><td></td><td>URL to the generated QR code in Object Storage</td></tr>
    <tr><td><code>created_at</code></td><td>TIMESTAMP</td><td></td><td>When the booking was confirmed</td></tr>
</table>
<p><strong>Indexes:</strong></p>
<ul>
    <li><strong>B-tree index on <code>user_id</code>:</strong> For the "My Tickets" page where a user views their bookings: <code>WHERE user_id = ?</code>. B-tree supports the <code>ORDER BY created_at DESC</code> for showing recent bookings first.</li>
    <li><strong>B-tree index on <code>event_id</code>:</strong> For organizer analytics: "how many tickets sold for my event?"</li>
</ul>
<p><strong>Sharding:</strong> Sharded by <code>event_id</code> for consistency with the seats and reservations tables. For the "My Tickets" query (by user_id), a scatter-gather across shards is needed, but this is an infrequent, low-priority query that can tolerate slightly higher latency. An alternative is to maintain a denormalized <code>user_bookings</code> lookup table sharded by <code>user_id</code>.</p>
<p><strong>Denormalization note:</strong> The <code>seat_ids</code> array is denormalized (duplicated from the seats table) so that the booking record is self-contained for read queries. This avoids a JOIN with the seats table when displaying a user's tickets. The trade-off is slightly increased storage and the need to keep both in sync, but since bookings are immutable (write-once), there's no ongoing sync concern.</p>

<p><strong>Read/Write triggers:</strong></p>
<ul>
    <li><em>Written when:</em> Payment succeeds (INSERT), event cancelled (status â†’ REFUNDED).</li>
    <li><em>Read when:</em> User views "My Tickets" page, organizer views sales dashboard.</li>
</ul>

<h4>Table: <code>payments</code></h4>
<table>
    <tr><th>Column</th><th>Type</th><th>Key</th><th>Description</th></tr>
    <tr><td><code>payment_id</code></td><td>UUID</td><td>PK</td><td>Unique payment identifier</td></tr>
    <tr><td><code>reservation_id</code></td><td>UUID</td><td>FK â†’ reservations</td><td>Associated reservation</td></tr>
    <tr><td><code>user_id</code></td><td>UUID</td><td>FK â†’ users</td><td>The payer</td></tr>
    <tr><td><code>amount_cents</code></td><td>BIGINT</td><td></td><td>Amount charged</td></tr>
    <tr><td><code>currency</code></td><td>VARCHAR(3)</td><td></td><td>Currency code (e.g., USD)</td></tr>
    <tr><td><code>status</code></td><td>ENUM</td><td></td><td>PENDING / SUCCESS / FAILED / REFUNDED</td></tr>
    <tr><td><code>idempotency_key</code></td><td>VARCHAR</td><td>Unique Index</td><td>Client-generated key for idempotent retries</td></tr>
    <tr><td><code>gateway_transaction_id</code></td><td>VARCHAR</td><td></td><td>External payment gateway reference</td></tr>
    <tr><td><code>created_at</code></td><td>TIMESTAMP</td><td></td><td>When payment was initiated</td></tr>
    <tr><td><code>updated_at</code></td><td>TIMESTAMP</td><td></td><td>Last status update</td></tr>
</table>
<p><strong>Indexes:</strong></p>
<ul>
    <li><strong>Hash index on <code>idempotency_key</code> (UNIQUE):</strong> For fast exact-match lookups to detect duplicate payment requests. Hash index is chosen over B-tree because we only need equality checks, never range queries. The uniqueness constraint prevents double-charging.</li>
    <li><strong>B-tree index on <code>reservation_id</code>:</strong> To quickly find the payment for a given reservation.</li>
</ul>
<p><strong>Sharding:</strong> Sharded by <code>reservation_id</code> (which correlates to event_id), co-located with reservations.</p>
<p><strong>Read/Write triggers:</strong></p>
<ul>
    <li><em>Written when:</em> User initiates payment (INSERT with PENDING), payment gateway responds (UPDATE status), refund processed (UPDATE status to REFUNDED).</li>
    <li><em>Read when:</em> Checking idempotency (on payment request), Booking Service confirming payment succeeded.</li>
</ul>

<h4>Table: <code>users</code></h4>
<table>
    <tr><th>Column</th><th>Type</th><th>Key</th><th>Description</th></tr>
    <tr><td><code>user_id</code></td><td>UUID</td><td>PK</td><td>Unique user identifier</td></tr>
    <tr><td><code>email</code></td><td>VARCHAR</td><td>Unique Index</td><td>User's email address</td></tr>
    <tr><td><code>password_hash</code></td><td>VARCHAR</td><td></td><td>Bcrypt/Argon2 hashed password</td></tr>
    <tr><td><code>name</code></td><td>VARCHAR</td><td></td><td>Display name</td></tr>
    <tr><td><code>phone</code></td><td>VARCHAR</td><td></td><td>Phone number (for push notifications)</td></tr>
    <tr><td><code>role</code></td><td>ENUM</td><td></td><td>USER / ORGANIZER / ADMIN</td></tr>
    <tr><td><code>created_at</code></td><td>TIMESTAMP</td><td></td><td>Registration timestamp</td></tr>
</table>
<p><strong>Indexes:</strong></p>
<ul>
    <li><strong>Hash index on <code>email</code> (UNIQUE):</strong> For login lookups. Exact-match only, so hash is optimal.</li>
</ul>
<p><strong>Sharding:</strong> Sharded by <code>user_id</code>. User lookups are always by user_id (from JWT) or email (at login). Range queries on users are not needed.</p>
<p><strong>Read/Write triggers:</strong></p>
<ul>
    <li><em>Written when:</em> User registers.</li>
    <li><em>Read when:</em> User logs in (by email), any authenticated request (by user_id from JWT â€” though typically the JWT is self-contained and doesn't require a DB lookup per request).</li>
</ul>

<h3>NoSQL Tables (Event DB)</h3>
<p>NoSQL (document database) was chosen for the event catalog because: (a) events have a flexible/variable schema â€” a concert has different attributes than a sports game or a theater show, (b) the event catalog is extremely read-heavy (100:1 read/write ratio), and (c) document databases support horizontal scaling easily, which is needed for serving millions of browse/search queries.</p>

<h4>Collection: <code>events</code></h4>
<table>
    <tr><th>Field</th><th>Type</th><th>Key</th><th>Description</th></tr>
    <tr><td><code>event_id</code></td><td>UUID</td><td>PK (partition key)</td><td>Unique event identifier</td></tr>
    <tr><td><code>name</code></td><td>String</td><td></td><td>Event name</td></tr>
    <tr><td><code>description</code></td><td>String</td><td></td><td>Full description</td></tr>
    <tr><td><code>category</code></td><td>String</td><td></td><td>"concert", "sports", "theater", "comedy"</td></tr>
    <tr><td><code>subcategory</code></td><td>String</td><td></td><td>"pop", "nba", "broadway", etc.</td></tr>
    <tr><td><code>artists</code></td><td>String[]</td><td></td><td>Performing artists/teams</td></tr>
    <tr><td><code>venue</code></td><td>Object</td><td></td><td>Denormalized: <code>{ venue_id, name, address, city, state, country, lat, lng, capacity }</code></td></tr>
    <tr><td><code>date</code></td><td>Timestamp</td><td></td><td>Event date/time</td></tr>
    <tr><td><code>on_sale_date</code></td><td>Timestamp</td><td></td><td>When tickets go on sale</td></tr>
    <tr><td><code>pricing_tiers</code></td><td>Object[]</td><td></td><td><code>[{ tier: "Floor", price_cents: 35000, currency: "USD" }, ...]</code></td></tr>
    <tr><td><code>images</code></td><td>String[]</td><td></td><td>URLs to poster/banner images in Object Storage</td></tr>
    <tr><td><code>seating_chart_url</code></td><td>String</td><td></td><td>URL to the seating chart SVG</td></tr>
    <tr><td><code>status</code></td><td>String</td><td></td><td>"DRAFT", "ON_SALE", "SOLD_OUT", "CANCELLED", "COMPLETED"</td></tr>
    <tr><td><code>queue_enabled</code></td><td>Boolean</td><td></td><td>Whether the virtual waiting queue is active</td></tr>
    <tr><td><code>organizer_id</code></td><td>UUID</td><td></td><td>Reference to the organizer user</td></tr>
    <tr><td><code>tags</code></td><td>String[]</td><td></td><td>Freeform tags for discoverability</td></tr>
    <tr><td><code>created_at</code></td><td>Timestamp</td><td></td><td>Creation timestamp</td></tr>
    <tr><td><code>updated_at</code></td><td>Timestamp</td><td></td><td>Last update timestamp</td></tr>
</table>
<p><strong>Denormalization:</strong> The <code>venue</code> field is denormalized (embedded) within the event document. This eliminates a separate lookup/JOIN to a venues collection for every event detail read &mdash; which would be extremely costly at 100:1 read/write ratio. Since venue information changes very rarely, the denormalization cost (updating venue info across all events for that venue) is low and can be done as a batch operation.</p>
<p><strong>Indexing:</strong></p>
<ul>
    <li>The primary index is on <code>event_id</code> (partition key) for direct lookups.</li>
    <li>A secondary index on <code>date</code> and <code>venue.city</code> for browsing events by date and location.</li>
    <li>Full-text search is handled by the separate Search Index (inverted index), not by database queries. The Search Index indexes: <code>name</code>, <code>description</code>, <code>artists</code>, <code>venue.name</code>, <code>venue.city</code>, <code>category</code>, <code>subcategory</code>, <code>tags</code>.</li>
</ul>
<p><strong>Sharding:</strong> Sharded by <code>event_id</code>. This distributes events evenly across nodes. Queries by event_id are single-shard. Queries by date/location require scatter-gather but are typically served from the cache or Search Index rather than hitting the database directly.</p>
<p><strong>Read/Write triggers:</strong></p>
<ul>
    <li><em>Written when:</em> Organizer creates/updates/cancels an event.</li>
    <li><em>Read when:</em> User views event details (via cache on a hit, directly on a miss), Event Service populates cache.</li>
</ul>

<h4>Collection: <code>venues</code> (Reference Data)</h4>
<table>
    <tr><th>Field</th><th>Type</th><th>Key</th><th>Description</th></tr>
    <tr><td><code>venue_id</code></td><td>UUID</td><td>PK</td><td>Unique venue identifier</td></tr>
    <tr><td><code>name</code></td><td>String</td><td></td><td>Venue name</td></tr>
    <tr><td><code>address</code></td><td>String</td><td></td><td>Street address</td></tr>
    <tr><td><code>city</code></td><td>String</td><td></td><td>City</td></tr>
    <tr><td><code>state</code></td><td>String</td><td></td><td>State/province</td></tr>
    <tr><td><code>country</code></td><td>String</td><td></td><td>Country</td></tr>
    <tr><td><code>lat</code></td><td>Float</td><td></td><td>Latitude</td></tr>
    <tr><td><code>lng</code></td><td>Float</td><td></td><td>Longitude</td></tr>
    <tr><td><code>capacity</code></td><td>Int</td><td></td><td>Total seating capacity</td></tr>
    <tr><td><code>seating_template</code></td><td>Object</td><td></td><td>Template defining sections, rows, seats per row. Used when creating seat inventory for a new event.</td></tr>
</table>
<p>This collection is low-volume reference data (thousands of venues, not millions). It's used primarily when an organizer creates an event â€” the seating template from this collection drives the bulk insert into the <code>seats</code> SQL table.</p>

<!-- ================================================================== -->
<!-- CDN & CACHING -->
<!-- ================================================================== -->
<h2 id="cdn-cache">9. CDN &amp; Caching Strategy</h2>

<h3>CDN</h3>
<p>A CDN is <strong>highly appropriate</strong> for this system. The event discovery flow is extremely read-heavy and serves many static or semi-static assets:</p>
<ul>
    <li><strong>Event images / posters:</strong> Immutable once uploaded. Served with long cache headers (1 year) and cache-busted via versioned URLs (e.g., <code>/images/event_abc_v2.jpg</code>).</li>
    <li><strong>Seating chart SVGs:</strong> The venue layout is static. The SVG defining sections and seat positions is served from CDN. Dynamic seat availability (which seats are taken) is overlaid by the client using data from the Booking Service API â€” the SVG itself doesn't change.</li>
    <li><strong>JS/CSS bundles:</strong> Frontend application code is served from CDN with content-hash filenames.</li>
</ul>
<p>The CDN's origin is the Object Storage. When an asset is requested, the CDN edge server checks its local cache. On a miss, it fetches from Object Storage, caches it, and serves subsequent requests from the edge. This dramatically reduces latency for geographically distributed users and offloads traffic from our origin servers.</p>

<h3>In-Memory Cache</h3>
<p>An in-memory distributed cache sits between the Event Service and the Event DB (NoSQL).</p>

<h4>What is Cached</h4>
<table>
    <tr><th>Cache Key Pattern</th><th>Value</th><th>TTL</th><th>Rationale</th></tr>
    <tr>
        <td><code>event:{event_id}</code></td>
        <td>Full event detail document</td>
        <td>5 minutes</td>
        <td>Event details are read ~100x for every 1 write. Caching eliminates most DB reads. 5-min TTL balances freshness (organizer updates) with cache hit rate.</td>
    </tr>
    <tr>
        <td><code>search:{query_hash}</code></td>
        <td>Paginated event ID list + metadata</td>
        <td>2 minutes</td>
        <td>Popular searches (e.g., "concerts this weekend") are repeated by many users. Short TTL ensures new events appear quickly.</td>
    </tr>
    <tr>
        <td><code>event:{event_id}:availability</code></td>
        <td>Seat availability summary (counts per section/tier)</td>
        <td>10 seconds</td>
        <td>Seat availability changes rapidly during sales. A 10-second TTL provides approximate availability for the browsing view. The actual reservation uses the SQL database (not cache) for consistency.</td>
    </tr>
</table>

<h4>Caching Strategy: Cache-Aside (Lazy Loading)</h4>
<p>The Event Service follows a <strong>cache-aside</strong> pattern:</p>
<ol>
    <li>On a read request, check the cache.</li>
    <li>If cache hit â†’ return cached data.</li>
    <li>If cache miss â†’ query the Event DB (NoSQL), write the result to cache with appropriate TTL, then return.</li>
</ol>
<p><strong>Why cache-aside over write-through?</strong> Write-through would populate the cache on every write, but events are written infrequently and many events are never read again (e.g., past events). Cache-aside avoids polluting the cache with data that may never be requested. The cache only holds data that has been actually requested, optimizing memory usage.</p>
<p><strong>Exception:</strong> When an organizer creates a new high-profile event, the Event Service proactively warms the cache (write-through for this specific case) because we expect an immediate flood of reads.</p>

<h4>Cache Invalidation</h4>
<p>When an organizer updates or cancels an event, the Event Service explicitly deletes the cache entry <code>event:{event_id}</code>. The next read triggers a cache miss and repopulates from the DB. This is a <strong>delete-on-write</strong> invalidation strategy, which is simpler and less error-prone than updating the cache value directly (avoids race conditions between concurrent writes and cache updates).</p>

<h4>Eviction Policy: LRU (Least Recently Used)</h4>
<p>When the cache approaches memory capacity, the least recently used entries are evicted first. This is appropriate because event popularity follows a power-law distribution â€” a small number of "hot" events (upcoming major concerts, sporting events) account for the vast majority of reads. LRU naturally retains these hot entries and evicts long-tail events that receive little traffic.</p>

<h4>Why Not Cache Seat-Level Availability?</h4>
<p>Individual seat availability (is seat X available?) is NOT cached at the seat level. The reason: during an active sale, seat availability changes every second, and serving stale seat data causes poor user experience (selecting a seat that's actually taken). Instead, the actual seat check and reservation is always done transactionally against the SQL database. The only cached availability data is a high-level summary (e.g., "Section 102: 45 seats remaining") with a very short TTL (10s), used for the browse view.</p>

<!-- ================================================================== -->
<!-- SCALING CONSIDERATIONS -->
<!-- ================================================================== -->
<h2 id="scaling">10. Scaling Considerations</h2>

<h3>Load Balancers</h3>
<p>Load balancers are placed at two levels:</p>
<ol>
    <li><strong>External L7 Load Balancer (Internet â†’ API Gateway):</strong> Sits at the public edge. Distributes incoming HTTPS requests across multiple API Gateway instances. Performs TLS termination, health checks, and connection draining. Uses <strong>round-robin</strong> or <strong>least-connections</strong> strategy. During flash sales, this layer absorbs the initial traffic spike. Multiple load balancer instances can be deployed across availability zones for redundancy.</li>
    <li><strong>Internal L4/L7 Load Balancers (between services):</strong> Service-to-service calls (e.g., Event Service â†’ Search Service, API Gateway â†’ Booking Service) go through internal load balancers. These use service discovery and route traffic to healthy instances. <strong>Least-connections</strong> strategy is preferred for the Booking Service to avoid overloading instances that are processing slow database transactions.</li>
</ol>

<h3>Horizontal Scaling by Service</h3>
<table>
    <tr><th>Component</th><th>Scaling Strategy</th><th>Notes</th></tr>
    <tr><td>API Gateway</td><td>Horizontal auto-scaling based on request rate</td><td>Stateless; can add/remove instances freely</td></tr>
    <tr><td>Event Service</td><td>Horizontal auto-scaling based on CPU/request rate</td><td>Stateless; reads mostly served from cache</td></tr>
    <tr><td>Search Service</td><td>Horizontal scaling with search index replicas</td><td>Read replicas of the search index across nodes</td></tr>
    <tr><td>Queue Service</td><td>Horizontal; one instance group per high-demand event</td><td>SSE connections are stateful; use sticky sessions or store queue state in shared cache</td></tr>
    <tr><td>Booking Service</td><td>Horizontal; bottlenecked by DB write throughput</td><td>The Queue Service throttles inbound traffic to match DB capacity</td></tr>
    <tr><td>Payment Service</td><td>Horizontal auto-scaling</td><td>Stateless; bottlenecked by external Payment Gateway rate limits</td></tr>
    <tr><td>Notification Service</td><td>Horizontal auto-scaling based on queue depth</td><td>Scale consumers up when message queue depth increases</td></tr>
    <tr><td>Booking DB (SQL)</td><td>Sharded by event_id; read replicas per shard</td><td>Write-heavy during sales â†’ vertical scaling of master may also be needed</td></tr>
    <tr><td>Event DB (NoSQL)</td><td>Sharded by event_id; replicas for reads</td><td>Read-heavy; horizontal scaling is straightforward</td></tr>
    <tr><td>Cache</td><td>Distributed cluster with consistent hashing</td><td>Add nodes for more memory; consistent hashing minimizes cache invalidation on rebalance</td></tr>
    <tr><td>Message Queue</td><td>Partitioned by event_id; add partitions for throughput</td><td>Consumer groups scale independently</td></tr>
</table>

<h3>Flash Sale Specific Scaling</h3>
<ul>
    <li><strong>Pre-warming:</strong> Before a known high-demand on-sale, scale up Booking Service instances, Booking DB read replicas, and cache capacity proactively.</li>
    <li><strong>Queue Service as a throttle:</strong> The virtual waiting queue is the key scaling mechanism. It absorbs millions of simultaneous users and releases them at a controlled rate (e.g., 500 users/5 sec) that the Booking Service can handle. Without this, the Booking DB would be overwhelmed with lock contention.</li>
    <li><strong>Rate limiting at API Gateway:</strong> Per-event rate limits prevent any single event's traffic from impacting other events.</li>
    <li><strong>Static content offloaded to CDN:</strong> Event images, seating chart SVGs, and frontend assets are served from CDN, keeping origin traffic to API calls only.</li>
</ul>

<h3>Database Scaling</h3>
<ul>
    <li><strong>Booking DB (SQL):</strong> Horizontal sharding by <code>event_id</code>. Each shard handles all transactions for a subset of events. During a flash sale for one event, that event's shard handles all the write traffic. Read replicas on each shard serve read queries (seat availability summaries). The Queue Service's throttling prevents any single shard from being overwhelmed.</li>
    <li><strong>Event DB (NoSQL):</strong> Natively supports horizontal scaling. Add nodes as the event catalog grows. Read-heavy queries are served from cache, so the DB primarily handles cache misses and writes.</li>
</ul>

<!-- ================================================================== -->
<!-- TRADEOFFS & DEEP DIVES -->
<!-- ================================================================== -->
<h2 id="tradeoffs">11. Tradeoffs &amp; Deep Dives</h2>

<h3>Pessimistic Locking (SELECT FOR UPDATE) vs. Optimistic Locking</h3>
<p><strong>Chosen: Pessimistic Locking.</strong></p>
<p>In the seat reservation flow, we use <code>SELECT FOR UPDATE</code> to acquire an exclusive row-level lock on the seat before checking availability and updating status. This guarantees that concurrent requests for the same seat are serialized.</p>
<p><strong>Trade-off:</strong> Pessimistic locking increases lock contention during high concurrency. If 1,000 users try to reserve the same seat simultaneously, 999 will block waiting for the lock. However, this is mitigated by:</p>
<ul>
    <li>The Queue Service throttling the rate of users entering the booking flow.</li>
    <li>Lock hold time is very short (just an INSERT + UPDATE in the same transaction, ~5-10ms).</li>
    <li>In practice, most users select different seats, so lock contention is limited to truly popular seats.</li>
</ul>
<p><strong>Why not optimistic locking?</strong> Optimistic locking (check version, update, retry on conflict) works well when conflicts are rare. But during flash sales, the same popular seats may see dozens of simultaneous attempts, leading to high retry rates. Each retry re-reads and re-checks, wasting database resources. Pessimistic locking is more predictable under high contention: one request wins immediately, others wait briefly (or fail fast if using lock timeout).</p>

<h3>Randomized Queue Position vs. FIFO</h3>
<p><strong>Chosen: Randomized positioning.</strong></p>
<p>Users who join the queue within the same window (e.g., the first 30 seconds after on-sale time) are shuffled randomly instead of strict first-come-first-served order.</p>
<p><strong>Why:</strong> Strict FIFO incentivizes users and bots to connect as early as possible, creating thundering herd at the exact on-sale second. Bots with faster connections and automated scripts gain an unfair advantage. Randomized positioning within windows neutralizes this advantage and promotes fairness.</p>
<p><strong>Trade-off:</strong> Users who arrive genuinely early may feel they "should" be first. This can be mitigated by communicating the randomization policy clearly to users.</p>

<h3>SSE for Queue Updates</h3>
<p><strong>How SSE connections are established and managed:</strong></p>
<ol>
    <li>Client sends <code>POST /api/v1/events/{event_id}/queue/join</code>.</li>
    <li>Server responds with status 200 and opens an SSE stream (<code>Content-Type: text/event-stream</code>).</li>
    <li>The connection is a persistent HTTP connection kept alive by the server.</li>
    <li>The Queue Service instance maintains an in-memory mapping: <code>{ user_id â†’ SSE connection }</code>.</li>
    <li>If the Queue Service instance has N users connected, it sends periodic batch updates to all N users every few seconds.</li>
    <li>If the client disconnects (network issue), the browser's <code>EventSource</code> API automatically reconnects. The server recognizes the user's queue position from their auth token and resumes sending updates.</li>
    <li>Queue state (positions, batch pointers) is stored in the distributed cache, not just in the Queue Service instance memory. This allows any Queue Service instance to serve any user's reconnection.</li>
</ol>
<p><strong>Why not WebSocket?</strong> WebSocket supports bidirectional communication, but the queue update flow is server â†’ client only. WebSocket requires a protocol upgrade handshake, is harder to load-balance (requires sticky sessions or connection state sharing), and is more complex to implement. SSE is sufficient and simpler.</p>
<p><strong>Why not Long Polling?</strong> Long polling requires the client to repeatedly open new HTTP connections. Each poll has TCP + TLS overhead. For thousands of users in a queue receiving updates every 3-5 seconds, this creates significantly more network overhead than a single persistent SSE connection.</p>

<h3>SQL for Bookings vs. NoSQL for Events</h3>
<p>This is a deliberate hybrid approach. Each data store is optimized for its access pattern:</p>
<ul>
    <li><strong>SQL for bookings:</strong> ACID transactions are non-negotiable. Double-selling a seat is a business-critical failure. SQL's row-level locking, transactions, and constraint enforcement (unique indexes) provide the strongest guarantees.</li>
    <li><strong>NoSQL for events:</strong> The event catalog is read-heavy, has variable schema (different event types), and needs to scale horizontally to serve millions of search/browse queries. Eventual consistency is acceptable for event details (a user seeing a slightly stale event description for a few seconds is fine).</li>
</ul>

<h3>Message Queue Delivery Guarantees</h3>
<p>The Message Queue uses <strong>at-least-once delivery</strong> semantics. This means a confirmation message may be delivered more than once to the Notification Service (e.g., if the consumer crashes after processing but before acknowledging). The Notification Service must be <strong>idempotent</strong> â€” it checks if a notification for this booking_id was already sent before sending again. This is tracked in a simple notification log table.</p>

<h3>Idempotency in the Payment API</h3>
<p>The <code>idempotency_key</code> field in the payments table ensures that if a client retries a payment request (due to network timeout, for example), the system recognizes the duplicate and returns the original result without charging the user again. The unique index on <code>idempotency_key</code> enforces this at the database level.</p>

<!-- ================================================================== -->
<!-- ALTERNATIVE APPROACHES -->
<!-- ================================================================== -->
<h2 id="alternatives">12. Alternative Approaches</h2>

<h3>Alternative 1: No Virtual Queue â€” Direct Booking</h3>
<p><strong>Description:</strong> Skip the Queue Service entirely. All users go directly to seat selection during flash sales.</p>
<p><strong>Why not chosen:</strong> Without throttling, millions of simultaneous requests hit the Booking Service and SQL database. Lock contention on popular seats becomes extreme. The database would need to handle 10K+ concurrent transactions for a single event, leading to connection pool exhaustion, high latency, and potential deadlocks. The Queue Service acts as a controlled funnel that protects downstream services.</p>

<h3>Alternative 2: Optimistic Concurrency Control (OCC) Instead of Pessimistic Locking</h3>
<p><strong>Description:</strong> Use a version column on the seats table. Read the seat with its version, attempt to update with a <code>WHERE version = X</code> condition, retry on conflict.</p>
<p><strong>Why not chosen:</strong> Under high contention (popular seats), OCC leads to a retry storm. If 50 users try to reserve the same seat, 49 fail and must retry, re-reading the row. This wastes database I/O and adds unpredictable latency. Pessimistic locking is more deterministic: one winner, others wait briefly or fail fast. We do, however, keep the <code>version</code> column as a secondary safety mechanism.</p>

<h3>Alternative 3: Pre-claim Tickets (General Admission) Instead of Seat Selection</h3>
<p><strong>Description:</strong> Instead of selecting specific seats, users just claim N tickets from a pool (like general admission). A simple counter decrement replaces the seat-level locking.</p>
<p><strong>Why not chosen:</strong> This simplifies the system dramatically (use an atomic counter instead of row-level locks). However, Ticketmaster's core value proposition is assigned seating for most events. General admission eliminates the ability to choose where you sit, which is a critical feature. For events that <em>are</em> general admission, we can use this simpler approach as a special case.</p>

<h3>Alternative 4: All SQL (No NoSQL for Events)</h3>
<p><strong>Description:</strong> Store events in the same SQL database as bookings.</p>
<p><strong>Why not chosen:</strong> Events have highly variable schema (concerts vs. sports vs. theater have different attributes). Modeling this in SQL requires either a wide table with many nullable columns or an EAV (entity-attribute-value) pattern, both of which are suboptimal. SQL also scales vertically (more expensive) rather than horizontally. The read-heavy event catalog (100:1 ratio) benefits from NoSQL's horizontal scaling and flexible schema.</p>

<h3>Alternative 5: WebSocket for All Real-Time Communication</h3>
<p><strong>Description:</strong> Use WebSocket connections for queue updates and seat availability changes.</p>
<p><strong>Why not chosen:</strong> WebSocket is bidirectional and requires a protocol upgrade from HTTP. Queue updates are unidirectional (server â†’ client), making SSE a better fit. WebSocket connections are harder to load-balance (need sticky sessions or connection-aware routing), require more server resources (maintain full-duplex connection), and don't auto-reconnect on failure (SSE does). WebSocket would be appropriate if we needed client â†’ server streaming (e.g., real-time bidding), but we don't in this use case.</p>

<h3>Alternative 6: Distributed Lock (via Cache) Instead of Database Locking</h3>
<p><strong>Description:</strong> Use a distributed lock in the cache layer (e.g., a lock key with TTL) to prevent double-booking, then write to the database.</p>
<p><strong>Why not chosen:</strong> Distributed locks via cache are less reliable than database-level locks. If the cache node crashes between acquiring the lock and writing to the database, the lock is lost and inconsistency can occur. Database-level <code>SELECT FOR UPDATE</code> ties the lock to the transaction, providing a single source of truth. The cache-based lock also introduces an additional layer of complexity for failure handling.</p>

<!-- ================================================================== -->
<!-- ADDITIONAL CONSIDERATIONS -->
<!-- ================================================================== -->
<h2 id="additional">13. Additional Considerations</h2>

<h3>Bot Prevention &amp; Anti-Fraud</h3>
<ul>
    <li><strong>CAPTCHA:</strong> Present a CAPTCHA challenge before joining the queue for high-demand events.</li>
    <li><strong>Device fingerprinting:</strong> Detect and block suspicious patterns (same device, multiple accounts).</li>
    <li><strong>Purchase limits:</strong> Enforce per-user-per-event seat limits (e.g., max 4-6 tickets) at the Booking Service level.</li>
    <li><strong>Verified Fan program:</strong> Pre-register fans for high-demand events. Only verified fans receive access codes to enter the queue.</li>
</ul>

<h3>Multi-Region Deployment</h3>
<p>For global availability and low latency, the system can be deployed across multiple geographic regions. The Event DB and Cache can have regional replicas. However, the Booking DB for a specific event should have a <strong>single write master</strong> (in the region closest to the venue) to maintain strong consistency. All booking requests for that event are routed to the master region.</p>

<h3>Monitoring &amp; Alerting</h3>
<ul>
    <li>Track queue depth, reservation rate, payment success/failure rate in real time.</li>
    <li>Alert on anomalies: sudden spike in failed payments, unusual bot patterns, database lock wait times exceeding thresholds.</li>
    <li>Dashboard for organizers showing live ticket sales, revenue, and seat availability.</li>
</ul>

<h3>Graceful Degradation</h3>
<ul>
    <li>If the Search Service is down, fall back to basic database queries (slower, but functional).</li>
    <li>If the Notification Service is down, messages remain in the queue and are processed when service recovers.</li>
    <li>If the Payment Gateway is temporarily unavailable, hold the reservation and allow the user to retry. Extend reservation TTL if needed.</li>
</ul>

<h3>Ticket Transfer / Resale</h3>
<p>A future extension could allow ticket holders to transfer or resell tickets. This would require an additional Transfer Service that updates the booking's <code>user_id</code> and generates a new QR code, potentially with anti-scalping price caps enforced at the system level.</p>

<h3>Data Retention &amp; Archiving</h3>
<p>Past event data (completed events, old bookings) should be archived to cold storage after a retention period (e.g., 2 years). This keeps the active database size manageable and improves query performance. Archived data remains available for auditing and analytics via batch query systems.</p>

<!-- ================================================================== -->
<!-- VENDOR RECOMMENDATIONS -->
<!-- ================================================================== -->
<h2 id="vendors">14. Vendor Recommendations</h2>
<p>The above design is vendor-agnostic. Below are vendor options that would work well for each component:</p>

<table>
    <tr><th>Component</th><th>Vendors</th><th>Rationale</th></tr>
    <tr>
        <td>SQL Database (Booking DB)</td>
        <td>PostgreSQL, MySQL, CockroachDB, Amazon Aurora</td>
        <td>PostgreSQL and MySQL are battle-tested for ACID transactions. CockroachDB adds distributed SQL with built-in sharding. Aurora provides managed, auto-scaling SQL.</td>
    </tr>
    <tr>
        <td>NoSQL Database (Event DB)</td>
        <td>MongoDB, DynamoDB, Couchbase</td>
        <td>MongoDB excels at flexible document schemas with rich querying. DynamoDB provides fully managed, auto-scaling with single-digit ms latency. Couchbase offers built-in caching + document store.</td>
    </tr>
    <tr>
        <td>In-Memory Cache</td>
        <td>Redis, Memcached, Hazelcast</td>
        <td>Redis supports rich data structures (hashes for event objects, sorted sets for leaderboards) and TTL-based expiry. Memcached is simpler and optimized for pure key-value caching. Redis is recommended for its versatility.</td>
    </tr>
    <tr>
        <td>Search Index</td>
        <td>Elasticsearch, Apache Solr, Meilisearch</td>
        <td>Elasticsearch is the industry standard for full-text search with geospatial queries, faceted search, and relevance tuning. Solr is a comparable alternative. Meilisearch is a lighter option for simpler use cases.</td>
    </tr>
    <tr>
        <td>Message Queue</td>
        <td>Apache Kafka, RabbitMQ, Amazon SQS, Apache Pulsar</td>
        <td>Kafka provides high-throughput, durable, partitioned message streaming with excellent support for at-least-once delivery. RabbitMQ is simpler for point-to-point queuing with dead-letter support. SQS is fully managed. Kafka is recommended for flash-sale throughput.</td>
    </tr>
    <tr>
        <td>Object Storage</td>
        <td>Amazon S3, Google Cloud Storage, Azure Blob Storage, MinIO</td>
        <td>S3 is the de facto standard for scalable, durable, cost-effective object storage. MinIO is an S3-compatible self-hosted option.</td>
    </tr>
    <tr>
        <td>CDN</td>
        <td>Cloudflare, AWS CloudFront, Akamai, Fastly</td>
        <td>Cloudflare offers a global edge network with DDoS protection built in (useful during flash sales). CloudFront integrates natively with S3. Akamai and Fastly are premium options with advanced edge compute capabilities.</td>
    </tr>
    <tr>
        <td>Load Balancer</td>
        <td>AWS ALB/NLB, Nginx, HAProxy, Envoy</td>
        <td>AWS ALB for managed L7 load balancing in cloud. Nginx and HAProxy are proven self-managed options. Envoy is popular in microservice mesh architectures.</td>
    </tr>
    <tr>
        <td>API Gateway</td>
        <td>Kong, AWS API Gateway, Apigee, custom (Nginx + Lua)</td>
        <td>Kong provides rate limiting, auth, and routing with plugin ecosystem. AWS API Gateway is fully managed. A custom Nginx-based gateway offers maximum control.</td>
    </tr>
</table>

<script>
    mermaid.initialize({
        startOnLoad: true,
        theme: 'default',
        flowchart: {
            useMaxWidth: true,
            htmlLabels: true,
            curve: 'basis'
        }
    });
</script>

</body>
</html>
