<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>System Design: Wikipedia</title>
    <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
    <style>
        :root {
            --bg: #0d1117;
            --surface: #161b22;
            --border: #30363d;
            --text: #e6edf3;
            --text-muted: #8b949e;
            --accent: #58a6ff;
            --accent2: #3fb950;
            --accent3: #d2a8ff;
            --accent4: #f78166;
            --code-bg: #1c2128;
        }
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, sans-serif;
            background: var(--bg);
            color: var(--text);
            line-height: 1.7;
            padding: 2rem;
            max-width: 1200px;
            margin: 0 auto;
        }
        h1 {
            font-size: 2.4rem;
            margin-bottom: 0.5rem;
            color: var(--accent);
            border-bottom: 2px solid var(--accent);
            padding-bottom: 0.5rem;
        }
        h2 {
            font-size: 1.8rem;
            margin-top: 2.5rem;
            margin-bottom: 1rem;
            color: var(--accent2);
            border-bottom: 1px solid var(--border);
            padding-bottom: 0.4rem;
        }
        h3 {
            font-size: 1.35rem;
            margin-top: 1.8rem;
            margin-bottom: 0.7rem;
            color: var(--accent3);
        }
        h4 {
            font-size: 1.1rem;
            margin-top: 1.2rem;
            margin-bottom: 0.5rem;
            color: var(--accent4);
        }
        p, li {
            margin-bottom: 0.6rem;
            color: var(--text);
        }
        ul, ol {
            padding-left: 1.8rem;
            margin-bottom: 1rem;
        }
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 1rem 0;
            background: var(--surface);
            border-radius: 8px;
            overflow: hidden;
        }
        th, td {
            text-align: left;
            padding: 0.7rem 1rem;
            border: 1px solid var(--border);
        }
        th {
            background: #21262d;
            color: var(--accent);
            font-weight: 600;
        }
        tr:hover { background: #1c2128; }
        code {
            background: var(--code-bg);
            padding: 0.15rem 0.4rem;
            border-radius: 4px;
            font-size: 0.92rem;
            color: var(--accent3);
        }
        .diagram-container {
            background: #ffffff;
            border-radius: 10px;
            padding: 1.5rem;
            margin: 1.5rem 0;
            overflow-x: auto;
        }
        .example-box {
            background: var(--surface);
            border-left: 4px solid var(--accent);
            padding: 1rem 1.2rem;
            margin: 1rem 0;
            border-radius: 0 8px 8px 0;
        }
        .example-box strong { color: var(--accent); }
        .callout {
            background: var(--surface);
            border-left: 4px solid var(--accent4);
            padding: 1rem 1.2rem;
            margin: 1rem 0;
            border-radius: 0 8px 8px 0;
        }
        .callout strong { color: var(--accent4); }
        .alt-box {
            background: var(--surface);
            border-left: 4px solid var(--accent3);
            padding: 1rem 1.2rem;
            margin: 1rem 0;
            border-radius: 0 8px 8px 0;
        }
        .alt-box strong { color: var(--accent3); }
        .section { margin-bottom: 2.5rem; }
        .tag {
            display: inline-block;
            padding: 0.15rem 0.5rem;
            border-radius: 4px;
            font-size: 0.82rem;
            font-weight: 600;
            margin-right: 0.3rem;
        }
        .tag-pk { background: #1f6feb33; color: #58a6ff; }
        .tag-fk { background: #3fb95033; color: #3fb950; }
        .tag-idx { background: #d2a8ff33; color: #d2a8ff; }
        .tag-shard { background: #f7816633; color: #f78166; }
    </style>
</head>
<body>

<h1>üìñ System Design: Wikipedia</h1>
<p style="color: var(--text-muted); font-size: 1.05rem;">A comprehensive system design for a collaborative online encyclopedia serving billions of page views monthly with millions of articles across hundreds of languages.</p>

<!-- ============================================================ -->
<h2>1. Functional Requirements</h2>
<!-- ============================================================ -->
<div class="section">
<ol>
    <li><strong>View Articles</strong> ‚Äî Users can read any article by navigating to its title/URL. Articles render wiki markup into formatted HTML with embedded media.</li>
    <li><strong>Search Articles</strong> ‚Äî Users can search for articles by keyword. The search returns ranked results with title matches and content snippets.</li>
    <li><strong>Edit Articles</strong> ‚Äî Registered users can edit existing articles. The system must detect and surface edit conflicts when two users edit the same article concurrently.</li>
    <li><strong>Create Articles</strong> ‚Äî Registered users can create new articles with a unique title, content, and category assignment.</li>
    <li><strong>View Revision History</strong> ‚Äî Users can view the chronological list of all edits ever made to an article, including who edited it and when.</li>
    <li><strong>View Diffs Between Revisions</strong> ‚Äî Users can compare any two revisions of an article side-by-side to see exactly what changed.</li>
    <li><strong>Media Support</strong> ‚Äî Articles can embed images, diagrams, and other media files hosted in an object storage layer.</li>
    <li><strong>Talk/Discussion Pages</strong> ‚Äî Each article has an associated discussion page where editors can discuss changes.</li>
    <li><strong>User Accounts &amp; Authentication</strong> ‚Äî Users can register, log in, and manage their profile. Certain articles may be protected so only privileged users can edit them.</li>
    <li><strong>Watchlist</strong> ‚Äî Logged-in users can add articles to a personal watchlist and see recent changes to those articles.</li>
</ol>
</div>

<!-- ============================================================ -->
<h2>2. Non-Functional Requirements</h2>
<!-- ============================================================ -->
<div class="section">
<ol>
    <li><strong>High Availability</strong> ‚Äî Target 99.99% uptime. Wikipedia is a critical global knowledge resource.</li>
    <li><strong>Low Read Latency</strong> ‚Äî Article pages should render in &lt;200 ms for the 95th percentile user.</li>
    <li><strong>Extreme Read-Heavy Workload</strong> ‚Äî Read-to-write ratio is approximately <strong>1000:1</strong>. The system must be architected for reads.</li>
    <li><strong>Global Scalability</strong> ‚Äî Support ~10 billion page views per month from users worldwide.</li>
    <li><strong>Durability</strong> ‚Äî Every single revision must be durably stored. Data loss is unacceptable.</li>
    <li><strong>Eventual Consistency for Reads</strong> ‚Äî After an edit, it is acceptable for cached/CDN pages to take a few seconds to reflect changes. Edits themselves must be strongly consistent.</li>
    <li><strong>Conflict Detection</strong> ‚Äî When two editors simultaneously edit the same article, the system must detect the conflict and prevent silent overwrites.</li>
    <li><strong>Multi-Language Support</strong> ‚Äî Support hundreds of independent language editions (en, fr, de, ja, etc.), each with its own article set.</li>
</ol>
</div>

<!-- ============================================================ -->
<h2>3. System Design ‚Äî Individual Flows</h2>
<!-- ============================================================ -->

<!-- ==================== FLOW 1 ==================== -->
<h3>Flow 1: Article Read</h3>
<div class="diagram-container">
    <pre class="mermaid">
graph TD
    U["üë§ User Browser"]
    CDN["üåê CDN<br/>(Edge Nodes Worldwide)"]
    LB["‚öñÔ∏è Load Balancer"]
    AS["üìÑ Article Service"]
    MC["‚ö° In-Memory Cache"]
    SQL["üóÑÔ∏è SQL Database<br/>(Article Metadata)"]
    NOSQL["üì¶ NoSQL Content Store<br/>(Article Body)"]
    OBJ["üñºÔ∏è Object Storage<br/>(Media Files)"]

    U -->|"1. HTTP GET /wiki/{title}"| CDN
    CDN -->|"2a. Cache HIT ‚Üí return HTML"| U
    CDN -->|"2b. Cache MISS ‚Üí"| LB
    LB -->|"3. Route to healthy instance"| AS
    AS -->|"4. Lookup article"| MC
    MC -.->|"5a. Cache HIT ‚Üí return"| AS
    AS -->|"5b. Cache MISS ‚Üí fetch metadata"| SQL
    AS -->|"6. Fetch content by revision_id"| NOSQL
    AS -->|"7. Resolve media URLs"| OBJ
    AS -->|"8. Populate cache"| MC
    AS -->|"9. Return rendered HTML"| LB
    LB --> CDN
    CDN -->|"10. Cache at edge + return to user"| U

    style CDN fill:#1a5276,stroke:#2980b9,color:#fff
    style MC fill:#7d3c98,stroke:#a569bd,color:#fff
    style SQL fill:#1e8449,stroke:#27ae60,color:#fff
    style NOSQL fill:#b9770e,stroke:#f39c12,color:#fff
    style AS fill:#2c3e50,stroke:#58a6ff,color:#fff
    style LB fill:#2c3e50,stroke:#58a6ff,color:#fff
    style OBJ fill:#7e5109,stroke:#d4ac0d,color:#fff
    </pre>
</div>

<h4>Examples</h4>

<div class="example-box">
<strong>Example 1 ‚Äî CDN Cache Hit (most common, ~95% of reads):</strong><br/>
A user in Tokyo types "Albert Einstein" into their browser. The browser sends <code>HTTP GET /wiki/Albert_Einstein</code>. The CDN edge node in Tokyo has a cached copy of this popular article and returns the rendered HTML immediately with a <code>200 OK</code>. The user sees the article in ~30 ms. No backend servers are contacted.
</div>

<div class="example-box">
<strong>Example 2 ‚Äî CDN Miss, In-Memory Cache Hit:</strong><br/>
A user requests a moderately popular article like "Guava". The CDN edge node does not have this cached (TTL expired). The request passes through the Load Balancer to an Article Service instance. The Article Service checks the in-memory cache and finds the rendered HTML. It returns the HTML upstream, the CDN caches it at the edge, and the user sees the article in ~80 ms.
</div>

<div class="example-box">
<strong>Example 3 ‚Äî Full Cache Miss (cold path):</strong><br/>
A user requests a rarely-viewed article like "Devonian-period brachiopods". Neither the CDN nor the in-memory cache has this article. The Article Service queries the SQL database for article metadata (title, current_revision_id, namespace), then fetches the full wiki-markup content from the NoSQL content store using the revision_id, resolves embedded image URLs from object storage, renders the markup into HTML, stores the result in the in-memory cache, and returns the HTML. The CDN caches the response at the edge. Total latency: ~200 ms.
</div>

<h4>Component Deep Dive ‚Äî Flow 1</h4>

<table>
<tr><th>Component</th><th>Protocol / Type</th><th>Input</th><th>Output</th><th>Details</th></tr>
<tr>
    <td><strong>CDN</strong></td>
    <td>HTTP/HTTPS</td>
    <td><code>GET /wiki/{title}</code></td>
    <td>Rendered HTML page</td>
    <td>Pull-based CDN with global edge nodes. Caches rendered HTML by URL path. TTL-based expiration (5‚Äì10 min for popular articles, 1 hr for static assets). Cache key: <code>language_code:article_title</code>.</td>
</tr>
<tr>
    <td><strong>Load Balancer</strong></td>
    <td>Layer 7 (HTTP)</td>
    <td>HTTP request</td>
    <td>Routed HTTP request</td>
    <td>Routes by URL path prefix: <code>/wiki/*</code> ‚Üí Article Service, <code>/api/search</code> ‚Üí Search Service, etc. Uses least-connections algorithm. Performs health checks on backend instances every 5 seconds.</td>
</tr>
<tr>
    <td><strong>Article Service</strong></td>
    <td>HTTP REST</td>
    <td><code>GET /api/articles/{title}?lang={code}</code></td>
    <td><code>{ article_id, title, content_html, updated_at, categories }</code></td>
    <td>Stateless service. Looks up article metadata from SQL, content from NoSQL, renders wiki markup to HTML, and resolves media URLs. Horizontally scalable.</td>
</tr>
<tr>
    <td><strong>In-Memory Cache</strong></td>
    <td>TCP (cache protocol)</td>
    <td>Cache key: <code>article:{lang}:{title}</code></td>
    <td>Rendered HTML string or null</td>
    <td>Stores pre-rendered HTML for hot articles. Cache-aside (lazy-loading) strategy. LRU eviction. TTL: 5 min. Invalidated on edit via message queue. See Cache Deep Dive section for details.</td>
</tr>
<tr>
    <td><strong>SQL Database</strong></td>
    <td>TCP (database wire protocol)</td>
    <td>Query by title + language_code</td>
    <td>Article metadata row</td>
    <td>Stores <code>articles</code>, <code>users</code>, <code>revisions</code> metadata tables. Read replicas serve read traffic. Primary handles writes.</td>
</tr>
<tr>
    <td><strong>NoSQL Content Store</strong></td>
    <td>TCP (database wire protocol)</td>
    <td>Key: <code>revision_id</code></td>
    <td>Raw wiki markup + optionally pre-rendered HTML</td>
    <td>Key-value / document store optimized for large text blobs. Simple access pattern: point read by revision_id.</td>
</tr>
<tr>
    <td><strong>Object Storage</strong></td>
    <td>HTTP REST</td>
    <td><code>GET /{media_id}</code></td>
    <td>Binary media file (image, diagram, etc.)</td>
    <td>Stores all media. Served directly via CDN with long TTLs (media is immutable once uploaded). Media URLs are embedded in article markup.</td>
</tr>
</table>

<!-- ==================== FLOW 2 ==================== -->
<h3>Flow 2: Article Edit</h3>
<div class="diagram-container">
    <pre class="mermaid">
graph TD
    U["üë§ Editor Browser"]
    LB["‚öñÔ∏è Load Balancer"]
    ES["‚úèÔ∏è Edit Service"]
    SQL["üóÑÔ∏è SQL Database<br/>(Articles + Revisions)"]
    NOSQL["üì¶ NoSQL Content Store"]
    MQ["üì¨ Message Queue"]
    CW["üîÑ Cache Invalidation<br/>Worker"]
    SW["üîç Search Index<br/>Worker"]
    WW["üì¢ Watchlist<br/>Notification Worker"]
    MC["‚ö° In-Memory Cache"]
    CDN["üåê CDN"]
    SI["üîé Search Index"]

    U -->|"1. HTTP POST /api/articles/{id}/edit<br/>{base_revision_id, content, summary}"| LB
    LB -->|"2. Route"| ES
    ES -->|"3. Read current_revision_id"| SQL
    ES -->|"4. Compare base_revision_id<br/>vs current_revision_id"| ES
    ES -->|"5a. CONFLICT ‚Üí return 409"| U
    ES -->|"5b. NO CONFLICT ‚Üí<br/>Write new revision metadata"| SQL
    ES -->|"6. Store content blob"| NOSQL
    ES -->|"7. Publish edit event"| MQ
    ES -->|"8. Return 200 + new revision_id"| U
    MQ --> CW
    MQ --> SW
    MQ --> WW
    CW -->|"Invalidate article entry"| MC
    CW -->|"Purge edge cache"| CDN
    SW -->|"Update index"| SI

    style ES fill:#922b21,stroke:#e74c3c,color:#fff
    style MQ fill:#1a5276,stroke:#2980b9,color:#fff
    style CW fill:#7d3c98,stroke:#a569bd,color:#fff
    style SW fill:#1e8449,stroke:#27ae60,color:#fff
    style WW fill:#b9770e,stroke:#f39c12,color:#fff
    style SQL fill:#1e8449,stroke:#27ae60,color:#fff
    style NOSQL fill:#b9770e,stroke:#f39c12,color:#fff
    </pre>
</div>

<h4>Examples</h4>

<div class="example-box">
<strong>Example 1 ‚Äî Successful Edit (no conflict):</strong><br/>
Editor "HistoryBuff42" opens the article "Battle of Hastings" (currently at revision 5500). They fix a typo in the second paragraph and click "Save". The browser sends <code>HTTP POST /api/articles/1066/edit</code> with <code>{ base_revision_id: 5500, content: "...corrected text...", summary: "Fixed typo", is_minor: true }</code>. The Edit Service reads the article's <code>current_revision_id</code> from SQL and finds it is still 5500 ‚Äî no conflict. It writes a new revision row (revision_id: 5501), stores the content blob in the NoSQL content store, updates <code>articles.current_revision_id</code> to 5501 (in a single transaction), and publishes an <code>article_edited</code> event to the message queue. The response <code>200 OK { revision_id: 5501 }</code> is returned. Asynchronously, the cache invalidation worker purges the in-memory cache and CDN entry, the search index worker re-indexes the article, and the watchlist worker queues notifications for watchers.
</div>

<div class="example-box">
<strong>Example 2 ‚Äî Edit Conflict Detected:</strong><br/>
Editor "WikiScholar" opens "Quantum Mechanics" at revision 8800 and starts writing a long addition. Meanwhile, editor "PhysicsNerd" also opens the same article at revision 8800 and makes a quick grammar fix, saving it as revision 8801. When "WikiScholar" finally clicks "Save", the browser sends <code>{ base_revision_id: 8800, ... }</code>. The Edit Service checks and finds <code>current_revision_id</code> is now 8801 ‚â† 8800 ‚Äî conflict detected. It returns <code>409 Conflict</code> with <code>{ conflict: true, current_revision_id: 8801, diff_url: "/api/articles/42/diff?from=8800&to=8801" }</code>. The browser shows WikiScholar a merge interface with the diff. WikiScholar resolves the conflict, and the browser re-submits with <code>{ base_revision_id: 8801, ... }</code>, which succeeds as revision 8802.
</div>

<div class="example-box">
<strong>Example 3 ‚Äî Edit on a Protected Article (authorization failure):</strong><br/>
A new user with the "editor" role tries to edit the semi-protected article "Barack Obama". The Edit Service checks the article's <code>protection_level</code> and the user's role/edit count. The user does not meet the autoconfirmed threshold (e.g., 500+ edits, 30+ days old account). The service returns <code>403 Forbidden</code> with <code>{ error: "Article is semi-protected. Autoconfirmed status required." }</code>.
</div>

<h4>Component Deep Dive ‚Äî Flow 2</h4>

<table>
<tr><th>Component</th><th>Protocol / Type</th><th>Input</th><th>Output</th><th>Details</th></tr>
<tr>
    <td><strong>Edit Service</strong></td>
    <td>HTTP REST</td>
    <td><code>POST /api/articles/{id}/edit</code><br/>Body: <code>{ base_revision_id, content, summary, is_minor }</code></td>
    <td><code>200 { revision_id }</code> or <code>409 { conflict details }</code> or <code>403</code></td>
    <td>Handles optimistic concurrency control. Reads the current revision ID, compares with the base revision submitted by the editor. If they match, creates a new revision atomically (SQL transaction: INSERT revision + UPDATE article). If they don't match, returns a conflict. Also checks article protection level against user role.</td>
</tr>
<tr>
    <td><strong>Message Queue</strong></td>
    <td>TCP (AMQP-like protocol)</td>
    <td>Message: <code>{ event: "article_edited", article_id, revision_id, language_code }</code></td>
    <td>Consumed by workers</td>
    <td>Decouples the synchronous edit path from asynchronous side effects. Guarantees at-least-once delivery. Three consumer groups: cache invalidation, search indexing, watchlist notification. See Message Queue Deep Dive for details.</td>
</tr>
<tr>
    <td><strong>Cache Invalidation Worker</strong></td>
    <td>TCP (internal)</td>
    <td>Edit event from queue</td>
    <td>Cache DELETE + CDN purge request</td>
    <td>On receiving an edit event: (1) deletes the in-memory cache key <code>article:{lang}:{title}</code>, (2) sends an HTTP PURGE request to the CDN for <code>/wiki/{title}</code>. This ensures the next reader gets fresh content.</td>
</tr>
<tr>
    <td><strong>Search Index Worker</strong></td>
    <td>TCP (internal)</td>
    <td>Edit event from queue</td>
    <td>Index update</td>
    <td>Fetches the new content from the NoSQL content store, tokenizes and parses it, and updates the search index entry for this article. Slight delay (seconds) is acceptable.</td>
</tr>
<tr>
    <td><strong>Watchlist Notification Worker</strong></td>
    <td>TCP (internal)</td>
    <td>Edit event from queue</td>
    <td>Watchlist update entries</td>
    <td>Queries the <code>watchlist</code> table for all users watching this article. Writes a notification entry to a <code>watchlist_changes</code> table so users see it on their next watchlist page load.</td>
</tr>
</table>

<!-- ==================== FLOW 3 ==================== -->
<h3>Flow 3: Article Search</h3>
<div class="diagram-container">
    <pre class="mermaid">
graph TD
    U["üë§ User Browser"]
    LB["‚öñÔ∏è Load Balancer"]
    SS["üîç Search Service"]
    SI["üîé Search Index<br/>(Inverted Index)"]
    SQL["üóÑÔ∏è SQL Database"]

    U -->|"1. HTTP GET /api/search?q={query}&lang={lang}&page={n}"| LB
    LB -->|"2. Route to Search Service"| SS
    SS -->|"3. Query inverted index"| SI
    SI -->|"4. Return ranked article_ids + snippets"| SS
    SS -->|"5. Hydrate metadata (titles, thumbnails)"| SQL
    SS -->|"6. Return search results"| U

    style SS fill:#1a5276,stroke:#2980b9,color:#fff
    style SI fill:#7d3c98,stroke:#a569bd,color:#fff
    style SQL fill:#1e8449,stroke:#27ae60,color:#fff
    </pre>
</div>

<h4>Examples</h4>

<div class="example-box">
<strong>Example 1 ‚Äî Title Match (high relevance):</strong><br/>
A user types "photosynthesis" in the search bar and presses Enter. The browser sends <code>HTTP GET /api/search?q=photosynthesis&lang=en&page=1</code>. The Search Service queries the inverted index. The article titled "Photosynthesis" scores highest (exact title match). The service hydrates metadata from the SQL database (thumbnail URL, article snippet, last updated date) and returns <code>{ results: [{ title: "Photosynthesis", snippet: "Photosynthesis is a biological process...", article_id: 12345 }, ...], total: 4820, page: 1 }</code>.
</div>

<div class="example-box">
<strong>Example 2 ‚Äî Fuzzy / Content Match:</strong><br/>
A user types "green energy from sun" ‚Äî no article has this exact title. The Search Service uses the inverted index to find articles containing these terms in their body, applies TF-IDF or BM25 ranking, and returns articles like "Solar Energy", "Photovoltaics", "Renewable Energy" ranked by relevance.
</div>

<div class="example-box">
<strong>Example 3 ‚Äî Autocomplete / Prefix Search:</strong><br/>
As the user types "Einst" in the search box, the browser fires <code>HTTP GET /api/search/suggest?prefix=Einst&lang=en</code>. The Search Service hits a prefix-optimized index (trie or prefix tree) that returns <code>["Albert Einstein", "Einstein field equations", "Einstein‚ÄìRosen bridge"]</code> within 20 ms.
</div>

<h4>Component Deep Dive ‚Äî Flow 3</h4>

<table>
<tr><th>Component</th><th>Protocol / Type</th><th>Input</th><th>Output</th><th>Details</th></tr>
<tr>
    <td><strong>Search Service</strong></td>
    <td>HTTP REST</td>
    <td><code>GET /api/search?q={query}&lang={lang}&page={n}</code></td>
    <td><code>{ results: [{ article_id, title, snippet, score }], total, page }</code></td>
    <td>Stateless service. Parses the query, applies tokenization and stemming, queries the search index, then hydrates results with metadata from SQL. Supports pagination.</td>
</tr>
<tr>
    <td><strong>Search Index</strong></td>
    <td>TCP (internal protocol)</td>
    <td>Query tokens + language filter</td>
    <td>Ranked list of (article_id, score, snippet)</td>
    <td>Inverted index partitioned by language. Uses BM25 scoring algorithm. Updated asynchronously by the Search Index Worker via message queue. Also maintains a prefix index (trie) for autocomplete. See Search Index Deep Dive for details.</td>
</tr>
</table>

<!-- ==================== FLOW 4 ==================== -->
<h3>Flow 4: Revision History &amp; Diff View</h3>
<div class="diagram-container">
    <pre class="mermaid">
graph TD
    U["üë§ User Browser"]
    LB["‚öñÔ∏è Load Balancer"]
    RS["üìú Revision Service"]
    SQL["üóÑÔ∏è SQL Database<br/>(Revisions Table)"]
    NOSQL["üì¶ NoSQL Content Store"]

    U -->|"1a. HTTP GET /api/articles/{id}/history?page=1<br/>OR<br/>1b. HTTP GET /api/articles/{id}/diff?from=100&to=105"| LB
    LB -->|"2. Route"| RS
    RS -->|"3. Query revisions for article"| SQL
    RS -->|"4. (Diff only) Fetch content<br/>of both revisions"| NOSQL
    RS -->|"5. (Diff only) Compute diff"| RS
    RS -->|"6. Return history list or diff HTML"| U

    style RS fill:#b9770e,stroke:#f39c12,color:#fff
    style SQL fill:#1e8449,stroke:#27ae60,color:#fff
    style NOSQL fill:#b9770e,stroke:#f39c12,color:#fff
    </pre>
</div>

<h4>Examples</h4>

<div class="example-box">
<strong>Example 1 ‚Äî Viewing Revision History:</strong><br/>
A user clicks "View History" on the article "Moon Landing". The browser sends <code>HTTP GET /api/articles/7890/history?page=1</code>. The Revision Service queries the SQL <code>revisions</code> table with <code>WHERE article_id = 7890 ORDER BY timestamp DESC LIMIT 50</code>. It returns a paginated list: <code>{ revisions: [{ revision_id: 6000, user: "SpaceEditor", timestamp: "2025-12-01T10:30Z", comment: "Added citation for Armstrong quote", size_change: +342 }, ...], page: 1, total_pages: 120 }</code>.
</div>

<div class="example-box">
<strong>Example 2 ‚Äî Viewing a Diff Between Two Revisions:</strong><br/>
A user selects revisions 5990 and 6000 and clicks "Compare". The browser sends <code>HTTP GET /api/articles/7890/diff?from=5990&to=6000</code>. The Revision Service fetches the full wiki-markup content of both revisions from the NoSQL content store, runs a line-by-line diff algorithm (similar to Unix <code>diff</code>), renders the additions/deletions into colored HTML, and returns <code>{ from_revision: 5990, to_revision: 6000, diff_html: "&lt;table&gt;...colored diff...&lt;/table&gt;" }</code>.
</div>

<h4>Component Deep Dive ‚Äî Flow 4</h4>

<table>
<tr><th>Component</th><th>Protocol / Type</th><th>Input</th><th>Output</th><th>Details</th></tr>
<tr>
    <td><strong>Revision Service</strong></td>
    <td>HTTP REST</td>
    <td>History: <code>GET /api/articles/{id}/history?page={n}</code><br/>Diff: <code>GET /api/articles/{id}/diff?from={rev}&to={rev}</code></td>
    <td>History: <code>{ revisions: [...], page, total_pages }</code><br/>Diff: <code>{ diff_html, from_revision, to_revision }</code></td>
    <td>For history: queries the SQL <code>revisions</code> table with a composite index on <code>(article_id, timestamp DESC)</code>. For diffs: fetches two content blobs from NoSQL and computes the diff on-the-fly using a longest-common-subsequence algorithm. Diffs are computed on-demand rather than pre-stored to save storage (tradeoff discussed in Tradeoffs section).</td>
</tr>
</table>

<!-- ============================================================ -->
<h2>4. Combined Overall Flow</h2>
<!-- ============================================================ -->
<div class="diagram-container">
    <pre class="mermaid">
graph TD
    U["üë§ User Browser"]
    CDN["üåê CDN"]
    LB["‚öñÔ∏è Load Balancer<br/>(Layer 7 ‚Äî routes by path)"]

    AS["üìÑ Article Service"]
    ES["‚úèÔ∏è Edit Service"]
    SS["üîç Search Service"]
    RS["üìú Revision Service"]
    US["üë• User Service"]
    MS["üñºÔ∏è Media Service"]

    MC["‚ö° In-Memory Cache"]
    SQL_P["üóÑÔ∏è SQL Primary"]
    SQL_R["üóÑÔ∏è SQL Read Replicas"]
    NOSQL["üì¶ NoSQL Content Store"]
    SI["üîé Search Index<br/>(Inverted Index)"]
    OBJ["üñºÔ∏è Object Storage"]
    MQ["üì¨ Message Queue"]

    CW["üîÑ Cache Invalidation Worker"]
    SW["üîç Search Index Worker"]
    WW["üì¢ Watchlist Worker"]

    U -->|"Read article"| CDN
    CDN -->|"Cache miss"| LB
    U -->|"Edit / Search / History / Auth / Upload"| LB

    LB -->|"/wiki/*"| AS
    LB -->|"/api/articles/*/edit"| ES
    LB -->|"/api/search"| SS
    LB -->|"/api/articles/*/history<br/>/api/articles/*/diff"| RS
    LB -->|"/api/auth/*"| US
    LB -->|"/api/media/*"| MS

    AS --> MC
    AS --> SQL_R
    AS --> NOSQL
    AS --> OBJ

    ES --> SQL_P
    ES --> NOSQL
    ES --> MQ

    SS --> SI
    SS --> SQL_R

    RS --> SQL_R
    RS --> NOSQL

    US --> SQL_P
    US --> SQL_R

    MS --> OBJ
    MS --> SQL_P

    MQ --> CW
    MQ --> SW
    MQ --> WW

    CW --> MC
    CW --> CDN
    SW --> SI

    style CDN fill:#1a5276,stroke:#2980b9,color:#fff
    style LB fill:#2c3e50,stroke:#58a6ff,color:#fff
    style AS fill:#2c3e50,stroke:#58a6ff,color:#fff
    style ES fill:#922b21,stroke:#e74c3c,color:#fff
    style SS fill:#1a5276,stroke:#2980b9,color:#fff
    style RS fill:#b9770e,stroke:#f39c12,color:#fff
    style US fill:#2c3e50,stroke:#58a6ff,color:#fff
    style MS fill:#7e5109,stroke:#d4ac0d,color:#fff
    style MC fill:#7d3c98,stroke:#a569bd,color:#fff
    style SQL_P fill:#1e8449,stroke:#27ae60,color:#fff
    style SQL_R fill:#1e8449,stroke:#27ae60,color:#fff
    style NOSQL fill:#b9770e,stroke:#f39c12,color:#fff
    style SI fill:#7d3c98,stroke:#a569bd,color:#fff
    style OBJ fill:#7e5109,stroke:#d4ac0d,color:#fff
    style MQ fill:#1a5276,stroke:#2980b9,color:#fff
    style CW fill:#7d3c98,stroke:#a569bd,color:#fff
    style SW fill:#1e8449,stroke:#27ae60,color:#fff
    style WW fill:#b9770e,stroke:#f39c12,color:#fff
    </pre>
</div>

<h4>Combined Flow Examples</h4>

<div class="example-box">
<strong>Example 1 ‚Äî Full Read-then-Edit Lifecycle:</strong><br/>
User "BioEditor" navigates to <code>/wiki/CRISPR</code>. The CDN returns a cached copy (cache hit). BioEditor reads the article, spots an outdated statistic, and clicks "Edit". The browser fetches the current wiki markup via <code>GET /api/articles/CRISPR/raw</code> (routed to Article Service ‚Üí NoSQL). BioEditor updates the stat, writes an edit summary, and clicks Save. <code>POST /api/articles/999/edit</code> is routed to the Edit Service, which performs conflict detection (no conflict), writes the new revision to SQL + NoSQL, and publishes an event to the message queue. The Edit Service returns <code>200 OK</code>. Asynchronously, the Cache Invalidation Worker purges the in-memory cache key and sends a CDN purge for <code>/wiki/CRISPR</code>. The Search Index Worker re-indexes the article content. The Watchlist Worker creates notification rows for the 2,300 users watching this article. When another user loads <code>/wiki/CRISPR</code> moments later, the CDN has been purged, so the request goes through to the Article Service, which re-renders from the new revision and caches the result.
</div>

<div class="example-box">
<strong>Example 2 ‚Äî Search-to-Read-to-History:</strong><br/>
A student searching for homework help types "mitochondria function" in the search box. <code>GET /api/search?q=mitochondria+function&lang=en</code> is routed to the Search Service, which queries the inverted search index and returns results led by "Mitochondrion". The student clicks the result, firing <code>GET /wiki/Mitochondrion</code>, which is served from the CDN cache. The student notices the article seems recently vandalized, so they click "View History" ‚Üí <code>GET /api/articles/4444/history?page=1</code> routed to the Revision Service, which reads from a SQL read replica. They see a suspicious edit from 5 minutes ago. They click "Compare with previous" ‚Üí <code>GET /api/articles/4444/diff?from=9100&to=9101</code> ‚Üí Revision Service fetches both contents from the NoSQL store, computes the diff, and returns the colored diff HTML showing the vandalism.
</div>

<div class="example-box">
<strong>Example 3 ‚Äî Media Upload and Embedding:</strong><br/>
An editor uploads a new diagram for the "Solar System" article. <code>POST /api/media/upload</code> (multipart form data) is routed to the Media Service, which validates the file (format, size, license), stores it in object storage, writes metadata to SQL, and returns <code>{ media_id: "File:Solar_system_diagram.svg", url: "https://media.wiki.org/..." }</code>. The editor then edits the article to embed the image using wiki markup <code>[[File:Solar_system_diagram.svg|300px]]</code> and saves. The edit flows through the normal edit path. When a reader views the article, the Article Service resolves the media URL, and the CDN serves the image from object storage via its own edge cache (long TTL since media is immutable).
</div>

<!-- ============================================================ -->
<h2>5. Database Schema</h2>
<!-- ============================================================ -->

<!-- ========== SQL TABLES ========== -->
<h3>5.1 SQL Tables</h3>

<h4><code>users</code></h4>
<table>
<tr><th>Column</th><th>Type</th><th>Constraints</th><th>Notes</th></tr>
<tr><td>user_id</td><td>BIGINT</td><td><span class="tag tag-pk">PRIMARY KEY</span> Auto-increment</td><td></td></tr>
<tr><td>username</td><td>VARCHAR(255)</td><td>UNIQUE, NOT NULL <span class="tag tag-idx">INDEX: B-tree</span></td><td>Indexed for login and display lookups</td></tr>
<tr><td>email</td><td>VARCHAR(255)</td><td>UNIQUE, NOT NULL</td><td></td></tr>
<tr><td>password_hash</td><td>VARCHAR(255)</td><td>NOT NULL</td><td>bcrypt or argon2 hash</td></tr>
<tr><td>role</td><td>ENUM('reader','editor','admin','bot')</td><td>NOT NULL, DEFAULT 'reader'</td><td></td></tr>
<tr><td>edit_count</td><td>INT</td><td>DEFAULT 0</td><td><strong>Denormalized</strong> ‚Äî avoids COUNT(*) on revisions table for every permission check. Incremented via trigger/application on edit.</td></tr>
<tr><td>created_at</td><td>TIMESTAMP</td><td>NOT NULL</td><td></td></tr>
</table>
<ul>
    <li><strong>Why SQL:</strong> User data is highly structured and relational. ACID transactions are required for authentication (register, login, role changes). Joins needed to display editor names on revision history. The dataset is bounded (tens of millions of users) so it fits comfortably in SQL.</li>
    <li><strong>Read events:</strong> Login authentication, displaying editor name on revision history, permission checks during edit.</li>
    <li><strong>Write events:</strong> User registration, profile updates, edit_count increment on each edit.</li>
    <li><strong>Denormalization note:</strong> <code>edit_count</code> is denormalized from a <code>COUNT(*) FROM revisions WHERE user_id = ?</code>. This is done because edit count is checked on every edit (for protection-level gating) and displayed on user profiles. Recomputing it each time would require scanning millions of rows for active editors.</li>
    <li><strong>Index:</strong> B-tree index on <code>username</code> ‚Äî B-tree chosen over hash because we may need prefix lookups (user search) and ordering, not just exact matches.</li>
</ul>

<h4><code>articles</code></h4>
<table>
<tr><th>Column</th><th>Type</th><th>Constraints</th><th>Notes</th></tr>
<tr><td>article_id</td><td>BIGINT</td><td><span class="tag tag-pk">PRIMARY KEY</span> Auto-increment</td><td></td></tr>
<tr><td>title</td><td>VARCHAR(512)</td><td>NOT NULL <span class="tag tag-idx">INDEX: B-tree (title, language_code)</span></td><td>Composite index for lookups</td></tr>
<tr><td>language_code</td><td>VARCHAR(10)</td><td>NOT NULL</td><td>e.g., 'en', 'fr', 'de', 'ja'</td></tr>
<tr><td>namespace</td><td>INT</td><td>NOT NULL, DEFAULT 0</td><td>0=Article, 1=Talk, 2=User, 3=User Talk, etc.</td></tr>
<tr><td>current_revision_id</td><td>BIGINT</td><td><span class="tag tag-fk">FOREIGN KEY ‚Üí revisions.revision_id</span></td><td>Points to the latest revision</td></tr>
<tr><td>protection_level</td><td>ENUM('none','semi','full')</td><td>DEFAULT 'none'</td><td></td></tr>
<tr><td>is_redirect</td><td>BOOLEAN</td><td>DEFAULT FALSE</td><td></td></tr>
<tr><td>created_at</td><td>TIMESTAMP</td><td>NOT NULL</td><td></td></tr>
<tr><td>updated_at</td><td>TIMESTAMP</td><td>NOT NULL</td><td></td></tr>
</table>
<ul>
    <li><strong>Why SQL:</strong> Article metadata is structured. The <code>current_revision_id</code> foreign key must be updated atomically with the revision insert (ACID transaction). Need joins with revisions and users. Need composite index for title + language lookups.</li>
    <li><strong>Read events:</strong> Every article read (fetch metadata + current_revision_id), edit conflict checks, article creation duplicate-title check.</li>
    <li><strong>Write events:</strong> Article creation, every edit (updates <code>current_revision_id</code> and <code>updated_at</code>), protection level changes by admins.</li>
    <li><strong>Index:</strong> Composite B-tree index on <code>(title, language_code)</code> ‚Äî this is the primary lookup pattern. B-tree supports exact match and prefix queries on title (e.g., autocomplete). The composite index allows the query <code>WHERE title = ? AND language_code = ?</code> to use a single index scan.</li>
    <li><strong>Sharding:</strong> <span class="tag tag-shard">Shard by language_code</span> ‚Äî Each language edition is largely independent (different articles, different editors). Sharding by language_code ensures that all articles in a language are co-located, which aligns with Wikipedia's architecture of independent language wikis. This avoids cross-shard queries for within-language operations. For extremely large languages (English has 6M+ articles), further sub-sharding by <code>hash(article_id)</code> within the language shard can be done.</li>
</ul>

<h4><code>revisions</code></h4>
<table>
<tr><th>Column</th><th>Type</th><th>Constraints</th><th>Notes</th></tr>
<tr><td>revision_id</td><td>BIGINT</td><td><span class="tag tag-pk">PRIMARY KEY</span> Auto-increment</td><td></td></tr>
<tr><td>article_id</td><td>BIGINT</td><td><span class="tag tag-fk">FOREIGN KEY ‚Üí articles.article_id</span> NOT NULL</td><td></td></tr>
<tr><td>user_id</td><td>BIGINT</td><td><span class="tag tag-fk">FOREIGN KEY ‚Üí users.user_id</span></td><td>NULL for anonymous edits</td></tr>
<tr><td>parent_revision_id</td><td>BIGINT</td><td><span class="tag tag-fk">FOREIGN KEY ‚Üí revisions.revision_id</span> NULLABLE</td><td>NULL for the first revision</td></tr>
<tr><td>timestamp</td><td>TIMESTAMP</td><td>NOT NULL <span class="tag tag-idx">INDEX (composite)</span></td><td></td></tr>
<tr><td>comment</td><td>VARCHAR(500)</td><td></td><td>Edit summary</td></tr>
<tr><td>content_length</td><td>INT</td><td></td><td>Byte size of content</td></tr>
<tr><td>is_minor_edit</td><td>BOOLEAN</td><td>DEFAULT FALSE</td><td></td></tr>
</table>
<ul>
    <li><strong>Why SQL:</strong> Revisions need to be ordered by timestamp, joined with users and articles, and counted. The insert of a revision and the update of <code>articles.current_revision_id</code> must be in the same ACID transaction to prevent inconsistency. This table grows large (billions of rows) but access is always by <code>article_id</code> (which maps to shard key).</li>
    <li><strong>Read events:</strong> Viewing revision history (query by article_id, ordered by timestamp), conflict detection during edit (read current_revision_id).</li>
    <li><strong>Write events:</strong> Every article edit creates one new row.</li>
    <li><strong>Index:</strong> Composite B-tree index on <code>(article_id, timestamp DESC)</code> ‚Äî This is the critical index. Revision history queries always filter by article_id and sort by timestamp descending. This composite index allows a single index range scan for the query <code>WHERE article_id = ? ORDER BY timestamp DESC LIMIT 50</code> without touching the base table (covering index if comment and user_id are included).</li>
    <li><strong>Sharding:</strong> <span class="tag tag-shard">Shard by article_id (same shard as the articles table)</span> ‚Äî Co-locating revisions with their article avoids cross-shard joins and transactions. When the Edit Service creates a revision and updates the article in a single transaction, both rows are on the same shard.</li>
</ul>

<h4><code>categories</code></h4>
<table>
<tr><th>Column</th><th>Type</th><th>Constraints</th><th>Notes</th></tr>
<tr><td>category_id</td><td>BIGINT</td><td><span class="tag tag-pk">PRIMARY KEY</span></td><td></td></tr>
<tr><td>name</td><td>VARCHAR(255)</td><td>UNIQUE, NOT NULL <span class="tag tag-idx">INDEX: B-tree</span></td><td></td></tr>
<tr><td>parent_category_id</td><td>BIGINT</td><td><span class="tag tag-fk">FOREIGN KEY ‚Üí categories.category_id</span> NULLABLE</td><td>Self-referential for hierarchy</td></tr>
</table>
<ul>
    <li><strong>Why SQL:</strong> The category hierarchy is a tree structure naturally modeled with a self-referential foreign key. The dataset is small (~millions of categories). SQL handles the many-to-many relationship with articles via the join table below.</li>
    <li><strong>Read events:</strong> Displaying categories on article pages, browsing category trees.</li>
    <li><strong>Write events:</strong> Admin creates/renames/reorganizes categories (rare).</li>
</ul>

<h4><code>article_categories</code> (Join Table)</h4>
<table>
<tr><th>Column</th><th>Type</th><th>Constraints</th><th>Notes</th></tr>
<tr><td>article_id</td><td>BIGINT</td><td><span class="tag tag-pk">PK (composite)</span> <span class="tag tag-fk">FOREIGN KEY ‚Üí articles</span></td><td></td></tr>
<tr><td>category_id</td><td>BIGINT</td><td><span class="tag tag-pk">PK (composite)</span> <span class="tag tag-fk">FOREIGN KEY ‚Üí categories</span></td><td></td></tr>
</table>
<ul>
    <li><strong>Why SQL:</strong> Many-to-many relationship best modeled with a join table in a relational database. Composite primary key prevents duplicates.</li>
    <li><strong>Index:</strong> In addition to the composite PK index <code>(article_id, category_id)</code>, a reverse B-tree index on <code>(category_id, article_id)</code> supports the query "find all articles in category X".</li>
</ul>

<h4><code>watchlist</code></h4>
<table>
<tr><th>Column</th><th>Type</th><th>Constraints</th><th>Notes</th></tr>
<tr><td>user_id</td><td>BIGINT</td><td><span class="tag tag-pk">PK (composite)</span> <span class="tag tag-fk">FOREIGN KEY ‚Üí users</span></td><td></td></tr>
<tr><td>article_id</td><td>BIGINT</td><td><span class="tag tag-pk">PK (composite)</span> <span class="tag tag-fk">FOREIGN KEY ‚Üí articles</span></td><td></td></tr>
<tr><td>added_at</td><td>TIMESTAMP</td><td></td><td></td></tr>
</table>
<ul>
    <li><strong>Why SQL:</strong> Relational data linking users and articles. Need to efficiently query "which users watch article X?" (for notifications) and "which articles does user Y watch?" (for watchlist page).</li>
    <li><strong>Read events:</strong> Watchlist Notification Worker queries by article_id on edit. User views their watchlist page.</li>
    <li><strong>Write events:</strong> User adds/removes article from watchlist.</li>
    <li><strong>Indexes:</strong> The composite PK <code>(user_id, article_id)</code> supports "all articles user watches". An additional B-tree index on <code>(article_id)</code> supports "all users watching this article" (used by Watchlist Worker).</li>
</ul>

<!-- ========== NoSQL TABLES ========== -->
<h3>5.2 NoSQL Tables</h3>

<h4><code>article_content</code> (Key-Value / Document Store)</h4>
<table>
<tr><th>Field</th><th>Type</th><th>Notes</th></tr>
<tr><td>revision_id</td><td>BIGINT</td><td><span class="tag tag-pk">PARTITION KEY</span></td></tr>
<tr><td>raw_markup</td><td>TEXT (large)</td><td>Wiki markup source. Can be 10 KB to 500+ KB for large articles.</td></tr>
<tr><td>rendered_html</td><td>TEXT (large)</td><td>Pre-rendered HTML. Stored to avoid re-rendering on every read. Invalidated/regenerated on edit.</td></tr>
<tr><td>content_hash</td><td>VARCHAR(64)</td><td>SHA-256 hash for integrity verification and deduplication.</td></tr>
</table>
<ul>
    <li><strong>Why NoSQL (Key-Value/Document Store):</strong> Article content is a large text blob (potentially hundreds of KB) accessed exclusively by a single key (<code>revision_id</code>). There are no complex queries, no joins, no transactions needed on this data. A key-value store provides:
        <ul>
            <li>Horizontal scalability ‚Äî content is the largest dataset (billions of revisions √ó avg 50 KB = petabytes). Key-value stores shard trivially by key.</li>
            <li>High throughput for simple point reads ‚Äî Wikipedia's dominant access pattern.</li>
            <li>Flexible schema ‚Äî can easily add fields like <code>rendered_html</code> without migrations.</li>
        </ul>
    </li>
    <li><strong>Read events:</strong> Article read (fetch by <code>current_revision_id</code> from articles table), diff computation (fetch both revisions).</li>
    <li><strong>Write events:</strong> Every article edit stores a new document keyed by the new <code>revision_id</code>.</li>
    <li><strong>Sharding:</strong> <span class="tag tag-shard">Auto-sharded by revision_id (hash partitioning)</span> ‚Äî Key-value stores inherently partition by key. Hash partitioning of revision_id ensures even data distribution. No hotspot concern since revision IDs are sequential and distribute uniformly across hash space.</li>
</ul>

<h4><code>media_metadata</code> (Document Store ‚Äî could also be SQL)</h4>
<table>
<tr><th>Field</th><th>Type</th><th>Notes</th></tr>
<tr><td>media_id</td><td>VARCHAR(255)</td><td><span class="tag tag-pk">PARTITION KEY</span> e.g., "File:Solar_system.svg"</td></tr>
<tr><td>storage_url</td><td>VARCHAR(1024)</td><td>URL/path in object storage</td></tr>
<tr><td>content_type</td><td>VARCHAR(50)</td><td>MIME type: image/svg+xml, image/jpeg, etc.</td></tr>
<tr><td>file_size</td><td>BIGINT</td><td>Bytes</td></tr>
<tr><td>uploader_id</td><td>BIGINT</td><td>FK to users (logical, not enforced in NoSQL)</td></tr>
<tr><td>license</td><td>VARCHAR(100)</td><td>Creative Commons, public domain, etc.</td></tr>
<tr><td>description</td><td>TEXT</td><td></td></tr>
<tr><td>uploaded_at</td><td>TIMESTAMP</td><td></td></tr>
<tr><td>thumbnails</td><td>MAP&lt;String, String&gt;</td><td>Pre-generated thumbnail URLs at various sizes</td></tr>
</table>
<ul>
    <li><strong>Why NoSQL (Document Store):</strong> Media metadata is accessed by media_id (point reads). The <code>thumbnails</code> field is a nested map of varying sizes, which fits naturally in a document model. No joins needed ‚Äî media metadata is self-contained. Horizontal scalability for potentially billions of media files.</li>
    <li><strong>Read events:</strong> Article rendering resolves media_id to storage_url and thumbnail URLs.</li>
    <li><strong>Write events:</strong> Media upload.</li>
</ul>

<h3>5.3 Search Index (Inverted Index ‚Äî Specialized)</h3>
<table>
<tr><th>Field</th><th>Type</th><th>Notes</th></tr>
<tr><td>token</td><td>VARCHAR</td><td>Stemmed/normalized word</td></tr>
<tr><td>posting_list</td><td>LIST&lt;(article_id, positions, tf_score)&gt;</td><td>Sorted by relevance</td></tr>
<tr><td>document_frequency</td><td>INT</td><td>Number of articles containing this token</td></tr>
</table>
<ul>
    <li>This is a dedicated <strong>inverted index</strong>, which is the standard data structure for full-text search. Each token maps to a posting list of articles containing that token, along with positional information for phrase queries and term frequency for BM25 ranking.</li>
    <li><strong>Why inverted index:</strong> An inverted index is specifically designed for full-text search. It allows O(1) lookup of all articles containing a given word, and intersection of posting lists for multi-word queries. Alternatives like LIKE queries or regex on a SQL database would require full table scans on millions of articles ‚Äî completely infeasible at Wikipedia's scale.</li>
    <li><strong>Index on article titles:</strong> A separate <strong>trie (prefix tree)</strong> index on article titles supports autocomplete/suggest functionality. This allows O(k) prefix lookups where k is the length of the typed prefix.</li>
    <li><strong>Partitioning:</strong> The search index is partitioned by <code>language_code</code>. Each language has its own independent index, since searches are almost always within a single language. This also allows language-specific tokenization and stemming.</li>
    <li><strong>Updated asynchronously</strong> by the Search Index Worker via the message queue after every edit.</li>
</ul>

<!-- ============================================================ -->
<h2>6. CDN Deep Dive</h2>
<!-- ============================================================ -->
<div class="section">
<p>The CDN is the <strong>most critical scaling component</strong> of Wikipedia's architecture. Given the ~1000:1 read-to-write ratio, the CDN absorbs the vast majority of read traffic, preventing it from ever reaching backend servers.</p>

<h4>Why CDN is Appropriate</h4>
<ul>
    <li><strong>Extreme read-heaviness:</strong> ~95% of traffic is anonymous users reading articles. The content is the same for every reader (no personalization on article pages), making it perfectly cacheable.</li>
    <li><strong>Global user base:</strong> Users access Wikipedia from every country. CDN edge nodes worldwide reduce latency from seconds to milliseconds.</li>
    <li><strong>Static-like content:</strong> Most articles are edited infrequently. The median article is edited less than once per month, meaning cached versions are valid for long periods.</li>
    <li><strong>Large media files:</strong> Images, diagrams, and other media are immutable once uploaded and are served for years. CDN caching is ideal.</li>
</ul>

<h4>CDN Caching Strategy</h4>
<table>
<tr><th>Aspect</th><th>Strategy</th><th>Rationale</th></tr>
<tr>
    <td><strong>Cache Model</strong></td>
    <td>Pull-based (origin-pull)</td>
    <td>CDN fetches from origin on cache miss and caches the response. Simpler than push-based; the CDN automatically handles cache population. Push-based would require us to proactively push all 60M+ articles to all edge nodes, which is wasteful since most articles are rarely accessed.</td>
</tr>
<tr>
    <td><strong>Cache Key</strong></td>
    <td><code>URL path</code> (e.g., <code>/en/wiki/Albert_Einstein</code>)</td>
    <td>Article pages are not personalized for anonymous users. The URL uniquely identifies the content. Logged-in users are served a slightly different page (with edit buttons), so the cache key includes a <code>Vary: Cookie</code> header to serve uncached versions to logged-in users.</td>
</tr>
<tr>
    <td><strong>Expiration (TTL)</strong></td>
    <td>Article HTML: <strong>5‚Äì10 minutes</strong><br/>Static assets (CSS/JS): <strong>1 year</strong> (fingerprinted)<br/>Media (images): <strong>30 days</strong></td>
    <td>Short TTL for articles balances freshness and cache hit rate. Static assets use content-hash fingerprinting so the URL changes on deploy, allowing very long TTLs. Media is immutable.</td>
</tr>
<tr>
    <td><strong>Invalidation</strong></td>
    <td>Active purge on edit via Cache Invalidation Worker</td>
    <td>When an article is edited, the worker sends an HTTP PURGE request to the CDN for that article's URL. This ensures stale content is not served after an edit. Without active purging, users could see stale content for up to 10 minutes after an edit.</td>
</tr>
<tr>
    <td><strong>Eviction</strong></td>
    <td>LRU (managed by CDN infrastructure)</td>
    <td>CDN edge nodes have finite storage. LRU ensures the most popular articles (Einstein, COVID-19, etc.) stay cached while rarely accessed articles are evicted. This aligns with Wikipedia's access pattern, which follows a power-law distribution.</td>
</tr>
</table>
</div>

<!-- ============================================================ -->
<h2>7. In-Memory Cache Deep Dive</h2>
<!-- ============================================================ -->
<div class="section">
<p>An in-memory cache sits between the application services and the databases. It serves as a <strong>second layer of caching</strong> behind the CDN, reducing database load for requests that miss the CDN but hit the application layer.</p>

<h4>Why In-Memory Cache is Appropriate</h4>
<ul>
    <li>Even with a CDN, ~5% of reads reach the origin. At 10 billion page views/month, that's ~500 million requests/month hitting the application layer. Without a cache, every one of those would query both SQL (metadata) and NoSQL (content), putting enormous pressure on the databases.</li>
    <li>Hot articles (trending topics, current events, perennially popular articles) are read millions of times. Caching them in memory avoids repeated database roundtrips.</li>
    <li>The in-memory cache can also cache rendered HTML, eliminating wiki-markup-to-HTML rendering overhead on cache hits.</li>
</ul>

<table>
<tr><th>Aspect</th><th>Strategy</th><th>Rationale</th></tr>
<tr>
    <td><strong>Caching Strategy</strong></td>
    <td><strong>Cache-Aside (Lazy Loading)</strong></td>
    <td>On a read request, the Article Service first checks the cache. On a hit, it returns the cached data. On a miss, it queries the databases, constructs the response, stores it in the cache, and then returns it. This is preferred over write-through because: (1) Wikipedia is overwhelmingly read-heavy, so populating the cache on writes would cache articles that may never be read, (2) Cache-aside is simpler and tolerates cache failures gracefully (requests fall through to DB).</td>
</tr>
<tr>
    <td><strong>Cache Key</strong></td>
    <td><code>article:{language_code}:{title}</code></td>
    <td>Includes language to partition by wiki edition. Title is the natural lookup key.</td>
</tr>
<tr>
    <td><strong>Cached Value</strong></td>
    <td>Rendered HTML + metadata JSON</td>
    <td>Caching the fully rendered HTML eliminates both database lookups and wiki-to-HTML rendering on hits.</td>
</tr>
<tr>
    <td><strong>Eviction Policy</strong></td>
    <td><strong>LRU (Least Recently Used)</strong></td>
    <td>Wikipedia's access pattern follows a Zipf/power-law distribution: a small fraction of articles receive the vast majority of traffic. LRU naturally keeps these hot articles in cache while evicting rarely-accessed ones. LRU is preferred over LFU here because trending articles (e.g., a person who just won an award) need to be cached quickly based on recent access, not long-term frequency.</td>
</tr>
<tr>
    <td><strong>Expiration Policy</strong></td>
    <td><strong>TTL of 5 minutes</strong></td>
    <td>Acts as a safety net in case an invalidation message is lost. Even if the message queue drops a cache invalidation event, the stale data will expire within 5 minutes. This bounds the staleness window.</td>
</tr>
<tr>
    <td><strong>Invalidation</strong></td>
    <td><strong>Event-driven invalidation via message queue</strong></td>
    <td>When the Cache Invalidation Worker receives an <code>article_edited</code> event, it sends a DELETE command to the cache for the affected article's key. This is proactive invalidation layered on top of the TTL safety net. Together, they ensure that edits are reflected quickly (~seconds) while tolerating message delivery failures gracefully.</td>
</tr>
<tr>
    <td><strong>Cache Population</strong></td>
    <td>On-demand (cache miss triggers population)</td>
    <td>The cache is populated when a cache miss occurs and the Article Service fetches from the databases. We do NOT pre-warm the cache because (1) it's impossible to predict which of 60M+ articles will be requested, and (2) the CDN already handles the most popular articles. However, on a cold start or after a cache cluster restart, we could optionally pre-warm with the top 10,000 most popular articles to reduce the thundering herd effect.</td>
</tr>
</table>
</div>

<!-- ============================================================ -->
<h2>8. Message Queue Deep Dive</h2>
<!-- ============================================================ -->
<div class="section">
<h4>Why a Message Queue?</h4>
<p>When an article is edited, three side effects must occur: (1) cache/CDN invalidation, (2) search index update, (3) watchlist notifications. If the Edit Service performed all three synchronously, the edit request latency would increase dramatically (index update alone can take seconds) and a failure in any one subsystem would block the edit. The message queue decouples these concerns:</p>
<ul>
    <li><strong>Reduced latency:</strong> The Edit Service writes the revision and publishes a single message, then returns immediately. Side effects happen asynchronously.</li>
    <li><strong>Fault isolation:</strong> If the search index is temporarily down, edits still succeed. Messages queue up and are processed when the search index recovers.</li>
    <li><strong>Independent scaling:</strong> Each consumer (cache worker, search worker, watchlist worker) can be scaled independently based on its own throughput needs.</li>
</ul>

<h4>Why Not Alternatives?</h4>
<table>
<tr><th>Alternative</th><th>Why Not Used</th></tr>
<tr>
    <td><strong>Pub/Sub</strong></td>
    <td>Pub/sub is designed for fan-out to dynamic, unknown subscribers. Wikipedia's edit-event consumers are fixed and well-known (3 consumer types). A message queue with consumer groups provides the same fan-out with better durability guarantees and replay ability. Pub/sub's "fire and forget" nature makes it harder to guarantee at-least-once processing.</td>
</tr>
<tr>
    <td><strong>Synchronous RPC</strong></td>
    <td>Calling each downstream service synchronously would triple edit latency and create tight coupling. A failure in the search index would cause edits to fail, which is unacceptable.</td>
</tr>
<tr>
    <td><strong>Polling (DB-based outbox)</strong></td>
    <td>Workers could poll a database outbox table for new edits. This works but introduces polling latency (seconds to minutes), wastes resources with empty polls, and puts additional load on the database. A message queue provides push-based, real-time delivery.</td>
</tr>
<tr>
    <td><strong>WebSocket / Server-Sent Events</strong></td>
    <td>These are client-facing push mechanisms. Our need is backend-to-backend async processing, which is better served by a message queue.</td>
</tr>
</table>

<h4>Message Queue Mechanics</h4>
<ul>
    <li><strong>Publishing:</strong> The Edit Service publishes a message to the queue after successfully committing the revision to the database. The message format is <code>{ event: "article_edited", article_id: 1066, revision_id: 5501, title: "Battle_of_Hastings", language_code: "en", editor_id: 42, timestamp: "..." }</code>.</li>
    <li><strong>Consumer Groups:</strong> Three independent consumer groups subscribe to the queue:
        <ol>
            <li><strong>cache-invalidation-group</strong> ‚Äî deletes cache entries and purges CDN.</li>
            <li><strong>search-index-group</strong> ‚Äî re-indexes article content.</li>
            <li><strong>watchlist-notification-group</strong> ‚Äî creates notification entries.</li>
        </ol>
        Each consumer group gets its own copy of every message and maintains its own offset/cursor. This ensures all three side effects are triggered independently.
    </li>
    <li><strong>Delivery guarantee:</strong> At-least-once delivery. If a consumer crashes mid-processing, the message is redelivered. Consumers must be idempotent (e.g., cache DELETE is naturally idempotent; search index update is idempotent because it overwrites the full document).</li>
    <li><strong>Message removal:</strong> Messages are acknowledged (committed) by each consumer group after successful processing. The queue retains messages until all consumer groups have acknowledged them (or until a retention period expires). This allows slow consumers to catch up and supports replaying from a past offset if needed.</li>
    <li><strong>Ordering:</strong> Messages for the same <code>article_id</code> are routed to the same partition (using article_id as the partition key). This ensures that edits to the same article are processed in order within each consumer group, preventing race conditions (e.g., an older edit's index update overwriting a newer one).</li>
</ul>
</div>

<!-- ============================================================ -->
<h2>9. Scaling Considerations</h2>
<!-- ============================================================ -->
<div class="section">

<h3>9.1 Load Balancers</h3>
<p>Load balancers are placed at two levels in the architecture:</p>
<table>
<tr><th>Position</th><th>Type</th><th>Algorithm</th><th>Role</th></tr>
<tr>
    <td><strong>Between CDN and Application Layer</strong></td>
    <td>Layer 7 (HTTP/Application)</td>
    <td>Least Connections with path-based routing</td>
    <td>Routes requests by URL prefix to the appropriate service: <code>/wiki/*</code> ‚Üí Article Service, <code>/api/articles/*/edit</code> ‚Üí Edit Service, <code>/api/search</code> ‚Üí Search Service, <code>/api/articles/*/history</code> ‚Üí Revision Service, <code>/api/auth/*</code> ‚Üí User Service, <code>/api/media/*</code> ‚Üí Media Service. Performs TLS termination. Health checks every 5 seconds with automatic instance removal on 3 consecutive failures.</td>
</tr>
<tr>
    <td><strong>Between Application Layer and Database Read Replicas</strong></td>
    <td>Layer 4 (TCP/Transport)</td>
    <td>Round Robin</td>
    <td>Distributes read queries across multiple SQL read replicas. The primary SQL instance handles all writes. Read-heavy services (Article Service, Search Service, Revision Service) exclusively query through the read-replica load balancer.</td>
</tr>
</table>

<h4>Load Balancer Deep Dive</h4>
<ul>
    <li><strong>Why Layer 7 at the edge:</strong> Path-based routing is essential because different URL paths map to different backend services. Layer 7 load balancers inspect the HTTP request path and route accordingly. They also handle TLS termination, reducing CPU overhead on backend services.</li>
    <li><strong>Why Least Connections:</strong> Wikipedia's requests have variable processing times (a complex article render takes longer than serving from cache). Least-connections distributes load more evenly than round-robin when request durations vary. This prevents slow requests from backing up on a single instance.</li>
    <li><strong>High availability:</strong> Load balancers are deployed in active-passive pairs. If the active LB fails, the passive takes over via health check + virtual IP (VIP) failover. Multiple LB pairs can be deployed across availability zones.</li>
    <li><strong>Auto-scaling integration:</strong> The load balancer's target group is dynamically updated as application instances are added/removed by the auto-scaler.</li>
</ul>

<h3>9.2 Horizontal Scaling Strategy</h3>

<table>
<tr><th>Component</th><th>Scaling Approach</th><th>Details</th></tr>
<tr>
    <td><strong>CDN</strong></td>
    <td>Managed globally by CDN provider</td>
    <td>Absorbs ~95% of read traffic. The single most important scaling lever. Each edge node caches independently. Adding points of presence (PoPs) in new regions reduces latency for underserved areas.</td>
</tr>
<tr>
    <td><strong>Application Services</strong></td>
    <td>Horizontal auto-scaling</td>
    <td>All services (Article, Edit, Search, Revision, User, Media) are stateless and can be horizontally scaled behind the load balancer. Auto-scale based on CPU utilization (target: 60%) and request queue depth. Scale-up during peak hours, scale-down during off-peak.</td>
</tr>
<tr>
    <td><strong>In-Memory Cache</strong></td>
    <td>Cluster with consistent hashing</td>
    <td>Cache cluster uses consistent hashing to distribute keys across nodes. Adding a node only remaps ~1/N of keys. This allows the cache to scale with traffic without mass invalidation.</td>
</tr>
<tr>
    <td><strong>SQL Database</strong></td>
    <td>Primary-Replica + Sharding</td>
    <td>One primary for writes, multiple read replicas (scale out reads by adding replicas). For very large languages (English), shard by <code>hash(article_id)</code>. Cross-shard queries are rare because most operations are within a single article's shard.</td>
</tr>
<tr>
    <td><strong>NoSQL Content Store</strong></td>
    <td>Horizontal sharding by key</td>
    <td>NoSQL stores naturally shard by partition key (revision_id). Add nodes to the cluster to handle more data and traffic. Replication factor of 3 for durability.</td>
</tr>
<tr>
    <td><strong>Search Index</strong></td>
    <td>Partition by language + shard within language</td>
    <td>Each language has its own index partition. Large languages can be further sharded by hash(article_id) range. Index replicas handle read load for popular search queries.</td>
</tr>
<tr>
    <td><strong>Object Storage</strong></td>
    <td>Managed horizontal scaling</td>
    <td>Object stores scale inherently (virtually unlimited capacity). Served primarily through CDN, so origin load is minimal.</td>
</tr>
<tr>
    <td><strong>Message Queue</strong></td>
    <td>Partition-based scaling</td>
    <td>Message queue partitions by article_id. Adding partitions increases throughput. Consumer instances scale independently per consumer group.</td>
</tr>
</table>

<h3>9.3 Rate Limiting &amp; Abuse Prevention</h3>
<ul>
    <li>Rate limiting at the load balancer level: max 100 edits/minute per user, max 1000 reads/minute per IP.</li>
    <li>CAPTCHA on edit for anonymous/new users to prevent bot vandalism.</li>
    <li>Automated vandalism detection: a lightweight ML model or rule-based filter runs on edit content before committing.</li>
</ul>

<h3>9.4 Multi-Region Deployment</h3>
<ul>
    <li>Deploy application stacks in multiple geographic regions (e.g., US-East, Europe, Asia-Pacific).</li>
    <li>CDN routes users to the nearest region.</li>
    <li>Database replication across regions for disaster recovery. Writes go to the primary region and replicate asynchronously. Reads can be served from local replicas (eventual consistency).</li>
    <li>For strong consistency on edits, the edit request is routed to the primary region regardless of user location. This adds latency for cross-region editors but guarantees conflict detection accuracy.</li>
</ul>
</div>

<!-- ============================================================ -->
<h2>10. Tradeoffs &amp; Deep Dives</h2>
<!-- ============================================================ -->
<div class="section">

<h3>10.1 Full Content Storage vs. Delta/Diff Storage for Revisions</h3>
<div class="callout">
<strong>Tradeoff:</strong> Store full content for each revision, or store deltas (diffs) between consecutive revisions?
</div>
<table>
<tr><th>Approach</th><th>Pros</th><th>Cons</th></tr>
<tr>
    <td><strong>Full content per revision (chosen)</strong></td>
    <td>Simple reads ‚Äî fetch one key-value pair to get any revision's content. No chain of deltas to reconstruct. O(1) read complexity.</td>
    <td>Higher storage cost. A large article (200 KB) with 5,000 revisions stores ~1 GB (though many revisions differ by only a few bytes).</td>
</tr>
<tr>
    <td><strong>Delta storage</strong></td>
    <td>Dramatically less storage ‚Äî only store the diff between consecutive revisions.</td>
    <td>To read revision N, you may need to reconstruct from revision 1 + N-1 deltas. O(N) read complexity. Periodic snapshots mitigate this but add complexity.</td>
</tr>
</table>
<p><strong>Decision:</strong> We choose full content storage for simplicity and read performance. Storage is cheap and scales horizontally in the NoSQL content store. The dominant access pattern is reading the <em>current</em> revision (O(1) regardless), and diffs between revisions are computed on-the-fly. If storage becomes a concern, we can add transparent compression (articles compress very well due to repetitive wiki markup) or introduce periodic snapshots with deltas in between.</p>

<h3>10.2 Pre-computed Diffs vs. On-the-fly Diff Computation</h3>
<div class="callout">
<strong>Tradeoff:</strong> Pre-compute and store diffs between all consecutive revisions, or compute them on-demand?
</div>
<table>
<tr><th>Approach</th><th>Pros</th><th>Cons</th></tr>
<tr>
    <td><strong>On-the-fly computation (chosen)</strong></td>
    <td>No additional storage. Supports arbitrary revision-pair diffs (not just consecutive ones).</td>
    <td>Higher compute cost per diff view. Latency: ~50-200 ms per diff computation.</td>
</tr>
<tr>
    <td><strong>Pre-computed diffs</strong></td>
    <td>Instant diff display. Zero compute on read.</td>
    <td>Enormous storage for all possible pairs. Even consecutive-only diffs are billions of records. Cannot support arbitrary pair comparison without computing on-the-fly anyway.</td>
</tr>
</table>
<p><strong>Decision:</strong> On-the-fly computation. Diff views are a tiny fraction (&lt;0.1%) of total traffic. The computation is cheap (diff algorithm on two text documents) and can be cached in the in-memory cache if a particular diff is accessed frequently. Pre-computing all diffs would be a massive storage waste for a rarely-used feature.</p>

<h3>10.3 Rendered HTML Storage vs. On-demand Rendering</h3>
<div class="callout">
<strong>Tradeoff:</strong> Store pre-rendered HTML alongside wiki markup, or render on every request?
</div>
<p><strong>Decision:</strong> We store pre-rendered HTML in the <code>article_content</code> NoSQL table. Rendering wiki markup to HTML is CPU-intensive (parsing templates, resolving transclusions, formatting tables, etc.) and can take 100‚Äì500 ms for complex articles. By pre-rendering at edit time (or via an async worker), we eliminate this cost from the read path. The additional storage (~2√ó the raw markup) is negligible compared to the CPU savings at Wikipedia's read volume. The pre-rendered HTML is invalidated and regenerated on each edit.</p>

<h3>10.4 Optimistic Concurrency Control vs. Pessimistic Locking</h3>
<div class="callout">
<strong>Tradeoff:</strong> Use optimistic concurrency (detect conflicts at save time) or pessimistic locking (lock the article while editing)?
</div>
<table>
<tr><th>Approach</th><th>Pros</th><th>Cons</th></tr>
<tr>
    <td><strong>Optimistic (chosen)</strong></td>
    <td>No blocking. Multiple editors can work simultaneously. Better UX for a wiki where edit conflicts are rare.</td>
    <td>Conflict resolution UX needed when conflicts occur. Lost-update risk if not implemented correctly.</td>
</tr>
<tr>
    <td><strong>Pessimistic locking</strong></td>
    <td>Guaranteed no conflicts. Simpler mental model.</td>
    <td>Blocks other editors while one person is editing. Deadlocks. Lock expiry issues (what if a user opens edit and walks away?). Terrible UX for a collaborative platform.</td>
</tr>
</table>
<p><strong>Decision:</strong> Optimistic concurrency control using <code>base_revision_id</code> comparison. The vast majority of edits (>99%) do not conflict because articles have many editors but edits are infrequent relative to the article count. When a conflict does occur, the user is shown a diff and asked to merge ‚Äî this is the established Wikipedia workflow that editors are familiar with.</p>

<h3>10.5 Denormalization Decisions</h3>
<ul>
    <li><strong><code>users.edit_count</code></strong> (denormalized): Avoids <code>SELECT COUNT(*) FROM revisions WHERE user_id = ?</code> which would scan millions of rows for prolific editors. Updated incrementally on each edit. Acceptable because exact accuracy is not critical (off by one is fine) and the counter is read far more often than articles are edited.</li>
    <li><strong><code>articles.current_revision_id</code></strong> (denormalized pointer): Avoids <code>SELECT MAX(revision_id) FROM revisions WHERE article_id = ?</code> on every read. Updated atomically with revision insert. This is critical for read performance since every article view requires this value.</li>
</ul>
</div>

<!-- ============================================================ -->
<h2>11. Alternative Approaches</h2>
<!-- ============================================================ -->
<div class="section">

<div class="alt-box">
<strong>Alternative 1: CRDT-based Real-time Collaborative Editing (like Google Docs)</strong><br/>
<em>Description:</em> Instead of the edit-save-conflict model, use Conflict-free Replicated Data Types (CRDTs) to enable real-time collaborative editing where multiple users see each other's cursors and edits live.<br/>
<em>Why not chosen:</em> Wikipedia's editing model is intentionally deliberative. Editors make thoughtful edits, write edit summaries, and submit discrete revisions that can be reviewed and reverted. Real-time collaboration would change the fundamental editing paradigm, make individual revisions harder to attribute and review, and introduce significant infrastructure complexity (WebSocket connections, CRDT resolution, operational transforms). It's a different product.
</div>

<div class="alt-box">
<strong>Alternative 2: Graph Database for Category Hierarchy</strong><br/>
<em>Description:</em> Model the category hierarchy (which forms a directed acyclic graph) in a graph database instead of SQL self-referential foreign keys.<br/>
<em>Why not chosen:</em> While category traversal would be faster in a graph database, the category structure is queried infrequently (mainly for browsing categories and building breadcrumbs). The overhead of operating a separate graph database‚Äîadditional infrastructure, backup procedures, monitoring, and team expertise‚Äîis not justified for this minor query pattern. SQL handles the hierarchical queries adequately with recursive CTEs or pre-computed closure tables.
</div>

<div class="alt-box">
<strong>Alternative 3: Event Sourcing Architecture</strong><br/>
<em>Description:</em> Model the entire system as an event store where every action (create, edit, categorize, etc.) is an immutable event, and current state is derived by replaying events.<br/>
<em>Why not chosen:</em> Wikipedia's revision model is already a form of event sourcing for article content (every revision is an immutable snapshot). However, applying event sourcing globally (users, categories, watchlists, etc.) adds enormous complexity without proportional benefit. The overhead of event replay, snapshotting, and CQRS projection maintenance is not worth it for data that fits naturally in a simple CRUD model.
</div>

<div class="alt-box">
<strong>Alternative 4: Store All Content in SQL (no separate NoSQL store)</strong><br/>
<em>Description:</em> Use a TEXT column in the SQL <code>revisions</code> table to store article content directly, eliminating the NoSQL content store.<br/>
<em>Why not chosen:</em> While simpler architecturally (one fewer data store), large TEXT blobs in SQL rows cause performance issues: (1) row size becomes huge, degrading index performance, (2) SQL databases are optimized for structured data, not large unstructured blobs, (3) SQL scaling (sharding, replication) becomes harder with multi-hundred-KB rows, (4) the content store's access pattern (simple key-value lookup) maps perfectly to NoSQL's strengths. The marginal operational complexity of a separate NoSQL store is justified by the performance and scalability gains.
</div>

<div class="alt-box">
<strong>Alternative 5: Server-Sent Events (SSE) for Real-time Watchlist Notifications</strong><br/>
<em>Description:</em> Instead of polling or refreshing the watchlist page, push edit notifications to users in real-time via SSE or WebSocket connections.<br/>
<em>Why not chosen:</em> Wikipedia's watchlist is consumed by editors who check it periodically (it's not a real-time chat). Maintaining persistent connections for millions of logged-in users would consume significant server resources (memory for connection state, CPU for heartbeats). The benefit is marginal‚Äîeditors don't need sub-second notifications. The simpler approach of querying the watchlist on page load (a SQL query against a well-indexed table) is sufficient and dramatically simpler to operate.
</div>

<div class="alt-box">
<strong>Alternative 6: Separate Microservice Per Language Wiki</strong><br/>
<em>Description:</em> Deploy a completely independent infrastructure stack for each language edition (en.wikipedia, fr.wikipedia, etc.).<br/>
<em>Why not chosen:</em> While this provides strong isolation, it dramatically increases operational overhead (300+ independent deployments, each needing monitoring, scaling, and patching). A shared-infrastructure approach with language-code-based sharding achieves similar isolation at the data level while sharing compute, caching, and CDN infrastructure. Cross-language features (interlanguage links, Wikidata integration) also benefit from shared infrastructure.
</div>
</div>

<!-- ============================================================ -->
<h2>12. Additional Information</h2>
<!-- ============================================================ -->
<div class="section">

<h3>12.1 Vandalism Detection Pipeline</h3>
<p>Wikipedia is open for editing, which makes it a target for vandalism. A lightweight pipeline runs inline on every edit:</p>
<ol>
    <li><strong>Rule-based filters:</strong> Regex patterns detect profanity, mass deletion (&gt;80% content removed), and known spam URLs. These run in &lt;10 ms.</li>
    <li><strong>ML-based scoring:</strong> A trained classification model scores each edit for vandalism probability (0.0 to 1.0). Edits above 0.9 are auto-reverted. Edits between 0.7 and 0.9 are flagged for human review. This runs in ~50 ms.</li>
    <li>The pipeline runs synchronously in the Edit Service before committing the revision, so vandalism can be blocked or flagged before it's visible.</li>
</ol>

<h3>12.2 Wikipedia-Specific Rendering Complexity</h3>
<p>Wiki markup rendering is significantly more complex than Markdown rendering:</p>
<ul>
    <li><strong>Templates:</strong> Articles transclude shared templates (e.g., infoboxes, citation templates). Rendering requires fetching and parsing template definitions recursively.</li>
    <li><strong>Lua modules:</strong> Some templates execute Lua scripts for complex formatting logic.</li>
    <li><strong>Transclusion:</strong> Articles can embed content from other articles.</li>
    <li>This is why pre-rendering and caching HTML is especially important‚Äîre-rendering a complex article can take 500+ ms and involves fetching multiple template definitions.</li>
</ul>

<h3>12.3 Redirects</h3>
<p>Wikipedia has millions of redirect articles (e.g., "USA" ‚Üí "United States"). The Article Service handles redirects at the application level:</p>
<ol>
    <li>Fetch article metadata for the requested title.</li>
    <li>If <code>is_redirect = true</code>, parse the target from the content and issue an HTTP 301 redirect.</li>
    <li>The CDN caches the 301 redirect response, so subsequent requests for "USA" are redirected at the CDN edge without hitting the backend.</li>
</ol>

<h3>12.4 Talk Page Implementation</h3>
<p>Talk pages are stored as regular articles in the SQL <code>articles</code> table with <code>namespace = 1</code> (Talk namespace). They use the exact same edit, revision, and rendering infrastructure. The only difference is the namespace value, which determines how the UI renders them (threaded discussion view vs. article view). This reuse of infrastructure is a significant simplification.</p>

<h3>12.5 Data Backup &amp; Disaster Recovery</h3>
<ul>
    <li>SQL databases: Daily full backups + continuous Write-Ahead Log (WAL) streaming to object storage. Point-in-time recovery to any second within the retention window (30 days).</li>
    <li>NoSQL content store: Replication factor of 3 across availability zones. Daily snapshots to object storage.</li>
    <li>Object storage: Inherently durable (11 nines of durability in typical implementations). Cross-region replication for disaster recovery.</li>
    <li>Recovery Time Objective (RTO): &lt;1 hour. Recovery Point Objective (RPO): &lt;1 minute.</li>
</ul>
</div>

<!-- ============================================================ -->
<h2>13. Vendor Suggestions</h2>
<!-- ============================================================ -->
<div class="section">
<p>The design above is vendor-agnostic. Below are potential vendor choices for each component category, with rationale:</p>

<table>
<tr><th>Component</th><th>Vendor Options</th><th>Rationale</th></tr>
<tr>
    <td><strong>SQL Database</strong></td>
    <td>PostgreSQL, MySQL/MariaDB</td>
    <td>PostgreSQL offers excellent support for full-text search (supplementary to the dedicated search index), recursive CTEs for category hierarchies, and robust replication. MySQL/MariaDB is battle-tested at Wikipedia's actual scale (the real Wikipedia uses MariaDB). Both support read replicas and have mature sharding solutions.</td>
</tr>
<tr>
    <td><strong>NoSQL Content Store</strong></td>
    <td>Apache Cassandra, Amazon DynamoDB, ScyllaDB</td>
    <td>Cassandra excels at high-throughput key-value reads/writes with tunable consistency, scales horizontally, and handles large values. DynamoDB offers fully managed scaling with single-digit-ms latency. ScyllaDB offers Cassandra-compatible API with better per-node performance.</td>
</tr>
<tr>
    <td><strong>In-Memory Cache</strong></td>
    <td>Redis, Memcached</td>
    <td>Memcached is simple and purpose-built for caching (pure key-value, multi-threaded). Redis adds data structures (useful if we need sorted sets for other features) and persistence. For pure article HTML caching, Memcached's simplicity is appealing. The real Wikipedia uses Memcached.</td>
</tr>
<tr>
    <td><strong>Search Index</strong></td>
    <td>Elasticsearch, Apache Solr, Meilisearch</td>
    <td>Elasticsearch is the industry standard for full-text search at scale: inverted index, BM25 ranking, multi-language tokenizers/stemmers, horizontal scaling via shards and replicas, and built-in autocomplete. The real Wikipedia uses Elasticsearch (CirrusSearch). Solr is a viable alternative with similar capabilities.</td>
</tr>
<tr>
    <td><strong>Message Queue</strong></td>
    <td>Apache Kafka, RabbitMQ, Amazon SQS</td>
    <td>Kafka is ideal for our use case: multiple consumer groups on the same topic, message replay from offsets, partition-based ordering by article_id, and high throughput. RabbitMQ offers simpler setup but lacks replay and multi-consumer-group semantics. The real Wikipedia uses Kafka.</td>
</tr>
<tr>
    <td><strong>Object Storage</strong></td>
    <td>Amazon S3, Google Cloud Storage, MinIO (self-hosted)</td>
    <td>All offer virtually unlimited storage, high durability (11 nines), and HTTP-accessible objects. S3 is the most widely adopted. MinIO provides an S3-compatible self-hosted alternative. Wikipedia uses Swift (OpenStack Object Storage) for media.</td>
</tr>
<tr>
    <td><strong>CDN</strong></td>
    <td>Cloudflare, Fastly, Akamai, Amazon CloudFront</td>
    <td>Fastly offers instant purge (milliseconds), which is critical for our cache invalidation on edit. Cloudflare offers the broadest edge network. Wikipedia uses a custom CDN built on Varnish, but Fastly/Cloudflare are the closest commercial equivalents.</td>
</tr>
<tr>
    <td><strong>Load Balancer</strong></td>
    <td>HAProxy, Nginx, AWS ALB/NLB, Envoy</td>
    <td>HAProxy is extremely high-performance for Layer 7 routing with health checks. Nginx offers additional reverse proxy and static serving capabilities. Envoy is ideal in a microservices/service mesh context. Wikipedia uses LVS + HAProxy.</td>
</tr>
</table>
</div>

<hr style="border-color: var(--border); margin: 3rem 0;">
<p style="text-align: center; color: var(--text-muted);">System Design Document ‚Äî Wikipedia ‚Äî Generated February 2026</p>

<script>
    mermaid.initialize({ 
        startOnLoad: true, 
        theme: 'default',
        flowchart: {
            useMaxWidth: true,
            htmlLabels: true,
            curve: 'basis'
        },
        securityLevel: 'loose'
    });
</script>

</body>
</html>
