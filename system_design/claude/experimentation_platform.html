<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Experimentation Platform - System Design</title>
<style>
:root{--bg:#0f0f0f;--surface:#1a1a1a;--border:#2a2a2a;--text:#e0e0e0;--dim:#888;--accent:#4CAF50;--accent2:#FF9800;}
*{margin:0;padding:0;box-sizing:border-box}
body{background:var(--bg);color:var(--text);font-family:'Segoe UI',system-ui,sans-serif;line-height:1.7;padding:2rem;max-width:1400px;margin:0 auto}
h1{color:var(--accent);font-size:2.2rem;border-bottom:3px solid var(--accent);padding-bottom:.5rem;margin-bottom:1.5rem}
h2{color:var(--accent2);margin:2rem 0 1rem;font-size:1.5rem}
h3{color:#81D4FA;margin:1.5rem 0 .75rem}
h4{color:#CE93D8;margin:1rem 0 .5rem}
.card{background:var(--surface);border:1px solid var(--border);border-radius:12px;padding:1.5rem;margin:1rem 0}
.grid{display:grid;grid-template-columns:repeat(auto-fit,minmax(340px,1fr));gap:1rem}
table{width:100%;border-collapse:collapse;margin:1rem 0}
th,td{border:1px solid var(--border);padding:.75rem;text-align:left;font-size:.95rem}
th{background:#1e3a2e;color:var(--accent)}
.tag-pk{background:#E91E63;color:#fff;padding:2px 8px;border-radius:4px;font-size:.8rem}
.tag-fk{background:#2196F3;color:#fff;padding:2px 8px;border-radius:4px;font-size:.8rem}
.tag-idx{background:#FF9800;color:#fff;padding:2px 8px;border-radius:4px;font-size:.8rem}
.tag-shard{background:#9C27B0;color:#fff;padding:2px 8px;border-radius:4px;font-size:.8rem}
pre{background:#1e1e1e;padding:1rem;border-radius:8px;overflow-x:auto;margin:.5rem 0}
code{color:#89DDFF;font-size:.9rem}
svg text{font-family:'Segoe UI',system-ui,sans-serif}
.tradeoff-grid{display:grid;grid-template-columns:1fr 1fr;gap:1rem;margin:1rem 0}
.pro{border-left:4px solid #4CAF50;padding-left:1rem}
.con{border-left:4px solid #f44336;padding-left:1rem}
details{background:var(--surface);border:1px solid var(--border);border-radius:8px;padding:1rem;margin:.5rem 0}
summary{cursor:pointer;font-weight:600;color:var(--accent)}
.step{margin:.5rem 0;padding:.5rem 1rem;border-left:3px solid var(--accent2)}
</style>
</head>
<body>

<h1>üß™ Experimentation Platform (A/B Testing)</h1>
<p>Design a system like Optimizely, LaunchDarkly, or Meta's PlanOut that enables product teams to run A/B tests and feature experiments at scale with statistical rigor, targeting flexibility, and real-time metric analysis.</p>

<h2>üìã Functional Requirements</h2>
<div class="card">
<ul>
<li><strong>Experiment creation</strong> ‚Äî create experiments with variants, traffic allocation, targeting rules, and guardrail metrics</li>
<li><strong>User assignment</strong> ‚Äî deterministically assign users to experiment variants with consistent hashing</li>
<li><strong>Feature flags</strong> ‚Äî toggle features on/off with gradual rollouts and targeting rules</li>
<li><strong>Metric collection</strong> ‚Äî track experiment metrics (conversion, engagement, revenue) in real-time and batch</li>
<li><strong>Statistical analysis</strong> ‚Äî compute p-values, confidence intervals, and sequential testing with false-discovery-rate controls</li>
<li><strong>Mutual exclusion</strong> ‚Äî prevent conflicting experiments from running on same users</li>
<li><strong>Experiment lifecycle</strong> ‚Äî draft ‚Üí running ‚Üí paused ‚Üí completed ‚Üí archived with rollback</li>
</ul>
</div>

<h2>üìã Non-Functional Requirements</h2>
<div class="card">
<ul>
<li><strong>Low latency assignment</strong> ‚Äî variant lookup must be &lt; 5ms at p99 (on critical path)</li>
<li><strong>Consistency</strong> ‚Äî same user always gets same variant (deterministic hashing)</li>
<li><strong>Scale</strong> ‚Äî support 10K+ concurrent experiments, billions of assignments/day</li>
<li><strong>Statistical rigor</strong> ‚Äî control false positive rate at 5%, manage multiple comparisons</li>
<li><strong>Availability</strong> ‚Äî 99.99% for assignment service (fail-open to control group)</li>
<li><strong>Isolation</strong> ‚Äî experiments must not interfere with each other (interaction effects)</li>
</ul>
</div>

<h2>üîÑ Flow 1: Experiment Creation & Configuration</h2>
<svg viewBox="0 0 1000 320" style="width:100%;background:#111;border-radius:12px;margin:1rem 0">
<defs><marker id="a1" markerWidth="10" markerHeight="7" refX="10" refY="3.5" orient="auto"><polygon points="0 0,10 3.5,0 7" fill="#4CAF50"/></marker></defs>
<rect x="20" y="50" width="130" height="50" rx="8" fill="#2E7D32"/><text x="85" y="80" text-anchor="middle" fill="white" font-size="13">Product Team</text>
<rect x="200" y="50" width="140" height="50" rx="8" fill="#E65100"/><text x="270" y="80" text-anchor="middle" fill="white" font-size="13">Experiment UI</text>
<rect x="400" y="50" width="150" height="50" rx="8" fill="#1565C0"/><text x="475" y="80" text-anchor="middle" fill="white" font-size="13">Config Service</text>
<rect x="620" y="50" width="140" height="50" rx="8" fill="#6A1B9A"/><text x="690" y="80" text-anchor="middle" fill="white" font-size="13">Validation Engine</text>
<rect x="400" y="160" width="150" height="50" rx="8" fill="#4E342E"/><text x="475" y="190" text-anchor="middle" fill="white" font-size="13">Experiment DB</text>
<rect x="620" y="160" width="140" height="50" rx="8" fill="#00695C"/><text x="690" y="190" text-anchor="middle" fill="white" font-size="13">Config CDN Cache</text>
<rect x="200" y="250" width="140" height="50" rx="8" fill="#1565C0"/><text x="270" y="280" text-anchor="middle" fill="white" font-size="13">Assignment Service</text>
<line x1="150" y1="75" x2="195" y2="75" stroke="#4CAF50" stroke-width="2" marker-end="url(#a1)"/>
<line x1="340" y1="75" x2="395" y2="75" stroke="#4CAF50" stroke-width="2" marker-end="url(#a1)"/>
<line x1="550" y1="75" x2="615" y2="75" stroke="#4CAF50" stroke-width="2" marker-end="url(#a1)"/>
<line x1="475" y1="100" x2="475" y2="155" stroke="#FF9800" stroke-width="2" marker-end="url(#a1)"/>
<line x1="550" y1="185" x2="615" y2="185" stroke="#FF9800" stroke-width="2" marker-end="url(#a1)"/>
<line x1="475" y1="210" x2="270" y2="245" stroke="#81D4FA" stroke-width="2" marker-end="url(#a1)"/>
<text x="490" y="140" fill="#aaa" font-size="11">persist config</text>
<text x="560" y="175" fill="#aaa" font-size="11">push to CDN</text>
<text x="310" y="230" fill="#aaa" font-size="11">propagate to SDK</text>
</svg>

<div class="step"><strong>Step 1:</strong> Product team defines experiment: name, hypothesis, variants (control + treatments), traffic %, targeting rules (country, platform, user segment), guardrail metrics</div>
<div class="step"><strong>Step 2:</strong> Validation Engine checks: no conflicting experiments on same layer, sample size sufficient for MDE (minimum detectable effect), metrics exist in catalog</div>
<div class="step"><strong>Step 3:</strong> Config Service persists experiment definition to Experiment DB and pushes serialized config to CDN/edge caches</div>
<div class="step"><strong>Step 4:</strong> Assignment Service receives updated config for local evaluation (SDK model) or serves API lookups</div>

<details><summary>Example: Creating a checkout flow A/B test</summary>
<pre><code>POST /api/v1/experiments
{
  "name": "checkout_redesign_q1_2025",
  "hypothesis": "New checkout reduces cart abandonment by 5%",
  "layer": "checkout_layer",
  "variants": [
    {"name": "control", "weight": 50, "config": {"checkout_version": "v1"}},
    {"name": "treatment_a", "weight": 25, "config": {"checkout_version": "v2_single_page"}},
    {"name": "treatment_b", "weight": 25, "config": {"checkout_version": "v2_accordion"}}
  ],
  "targeting": {
    "countries": ["US", "CA"],
    "platforms": ["web", "ios"],
    "user_segment": "returning_users"
  },
  "primary_metric": "checkout_conversion_rate",
  "guardrail_metrics": ["p99_page_load", "revenue_per_user"],
  "min_detectable_effect": 0.02,
  "significance_level": 0.05,
  "power": 0.8,
  "estimated_duration_days": 14
}</code></pre>
</details>

<details><summary>Deep Dive: Experiment Layers & Mutual Exclusion</summary>
<div class="card">
<h4>Layer-Based Traffic Isolation (Google's Overlapping Experiments)</h4>
<p>Traffic is divided into <strong>layers</strong> (orthogonal dimensions) and <strong>domains</strong> (traffic slices):</p>
<pre><code>// Layer architecture (inspired by Google's paper)
// Each layer is an independent randomization unit
Layer: "UI" ‚Üí experiments affecting UI rendering
Layer: "Backend" ‚Üí experiments affecting backend logic
Layer: "ML Ranking" ‚Üí experiments affecting ML models

// Within a layer, experiments are mutually exclusive
// Across layers, experiments are orthogonal (can overlap)

// Mutual exclusion within a layer:
hash(user_id + layer_id) % 10000 ‚Üí traffic_bucket
experiment_A: buckets 0-4999 (50%)
experiment_B: buckets 5000-7499 (25%)
experiment_C: buckets 7500-9999 (25%)

// Cross-layer independence:
// User in experiment_A (UI layer) can also be in experiment_X (ML layer)
// because different layer_id produces different hash ‚Üí independent assignment</code></pre>
<p><strong>Why layers?</strong> Without layers, you'd need N! combinations for N experiments. Layers give O(N) complexity by ensuring independence. Google runs 10K+ concurrent experiments using this model.</p>
</div>
</details>

<h2>üîÑ Flow 2: User Assignment (Variant Lookup)</h2>
<svg viewBox="0 0 1000 300" style="width:100%;background:#111;border-radius:12px;margin:1rem 0">
<defs><marker id="a2" markerWidth="10" markerHeight="7" refX="10" refY="3.5" orient="auto"><polygon points="0 0,10 3.5,0 7" fill="#FF9800"/></marker></defs>
<rect x="20" y="60" width="120" height="50" rx="8" fill="#2E7D32"/><text x="80" y="90" text-anchor="middle" fill="white" font-size="13">Client App</text>
<rect x="190" y="60" width="140" height="50" rx="8" fill="#00695C"/><text x="260" y="80" text-anchor="middle" fill="white" font-size="12">Local SDK Cache</text><text x="260" y="95" text-anchor="middle" fill="white" font-size="10">(config snapshot)</text>
<rect x="380" y="30" width="150" height="50" rx="8" fill="#1565C0"/><text x="455" y="60" text-anchor="middle" fill="white" font-size="13">Hashing Function</text>
<rect x="380" y="100" width="150" height="50" rx="8" fill="#6A1B9A"/><text x="455" y="130" text-anchor="middle" fill="white" font-size="13">Targeting Engine</text>
<rect x="600" y="60" width="140" height="50" rx="8" fill="#E65100"/><text x="670" y="90" text-anchor="middle" fill="white" font-size="13">Variant Config</text>
<rect x="800" y="60" width="130" height="50" rx="8" fill="#2E7D32"/><text x="865" y="90" text-anchor="middle" fill="white" font-size="13">Feature Code</text>
<rect x="380" y="200" width="150" height="50" rx="8" fill="#4E342E"/><text x="455" y="230" text-anchor="middle" fill="white" font-size="13">Exposure Log</text>
<rect x="600" y="200" width="140" height="50" rx="8" fill="#6A1B9A"/><text x="670" y="230" text-anchor="middle" fill="white" font-size="13">Event Pipeline</text>
<line x1="140" y1="85" x2="185" y2="85" stroke="#FF9800" stroke-width="2" marker-end="url(#a2)"/>
<line x1="330" y1="75" x2="375" y2="55" stroke="#FF9800" stroke-width="2" marker-end="url(#a2)"/>
<line x1="330" y1="95" x2="375" y2="125" stroke="#FF9800" stroke-width="2" marker-end="url(#a2)"/>
<line x1="530" y1="55" x2="595" y2="75" stroke="#FF9800" stroke-width="2" marker-end="url(#a2)"/>
<line x1="530" y1="125" x2="595" y2="95" stroke="#FF9800" stroke-width="2" marker-end="url(#a2)"/>
<line x1="740" y1="85" x2="795" y2="85" stroke="#FF9800" stroke-width="2" marker-end="url(#a2)"/>
<line x1="455" y1="150" x2="455" y2="195" stroke="#81D4FA" stroke-width="2" marker-end="url(#a2)"/>
<line x1="530" y1="225" x2="595" y2="225" stroke="#81D4FA" stroke-width="2" marker-end="url(#a2)"/>
<text x="455" y="180" fill="#aaa" font-size="11" text-anchor="middle">log exposure</text>
</svg>

<div class="step"><strong>Step 1:</strong> Client SDK loads experiment config snapshot (pushed via CDN or streamed via SSE)</div>
<div class="step"><strong>Step 2:</strong> For each experiment, SDK evaluates targeting rules (country, platform, segment)</div>
<div class="step"><strong>Step 3:</strong> Deterministic hashing: variant = hash(user_id + experiment_salt) % 10000; map bucket to variant</div>
<div class="step"><strong>Step 4:</strong> SDK returns variant config to feature code and logs exposure event asynchronously</div>

<details><summary>Deep Dive: Deterministic Hashing for Consistent Assignment</summary>
<div class="card">
<h4>Why Deterministic Hashing?</h4>
<p>Users must always see the same variant. Storing assignments in a DB would add latency and a single point of failure. Instead, we use deterministic hashing:</p>
<pre><code>// Assignment algorithm (no DB needed!)
function assignVariant(userId, experiment) {
  // Salt ensures different experiments get independent assignments
  const hash = murmur3_32(userId + experiment.salt);
  const bucket = hash % 10000; // 0.01% granularity

  // Check targeting rules first
  if (!matchesTargeting(userId, experiment.targeting)) {
    return null; // not in experiment
  }

  // Map bucket to variant based on traffic allocation
  let cumulative = 0;
  for (const variant of experiment.variants) {
    cumulative += variant.weight * 100; // weight is percentage
    if (bucket < cumulative) {
      return variant;
    }
  }
  return experiment.variants[0]; // fallback to control
}

// Properties of murmur3:
// - Deterministic: same input ‚Üí same output (consistent assignment)
// - Uniform: buckets are evenly distributed (balanced groups)
// - Fast: ~3ns per hash (negligible latency)
// - Avalanche: small input change ‚Üí completely different output</code></pre>
<p><strong>Salt per experiment</strong> ensures that being in variant A of experiment 1 doesn't correlate with variant assignment in experiment 2 (independence).</p>
</div>
</details>

<details><summary>Deep Dive: Client SDK Architecture</summary>
<div class="card">
<h4>Two Models: Server-Side vs Client-Side Evaluation</h4>
<table>
<tr><th>Aspect</th><th>Server-Side (API)</th><th>Client-Side (SDK)</th></tr>
<tr><td>Latency</td><td>Network RTT (~20-50ms)</td><td>Local (~0.1ms)</td></tr>
<tr><td>Config freshness</td><td>Always latest</td><td>Eventual (polling/SSE)</td></tr>
<tr><td>Targeting data</td><td>Rich server context</td><td>Limited to client context</td></tr>
<tr><td>Offline support</td><td>No</td><td>Yes (cached config)</td></tr>
<tr><td>SDK complexity</td><td>Thin client</td><td>Thick client (eval engine)</td></tr>
</table>
<p><strong>Hybrid approach (LaunchDarkly model):</strong> SDK connects via streaming (SSE/WebSocket) for real-time config updates, evaluates locally for speed, falls back to cached config if connection lost.</p>
<pre><code>// SDK initialization flow
const client = ExperimentSDK.init({
  sdkKey: "sdk-xxx",
  user: { key: "user_123", country: "US", plan: "premium" },
  bootstrap: localStorageCache, // instant first render
  streaming: true, // SSE for real-time updates
  flushInterval: 30000, // batch exposure events every 30s
});</code></pre>
</div>
</details>

<h2>üîÑ Flow 3: Metric Analysis & Statistical Testing</h2>
<svg viewBox="0 0 1000 350" style="width:100%;background:#111;border-radius:12px;margin:1rem 0">
<defs><marker id="a3" markerWidth="10" markerHeight="7" refX="10" refY="3.5" orient="auto"><polygon points="0 0,10 3.5,0 7" fill="#81D4FA"/></marker></defs>
<rect x="20" y="50" width="130" height="50" rx="8" fill="#6A1B9A"/><text x="85" y="80" text-anchor="middle" fill="white" font-size="12">Exposure Events</text>
<rect x="20" y="130" width="130" height="50" rx="8" fill="#6A1B9A"/><text x="85" y="160" text-anchor="middle" fill="white" font-size="12">Metric Events</text>
<rect x="200" y="90" width="140" height="50" rx="8" fill="#4E342E"/><text x="270" y="120" text-anchor="middle" fill="white" font-size="13">Event Pipeline</text>
<rect x="400" y="50" width="150" height="50" rx="8" fill="#1565C0"/><text x="475" y="75" text-anchor="middle" fill="white" font-size="12">Exposure-Metric</text><text x="475" y="90" text-anchor="middle" fill="white" font-size="12">Join (Spark)</text>
<rect x="400" y="140" width="150" height="50" rx="8" fill="#00695C"/><text x="475" y="170" text-anchor="middle" fill="white" font-size="12">Stats Engine</text>
<rect x="620" y="50" width="160" height="50" rx="8" fill="#E65100"/><text x="700" y="80" text-anchor="middle" fill="white" font-size="12">Results Dashboard</text>
<rect x="620" y="140" width="160" height="50" rx="8" fill="#2E7D32"/><text x="700" y="160" text-anchor="middle" fill="white" font-size="12">Auto-Stop /</text><text x="700" y="175" text-anchor="middle" fill="white" font-size="12">Guardrail Alerts</text>
<rect x="400" y="260" width="150" height="50" rx="8" fill="#1565C0"/><text x="475" y="280" text-anchor="middle" fill="white" font-size="12">Variance Reduction</text><text x="475" y="295" text-anchor="middle" fill="white" font-size="12">(CUPED)</text>
<line x1="150" y1="75" x2="195" y2="105" stroke="#81D4FA" stroke-width="2" marker-end="url(#a3)"/>
<line x1="150" y1="155" x2="195" y2="125" stroke="#81D4FA" stroke-width="2" marker-end="url(#a3)"/>
<line x1="340" y1="115" x2="395" y2="80" stroke="#81D4FA" stroke-width="2" marker-end="url(#a3)"/>
<line x1="550" y1="75" x2="615" y2="75" stroke="#81D4FA" stroke-width="2" marker-end="url(#a3)"/>
<line x1="475" y1="100" x2="475" y2="135" stroke="#FF9800" stroke-width="2" marker-end="url(#a3)"/>
<line x1="550" y1="165" x2="615" y2="165" stroke="#FF9800" stroke-width="2" marker-end="url(#a3)"/>
<line x1="475" y1="190" x2="475" y2="255" stroke="#81D4FA" stroke-width="2" marker-end="url(#a3)"/>
<text x="475" y="240" fill="#aaa" font-size="11" text-anchor="middle">pre-experiment covariate</text>
</svg>

<div class="step"><strong>Step 1:</strong> Exposure events (who saw what variant) and metric events (conversions, clicks, revenue) flow into event pipeline (Kafka ‚Üí Spark)</div>
<div class="step"><strong>Step 2:</strong> Exposure-Metric Join: match metric events to the variant the user was assigned to via user_id + timestamp</div>
<div class="step"><strong>Step 3:</strong> Stats Engine computes per-variant aggregates, applies CUPED variance reduction, runs hypothesis tests</div>
<div class="step"><strong>Step 4:</strong> Dashboard shows lift, p-value, confidence intervals; guardrail alerts fire if degradation detected</div>

<details><summary>Deep Dive: Statistical Methods</summary>
<div class="card">
<h4>Fixed-Horizon vs Sequential Testing</h4>
<pre><code>// Fixed-Horizon (Classical A/B Test)
// - Set sample size upfront based on MDE, power, significance
// - Wait until sample collected, then analyze ONCE
// - Problem: "peeking" inflates false positive rate

n = (Z_Œ±/2 + Z_Œ≤)¬≤ √ó (œÉ‚ÇÅ¬≤ + œÉ‚ÇÇ¬≤) / (Œº‚ÇÅ - Œº‚ÇÇ)¬≤
// For 5% significance, 80% power, 2% MDE on 10% baseline conversion:
// n ‚âà 39,200 per variant

// Sequential Testing (Always-Valid p-values)
// - Can peek at results at any time without inflating false positives
// - Uses confidence sequences instead of confidence intervals
// - Methods: mSPRT (mixture Sequential Probability Ratio Test)

// CUPED (Controlled-experiment Using Pre-Experiment Data)
// Reduces variance by 20-50%, cutting experiment duration significantly
≈∂_cuped = Y - Œ∏ √ó (X - XÃÑ)
// where X = pre-experiment metric (covariate)
// Œ∏ = Cov(X,Y) / Var(X)
// This adjusts for pre-existing user behavior differences</code></pre>

<h4>Multiple Testing Correction</h4>
<pre><code>// Problem: Testing 20 metrics at Œ±=0.05 ‚Üí expect 1 false positive!
// Solutions:
// 1. Bonferroni: Œ±_adj = Œ± / m (too conservative)
// 2. Benjamini-Hochberg (FDR): controls false discovery rate
//    - Sort p-values: p(1) ‚â§ p(2) ‚â§ ... ‚â§ p(m)
//    - Find largest k where p(k) ‚â§ k/m √ó Œ±
//    - Reject all H0 for i ‚â§ k

// 3. Primary vs secondary metrics:
//    - Apply correction only to primary metrics
//    - Secondary metrics are exploratory (flag but don't decide)</code></pre>
</div>
</details>

<details><summary>Deep Dive: CUPED Variance Reduction</summary>
<div class="card">
<p><strong>CUPED</strong> (Microsoft Research, 2013) uses pre-experiment data to reduce noise:</p>
<pre><code>// Without CUPED: need 40K users for 2% MDE detection
// With CUPED: need ~20K users (50% variance reduction!)

// Algorithm:
// 1. For each user, get pre-experiment metric X (e.g., last 7 days revenue)
// 2. Compute Œ∏ = Cov(X, Y) / Var(X)
// 3. Adjusted metric: ≈∂ = Y - Œ∏(X - XÃÑ)
// 4. Run t-test on ≈∂ instead of Y

// Why it works:
// - Users who spent a lot before experiment will likely spend a lot during
// - Subtracting this "expected" behavior isolates the treatment effect
// - Mathematically: Var(≈∂) = Var(Y)(1 - œÅ¬≤) where œÅ = correlation(X,Y)
// - Higher correlation ‚Üí more variance reduction</code></pre>
</div>
</details>

<h2>üèóÔ∏è Combined Architecture</h2>
<svg viewBox="0 0 1100 600" style="width:100%;background:#111;border-radius:12px;margin:1rem 0">
<defs><marker id="ac" markerWidth="10" markerHeight="7" refX="10" refY="3.5" orient="auto"><polygon points="0 0,10 3.5,0 7" fill="#4CAF50"/></marker></defs>
<text x="550" y="25" text-anchor="middle" fill="#4CAF50" font-size="16" font-weight="bold">Experimentation Platform Architecture</text>
<rect x="20" y="45" width="120" height="45" rx="8" fill="#2E7D32"/><text x="80" y="73" text-anchor="middle" fill="white" font-size="12">Experiment UI</text>
<rect x="20" y="110" width="120" height="45" rx="8" fill="#2E7D32"/><text x="80" y="138" text-anchor="middle" fill="white" font-size="12">Client Apps</text>
<rect x="200" y="45" width="140" height="45" rx="8" fill="#E65100"/><text x="270" y="73" text-anchor="middle" fill="white" font-size="12">API Gateway</text>
<rect x="400" y="45" width="150" height="45" rx="8" fill="#1565C0"/><text x="475" y="73" text-anchor="middle" fill="white" font-size="12">Config Service</text>
<rect x="400" y="110" width="150" height="45" rx="8" fill="#1565C0"/><text x="475" y="138" text-anchor="middle" fill="white" font-size="12">Assignment Service</text>
<rect x="400" y="175" width="150" height="45" rx="8" fill="#1565C0"/><text x="475" y="203" text-anchor="middle" fill="white" font-size="12">Analysis Service</text>
<rect x="620" y="45" width="140" height="45" rx="8" fill="#4E342E"/><text x="690" y="73" text-anchor="middle" fill="white" font-size="12">Experiment DB</text>
<rect x="620" y="110" width="140" height="45" rx="8" fill="#00695C"/><text x="690" y="138" text-anchor="middle" fill="white" font-size="12">Config Cache</text>
<rect x="200" y="175" width="140" height="45" rx="8" fill="#6A1B9A"/><text x="270" y="203" text-anchor="middle" fill="white" font-size="12">Client SDK</text>
<rect x="200" y="290" width="140" height="45" rx="8" fill="#6A1B9A"/><text x="270" y="318" text-anchor="middle" fill="white" font-size="12">Kafka Events</text>
<rect x="400" y="290" width="150" height="45" rx="8" fill="#1565C0"/><text x="475" y="318" text-anchor="middle" fill="white" font-size="12">Flink Streaming</text>
<rect x="620" y="290" width="140" height="45" rx="8" fill="#4E342E"/><text x="690" y="318" text-anchor="middle" fill="white" font-size="12">ClickHouse</text>
<rect x="400" y="370" width="150" height="45" rx="8" fill="#1565C0"/><text x="475" y="398" text-anchor="middle" fill="white" font-size="12">Spark Batch</text>
<rect x="620" y="370" width="140" height="45" rx="8" fill="#1565C0"/><text x="690" y="398" text-anchor="middle" fill="white" font-size="12">Stats Engine</text>
<rect x="830" y="290" width="140" height="45" rx="8" fill="#E65100"/><text x="900" y="318" text-anchor="middle" fill="white" font-size="12">Results Dashboard</text>
<rect x="830" y="370" width="140" height="45" rx="8" fill="#2E7D32"/><text x="900" y="398" text-anchor="middle" fill="white" font-size="12">Alerting System</text>
<rect x="830" y="45" width="140" height="45" rx="8" fill="#607D8B"/><text x="900" y="73" text-anchor="middle" fill="white" font-size="12">CDN Edge</text>
<line x1="140" y1="67" x2="195" y2="67" stroke="#4CAF50" stroke-width="2" marker-end="url(#ac)"/>
<line x1="140" y1="132" x2="195" y2="197" stroke="#4CAF50" stroke-width="2" marker-end="url(#ac)"/>
<line x1="340" y1="67" x2="395" y2="67" stroke="#4CAF50" stroke-width="2" marker-end="url(#ac)"/>
<line x1="550" y1="67" x2="615" y2="67" stroke="#4CAF50" stroke-width="2" marker-end="url(#ac)"/>
<line x1="550" y1="132" x2="615" y2="132" stroke="#4CAF50" stroke-width="2" marker-end="url(#ac)"/>
<line x1="760" y1="67" x2="825" y2="67" stroke="#4CAF50" stroke-width="2" marker-end="url(#ac)"/>
<line x1="340" y1="197" x2="395" y2="197" stroke="#4CAF50" stroke-width="2" marker-end="url(#ac)"/>
<line x1="270" y1="220" x2="270" y2="285" stroke="#FF9800" stroke-width="2" marker-end="url(#ac)"/>
<line x1="340" y1="312" x2="395" y2="312" stroke="#FF9800" stroke-width="2" marker-end="url(#ac)"/>
<line x1="550" y1="312" x2="615" y2="312" stroke="#FF9800" stroke-width="2" marker-end="url(#ac)"/>
<line x1="760" y1="312" x2="825" y2="312" stroke="#FF9800" stroke-width="2" marker-end="url(#ac)"/>
<line x1="475" y1="335" x2="475" y2="365" stroke="#81D4FA" stroke-width="2" marker-end="url(#ac)"/>
<line x1="550" y1="392" x2="615" y2="392" stroke="#81D4FA" stroke-width="2" marker-end="url(#ac)"/>
<line x1="760" y1="392" x2="825" y2="392" stroke="#81D4FA" stroke-width="2" marker-end="url(#ac)"/>
</svg>

<h2>üíæ Database Schema</h2>
<div class="grid">
<div class="card">
<h3>PostgreSQL ‚Äî Experiment Config</h3>
<pre><code>CREATE TABLE experiments (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(), -- <span class="tag-pk">PK</span>
  name VARCHAR(255) NOT NULL UNIQUE, -- <span class="tag-idx">IDX</span>
  layer_id UUID NOT NULL REFERENCES layers(id), -- <span class="tag-fk">FK</span>
  status VARCHAR(20) DEFAULT 'draft', -- <span class="tag-idx">IDX</span>
  hypothesis TEXT,
  targeting_rules JSONB NOT NULL DEFAULT '{}',
  salt VARCHAR(64) NOT NULL DEFAULT gen_random_uuid(),
  traffic_percentage DECIMAL(5,2) DEFAULT 100.00,
  primary_metric_id UUID REFERENCES metrics(id),
  significance_level DECIMAL(4,3) DEFAULT 0.050,
  power DECIMAL(4,3) DEFAULT 0.800,
  min_detectable_effect DECIMAL(6,4),
  started_at TIMESTAMPTZ,
  ended_at TIMESTAMPTZ,
  created_by UUID NOT NULL,
  created_at TIMESTAMPTZ DEFAULT now(),
  version INT DEFAULT 1
);
-- <span class="tag-idx">IDX</span>: (layer_id, status) for finding active experiments per layer
-- <span class="tag-idx">IDX</span>: (status, started_at) for lifecycle management

CREATE TABLE variants (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  experiment_id UUID NOT NULL REFERENCES experiments(id), -- <span class="tag-fk">FK</span>
  name VARCHAR(100) NOT NULL,
  weight INT NOT NULL, -- percentage √ó 100 for precision
  config JSONB NOT NULL DEFAULT '{}',
  is_control BOOLEAN DEFAULT false,
  UNIQUE(experiment_id, name)
);

CREATE TABLE layers (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  name VARCHAR(255) NOT NULL UNIQUE,
  description TEXT,
  domain_start INT DEFAULT 0,
  domain_end INT DEFAULT 9999
);

CREATE TABLE experiment_guardrails (
  experiment_id UUID REFERENCES experiments(id),
  metric_id UUID REFERENCES metrics(id),
  direction VARCHAR(10) NOT NULL, -- 'increase' | 'decrease'
  threshold DECIMAL(8,4), -- alert if metric moves > threshold
  PRIMARY KEY (experiment_id, metric_id)
);</code></pre>
</div>
<div class="card">
<h3>ClickHouse ‚Äî Event Analytics</h3>
<pre><code>-- Exposure events (who saw which variant)
CREATE TABLE exposures (
  event_id UUID,
  user_id UInt64, -- <span class="tag-shard">SHARD KEY</span>
  experiment_id UUID,
  variant_id UUID,
  variant_name String,
  timestamp DateTime64(3),
  context Map(String, String), -- device, country, etc.
  sdk_version String
) ENGINE = MergeTree()
PARTITION BY toYYYYMMDD(timestamp)
ORDER BY (experiment_id, user_id, timestamp);

-- Metric events (what users did)
CREATE TABLE metric_events (
  event_id UUID,
  user_id UInt64,
  metric_name String,
  metric_value Float64,
  timestamp DateTime64(3),
  properties Map(String, String)
) ENGINE = MergeTree()
PARTITION BY toYYYYMMDD(timestamp)
ORDER BY (metric_name, user_id, timestamp);

-- Pre-aggregated experiment results (materialized view)
CREATE MATERIALIZED VIEW experiment_results_mv
ENGINE = AggregatingMergeTree()
ORDER BY (experiment_id, variant_id, metric_name, date)
AS SELECT
  e.experiment_id,
  e.variant_id,
  m.metric_name,
  toDate(m.timestamp) AS date,
  countState() AS sample_size,
  sumState(m.metric_value) AS total_value,
  avgState(m.metric_value) AS avg_value,
  varSampState(m.metric_value) AS variance
FROM exposures e
JOIN metric_events m ON e.user_id = m.user_id
GROUP BY experiment_id, variant_id, metric_name, date;</code></pre>
</div>
</div>

<h2>‚ö° Cache & CDN Deep Dive</h2>
<div class="card">
<h3>Experiment Config Distribution</h3>
<table>
<tr><th>Layer</th><th>Strategy</th><th>TTL</th><th>Purpose</th></tr>
<tr><td>CDN Edge</td><td>Push on change</td><td>5 min max</td><td>Serve SDK config downloads</td></tr>
<tr><td>SDK Local Cache</td><td>In-memory + localStorage</td><td>Until SSE update</td><td>Zero-latency variant lookup</td></tr>
<tr><td>Assignment Redis</td><td>Write-through</td><td>Experiment duration</td><td>Override assignments (debugging)</td></tr>
<tr><td>Results Cache</td><td>LRU + TTL</td><td>5 min</td><td>Dashboard query results</td></tr>
</table>
<h4>Config Propagation Protocol</h4>
<pre><code>// When experiment config changes:
1. Config Service updates Experiment DB (source of truth)
2. Publishes config change event to Kafka topic "experiment-config-changes"
3. CDN invalidation triggered (purge edge caches)
4. SSE fanout service pushes update to all connected SDKs
5. SDK receives update, atomically swaps local config
   Average propagation time: ~2-5 seconds globally

// Fail-safe: SDKs poll every 5 minutes as backup
// On SDK init with no cached config: synchronous HTTP fetch</code></pre>
</div>

<h2>üìà Scaling Considerations</h2>
<div class="card">
<h3>Assignment Service Scaling</h3>
<ul>
<li><strong>Stateless computation:</strong> assignment is pure function of (user_id, experiment_config) ‚Üí horizontally scalable</li>
<li><strong>Client-side evaluation:</strong> push config to SDKs, eliminate server calls entirely; this is how 10K+ experiments scale</li>
<li><strong>Config snapshot size:</strong> serialize active experiments as protobuf (~50KB for 1000 experiments); fits in single CDN fetch</li>
<li><strong>Event ingestion:</strong> Kafka with 100+ partitions for exposure/metric events; 1M+ events/sec</li>
</ul>
<h3>Analysis Pipeline Scaling</h3>
<ul>
<li><strong>Incremental computation:</strong> don't recompute all stats from scratch; use streaming aggregates in Flink for real-time and Spark batch for full precision</li>
<li><strong>Pre-aggregation:</strong> ClickHouse materialized views maintain running sums/counts per experiment√óvariant√ómetric</li>
<li><strong>Parallel experiment analysis:</strong> each experiment analyzed independently ‚Üí embarrassingly parallel</li>
<li><strong>CUPED at scale:</strong> pre-compute covariates in nightly batch job; store in feature store for fast access</li>
</ul>
</div>

<h2>‚öñÔ∏è Tradeoffs</h2>
<div class="tradeoff-grid">
<div class="card pro">
<h4>Client-Side Evaluation</h4>
<ul>
<li>Zero-latency variant lookup</li>
<li>Works offline</li>
<li>No server dependency</li>
</ul>
</div>
<div class="card con">
<h4>Client-Side Evaluation</h4>
<ul>
<li>Config size grows with experiments</li>
<li>SDK complexity increases</li>
<li>Config propagation delay</li>
</ul>
</div>
<div class="card pro">
<h4>Sequential Testing</h4>
<ul>
<li>Can stop experiments early</li>
<li>Peek at results safely</li>
<li>Faster iteration</li>
</ul>
</div>
<div class="card con">
<h4>Sequential Testing</h4>
<ul>
<li>Wider confidence intervals</li>
<li>More complex to implement</li>
<li>Requires larger sample for same power</li>
</ul>
</div>
</div>

<h2>üîÑ Alternative Approaches</h2>
<div class="grid">
<div class="card">
<h4>Multi-Armed Bandit (MAB)</h4>
<p>Instead of fixed allocation, dynamically shift traffic to winning variant. Thompson Sampling or UCB algorithm. Maximizes reward during experiment but trades off statistical rigor for practical optimization. Best for short-lived optimizations (ad copy, button color).</p>
</div>
<div class="card">
<h4>Bayesian A/B Testing</h4>
<p>Replace p-values with posterior probability distributions. Output: "92% probability that treatment is better" (more intuitive). Use Beta-Binomial for conversion metrics, Normal-Normal for continuous. No fixed sample size needed. Popular in industry (VWO, Dynamic Yield).</p>
</div>
<div class="card">
<h4>Interleaving (Ranking Experiments)</h4>
<p>For search/recommendation, interleave results from control and treatment in same result page. Each user sees both. Much more sensitive than A/B test (10x fewer users needed). Used by Netflix, Spotify for ranking model evaluation.</p>
</div>
</div>

<h2>üìö Additional Information</h2>
<div class="card">
<ul>
<li><strong>Novelty & primacy effects:</strong> new treatments often show initial lift that fades. Run experiments for at least 2 full business cycles (weekdays + weekends).</li>
<li><strong>Network effects / SUTVA violation:</strong> if users interact (social networks), treatment on user A affects control user B. Use cluster randomization (assign entire friend groups together) or switchback experiments.</li>
<li><strong>Simpson's Paradox:</strong> overall metric improves but subgroups worsen. Always do heterogeneous treatment effect analysis (slice by segments).</li>
<li><strong>Triggering / dilution:</strong> only analyze users who actually "triggered" the experiment (saw the feature), not all assigned users. Reduces dilution and increases sensitivity.</li>
<li><strong>Key papers:</strong> Google (Overlapping Experiment Infrastructure, 2010), Microsoft (Trustworthy Online Controlled Experiments, 2020), Netflix (Interleaving, 2018), Meta (PlanOut, 2014)</li>
</ul>
</div>

</body>
</html>
