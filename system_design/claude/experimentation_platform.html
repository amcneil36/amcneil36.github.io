<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>System Design: Experimentation Platform</title>
<script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
<style>
  :root {
    --bg: #0d1117;
    --card: #161b22;
    --border: #30363d;
    --text: #c9d1d9;
    --heading: #e6edf3;
    --accent: #58a6ff;
    --accent2: #3fb950;
    --accent3: #d2a8ff;
    --accent4: #f78166;
    --code-bg: #1c2128;
  }
  * { box-sizing: border-box; margin: 0; padding: 0; }
  body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif; background: var(--bg); color: var(--text); line-height: 1.7; padding: 2rem; max-width: 1200px; margin: 0 auto; }
  h1 { color: var(--heading); font-size: 2.4rem; border-bottom: 2px solid var(--accent); padding-bottom: 0.5rem; margin-bottom: 1.5rem; }
  h2 { color: var(--accent); font-size: 1.8rem; margin-top: 2.5rem; margin-bottom: 1rem; border-left: 4px solid var(--accent); padding-left: 0.75rem; }
  h3 { color: var(--accent3); font-size: 1.3rem; margin-top: 1.5rem; margin-bottom: 0.7rem; }
  h4 { color: var(--accent4); font-size: 1.1rem; margin-top: 1.2rem; margin-bottom: 0.5rem; }
  p { margin-bottom: 0.8rem; }
  ul, ol { margin-left: 1.5rem; margin-bottom: 1rem; }
  li { margin-bottom: 0.4rem; }
  .card { background: var(--card); border: 1px solid var(--border); border-radius: 8px; padding: 1.5rem; margin-bottom: 1.5rem; }
  .example { background: #1a2332; border-left: 4px solid var(--accent2); padding: 1rem 1.5rem; margin: 1rem 0; border-radius: 0 8px 8px 0; }
  .example strong { color: var(--accent2); }
  .mermaid { background: #fff; border-radius: 8px; padding: 1.5rem; margin: 1rem 0; text-align: center; }
  table { width: 100%; border-collapse: collapse; margin: 1rem 0; }
  th { background: #21262d; color: var(--accent); text-align: left; padding: 0.6rem 1rem; border: 1px solid var(--border); }
  td { padding: 0.6rem 1rem; border: 1px solid var(--border); }
  tr:nth-child(even) { background: #161b22; }
  code { background: var(--code-bg); padding: 2px 6px; border-radius: 4px; font-size: 0.9em; color: var(--accent4); }
  .tag { display: inline-block; padding: 2px 10px; border-radius: 20px; font-size: 0.8em; margin-right: 0.3rem; }
  .tag-sql { background: #1f3a5f; color: #58a6ff; }
  .tag-nosql { background: #2a1f3f; color: #d2a8ff; }
  .tag-cache { background: #1f3f2a; color: #3fb950; }
  .warn { background: #3d2b00; border-left: 4px solid #d29922; padding: 1rem 1.5rem; margin: 1rem 0; border-radius: 0 8px 8px 0; }
  .warn strong { color: #d29922; }
  hr { border: none; border-top: 1px solid var(--border); margin: 2rem 0; }
</style>
</head>
<body>

<h1>üß™ System Design: Experimentation Platform (A/B Testing)</h1>
<p>An experimentation platform allows product teams to run controlled experiments (A/B tests, multivariate tests, feature flags) on users, measure the impact of changes through statistical analysis, and make data-driven product decisions. Think of platforms similar to internal tools at large tech companies or third-party solutions like Optimizely or LaunchDarkly.</p>

<hr>

<!-- ========== FUNCTIONAL REQUIREMENTS ========== -->
<h2>1. Functional Requirements</h2>
<div class="card">
<ol>
  <li><strong>Experiment Creation &amp; Configuration</strong> ‚Äî Experimenters can create experiments with a name, description, hypothesis, variants (control + one or more treatments), traffic allocation percentages, targeting rules (e.g., geography, platform, user attributes), start/end dates, and associated metrics.</li>
  <li><strong>Experiment Lifecycle Management</strong> ‚Äî Support experiment states: <code>draft ‚Üí running ‚Üí paused ‚Üí completed/killed</code>. Allow pausing and resuming experiments. Auto-expire experiments when end date is reached.</li>
  <li><strong>Deterministic Variant Assignment</strong> ‚Äî Assign users to experiment variants deterministically so the same user always sees the same variant for a given experiment. Assignment must be consistent even across sessions and devices.</li>
  <li><strong>Mutual Exclusion Layers</strong> ‚Äî Allow grouping experiments into mutual exclusion layers so that a user is only in one experiment within a layer, preventing experiments from interfering with each other.</li>
  <li><strong>Feature Flags</strong> ‚Äî Support simple on/off feature flags as a degenerate case of an experiment (100% traffic to one variant).</li>
  <li><strong>Targeting &amp; Segmentation</strong> ‚Äî Allow experiments to target specific user segments based on attributes (country, platform, language, user tier, custom attributes).</li>
  <li><strong>Event Tracking</strong> ‚Äî Capture user interaction events and associate them with the user's active experiment assignments for later analysis.</li>
  <li><strong>Statistical Analysis &amp; Reporting</strong> ‚Äî Compute per-experiment metrics (mean, proportion, percentile), confidence intervals, p-values, and statistical significance. Support guardrail metrics that can auto-kill experiments if violated.</li>
  <li><strong>Experiment Dashboard</strong> ‚Äî A web UI for experimenters to create, manage, monitor, and analyze experiments.</li>
  <li><strong>Audit Trail</strong> ‚Äî Track all changes to experiments (who changed what and when).</li>
</ol>
</div>

<!-- ========== NON-FUNCTIONAL REQUIREMENTS ========== -->
<h2>2. Non-Functional Requirements</h2>
<div class="card">
<ol>
  <li><strong>Low Latency</strong> ‚Äî Variant assignment lookups must be &lt; 50ms p99. This is on the critical path of every user request to the product application.</li>
  <li><strong>High Availability (99.99%)</strong> ‚Äî The assignment service must be highly available. If it is down, applications should gracefully degrade to default/control experiences.</li>
  <li><strong>Deterministic Consistency</strong> ‚Äî A user must always receive the same variant for a given experiment regardless of which server handles the request.</li>
  <li><strong>Scalability</strong> ‚Äî Support 100,000+ concurrent experiments, billions of assignment lookups per day, and millions of events per second.</li>
  <li><strong>Data Integrity</strong> ‚Äî Zero event loss for tracked metrics. Accurate statistical computation.</li>
  <li><strong>Auditability</strong> ‚Äî Complete audit log of all experiment configuration changes.</li>
  <li><strong>Experiment Isolation</strong> ‚Äî Experiments within the same mutual exclusion layer must not pollute each other's results.</li>
  <li><strong>Graceful Degradation</strong> ‚Äî If the experimentation platform is unavailable, applications should fall back to default behavior without crashing.</li>
</ol>
</div>

<hr>

<!-- ========== FLOW 1: EXPERIMENT CREATION ========== -->
<h2>3. Flow 1 ‚Äî Experiment Creation &amp; Configuration</h2>
<div class="card">
<p>This flow covers how an experimenter creates and configures a new experiment using the dashboard, and how that configuration is distributed to the runtime systems.</p>

<div class="mermaid">
graph LR
    A["üë§ Experimenter<br/>(Browser)"] -->|"HTTP POST /experiments"| B["Experiment<br/>Management<br/>Service"]
    B -->|"Write config"| C[("SQL DB<br/>(Experiments,<br/>Variants,<br/>Metrics)")]
    B -->|"Write audit log"| C
    B -->|"Publish config update"| D["Config Distribution<br/>Service"]
    D -->|"Push config bundle<br/>(JSON)"| E["CDN<br/>(Config Files)"]
    D -->|"Invalidate"| F["Config Cache<br/>(In-Memory)"]
    F -.->|"Consumed by"| G["Assignment<br/>Service"]
    E -.->|"Consumed by"| H["Client-Side<br/>SDK"]
</div>

<h3>Examples</h3>

<div class="example">
<strong>Example 1 ‚Äî Creating a Simple A/B Test:</strong><br/>
A product manager on the checkout team opens the Experiment Dashboard and clicks "Create Experiment." They name it "checkout_button_color_test," set the hypothesis to "A green checkout button will increase conversion by 5%," define two variants ‚Äî control (blue button, 50%) and treatment (green button, 50%) ‚Äî target it to US users on mobile, set the primary metric to "checkout_conversion_rate," and add a guardrail metric "page_load_time_p95." They click "Create." This triggers an <code>HTTP POST /experiments</code> to the Experiment Management Service, which validates the config, writes the experiment, variants, and metrics to the SQL DB, writes an audit log entry, and publishes the config to the Config Distribution Service. The Config Distribution Service rebuilds the config bundle and pushes it to the CDN and invalidates the in-memory Config Cache so the Assignment Service picks up the new experiment.
</div>

<div class="example">
<strong>Example 2 ‚Äî Creating an Experiment in a Mutual Exclusion Layer:</strong><br/>
A data scientist wants to run two experiments on the homepage ‚Äî "homepage_hero_image" and "homepage_recommendation_algo." Since both affect the homepage experience, they should be mutually exclusive. The data scientist creates a mutual exclusion layer called "homepage_layer" with 100% traffic. They then create "homepage_hero_image" in this layer using 30% of the layer's traffic and "homepage_recommendation_algo" using another 40%, leaving 30% unexposed. The Experiment Management Service validates that total traffic in the layer does not exceed 100% and persists the configuration. Config is pushed downstream as in Example 1.
</div>

<div class="example">
<strong>Example 3 ‚Äî Pausing an Experiment:</strong><br/>
A guardrail alert fires showing that "checkout_button_color_test" has increased the error rate. The experimenter opens the dashboard and clicks "Pause." This triggers an <code>HTTP PATCH /experiments/{id}</code> with <code>{"status": "paused"}</code>. The Experiment Management Service updates the status in SQL, logs the audit entry, and pushes the updated config. The Assignment Service stops assigning new users to this experiment and returns the default/control experience for all users.
</div>

<h3>Component Deep Dive ‚Äî Flow 1</h3>

<h4>Experiment Dashboard (Web UI)</h4>
<ul>
  <li>Single-page web application (React or similar)</li>
  <li>Provides forms for creating/editing experiments, viewing results, managing lifecycle</li>
  <li>Communicates with the Experiment Management Service via REST API</li>
  <li>Static assets (JS, CSS, images) served via CDN</li>
</ul>

<h4>Experiment Management Service</h4>
<ul>
  <li><strong>Protocol:</strong> HTTP/REST</li>
  <li><strong>Endpoints:</strong></li>
</ul>
<table>
  <tr><th>Method</th><th>Endpoint</th><th>Input</th><th>Output</th></tr>
  <tr><td><code>POST</code></td><td><code>/experiments</code></td><td>JSON body: name, description, hypothesis, variants[], targeting_rules, metrics[], traffic_pct, mutual_exclusion_layer_id</td><td>201 Created ‚Äî experiment object with ID</td></tr>
  <tr><td><code>GET</code></td><td><code>/experiments/{id}</code></td><td>Path param: experiment_id</td><td>200 OK ‚Äî full experiment config</td></tr>
  <tr><td><code>PATCH</code></td><td><code>/experiments/{id}</code></td><td>JSON body: fields to update (status, traffic_pct, end_date, etc.)</td><td>200 OK ‚Äî updated experiment</td></tr>
  <tr><td><code>DELETE</code></td><td><code>/experiments/{id}</code></td><td>Path param: experiment_id</td><td>204 No Content</td></tr>
  <tr><td><code>GET</code></td><td><code>/experiments</code></td><td>Query params: status, owner, page, limit</td><td>200 OK ‚Äî paginated list of experiments</td></tr>
  <tr><td><code>POST</code></td><td><code>/mutual-exclusion-layers</code></td><td>JSON body: name, description</td><td>201 Created ‚Äî layer object</td></tr>
</table>
<ul>
  <li>Validates business rules: traffic percentages sum ‚â§ 100% per layer, at least one variant, valid targeting rules, valid metric definitions</li>
  <li>Writes audit log entries for every mutation</li>
  <li>Publishes config change events to the Config Distribution Service</li>
</ul>

<h4>SQL Database (Experiment Config Store)</h4>
<ul>
  <li>Stores experiment definitions, variants, metrics, mutual exclusion layers, and audit logs</li>
  <li>SQL is chosen because experiment configuration is relational (experiments ‚Üí variants, experiments ‚Üí metrics), requires ACID transactions, and involves complex queries for the dashboard (filtering, sorting, joining)</li>
  <li>Read/written by: Experiment Management Service</li>
</ul>

<h4>Config Distribution Service</h4>
<ul>
  <li>Receives config update notifications from the Experiment Management Service</li>
  <li>Rebuilds a compact config bundle (JSON) containing all active experiments, their variants, targeting rules, and mutual exclusion layer assignments</li>
  <li>Pushes the bundle to the CDN for client-side SDKs</li>
  <li>Invalidates the in-memory Config Cache used by the Assignment Service</li>
  <li>This is a critical component for ensuring config changes propagate within seconds</li>
  <li><strong>Protocol:</strong> Internal gRPC for receiving updates; HTTP for serving config bundles</li>
</ul>

<h4>CDN (Config Files)</h4>
<ul>
  <li>Caches the experiment config bundle as a static JSON file</li>
  <li>Client-side SDKs fetch this bundle on app initialization and periodically refresh</li>
  <li>Reduces load on the Config Distribution Service</li>
  <li>TTL: Short (30‚Äì60 seconds) to ensure timely propagation of config changes</li>
</ul>

<h4>Config Cache (In-Memory)</h4>
<ul>
  <li>An in-memory cache co-located with or consumed by the Assignment Service</li>
  <li>Stores the latest experiment config bundle</li>
  <li>Invalidated by the Config Distribution Service on config change</li>
  <li>Fallback: If cache is empty, the Assignment Service fetches from the Config Distribution Service or SQL DB</li>
</ul>
</div>

<hr>

<!-- ========== FLOW 2: VARIANT ASSIGNMENT ========== -->
<h2>4. Flow 2 ‚Äî Variant Assignment (Runtime)</h2>
<div class="card">
<p>This flow covers what happens when a user's application checks which variant of an experiment they should see. This is the most latency-sensitive flow ‚Äî it's on the hot path of every user request.</p>

<div class="mermaid">
graph LR
    A["üë§ User's App<br/>(Client or Server)"] -->|"getVariant(user_id,<br/>experiment_key)"| B["Experimentation<br/>SDK"]
    B -->|"Check"| C["Local Config<br/>Cache"]
    C -->|"Cache Hit"| B
    C -->|"Cache Miss"| D["Assignment<br/>Service"]
    D -->|"Read config"| E["Config Cache<br/>(In-Memory)"]
    B -->|"hash(user_id +<br/>experiment_id + salt)<br/>mod 10000"| F{"Targeting<br/>Check"}
    F -->|"User matches<br/>targeting rules"| G{"Mutual Exclusion<br/>Layer Check"}
    G -->|"User eligible<br/>in layer"| H["Return Variant<br/>(e.g., treatment_a)"]
    F -->|"User does NOT<br/>match"| I["Return Default<br/>(control)"]
    G -->|"User already in<br/>another experiment<br/>in same layer"| I
    B -->|"Log assignment<br/>(async, fire-and-forget)"| J["Message Queue"]
    J -->|"Consume"| K[("Assignment<br/>Log Store<br/>(NoSQL)")]
</div>

<h3>Examples</h3>

<div class="example">
<strong>Example 1 ‚Äî Standard Assignment (Cache Hit, User Eligible):</strong><br/>
A user on an iPhone in the US opens the app. The checkout page code calls <code>sdk.getVariant("checkout_button_color_test", user_id="u123")</code>. The SDK checks its local config cache (which was populated on app startup from the CDN config bundle) and finds the experiment config. It evaluates targeting: user is in the US and on mobile ‚Äî passes. It checks the mutual exclusion layer: user is not in any other experiment in this layer ‚Äî passes. It computes <code>hash("u123" + "exp_456" + "salt_xyz") mod 10000 = 3721</code>. Since the control variant covers buckets 0‚Äì4999 and treatment covers 5000‚Äì9999, the user gets <strong>control</strong> (blue button). The SDK fires an assignment log event asynchronously to the Message Queue. All of this happens in &lt; 5ms locally.
</div>

<div class="example">
<strong>Example 2 ‚Äî Cache Miss, Fallback to Assignment Service:</strong><br/>
A server-side application handling an API request calls <code>sdk.getVariant("search_ranking_v2", user_id="u789")</code>. The server-side SDK's local config cache has expired (TTL passed). The SDK makes an <code>HTTP GET /assignments?user_id=u789&experiment_key=search_ranking_v2</code> call to the Assignment Service. The Assignment Service reads the config from its in-memory Config Cache, computes the hash, checks targeting and mutual exclusion, and returns <code>{"variant": "treatment_b", "config": {"ranking_algo": "v2_boosted"}}</code>. The SDK caches this locally and returns the variant to the application. Latency: ~20‚Äì40ms.
</div>

<div class="example">
<strong>Example 3 ‚Äî User Does Not Match Targeting Rules:</strong><br/>
A user in Germany accesses the checkout page. The SDK evaluates the "checkout_button_color_test" experiment's targeting rules, which require <code>country = US</code>. The user does not match. The SDK returns the default/control experience without ever computing a hash or checking mutual exclusion. No assignment is logged for this user for this experiment.
</div>

<div class="example">
<strong>Example 4 ‚Äî Mutual Exclusion Conflict:</strong><br/>
User "u456" visits the homepage. Two experiments exist in the "homepage_layer": "homepage_hero_image" (buckets 0‚Äì2999) and "homepage_recommendation_algo" (buckets 3000‚Äì6999). The hash <code>hash("u456" + "homepage_layer" + "layer_salt") mod 10000 = 4200</code>. The user falls into the "homepage_recommendation_algo" bucket. When the code checks for "homepage_hero_image," the SDK sees the user is not in that experiment's bucket range within the layer, so it returns the default experience for hero image. When the code checks for "homepage_recommendation_algo," the SDK sees the user IS in that bucket range and returns the assigned variant.
</div>

<div class="example">
<strong>Example 5 ‚Äî Graceful Degradation (Service Down):</strong><br/>
The Assignment Service is experiencing an outage. A server-side SDK call to <code>getVariant</code> times out after 50ms. The SDK catches the timeout, returns the pre-configured default variant (control), and logs a degradation event. The user sees the default experience. The application is unaffected.
</div>

<h3>Component Deep Dive ‚Äî Flow 2</h3>

<h4>Experimentation SDK</h4>
<ul>
  <li>Lightweight library embedded into client apps (iOS, Android, web) or server-side applications</li>
  <li>Provides a simple API: <code>getVariant(experiment_key, user_id, user_attributes?) ‚Üí variant_name + config</code></li>
  <li>Contains the core assignment logic: targeting evaluation, mutual exclusion layer check, deterministic hashing</li>
  <li>Maintains a local config cache (refreshed periodically from CDN or Assignment Service)</li>
  <li>Logs assignment events asynchronously (fire-and-forget) to avoid adding latency</li>
  <li>Implements graceful degradation: returns defaults on any failure</li>
</ul>

<h4>Deterministic Hashing Algorithm</h4>
<ul>
  <li>Hash function: <code>MurmurHash3(user_id + experiment_id + salt) mod 10000</code></li>
  <li>Produces a bucket number from 0‚Äì9999 (0.01% granularity)</li>
  <li>Variants are mapped to contiguous bucket ranges based on their traffic allocation</li>
  <li>Salt is unique per experiment to ensure independent randomization across experiments</li>
  <li>For mutual exclusion layers, a separate hash is computed: <code>MurmurHash3(user_id + layer_id + layer_salt) mod 10000</code> to determine which experiment in the layer the user belongs to</li>
  <li>This approach ensures: (a) determinism ‚Äî same input always produces same bucket, (b) uniformity ‚Äî users are evenly distributed, (c) independence ‚Äî bucket for one experiment is independent of another</li>
</ul>

<h4>Assignment Service</h4>
<ul>
  <li><strong>Protocol:</strong> HTTP/REST</li>
  <li><strong>Endpoints:</strong></li>
</ul>
<table>
  <tr><th>Method</th><th>Endpoint</th><th>Input</th><th>Output</th></tr>
  <tr><td><code>GET</code></td><td><code>/assignments</code></td><td>Query params: user_id, experiment_key, user_attributes (optional JSON)</td><td>200 OK ‚Äî <code>{"variant": "treatment_a", "config": {...}}</code></td></tr>
  <tr><td><code>GET</code></td><td><code>/assignments/bulk</code></td><td>Query params: user_id, experiment_keys[] (comma-separated)</td><td>200 OK ‚Äî map of experiment_key ‚Üí variant + config</td></tr>
</table>
<ul>
  <li>Stateless ‚Äî all assignment logic is computed on-the-fly from cached config</li>
  <li>Reads experiment config from the in-memory Config Cache</li>
  <li>Horizontally scalable behind a load balancer</li>
</ul>

<h4>Message Queue (Assignment Logging)</h4>
<ul>
  <li>Receives fire-and-forget assignment log events from the SDK</li>
  <li>Events contain: user_id, experiment_id, variant_id, timestamp, user_attributes snapshot</li>
  <li>A consumer processes these events and writes them to the Assignment Log Store</li>
  <li>This decouples the latency-sensitive assignment path from the logging/storage path</li>
  <li>Why a message queue and not direct writes: Assignment lookups happen billions of times per day. Direct writes to a database from the SDK would add latency and could overwhelm the store. The message queue buffers writes and allows batch processing.</li>
  <li>Why not WebSocket or polling: Assignment logging is a one-way, fire-and-forget operation. There is no need for bidirectional communication or for the server to push data back to the client for this flow.</li>
</ul>

<h4>Assignment Log Store (NoSQL)</h4>
<ul>
  <li>Stores every assignment event: (user_id, experiment_id, variant_id, timestamp)</li>
  <li>Used by the analysis pipeline to know which users were in which experiments</li>
  <li>NoSQL (wide-column store) chosen because: extremely high write throughput, simple key-based access patterns (lookup by user_id or experiment_id), no complex joins needed, append-only workload</li>
</ul>
</div>

<hr>

<!-- ========== FLOW 3: EVENT TRACKING ========== -->
<h2>5. Flow 3 ‚Äî Event Tracking &amp; Metric Collection</h2>
<div class="card">
<p>This flow covers how user interaction events are captured, enriched with experiment assignment context, and stored for later analysis.</p>

<div class="mermaid">
graph LR
    A["üë§ User Action<br/>(click, purchase,<br/>page view, etc.)"] -->|"Event emitted<br/>by app code"| B["Experimentation<br/>SDK"]
    B -->|"Attach experiment<br/>assignments snapshot"| C["Event Payload"]
    C -->|"HTTP POST<br/>/events (batch)"| D["Event Ingestion<br/>Service"]
    D -->|"Validate &<br/>enqueue"| E["Message Queue<br/>(Events)"]
    E -->|"Consume"| F["Event Processing<br/>Worker"]
    F -->|"Enrich & write"| G[("Event Store<br/>(NoSQL /<br/>Columnar)")]
    F -->|"Update real-time<br/>counters (optional)"| H["Real-Time<br/>Aggregation<br/>Cache"]
</div>

<h3>Examples</h3>

<div class="example">
<strong>Example 1 ‚Äî Tracking a Conversion Event:</strong><br/>
User "u123" (who is in the "checkout_button_color_test" control variant) clicks the checkout button and completes a purchase. The app code calls <code>sdk.track("purchase_completed", {order_value: 49.99, item_count: 3})</code>. The SDK attaches the user's current experiment assignments: <code>{"checkout_button_color_test": "control"}</code>. It batches this event with other recent events and sends an <code>HTTP POST /events</code> to the Event Ingestion Service with a payload of <code>[{user_id: "u123", event: "purchase_completed", properties: {order_value: 49.99, item_count: 3}, experiments: {"checkout_button_color_test": "control"}, timestamp: "2026-02-13T15:30:00Z"}]</code>. The Event Ingestion Service validates the payload, enqueues it on the Message Queue. An Event Processing Worker consumes the message, enriches it (e.g., adds user segment info from a user profile service if needed), and writes it to the Event Store.
</div>

<div class="example">
<strong>Example 2 ‚Äî Tracking a Page View (High Volume):</strong><br/>
Every time a user loads the homepage, a <code>page_view</code> event is tracked. During peak traffic, millions of page_view events per second flow in. The SDK on each user's device batches events (e.g., every 10 seconds or every 20 events) and sends them to the Event Ingestion Service. The Message Queue absorbs the burst. Event Processing Workers scale horizontally to keep up. Events are written in bulk to the Event Store, which is optimized for append-heavy workloads.
</div>

<div class="example">
<strong>Example 3 ‚Äî Event with No Active Experiments:</strong><br/>
A new user who hasn't been assigned to any experiments triggers a <code>page_view</code> event. The SDK attaches an empty experiments map: <code>{}</code>. The event is still tracked in the Event Store (it may be useful for baseline metrics), but it will not contribute to any experiment's analysis.
</div>

<h3>Component Deep Dive ‚Äî Flow 3</h3>

<h4>Event Ingestion Service</h4>
<ul>
  <li><strong>Protocol:</strong> HTTP/REST</li>
  <li><strong>Endpoints:</strong></li>
</ul>
<table>
  <tr><th>Method</th><th>Endpoint</th><th>Input</th><th>Output</th></tr>
  <tr><td><code>POST</code></td><td><code>/events</code></td><td>JSON body: array of event objects, each with user_id, event_name, properties, experiments, timestamp</td><td>202 Accepted</td></tr>
</table>
<ul>
  <li>Stateless, horizontally scalable</li>
  <li>Validates event schema, rejects malformed events</li>
  <li>Enqueues valid events onto the Message Queue</li>
  <li>Returns <code>202 Accepted</code> immediately (async processing) to minimize client latency</li>
</ul>

<h4>Message Queue (Events)</h4>
<ul>
  <li>Buffers events between ingestion and processing</li>
  <li>Partitioned by <code>user_id</code> to ensure ordering of events per user</li>
  <li>Provides durability ‚Äî events are not lost if processing workers are temporarily down</li>
  <li>Allows independent scaling of ingestion and processing</li>
  <li>Why a message queue: The event ingestion rate (millions/sec) can spike unpredictably. The queue absorbs bursts and allows the processing workers to consume at a sustainable rate. It also decouples the ingestion service from the event store, providing fault tolerance.</li>
</ul>

<h4>Event Processing Worker</h4>
<ul>
  <li>Consumes events from the Message Queue</li>
  <li>Enriches events with additional context if needed (e.g., user segment lookup)</li>
  <li>Writes events in batches to the Event Store for efficiency</li>
  <li>Optionally updates real-time aggregation counters in the cache (for live dashboards)</li>
  <li>Horizontally scalable ‚Äî add more workers to handle increased throughput</li>
</ul>

<h4>Event Store (NoSQL / Columnar)</h4>
<ul>
  <li>Stores all raw events for analysis</li>
  <li>Chosen as NoSQL/columnar because: (a) append-only, extremely high write throughput, (b) analysis queries scan large ranges of data by time + experiment, which columnar storage excels at, (c) no need for ACID transactions on event data, (d) schema flexibility for different event types with varying properties</li>
  <li>Partitioned by time (e.g., daily partitions) for efficient range scans and retention management</li>
</ul>

<h4>Real-Time Aggregation Cache (Optional)</h4>
<ul>
  <li>In-memory cache storing running counters per experiment per variant (e.g., count of purchases, sum of order values)</li>
  <li>Updated by event processing workers as they process events</li>
  <li>Read by the dashboard for approximate real-time metrics (before the batch analysis pipeline runs)</li>
  <li>Eviction: Time-based expiration (e.g., 24-hour TTL); counters are recomputed by the batch pipeline</li>
</ul>
</div>

<hr>

<!-- ========== FLOW 4: ANALYSIS & REPORTING ========== -->
<h2>6. Flow 4 ‚Äî Statistical Analysis &amp; Reporting</h2>
<div class="card">
<p>This flow covers how experiment results are computed, statistical significance is determined, guardrails are checked, and results are displayed to the experimenter.</p>

<div class="mermaid">
graph LR
    A["‚è∞ Scheduler<br/>(Daily / Hourly)"] -->|"Trigger analysis<br/>for active experiments"| B["Analysis<br/>Pipeline"]
    B -->|"Read events"| C[("Event Store<br/>(NoSQL /<br/>Columnar)")]
    B -->|"Read assignments"| D[("Assignment<br/>Log Store<br/>(NoSQL)")]
    B -->|"Read experiment<br/>config"| E[("SQL DB<br/>(Experiments)")]
    B -->|"Compute metrics,<br/>p-values, CI,<br/>SRM check"| F["Statistical<br/>Engine"]
    F -->|"Write results"| G[("Results Store<br/>(SQL)")]
    F -->|"Guardrail<br/>violation?"| H{"Auto-Kill<br/>Check"}
    H -->|"Yes"| I["Experiment<br/>Management<br/>Service<br/>(PATCH: kill)"]
    H -->|"No"| J["Continue"]
    K["üë§ Experimenter"] -->|"HTTP GET<br/>/experiments/{id}/results"| L["Results<br/>Service"]
    L -->|"Read"| G
    L -->|"Return results"| K
</div>

<h3>Examples</h3>

<div class="example">
<strong>Example 1 ‚Äî Daily Analysis Run (Statistically Significant Result):</strong><br/>
At 2:00 AM UTC, the scheduler triggers the analysis pipeline for all running experiments. For "checkout_button_color_test," the pipeline reads all "purchase_completed" events from the Event Store where the user was in this experiment (matched via assignment log). It groups events by variant (control vs. treatment). It computes the conversion rate for each variant: control = 3.2%, treatment = 3.1%. It runs a two-sample z-test for proportions: p-value = 0.42, confidence interval for the difference overlaps zero. Result: <strong>not statistically significant</strong>. The pipeline writes this result to the Results Store. The experimenter checks the dashboard the next morning via <code>HTTP GET /experiments/exp_456/results</code> and sees "No significant difference detected. Consider running longer to reach required sample size."
</div>

<div class="example">
<strong>Example 2 ‚Äî Guardrail Violation Triggers Auto-Kill:</strong><br/>
The analysis pipeline runs for "search_ranking_v2." It computes the guardrail metric "p95_page_load_time." The treatment variant shows a p95 load time of 3.2 seconds vs. control's 1.8 seconds ‚Äî a 78% increase. This exceeds the configured guardrail threshold of 20% degradation with statistical significance (p &lt; 0.001). The pipeline triggers an auto-kill by calling <code>HTTP PATCH /experiments/search_ranking_v2</code> with <code>{"status": "killed", "kill_reason": "guardrail_violation: p95_page_load_time degraded by 78%"}</code> on the Experiment Management Service. The experiment is stopped immediately. The experimenter receives a notification (email/Slack) about the auto-kill.
</div>

<div class="example">
<strong>Example 3 ‚Äî Sample Ratio Mismatch (SRM) Detection:</strong><br/>
The analysis pipeline for "homepage_recommendation_algo" checks for SRM. The experiment was configured for a 50/50 split. Actual assignment counts: control = 502,341, treatment = 487,112. A chi-squared test for goodness-of-fit yields p = 0.00001, indicating a significant sample ratio mismatch. The pipeline flags this experiment with an SRM warning. Results are still computed but marked as "unreliable ‚Äî SRM detected." The experimenter is alerted to investigate potential bugs in the assignment logic or targeting.
</div>

<h3>Component Deep Dive ‚Äî Flow 4</h3>

<h4>Scheduler</h4>
<ul>
  <li>A cron-like service that triggers analysis runs on a configurable schedule (hourly, daily)</li>
  <li>For each running experiment, it enqueues an analysis job</li>
  <li>Can also be triggered on-demand by an experimenter from the dashboard</li>
</ul>

<h4>Analysis Pipeline</h4>
<ul>
  <li>A batch data processing pipeline (e.g., MapReduce / distributed compute framework)</li>
  <li>For each experiment, it: (1) joins events with assignments, (2) groups by variant, (3) computes per-variant metrics, (4) passes to the statistical engine</li>
  <li>Reads from: Event Store, Assignment Log Store, SQL DB (for experiment config and metric definitions)</li>
  <li>Writes to: Results Store</li>
</ul>

<h4>Statistical Engine</h4>
<ul>
  <li>Computes:
    <ul>
      <li><strong>Mean/proportion per variant</strong> for each metric</li>
      <li><strong>Confidence intervals</strong> (typically 95%)</li>
      <li><strong>P-values</strong> using appropriate tests:
        <ul>
          <li>Two-sample t-test for continuous metrics (e.g., revenue per user)</li>
          <li>Two-sample z-test for proportions (e.g., conversion rate)</li>
          <li>Mann-Whitney U test for non-normal distributions</li>
        </ul>
      </li>
      <li><strong>Bonferroni correction</strong> for multiple comparisons when there are multiple treatment variants</li>
      <li><strong>Sample Ratio Mismatch (SRM)</strong> detection via chi-squared goodness-of-fit test</li>
      <li><strong>Sequential analysis</strong> (optional) ‚Äî always-valid p-values for continuous monitoring without inflating false positive rate</li>
    </ul>
  </li>
</ul>

<h4>Results Store (SQL)</h4>
<ul>
  <li>Stores computed results: per-experiment per-metric per-variant statistics, p-values, confidence intervals, SRM flags</li>
  <li>SQL chosen because: results have a well-defined relational schema, the dashboard needs complex queries (filter by metric, sort by significance, join with experiment metadata), and the write volume is low (computed in batches)</li>
</ul>

<h4>Results Service</h4>
<ul>
  <li><strong>Protocol:</strong> HTTP/REST</li>
  <li><strong>Endpoints:</strong></li>
</ul>
<table>
  <tr><th>Method</th><th>Endpoint</th><th>Input</th><th>Output</th></tr>
  <tr><td><code>GET</code></td><td><code>/experiments/{id}/results</code></td><td>Path param: experiment_id; Query params: metric_name (optional), date_range</td><td>200 OK ‚Äî results object with per-variant metrics, p-values, CIs, SRM status, significance flag</td></tr>
  <tr><td><code>GET</code></td><td><code>/experiments/{id}/results/timeseries</code></td><td>Path param: experiment_id; Query params: metric_name, granularity (daily/hourly)</td><td>200 OK ‚Äî array of metric values over time per variant</td></tr>
</table>
</div>

<hr>

<!-- ========== COMBINED FLOW ========== -->
<h2>7. Combined Overall Flow</h2>
<div class="card">
<p>This diagram combines all four flows into one unified view of the experimentation platform.</p>

<div class="mermaid">
graph TB
    subgraph "Experiment Creation (Flow 1)"
        EXP["üë§ Experimenter<br/>(Dashboard)"] -->|"HTTP POST/PATCH<br/>/experiments"| EMS["Experiment<br/>Management<br/>Service"]
        EMS -->|"Write config<br/>+ audit log"| SQLDB[("SQL DB<br/>(Config,<br/>Audit)")]
        EMS -->|"Publish update"| CDS["Config<br/>Distribution<br/>Service"]
        CDS -->|"Push bundle"| CDN["CDN"]
        CDS -->|"Invalidate"| CCACHE["Config Cache<br/>(In-Memory)"]
    end

    subgraph "Variant Assignment (Flow 2)"
        USER["üë§ User App"] -->|"getVariant()"| SDK["Experimentation<br/>SDK"]
        SDK -->|"Fetch config"| CDN
        SDK -->|"Fallback"| ASVC["Assignment<br/>Service"]
        ASVC -->|"Read config"| CCACHE
        SDK -->|"Deterministic<br/>hash + targeting<br/>+ ME layer"| VARIANT["Return<br/>Variant"]
        SDK -->|"Log assignment<br/>(async)"| MQ1["Message Queue<br/>(Assignments)"]
        MQ1 -->|"Consume"| ALOG[("Assignment<br/>Log Store<br/>(NoSQL)")]
    end

    subgraph "Event Tracking (Flow 3)"
        USER -->|"User actions"| SDK2["SDK<br/>(track)"]
        SDK2 -->|"Attach experiment<br/>assignments"| EIS["Event Ingestion<br/>Service"]
        EIS -->|"Enqueue"| MQ2["Message Queue<br/>(Events)"]
        MQ2 -->|"Consume"| EPW["Event<br/>Processing<br/>Worker"]
        EPW -->|"Write"| ESTORE[("Event Store<br/>(NoSQL /<br/>Columnar)")]
        EPW -->|"Update counters"| RTCACHE["Real-Time<br/>Aggregation<br/>Cache"]
    end

    subgraph "Analysis & Reporting (Flow 4)"
        SCHED["‚è∞ Scheduler"] -->|"Trigger"| PIPELINE["Analysis<br/>Pipeline"]
        PIPELINE -->|"Read events"| ESTORE
        PIPELINE -->|"Read assignments"| ALOG
        PIPELINE -->|"Read config"| SQLDB
        PIPELINE -->|"Compute stats"| STATS["Statistical<br/>Engine"]
        STATS -->|"Write results"| RSTORE[("Results Store<br/>(SQL)")]
        STATS -->|"Guardrail<br/>violation"| EMS
        EXP -->|"HTTP GET /results"| RSVC["Results<br/>Service"]
        RSVC -->|"Read"| RSTORE
        RSVC -->|"Real-time data"| RTCACHE
    end
</div>

<h3>End-to-End Examples</h3>

<div class="example">
<strong>Example 1 ‚Äî Full Lifecycle of a Successful Experiment:</strong><br/>
<ol>
  <li><strong>Creation (Flow 1):</strong> Product Manager "Alice" creates "checkout_button_color_test" via the dashboard. <code>HTTP POST /experiments</code> ‚Üí Experiment Management Service ‚Üí SQL DB + Config Distribution Service ‚Üí CDN + Config Cache.</li>
  <li><strong>Assignment (Flow 2):</strong> User "u123" opens the checkout page. The app calls <code>sdk.getVariant("checkout_button_color_test", "u123")</code>. The SDK reads the config from its local cache (fetched from CDN), computes <code>hash("u123" + "exp_456" + "salt") mod 10000 = 3721</code> ‚Üí control variant. Assignment logged async to Message Queue ‚Üí Assignment Log Store.</li>
  <li><strong>Event Tracking (Flow 3):</strong> User "u123" completes a purchase. App calls <code>sdk.track("purchase_completed", {order_value: 49.99})</code>. SDK attaches experiment assignments, sends to Event Ingestion Service ‚Üí Message Queue ‚Üí Event Processing Worker ‚Üí Event Store.</li>
  <li><strong>Analysis (Flow 4):</strong> After 2 weeks, the daily analysis pipeline runs. It joins events and assignments, computes conversion rates (control: 3.2%, treatment: 3.8%), runs a z-test (p = 0.003). Result: statistically significant. Alice opens the dashboard, sees a 19% lift with p &lt; 0.01. She decides to ship the green button and ends the experiment via <code>HTTP PATCH /experiments/exp_456 {"status": "completed"}</code>.</li>
</ol>
</div>

<div class="example">
<strong>Example 2 ‚Äî Experiment Killed by Guardrail:</strong><br/>
<ol>
  <li><strong>Creation (Flow 1):</strong> Engineer "Bob" creates "new_image_compression_algo" with a guardrail metric "image_load_time_p99" (max 10% degradation allowed).</li>
  <li><strong>Assignment (Flow 2):</strong> Users across the platform are assigned to control or treatment as they load images.</li>
  <li><strong>Event Tracking (Flow 3):</strong> Image load events with timing data flow into the Event Store.</li>
  <li><strong>Analysis (Flow 4):</strong> After 24 hours, the analysis pipeline detects that the treatment variant's p99 image load time is 45% worse than control with p &lt; 0.0001. This violates the guardrail. The pipeline calls <code>HTTP PATCH /experiments/{id} {"status": "killed"}</code>. The Experiment Management Service updates the config and pushes it out. Within 60 seconds, the CDN and Config Cache are updated. All users now get the control experience. Bob is notified via alert.</li>
</ol>
</div>
</div>

<hr>

<!-- ========== DATABASE SCHEMA ========== -->
<h2>8. Database Schema</h2>

<h3>SQL Tables</h3>
<div class="card">
<p><span class="tag tag-sql">SQL</span> SQL is used for experiment configuration, results, and audit data because these require ACID transactions, have relational structures (experiments ‚Üí variants ‚Üí metrics), and need complex queries for dashboard functionality (filtering, sorting, pagination, joins).</p>

<h4>Table: <code>experiments</code></h4>
<table>
  <tr><th>Column</th><th>Type</th><th>Key</th><th>Description</th></tr>
  <tr><td>experiment_id</td><td>UUID</td><td>PK</td><td>Unique experiment identifier</td></tr>
  <tr><td>name</td><td>VARCHAR(255)</td><td>UNIQUE INDEX</td><td>Human-readable experiment name/key</td></tr>
  <tr><td>description</td><td>TEXT</td><td></td><td>Experiment description and hypothesis</td></tr>
  <tr><td>owner_id</td><td>UUID</td><td>FK ‚Üí users</td><td>Experimenter who created this</td></tr>
  <tr><td>status</td><td>ENUM</td><td>INDEX (B-tree)</td><td>draft, running, paused, completed, killed</td></tr>
  <tr><td>traffic_percentage</td><td>DECIMAL(5,2)</td><td></td><td>% of eligible users included (0.00‚Äì100.00)</td></tr>
  <tr><td>targeting_rules</td><td>JSON</td><td></td><td>Targeting criteria (country, platform, etc.)</td></tr>
  <tr><td>mutual_exclusion_layer_id</td><td>UUID</td><td>FK ‚Üí mutual_exclusion_layers (nullable)</td><td>Layer for mutual exclusion</td></tr>
  <tr><td>salt</td><td>VARCHAR(64)</td><td></td><td>Random salt for deterministic hashing</td></tr>
  <tr><td>start_date</td><td>TIMESTAMP</td><td>INDEX (B-tree)</td><td>When the experiment starts</td></tr>
  <tr><td>end_date</td><td>TIMESTAMP</td><td></td><td>When the experiment ends</td></tr>
  <tr><td>created_at</td><td>TIMESTAMP</td><td></td><td>Record creation time</td></tr>
  <tr><td>updated_at</td><td>TIMESTAMP</td><td></td><td>Last modification time</td></tr>
</table>
<p><strong>Indexes:</strong></p>
<ul>
  <li><strong>B-tree index on <code>status</code></strong> ‚Äî The dashboard frequently queries experiments by status (e.g., "show all running experiments"). B-tree supports equality lookups efficiently and is the default index type for enumerated values.</li>
  <li><strong>B-tree index on <code>start_date</code></strong> ‚Äî Supports range queries for finding experiments that started within a date range (e.g., "experiments started this month").</li>
  <li><strong>Hash index on <code>name</code></strong> ‚Äî The Assignment Service and SDK look up experiments by their unique name/key. A hash index provides O(1) equality lookups which is optimal for this access pattern.</li>
</ul>
<p><strong>Sharding:</strong> Not required. Experiment config is a low-volume dataset (hundreds of thousands of rows, not billions). A single SQL instance with read replicas is sufficient.</p>
<p><strong>Read from:</strong> Config Distribution Service (to build config bundles), Analysis Pipeline (to get metric definitions), Results Service (to join with results), Dashboard (to list/view experiments).</p>
<p><strong>Written to:</strong> Experiment Management Service when an experimenter creates/updates/deletes an experiment, or when the auto-kill system patches an experiment status.</p>

<h4>Table: <code>variants</code></h4>
<table>
  <tr><th>Column</th><th>Type</th><th>Key</th><th>Description</th></tr>
  <tr><td>variant_id</td><td>UUID</td><td>PK</td><td>Unique variant identifier</td></tr>
  <tr><td>experiment_id</td><td>UUID</td><td>FK ‚Üí experiments, INDEX (B-tree)</td><td>Parent experiment</td></tr>
  <tr><td>name</td><td>VARCHAR(100)</td><td></td><td>e.g., "control", "treatment_a"</td></tr>
  <tr><td>weight</td><td>DECIMAL(5,2)</td><td></td><td>Traffic allocation weight (percentage)</td></tr>
  <tr><td>config_payload</td><td>JSON</td><td></td><td>Feature flag values / parameters for this variant</td></tr>
  <tr><td>is_control</td><td>BOOLEAN</td><td></td><td>Whether this is the control variant</td></tr>
</table>
<p><strong>Indexes:</strong></p>
<ul>
  <li><strong>B-tree index on <code>experiment_id</code></strong> ‚Äî Variants are always queried by their parent experiment. B-tree supports efficient lookups for all variants belonging to a given experiment.</li>
</ul>
<p><strong>Read from:</strong> Config Distribution Service, Analysis Pipeline.</p>
<p><strong>Written to:</strong> Experiment Management Service on experiment creation/update.</p>

<h4>Table: <code>metrics</code></h4>
<table>
  <tr><th>Column</th><th>Type</th><th>Key</th><th>Description</th></tr>
  <tr><td>metric_id</td><td>UUID</td><td>PK</td><td>Unique metric identifier</td></tr>
  <tr><td>experiment_id</td><td>UUID</td><td>FK ‚Üí experiments, INDEX (B-tree)</td><td>Parent experiment</td></tr>
  <tr><td>name</td><td>VARCHAR(255)</td><td></td><td>Metric display name</td></tr>
  <tr><td>type</td><td>ENUM</td><td></td><td>primary, secondary, guardrail</td></tr>
  <tr><td>event_name</td><td>VARCHAR(255)</td><td></td><td>Which event to aggregate</td></tr>
  <tr><td>aggregation</td><td>ENUM</td><td></td><td>mean, sum, count, ratio, percentile</td></tr>
  <tr><td>min_detectable_effect</td><td>DECIMAL(5,4)</td><td></td><td>MDE for power analysis</td></tr>
  <tr><td>guardrail_threshold</td><td>DECIMAL(5,4)</td><td></td><td>Max allowed degradation (for guardrail type)</td></tr>
</table>
<p><strong>Indexes:</strong></p>
<ul>
  <li><strong>B-tree index on <code>experiment_id</code></strong> ‚Äî Metrics are always queried by their parent experiment.</li>
</ul>
<p><strong>Read from:</strong> Analysis Pipeline (to know what to compute), Dashboard (to display metric config).</p>
<p><strong>Written to:</strong> Experiment Management Service on experiment creation/update.</p>

<h4>Table: <code>mutual_exclusion_layers</code></h4>
<table>
  <tr><th>Column</th><th>Type</th><th>Key</th><th>Description</th></tr>
  <tr><td>layer_id</td><td>UUID</td><td>PK</td><td>Unique layer identifier</td></tr>
  <tr><td>name</td><td>VARCHAR(255)</td><td>UNIQUE</td><td>Layer name</td></tr>
  <tr><td>description</td><td>TEXT</td><td></td><td>Purpose of this layer</td></tr>
  <tr><td>salt</td><td>VARCHAR(64)</td><td></td><td>Random salt for layer-level hashing</td></tr>
</table>
<p><strong>Read from:</strong> Config Distribution Service, Assignment Service/SDK.</p>
<p><strong>Written to:</strong> Experiment Management Service when a layer is created.</p>

<h4>Table: <code>experiment_audit_log</code></h4>
<table>
  <tr><th>Column</th><th>Type</th><th>Key</th><th>Description</th></tr>
  <tr><td>log_id</td><td>UUID</td><td>PK</td><td>Unique log entry ID</td></tr>
  <tr><td>experiment_id</td><td>UUID</td><td>FK ‚Üí experiments, INDEX (B-tree)</td><td>Which experiment was modified</td></tr>
  <tr><td>actor_id</td><td>UUID</td><td></td><td>Who made the change</td></tr>
  <tr><td>action</td><td>VARCHAR(50)</td><td></td><td>created, updated, paused, resumed, killed, completed</td></tr>
  <tr><td>old_value</td><td>JSON</td><td></td><td>Previous state (snapshot)</td></tr>
  <tr><td>new_value</td><td>JSON</td><td></td><td>New state (snapshot)</td></tr>
  <tr><td>created_at</td><td>TIMESTAMP</td><td>INDEX (B-tree)</td><td>When the change occurred</td></tr>
</table>
<p><strong>Indexes:</strong></p>
<ul>
  <li><strong>B-tree index on <code>experiment_id</code></strong> ‚Äî Audit logs are queried per experiment.</li>
  <li><strong>B-tree index on <code>created_at</code></strong> ‚Äî Supports time-range queries on audit history.</li>
</ul>
<p><strong>Read from:</strong> Dashboard (to display audit trail).</p>
<p><strong>Written to:</strong> Experiment Management Service on every mutation to an experiment.</p>

<h4>Table: <code>experiment_results</code></h4>
<table>
  <tr><th>Column</th><th>Type</th><th>Key</th><th>Description</th></tr>
  <tr><td>result_id</td><td>UUID</td><td>PK</td><td>Unique result ID</td></tr>
  <tr><td>experiment_id</td><td>UUID</td><td>FK ‚Üí experiments, INDEX (B-tree)</td><td>Which experiment</td></tr>
  <tr><td>variant_id</td><td>UUID</td><td>FK ‚Üí variants</td><td>Which variant</td></tr>
  <tr><td>metric_id</td><td>UUID</td><td>FK ‚Üí metrics</td><td>Which metric</td></tr>
  <tr><td>analysis_date</td><td>DATE</td><td>INDEX (B-tree, composite with experiment_id)</td><td>Date of this analysis run</td></tr>
  <tr><td>sample_size</td><td>BIGINT</td><td></td><td>Number of users in this variant</td></tr>
  <tr><td>metric_value</td><td>DECIMAL(20,6)</td><td></td><td>Computed metric value (mean, proportion, etc.)</td></tr>
  <tr><td>confidence_interval_lower</td><td>DECIMAL(20,6)</td><td></td><td>95% CI lower bound</td></tr>
  <tr><td>confidence_interval_upper</td><td>DECIMAL(20,6)</td><td></td><td>95% CI upper bound</td></tr>
  <tr><td>p_value</td><td>DECIMAL(10,8)</td><td></td><td>Statistical p-value</td></tr>
  <tr><td>is_significant</td><td>BOOLEAN</td><td></td><td>Whether result is statistically significant</td></tr>
  <tr><td>srm_p_value</td><td>DECIMAL(10,8)</td><td></td><td>Sample ratio mismatch p-value</td></tr>
  <tr><td>srm_detected</td><td>BOOLEAN</td><td></td><td>Whether SRM was detected</td></tr>
</table>
<p><strong>Indexes:</strong></p>
<ul>
  <li><strong>Composite B-tree index on <code>(experiment_id, analysis_date)</code></strong> ‚Äî The dashboard queries results for a specific experiment, often for the latest analysis date or a date range. This composite index serves both patterns efficiently.</li>
</ul>
<p><strong>Read from:</strong> Results Service (to serve dashboard).</p>
<p><strong>Written to:</strong> Analysis Pipeline after each analysis run.</p>
</div>

<h3>NoSQL Tables</h3>
<div class="card">

<h4>Table: <code>assignment_log</code> <span class="tag tag-nosql">NoSQL (Wide-Column)</span></h4>
<table>
  <tr><th>Column</th><th>Type</th><th>Key</th><th>Description</th></tr>
  <tr><td>user_id</td><td>STRING</td><td>Partition Key</td><td>User identifier</td></tr>
  <tr><td>experiment_id</td><td>STRING</td><td>Sort Key</td><td>Experiment identifier</td></tr>
  <tr><td>variant_id</td><td>STRING</td><td></td><td>Assigned variant</td></tr>
  <tr><td>assigned_at</td><td>TIMESTAMP</td><td></td><td>When the assignment was made</td></tr>
  <tr><td>user_attributes</td><td>MAP</td><td></td><td>Snapshot of user attributes at assignment time</td></tr>
</table>
<p><strong>Why NoSQL:</strong> This table sees <strong>extremely high write throughput</strong> (billions of assignments per day). The access pattern is simple: write on assignment, read by (user_id) or scan by (experiment_id) during analysis. No joins or transactions are needed. A wide-column NoSQL store handles this scale efficiently with horizontal scaling.</p>
<p><strong>Sharding:</strong> <strong>Hash-based sharding on <code>user_id</code></strong>. This distributes writes evenly across nodes (user IDs are naturally well-distributed). The analysis pipeline reads by experiment_id, which requires a scatter-gather across shards, but this is acceptable for batch processing.</p>
<p><strong>Read from:</strong> Analysis Pipeline (batch reads per experiment for analysis).</p>
<p><strong>Written to:</strong> Assignment logging consumer (from Message Queue) whenever a user is assigned to a variant.</p>

<h4>Table: <code>events</code> <span class="tag tag-nosql">NoSQL (Columnar / Time-Series)</span></h4>
<table>
  <tr><th>Column</th><th>Type</th><th>Key</th><th>Description</th></tr>
  <tr><td>event_id</td><td>UUID</td><td>PK</td><td>Unique event identifier</td></tr>
  <tr><td>user_id</td><td>STRING</td><td>INDEX</td><td>User who triggered the event</td></tr>
  <tr><td>event_name</td><td>STRING</td><td>INDEX</td><td>Event type (e.g., "purchase_completed")</td></tr>
  <tr><td>event_properties</td><td>MAP</td><td></td><td>Event-specific data (order_value, item_count, etc.)</td></tr>
  <tr><td>experiment_assignments</td><td>MAP</td><td></td><td>Snapshot of user's experiment assignments at event time</td></tr>
  <tr><td>timestamp</td><td>TIMESTAMP</td><td>Partition Key (date)</td><td>When the event occurred</td></tr>
  <tr><td>user_attributes</td><td>MAP</td><td></td><td>User segment info at event time</td></tr>
</table>
<p><strong>Why NoSQL (Columnar):</strong> Events are the highest-volume data in the system (millions per second). They are append-only (never updated), have a flexible schema (different events have different properties), and are queried analytically (large scans with aggregations). A columnar store excels at compressing and scanning large volumes of data for analytical queries.</p>
<p><strong>Sharding:</strong> <strong>Range-based sharding on <code>timestamp</code> (daily partitions)</strong>. Analysis queries are always bounded by time range. Range partitioning by date ensures that a query for "last 7 days" only scans 7 partitions. It also enables efficient data retention (drop old partitions to free space).</p>
<p><strong>Denormalization Note:</strong> The <code>experiment_assignments</code> field is denormalized ‚Äî it's a snapshot of the user's assignments embedded directly in each event. This denormalization is done because:
<ul>
  <li>It avoids a join between the events table and assignment_log table during analysis (which would be extremely expensive at billions-of-rows scale)</li>
  <li>It captures the assignment <em>at the time of the event</em>, which is important for correctness (a user might be reassigned to a different experiment later, but the event should reflect the assignment when it happened)</li>
  <li>The trade-off is increased storage (the same assignment data is repeated across events), which is acceptable given the cost efficiency of columnar storage with compression</li>
</ul>
</p>
<p><strong>Read from:</strong> Analysis Pipeline (batch scans for experiment analysis).</p>
<p><strong>Written to:</strong> Event Processing Worker as events are consumed from the Message Queue.</p>
</div>

<hr>

<!-- ========== CDN & CACHE ========== -->
<h2>9. CDN &amp; Cache Deep Dive</h2>

<h3>CDN</h3>
<div class="card">
<p>A CDN is appropriate for two purposes in this system:</p>
<ol>
  <li><strong>Static Dashboard Assets:</strong> The experiment dashboard's HTML, JS, CSS, and images are served via CDN. This is standard for any web application.</li>
  <li><strong>Experiment Config Bundles:</strong> The Config Distribution Service publishes the active experiment config as a JSON bundle to the CDN. Client-side SDKs (mobile apps, web apps) fetch this bundle on initialization and periodically refresh it. The CDN dramatically reduces load on the Config Distribution Service and provides low-latency access from edge locations worldwide.</li>
</ol>
<p><strong>CDN is NOT used for:</strong> Event tracking (dynamic write traffic), assignment logging (write traffic), or analysis data (batch processing, not user-facing).</p>

<h4>CDN Configuration</h4>
<ul>
  <li><strong>TTL:</strong> 30‚Äì60 seconds for config bundles. This is a trade-off between freshness (how quickly config changes propagate) and CDN cache hit rate (higher TTL = better hit rate). 30‚Äì60 seconds is acceptable because experiment config changes are infrequent (maybe a few times a day) and a delay of up to 60 seconds in propagation is tolerable.</li>
  <li><strong>Cache Invalidation:</strong> The Config Distribution Service explicitly invalidates the CDN cache when a new config bundle is published, ensuring propagation within seconds even before TTL expiry.</li>
</ul>
</div>

<h3>In-Memory Config Cache (Assignment Service)</h3>
<div class="card">
<h4>Purpose</h4>
<p>The Assignment Service keeps the full experiment config bundle in an in-memory cache. This allows it to compute variant assignments without any database or network calls, achieving sub-millisecond assignment computation.</p>

<h4>Caching Strategy: Write-Through</h4>
<ul>
  <li>When the Config Distribution Service publishes a new config bundle, it <strong>writes through</strong> to the cache (pushes the update directly to the cache)</li>
  <li>Simultaneously, the config is written to the CDN and the SQL DB</li>
  <li>This ensures the cache is always up-to-date with the latest config</li>
</ul>

<h4>Eviction Policy: LRU (Least Recently Used)</h4>
<ul>
  <li>The cache stores the config for all active experiments, which is bounded (typically under 1 GB even for 100K experiments)</li>
  <li>LRU eviction is a safety mechanism in case memory pressure occurs; in practice, the full config fits in memory</li>
</ul>

<h4>Expiration Policy: TTL of 5 minutes</h4>
<ul>
  <li>Each cache entry has a TTL of 5 minutes as a safety net</li>
  <li>If the write-through push fails (e.g., network partition), the cache will eventually expire stale entries and re-fetch from the Config Distribution Service</li>
  <li>5 minutes is chosen as a balance: short enough that stale config is limited, long enough that re-fetches don't overload the system</li>
</ul>

<h4>Why write-through:</h4>
<ul>
  <li>Config changes must propagate quickly (experimenters expect changes to take effect within seconds)</li>
  <li>The Config Distribution Service knows exactly when changes occur, making push-based (write-through) more efficient than pull-based (lazy loading)</li>
  <li>Alternative considered: <em>Write-behind</em> (async write to cache) ‚Äî rejected because we need the cache to be consistent with the source of truth immediately</li>
  <li>Alternative considered: <em>Cache-aside / Lazy loading</em> ‚Äî rejected because it introduces a cold-start problem and potential cache-miss latency on the assignment hot path</li>
</ul>
</div>

<h3>Real-Time Aggregation Cache</h3>
<div class="card">
<h4>Purpose</h4>
<p>Stores running metric counters per experiment per variant for near-real-time dashboard display. This is optional ‚Äî the primary analysis is done by the batch pipeline.</p>

<h4>Caching Strategy: Write-Behind (Async Updates)</h4>
<ul>
  <li>Event Processing Workers update counters in the cache as they process events</li>
  <li>These are approximate running tallies, not exact figures</li>
  <li>The batch analysis pipeline produces the authoritative results</li>
</ul>

<h4>Eviction Policy: LRU</h4>
<ul>
  <li>Counters for completed/killed experiments are evicted first</li>
</ul>

<h4>Expiration Policy: TTL of 24 hours</h4>
<ul>
  <li>Counters expire after 24 hours and are recomputed by the batch pipeline</li>
  <li>This prevents stale real-time counters from diverging too far from the batch results</li>
</ul>
</div>

<hr>

<!-- ========== SCALING CONSIDERATIONS ========== -->
<h2>10. Scaling Considerations</h2>
<div class="card">

<h3>Load Balancers</h3>
<p>Load balancers are placed in front of every stateless service to distribute traffic and enable horizontal scaling:</p>

<h4>1. Load Balancer ‚Üí Assignment Service</h4>
<ul>
  <li><strong>Type:</strong> Layer 7 (HTTP) load balancer</li>
  <li><strong>Algorithm:</strong> Round-robin (all instances are stateless and equivalent)</li>
  <li><strong>Health checks:</strong> HTTP health endpoint on each instance</li>
  <li><strong>Why:</strong> The Assignment Service handles the highest request volume (billions of lookups/day). A load balancer distributes traffic across many instances and enables auto-scaling.</li>
  <li><strong>SSL termination:</strong> Handled at the load balancer to offload TLS from application instances</li>
</ul>

<h4>2. Load Balancer ‚Üí Event Ingestion Service</h4>
<ul>
  <li><strong>Type:</strong> Layer 7 (HTTP) load balancer</li>
  <li><strong>Algorithm:</strong> Least-connections (event batches vary in size, so some requests take longer than others)</li>
  <li><strong>Health checks:</strong> HTTP health endpoint</li>
  <li><strong>Why:</strong> Event ingestion is the second-highest-volume service. Load balancing ensures no single instance is overwhelmed during traffic spikes.</li>
</ul>

<h4>3. Load Balancer ‚Üí Experiment Management Service</h4>
<ul>
  <li><strong>Type:</strong> Layer 7 (HTTP) load balancer</li>
  <li><strong>Algorithm:</strong> Round-robin</li>
  <li><strong>Why:</strong> Lower volume than the above services (only experimenters use it), but still needs load balancing for availability and zero-downtime deployments.</li>
</ul>

<h4>4. Load Balancer ‚Üí Results Service</h4>
<ul>
  <li><strong>Type:</strong> Layer 7 (HTTP) load balancer</li>
  <li><strong>Algorithm:</strong> Round-robin</li>
  <li><strong>Why:</strong> Dashboard traffic is moderate but needs to be reliably available for experimenters.</li>
</ul>

<h3>Horizontal Scaling Strategy</h3>
<table>
  <tr><th>Component</th><th>Scaling Approach</th><th>Scaling Trigger</th></tr>
  <tr><td>Assignment Service</td><td>Horizontal auto-scaling (stateless)</td><td>CPU/request-rate-based</td></tr>
  <tr><td>Event Ingestion Service</td><td>Horizontal auto-scaling (stateless)</td><td>CPU/request-rate-based</td></tr>
  <tr><td>Event Processing Workers</td><td>Horizontal scaling (add more consumers)</td><td>Message queue lag (consumer lag)</td></tr>
  <tr><td>Analysis Pipeline</td><td>Scale compute resources (more parallelism)</td><td>Number of active experiments / data volume</td></tr>
  <tr><td>SQL DB (Config)</td><td>Read replicas for read-heavy queries; single writer</td><td>Read latency / connection count</td></tr>
  <tr><td>NoSQL Event Store</td><td>Add nodes; range-partitioned by date</td><td>Storage / write throughput</td></tr>
  <tr><td>NoSQL Assignment Log</td><td>Add nodes; hash-partitioned by user_id</td><td>Write throughput</td></tr>
  <tr><td>Message Queues</td><td>Add partitions</td><td>Throughput / consumer lag</td></tr>
  <tr><td>Config Cache</td><td>Replicate across regions</td><td>Request latency from distant regions</td></tr>
  <tr><td>CDN</td><td>Inherently distributed globally</td><td>N/A (managed)</td></tr>
</table>

<h3>Global Distribution</h3>
<ul>
  <li>The Assignment Service and Config Cache should be deployed in multiple regions for low-latency assignment lookups worldwide</li>
  <li>Event Ingestion Services should also be multi-region to reduce client-to-server latency for event tracking</li>
  <li>The Analysis Pipeline and SQL DB can be centralized (analysis is batch, not latency-sensitive)</li>
</ul>
</div>

<hr>

<!-- ========== TRADEOFFS & DEEP DIVES ========== -->
<h2>11. Tradeoffs &amp; Deep Dives</h2>
<div class="card">

<h3>Client-Side vs. Server-Side Assignment</h3>
<table>
  <tr><th>Approach</th><th>Pros</th><th>Cons</th></tr>
  <tr><td><strong>Client-side (SDK computes locally)</strong></td><td>Zero network latency for assignment; works offline; no server dependency</td><td>Larger SDK; config bundle must be downloaded; harder to enforce server-side logic</td></tr>
  <tr><td><strong>Server-side (Assignment Service computes)</strong></td><td>Centralized control; smaller client SDK; easier to enforce business rules</td><td>Network call on hot path; single point of failure; added latency</td></tr>
  <tr><td><strong>Hybrid (our approach)</strong></td><td>Client-side SDK with local config cache (fast path) + server-side Assignment Service as fallback</td><td>More complex architecture; need to keep SDK and server logic in sync</td></tr>
</table>
<p>We chose the <strong>hybrid approach</strong> because it provides the best of both worlds: sub-millisecond assignment for the common case (cache hit) with a reliable server-side fallback. The complexity cost is managed by having a shared assignment library used by both SDK and server.</p>

<h3>Batch vs. Real-Time Analysis</h3>
<table>
  <tr><th>Approach</th><th>Pros</th><th>Cons</th></tr>
  <tr><td><strong>Batch (our primary approach)</strong></td><td>More accurate; handles large datasets; can apply complex statistical corrections; cost-efficient</td><td>Delayed results (hours); not suitable for urgent guardrail checks</td></tr>
  <tr><td><strong>Real-time streaming</strong></td><td>Immediate results; fast guardrail checks</td><td>Harder to implement complex statistics correctly; higher infrastructure cost; potential for peeking problems</td></tr>
  <tr><td><strong>Hybrid (our approach)</strong></td><td>Batch for authoritative results + real-time approximate counters for monitoring</td><td>Two systems to maintain; potential for dashboard confusion between approximate and authoritative numbers</td></tr>
</table>
<p>We chose a <strong>hybrid approach</strong>: batch analysis is the source of truth for statistical results, while real-time counters in the aggregation cache provide approximate live monitoring for guardrail alerting. This balances accuracy with responsiveness.</p>

<h3>Embedded Assignment Snapshot (Denormalization) vs. Join at Analysis Time</h3>
<p>We denormalize experiment assignments into each event record. The alternative would be to store events without assignments and join with the assignment_log during analysis.</p>
<ul>
  <li><strong>Why denormalized:</strong> Joining billions of events with billions of assignments at analysis time is extremely expensive and slow. Embedding the assignment snapshot eliminates this join.</li>
  <li><strong>Trade-off:</strong> Increased storage (~100‚Äì200 bytes per event for the assignment map). At millions of events/second, this adds terabytes of data. However, columnar stores compress repeated values efficiently, and storage is cheap compared to compute.</li>
  <li><strong>Correctness benefit:</strong> The snapshot captures the assignment at event time, which is the correct assignment for analysis. If we used a join, we'd need point-in-time lookups into the assignment log, which is more complex.</li>
</ul>

<h3>Mutual Exclusion via Hashing vs. Pre-Assignment</h3>
<table>
  <tr><th>Approach</th><th>Pros</th><th>Cons</th></tr>
  <tr><td><strong>Layer-based hashing (our approach)</strong></td><td>No pre-computation needed; O(1) lookup; fully deterministic; no storage for per-user-per-layer assignments</td><td>Traffic allocation is less flexible (must partition hash space); can't easily override individual users</td></tr>
  <tr><td><strong>Pre-assignment (store user‚Üíexperiment mapping)</strong></td><td>Full flexibility; can manually assign users; exact control</td><td>Requires storing and querying a massive mapping table; slow to update; not scalable for billions of users</td></tr>
</table>
<p>We chose <strong>layer-based hashing</strong> because it scales to billions of users without any per-user storage, is deterministic and fast, and is the industry-standard approach used by major experimentation platforms.</p>

<h3>Sequential Testing vs. Fixed-Horizon Testing</h3>
<ul>
  <li><strong>Fixed-horizon:</strong> Determine sample size upfront, run experiment for that duration, then analyze once. Simple, but experimenters often peek at results early, inflating false positive rates.</li>
  <li><strong>Sequential testing (always-valid p-values):</strong> Allows continuous monitoring of results without inflating false positive rates. More complex to implement but provides better user experience.</li>
  <li>Our design supports both: the batch pipeline can compute always-valid confidence sequences in addition to fixed-horizon results. The dashboard shows both, with clear labeling.</li>
</ul>
</div>

<hr>

<!-- ========== MESSAGE QUEUE DEEP DIVE ========== -->
<h2>12. Message Queue Deep Dive</h2>
<div class="card">

<h3>Why Message Queue (and not alternatives)</h3>
<p>We use message queues in two places: assignment logging and event processing.</p>

<h4>Why not direct database writes?</h4>
<ul>
  <li>Direct writes from the SDK to the database would add latency to the assignment and event tracking hot paths</li>
  <li>Database writes can fail or slow down under load, which would impact the user experience</li>
  <li>The message queue provides buffering, durability, and backpressure handling</li>
</ul>

<h4>Why not WebSockets?</h4>
<ul>
  <li>Assignment logging and event tracking are <strong>unidirectional</strong> (client ‚Üí server). WebSockets are designed for bidirectional communication and would add unnecessary complexity (connection management, heartbeats, reconnection logic).</li>
  <li>Events are fire-and-forget; the client doesn't need a response or a persistent connection.</li>
</ul>

<h4>Why not long polling?</h4>
<ul>
  <li>Long polling is for server ‚Üí client push. In this system, events flow from client ‚Üí server, so long polling is not applicable.</li>
</ul>

<h4>Why not pub/sub?</h4>
<ul>
  <li>Pub/sub is appropriate when multiple subscribers need the same message. For assignment logging, there's one consumer (the assignment log writer). For event processing, there's one consumer group (event processing workers). A message queue with consumer groups is more appropriate than pub/sub for this one-to-one processing pattern.</li>
  <li>Exception: The Config Distribution Service <em>does</em> use a pub/sub-like mechanism to push config updates to multiple Assignment Service instances and CDN ‚Äî this is the one place where pub/sub semantics make sense, and we use the Config Distribution Service for this.</li>
</ul>

<h3>How Messages Are Produced and Consumed</h3>

<h4>Assignment Logging Queue</h4>
<ul>
  <li><strong>Producer:</strong> The Experimentation SDK, after computing a variant assignment, serializes the assignment event (user_id, experiment_id, variant_id, timestamp, user_attributes) to JSON and sends it to the message queue via an asynchronous HTTP POST or a lightweight queue client library. This is fire-and-forget ‚Äî the SDK does not wait for acknowledgment.</li>
  <li><strong>Consumer:</strong> Assignment Log Writer workers consume messages in batches, deserialize them, and write them in bulk to the Assignment Log Store (NoSQL). Consumer acknowledges messages after successful write.</li>
  <li><strong>Partitioning:</strong> By <code>user_id</code> to ensure all assignments for a user go to the same partition, maintaining ordering per user.</li>
  <li><strong>Delivery guarantee:</strong> At-least-once. Duplicate assignment logs are harmless (same user_id + experiment_id is idempotent).</li>
</ul>

<h4>Events Queue</h4>
<ul>
  <li><strong>Producer:</strong> The Event Ingestion Service validates incoming event batches and enqueues each event as a message on the queue.</li>
  <li><strong>Consumer:</strong> Event Processing Workers consume events, optionally enrich them, and write them in batches to the Event Store.</li>
  <li><strong>Partitioning:</strong> By <code>user_id</code> to maintain event ordering per user.</li>
  <li><strong>Delivery guarantee:</strong> At-least-once. Events are deduplicated by <code>event_id</code> during processing.</li>
</ul>
</div>

<hr>

<!-- ========== ALTERNATIVE APPROACHES ========== -->
<h2>13. Alternative Approaches</h2>
<div class="card">

<h3>Alternative 1: Fully Server-Side Assignment (No Client SDK Logic)</h3>
<p><strong>Description:</strong> Instead of a client-side SDK that computes assignments locally, every assignment request goes to the server-side Assignment Service via an API call.</p>
<p><strong>Why not chosen:</strong></p>
<ul>
  <li>Adds network latency (10‚Äì50ms) to every feature check, which is unacceptable on mobile and for server-side code that makes many experiment checks per request</li>
  <li>Creates a single point of failure ‚Äî if the Assignment Service goes down, no features can be gated</li>
  <li>Much higher server-side load and cost</li>
  <li>Doesn't work offline (mobile apps)</li>
</ul>

<h3>Alternative 2: Fully Client-Side (No Assignment Service at All)</h3>
<p><strong>Description:</strong> The SDK downloads the config bundle and does everything locally. No server-side Assignment Service exists.</p>
<p><strong>Why not chosen:</strong></p>
<ul>
  <li>Server-side applications (APIs, backend services) need a server-side SDK that can't rely on CDN-delivered config bundles (different deployment model)</li>
  <li>No fallback for client-side SDKs that fail to download the config bundle</li>
  <li>Less operational control (can't monitor assignments centrally)</li>
</ul>

<h3>Alternative 3: Pre-Computed Assignments (Store User‚ÜíVariant Mapping in a Database)</h3>
<p><strong>Description:</strong> When an experiment starts, pre-compute and store the variant for every eligible user in a lookup table. At runtime, simply look up the user's variant.</p>
<p><strong>Why not chosen:</strong></p>
<ul>
  <li>For billions of users and thousands of experiments, the mapping table would be enormous (trillions of rows)</li>
  <li>Adding a new experiment requires a massive batch write to pre-assign all users</li>
  <li>New users who sign up after the experiment starts wouldn't have an assignment until the next batch run</li>
  <li>Deterministic hashing achieves the same result (consistent assignment) without any storage, using O(1) computation</li>
</ul>

<h3>Alternative 4: Real-Time Streaming Analysis Only (No Batch Pipeline)</h3>
<p><strong>Description:</strong> Use a streaming pipeline to continuously compute experiment metrics in real time, eliminating the batch analysis pipeline.</p>
<p><strong>Why not chosen:</strong></p>
<ul>
  <li>Complex statistical corrections (Bonferroni, sequential testing, variance reduction like CUPED) are difficult to implement correctly in streaming</li>
  <li>Streaming results can be noisy and misleading early in an experiment</li>
  <li>Higher infrastructure cost for continuous processing</li>
  <li>Batch analysis is the industry standard for experiment analysis because statistical rigor requires processing the full dataset</li>
  <li>We do include optional real-time counters for monitoring, but the authoritative results come from the batch pipeline</li>
</ul>

<h3>Alternative 5: Feature Flag Service as a Separate System</h3>
<p><strong>Description:</strong> Build feature flags as a completely separate system from experimentation.</p>
<p><strong>Why not chosen:</strong></p>
<ul>
  <li>Feature flags and experiments share the same core mechanism: assigning users to variants and delivering config</li>
  <li>Treating feature flags as a degenerate case of experiments (100% traffic to one variant) allows reuse of the entire assignment infrastructure</li>
  <li>A unified platform reduces cognitive overhead for engineers (one SDK, one dashboard)</li>
</ul>
</div>

<hr>

<!-- ========== ADDITIONAL INFORMATION ========== -->
<h2>14. Additional Information</h2>
<div class="card">

<h3>Sample Ratio Mismatch (SRM)</h3>
<p>SRM is one of the most common issues in experimentation. It occurs when the actual ratio of users in each variant deviates significantly from the expected ratio. Causes include:</p>
<ul>
  <li>Bugs in the assignment logic</li>
  <li>Bots or automated traffic disproportionately hitting one variant</li>
  <li>Interaction with other systems (e.g., a caching layer that serves stale assignments)</li>
  <li>Redirect-based experiments where one variant has a redirect that loses users</li>
</ul>
<p>Our platform automatically checks for SRM using a chi-squared goodness-of-fit test on every analysis run and flags affected experiments.</p>

<h3>Variance Reduction Techniques</h3>
<p>The analysis pipeline can implement CUPED (Controlled-experiment Using Pre-Experiment Data) to reduce variance and detect smaller effects faster. CUPED uses pre-experiment covariates to adjust the metric, often reducing the required sample size by 30‚Äì50%.</p>

<h3>Interaction Detection</h3>
<p>When a user is in multiple experiments simultaneously (outside of mutual exclusion layers), there's a risk of interaction effects. The analysis pipeline should support interaction analysis ‚Äî checking whether the effect of experiment A depends on the variant in experiment B. This is computationally expensive but important for platform integrity.</p>

<h3>Ramp-Up Strategy</h3>
<p>Experiments should support gradual ramp-up: starting at 1% traffic, then increasing to 5%, 10%, 50%, 100% as confidence grows. The deterministic hashing approach supports this naturally ‚Äî increasing traffic percentage expands the bucket range, and previously-assigned users retain their variant.</p>

<h3>Holdout Groups</h3>
<p>A global holdout group (e.g., 1% of all users) is never exposed to any experiment. This allows measuring the cumulative impact of all experiments over time.</p>

<h3>Network Effects / SUTVA Violations</h3>
<p>For social products, user-level randomization can violate the Stable Unit Treatment Value Assumption (SUTVA) because users influence each other. For such experiments, the platform should support cluster-based randomization (e.g., randomize by geographic region or social cluster instead of by individual user). This is an advanced feature that requires changes to the hashing logic.</p>

<h3>SDK Resilience Patterns</h3>
<ul>
  <li><strong>Circuit breaker:</strong> If the Assignment Service is returning errors, the SDK trips a circuit breaker and stops calling it, relying on cached config or defaults.</li>
  <li><strong>Bulkhead:</strong> The SDK uses a separate thread/connection pool for experiment calls so that slow experiment calls don't affect the application's main thread.</li>
  <li><strong>Timeout:</strong> Hard timeout of 50ms for any server call. If exceeded, return default.</li>
</ul>

<h3>Data Retention</h3>
<ul>
  <li>Event data: Retained for 90 days (raw), then aggregated and archived</li>
  <li>Assignment logs: Retained for the lifetime of the experiment + 30 days</li>
  <li>Experiment results: Retained indefinitely (small dataset)</li>
  <li>Experiment config: Retained indefinitely for audit purposes</li>
</ul>
</div>

<hr>

<!-- ========== VENDOR SECTION ========== -->
<h2>15. Potential Vendors</h2>
<div class="card">

<table>
  <tr><th>Component</th><th>Vendor Options</th><th>Rationale</th></tr>
  <tr>
    <td><strong>SQL Database</strong><br/>(Config, Results, Audit)</td>
    <td>PostgreSQL, MySQL, Amazon Aurora, Google Cloud Spanner</td>
    <td>PostgreSQL is mature, feature-rich (JSON support, excellent indexing), and free. Aurora offers managed PostgreSQL-compatible with auto-scaling. Cloud Spanner for global distribution if needed.</td>
  </tr>
  <tr>
    <td><strong>NoSQL Wide-Column Store</strong><br/>(Assignment Log)</td>
    <td>Apache Cassandra, Amazon DynamoDB, Google Bigtable, ScyllaDB</td>
    <td>Cassandra offers high write throughput, tunable consistency, and linear horizontal scaling. DynamoDB for managed simplicity. Bigtable for tight integration with Google analytics ecosystem.</td>
  </tr>
  <tr>
    <td><strong>Columnar Event Store</strong></td>
    <td>Apache Druid, ClickHouse, Amazon Redshift, Google BigQuery, Apache Pinot</td>
    <td>ClickHouse offers best-in-class performance for analytical queries on large datasets with excellent compression. Druid/Pinot for real-time ingestion + analysis. BigQuery for serverless simplicity.</td>
  </tr>
  <tr>
    <td><strong>Message Queue</strong></td>
    <td>Apache Kafka, Amazon SQS/Kinesis, Google Pub/Sub, Apache Pulsar</td>
    <td>Kafka is the industry standard for high-throughput event streaming with durability, ordering, and consumer groups. Pulsar for multi-tenancy. Kinesis/SQS for AWS-managed simplicity.</td>
  </tr>
  <tr>
    <td><strong>In-Memory Cache</strong></td>
    <td>Redis, Memcached, Hazelcast</td>
    <td>Redis offers rich data structures (counters for real-time aggregation, hash maps for config), persistence options, and pub/sub for config invalidation. Memcached for simpler key-value caching.</td>
  </tr>
  <tr>
    <td><strong>CDN</strong></td>
    <td>Cloudflare, Amazon CloudFront, Fastly, Akamai</td>
    <td>Cloudflare for ease-of-use and global edge network. CloudFront for AWS integration. Fastly for instant purge (important for config bundle freshness).</td>
  </tr>
  <tr>
    <td><strong>Batch Processing</strong><br/>(Analysis Pipeline)</td>
    <td>Apache Spark, Apache Flink, Google Dataflow, Presto/Trino</td>
    <td>Spark is the standard for large-scale batch processing. Flink for unified batch + streaming. Presto/Trino for SQL-based analysis directly on the event store.</td>
  </tr>
  <tr>
    <td><strong>Object Storage</strong><br/>(Archived data, large exports)</td>
    <td>Amazon S3, Google Cloud Storage, Azure Blob Storage, MinIO</td>
    <td>S3 is the standard for durable, cost-effective object storage. MinIO for self-hosted S3-compatible option.</td>
  </tr>
</table>
</div>

<hr>

<p style="text-align:center; color:#666; margin-top:3rem;">
  Experimentation Platform System Design ‚Äî Generated February 2026
</p>

<script>
  mermaid.initialize({
    startOnLoad: true,
    theme: 'default',
    flowchart: { useMaxWidth: true, htmlLabels: true, curve: 'basis' },
    securityLevel: 'loose'
  });
</script>

</body>
</html>
