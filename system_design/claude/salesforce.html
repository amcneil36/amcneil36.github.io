<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>System Design: Salesforce (CRM Platform)</title>
<script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
<style>
  :root { --primary: #0070d2; --bg: #f4f6f9; --card: #ffffff; --text: #181818; --muted: #555; --border: #d8dde6; --accent: #e8f0fe; }
  * { box-sizing: border-box; margin: 0; padding: 0; }
  body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, sans-serif; background: var(--bg); color: var(--text); line-height: 1.7; padding: 2rem; max-width: 1200px; margin: auto; }
  h1 { font-size: 2.2rem; color: var(--primary); border-bottom: 3px solid var(--primary); padding-bottom: .5rem; margin-bottom: 1.5rem; }
  h2 { font-size: 1.6rem; color: var(--primary); margin: 2.5rem 0 1rem; border-left: 4px solid var(--primary); padding-left: .75rem; }
  h3 { font-size: 1.25rem; color: #333; margin: 1.5rem 0 .75rem; }
  h4 { font-size: 1.05rem; color: #444; margin: 1rem 0 .5rem; }
  p, li { font-size: .97rem; color: var(--text); }
  ul, ol { margin-left: 1.5rem; margin-bottom: 1rem; }
  li { margin-bottom: .35rem; }
  .card { background: var(--card); border: 1px solid var(--border); border-radius: 8px; padding: 1.5rem; margin: 1rem 0; box-shadow: 0 1px 3px rgba(0,0,0,.08); }
  .example { background: #eef6ee; border-left: 4px solid #2e7d32; padding: 1rem 1.25rem; margin: 1rem 0; border-radius: 0 6px 6px 0; }
  .example strong { color: #2e7d32; }
  .warn { background: #fff8e1; border-left: 4px solid #f9a825; padding: 1rem 1.25rem; margin: 1rem 0; border-radius: 0 6px 6px 0; }
  .info { background: var(--accent); border-left: 4px solid var(--primary); padding: 1rem 1.25rem; margin: 1rem 0; border-radius: 0 6px 6px 0; }
  table { border-collapse: collapse; width: 100%; margin: 1rem 0; font-size: .93rem; }
  th, td { border: 1px solid var(--border); padding: .55rem .75rem; text-align: left; }
  th { background: var(--primary); color: #fff; }
  tr:nth-child(even) { background: #f9fafb; }
  code { background: #e8edf3; padding: 2px 6px; border-radius: 4px; font-size: .9rem; }
  pre { background: #1e1e1e; color: #d4d4d4; padding: 1rem; border-radius: 6px; overflow-x: auto; margin: 1rem 0; font-size: .88rem; line-height: 1.5; }
  .mermaid { margin: 1.5rem 0; text-align: center; }
  .toc { background: var(--card); border: 1px solid var(--border); border-radius: 8px; padding: 1.5rem 2rem; margin: 1.5rem 0; }
  .toc a { color: var(--primary); text-decoration: none; }
  .toc a:hover { text-decoration: underline; }
  .toc ol { counter-reset: toc-counter; list-style: none; margin-left: 0; }
  .toc > ol > li { counter-increment: toc-counter; margin-bottom: .3rem; }
  .toc > ol > li::before { content: counter(toc-counter) ". "; font-weight: bold; color: var(--primary); }
</style>
</head>
<body>

<h1>System Design: Salesforce &mdash; CRM Platform</h1>

<div class="toc">
<h3>Table of Contents</h3>
<ol>
  <li><a href="#fr">Functional Requirements</a></li>
  <li><a href="#nfr">Non-Functional Requirements</a></li>
  <li><a href="#flow1">Flow 1 &mdash; Lead Capture &amp; Scoring</a></li>
  <li><a href="#flow2">Flow 2 &mdash; Lead Conversion to Opportunity</a></li>
  <li><a href="#flow3">Flow 3 &mdash; Opportunity Pipeline Management</a></li>
  <li><a href="#flow4">Flow 4 &mdash; Global Search</a></li>
  <li><a href="#flow5">Flow 5 &mdash; Reporting &amp; Dashboards</a></li>
  <li><a href="#flow6">Flow 6 &mdash; Real-Time Notifications</a></li>
  <li><a href="#combined">Combined Overall Architecture Diagram</a></li>
  <li><a href="#schema">Database Schema</a></li>
  <li><a href="#cdn">CDN Deep Dive</a></li>
  <li><a href="#cache">Cache Deep Dive</a></li>
  <li><a href="#mq">Message Queue Deep Dive</a></li>
  <li><a href="#ws">WebSocket Deep Dive</a></li>
  <li><a href="#scaling">Scaling Considerations</a></li>
  <li><a href="#tradeoffs">Tradeoffs &amp; Deep Dives</a></li>
  <li><a href="#alternatives">Alternative Approaches</a></li>
  <li><a href="#additional">Additional Considerations</a></li>
  <li><a href="#vendors">Vendor Section</a></li>
</ol>
</div>

<!-- ============================================================ -->
<h2 id="fr">1. Functional Requirements</h2>
<!-- ============================================================ -->
<div class="card">
<ol>
  <li><strong>Multi-Tenant Organization Management</strong> &mdash; Each customer organization (tenant) has isolated data, users, roles, and configuration. Admins can manage users and permissions within their org.</li>
  <li><strong>Lead Management</strong> &mdash; Capture leads from web forms, APIs, CSV imports, or manual entry. Score leads automatically based on configurable criteria. Assign leads to sales reps via round-robin or rules-based routing.</li>
  <li><strong>Lead Conversion</strong> &mdash; Convert a qualified lead into an Account, Contact, and Opportunity in one atomic operation.</li>
  <li><strong>Account &amp; Contact Management</strong> &mdash; CRUD operations on company accounts and their associated contacts. Link contacts to accounts with many-to-one relationships.</li>
  <li><strong>Opportunity / Pipeline Management</strong> &mdash; Track deals through configurable stages (e.g., Prospecting → Qualification → Proposal → Negotiation → Closed-Won / Closed-Lost). Probability and expected revenue auto-calculated per stage.</li>
  <li><strong>Case Management</strong> &mdash; Create and track customer support cases linked to accounts and contacts.</li>
  <li><strong>Activity Tracking</strong> &mdash; Log calls, emails, meetings, and tasks against any CRM record.</li>
  <li><strong>Global Search</strong> &mdash; Full-text search across all CRM entities (accounts, contacts, leads, opportunities, cases) scoped to the user's org and permissions.</li>
  <li><strong>Reporting &amp; Dashboards</strong> &mdash; Build reports with filters, groupings, and aggregations. Compose dashboards from multiple report widgets. Schedule recurring report exports.</li>
  <li><strong>Workflow Automation</strong> &mdash; Define rules triggered by record create/update/delete events. Actions include field updates, email alerts, task creation, and outbound webhooks.</li>
  <li><strong>Real-Time Notifications</strong> &mdash; Notify users in real time when records they own or follow are changed, when they are @mentioned, or when they are assigned a new record.</li>
  <li><strong>Custom Objects &amp; Fields</strong> &mdash; Admins can create custom objects (tables) and custom fields on standard or custom objects without schema migrations.</li>
  <li><strong>File Attachments</strong> &mdash; Attach documents, images, and other files to any CRM record.</li>
  <li><strong>Role-Based Access Control (RBAC)</strong> &mdash; Profiles, roles, and permission sets control which objects, fields, and records each user can access.</li>
  <li><strong>Audit Trail</strong> &mdash; Log all data changes with who, what, when, old value, and new value.</li>
</ol>
</div>

<!-- ============================================================ -->
<h2 id="nfr">2. Non-Functional Requirements</h2>
<!-- ============================================================ -->
<div class="card">
<table>
<tr><th>Requirement</th><th>Target</th><th>Notes</th></tr>
<tr><td>Availability</td><td>99.99% uptime</td><td>CRM is mission-critical for sales teams; downtime means lost revenue.</td></tr>
<tr><td>Read Latency</td><td>&lt; 200 ms (p95)</td><td>Users expect snappy page loads when viewing records.</td></tr>
<tr><td>Write Latency</td><td>&lt; 500 ms (p95)</td><td>Acceptable for creates/updates which may trigger workflows.</td></tr>
<tr><td>Consistency</td><td>Strong consistency for CRM data</td><td>A sales rep who updates a deal stage must see the change immediately. Eventual consistency acceptable for search index, analytics, and notifications.</td></tr>
<tr><td>Multi-Tenancy</td><td>Millions of orgs, billions of records</td><td>Shared infrastructure with strict data isolation.</td></tr>
<tr><td>Scalability</td><td>Horizontal scaling of all tiers</td><td>Stateless services, sharded databases.</td></tr>
<tr><td>Security</td><td>SOC 2 Type II, GDPR, HIPAA-ready</td><td>Encryption at rest and in transit. Tenant-scoped queries enforced at the data layer.</td></tr>
<tr><td>Extensibility</td><td>Metadata-driven customization</td><td>No code deploys for custom objects/fields.</td></tr>
<tr><td>Audit &amp; Compliance</td><td>Full audit trail on all mutations</td><td>Immutable audit log.</td></tr>
<tr><td>Throughput</td><td>~50k API requests/sec at peak</td><td>Across all tenants.</td></tr>
</table>
</div>

<!-- ============================================================ -->
<h2 id="flow1">3. Flow 1 &mdash; Lead Capture &amp; Scoring</h2>
<!-- ============================================================ -->
<p>This flow covers how a new lead enters the system, gets persisted, scored, and assigned to a sales rep.</p>

<div class="mermaid">
graph LR
    subgraph Clients
        WF["Web Form / Landing Page"]
        API_EXT["External API Client"]
    end

    LB["Load Balancer"]
    GW["API Gateway<br/>(Auth, Rate Limit, Routing)"]
    LS["Lead Service"]
    DB_SQL[("SQL Database<br/>(Leads table)")]
    MQ["Message Queue"]
    LSS["Lead Scoring<br/>Worker"]
    NS["Notification<br/>Service"]
    SI["Search Index<br/>Updater"]
    SEARCH[("Search Index")]
    WSS["WebSocket<br/>Server"]
    REP["Sales Rep<br/>(Browser)"]

    WF -->|"HTTPS POST /api/v1/leads"| LB
    API_EXT -->|"HTTPS POST /api/v1/leads"| LB
    LB --> GW
    GW -->|"gRPC CreateLead()"| LS
    LS -->|"INSERT lead"| DB_SQL
    LS -->|"Publish: lead.created"| MQ
    MQ --> LSS
    MQ --> NS
    MQ --> SI
    LSS -->|"UPDATE lead SET score"| DB_SQL
    SI -->|"Index lead doc"| SEARCH
    NS -->|"Push via WebSocket"| WSS
    WSS -->|"Real-time alert"| REP
</div>

<h3>Examples</h3>

<div class="example">
<strong>Example 1 — Web Form Submission:</strong> A visitor fills out a "Request a Demo" form on Acme Corp's marketing site. The form JavaScript sends an <code>HTTPS POST /api/v1/leads</code> with body <code>{first_name: "Jane", last_name: "Doe", email: "jane@bigco.com", company: "BigCo", source: "web_form"}</code> to the Load Balancer, which forwards to the API Gateway. The Gateway authenticates the request via an API key tied to Acme Corp's org, rate-limits it, and routes to the Lead Service via gRPC. The Lead Service inserts a new row into the <code>leads</code> SQL table with <code>org_id=acme</code>, <code>status=new</code>, and publishes a <code>lead.created</code> event to the Message Queue. Three consumers pick up the message: (1) the Lead Scoring Worker evaluates the lead against Acme's scoring rules (company size, industry match) and updates the score to 85, (2) the Search Index Updater indexes the lead for full-text search, and (3) the Notification Service determines that sales rep "Bob" is the round-robin assignee, stores a notification, and pushes it to Bob's browser via the WebSocket Server. Bob sees a real-time toast: "New lead: Jane Doe (BigCo) — Score: 85".
</div>

<div class="example">
<strong>Example 2 — Bulk CSV Import:</strong> An admin uploads a CSV with 5,000 leads via <code>HTTPS POST /api/v1/leads/bulk</code>. The API Gateway accepts the request and returns a <code>202 Accepted</code> with a job ID. The Lead Service publishes a <code>bulk_import.started</code> event to the Message Queue. A Bulk Import Worker processes rows in batches of 100, inserting them into SQL and publishing individual <code>lead.created</code> events. Each lead is scored and indexed asynchronously. The admin can poll <code>GET /api/v1/jobs/{job_id}</code> for progress.
</div>

<div class="example">
<strong>Example 3 — Duplicate Lead:</strong> A lead with email "jane@bigco.com" is submitted but one already exists in Acme Corp's org. The Lead Service checks for duplicates by querying the <code>leads</code> table on <code>(org_id, email)</code>. It finds the match, and based on Acme's duplicate-handling rules (configured as "merge"), it updates the existing lead with the new data and publishes a <code>lead.updated</code> event instead of <code>lead.created</code>. No new record is created; the score is recalculated.
</div>

<h3>Component Deep Dive</h3>

<div class="card">
<h4>Load Balancer</h4>
<ul>
  <li><strong>Protocol:</strong> HTTPS (TLS 1.3) termination at the LB; forwards to API Gateway over HTTPS or HTTP internally.</li>
  <li><strong>Algorithm:</strong> Round-robin with health checks (or least-connections for long-lived requests).</li>
  <li><strong>Purpose:</strong> Distributes traffic across multiple API Gateway instances for fault tolerance and even load distribution.</li>
</ul>
</div>

<div class="card">
<h4>API Gateway</h4>
<ul>
  <li><strong>Protocol:</strong> Accepts HTTPS from clients; communicates with downstream services via gRPC (binary, efficient for internal service-to-service calls).</li>
  <li><strong>Responsibilities:</strong> Authentication (validate API key or OAuth2 JWT token), rate limiting (per-tenant and per-user quotas), request routing to the correct microservice, request/response transformation.</li>
  <li><strong>Input:</strong> <code>POST /api/v1/leads</code> &mdash; JSON body with lead fields (<code>first_name</code>, <code>last_name</code>, <code>email</code>, <code>company</code>, <code>source</code>, plus any custom fields).</li>
  <li><strong>Output:</strong> <code>201 Created</code> with the created lead JSON including <code>lead_id</code>, or <code>202 Accepted</code> for bulk operations.</li>
</ul>
</div>

<div class="card">
<h4>Lead Service</h4>
<ul>
  <li><strong>Protocol:</strong> gRPC (internal).</li>
  <li><strong>Responsibilities:</strong> Validates lead data, checks for duplicates, enforces org-level configurations (required fields, field validation rules), inserts/updates the lead record in SQL, and publishes events to the Message Queue.</li>
  <li><strong>Input (gRPC):</strong> <code>CreateLead(org_id, lead_data)</code></li>
  <li><strong>Output (gRPC):</strong> <code>LeadResponse(lead_id, status, created_at)</code></li>
  <li><strong>Database interaction:</strong> Writes to the <code>leads</code> SQL table; reads from <code>leads</code> for duplicate detection.</li>
</ul>
</div>

<div class="card">
<h4>SQL Database (Leads Table)</h4>
<ul>
  <li><strong>Protocol:</strong> TCP (database wire protocol).</li>
  <li><strong>Purpose:</strong> Durable, strongly consistent storage for lead records. SQL chosen because leads have structured, relational data and we need ACID transactions (e.g., atomic duplicate check + insert).</li>
  <li><strong>Sharding:</strong> Sharded by <code>org_id</code> so all leads for one tenant are co-located.</li>
</ul>
</div>

<div class="card">
<h4>Message Queue</h4>
<ul>
  <li><strong>Protocol:</strong> TCP (message queue wire protocol, e.g., AMQP-like).</li>
  <li><strong>Purpose:</strong> Decouples the synchronous lead creation from asynchronous downstream processing (scoring, indexing, notifications). Guarantees at-least-once delivery.</li>
  <li><strong>Topics:</strong> <code>lead.created</code>, <code>lead.updated</code>, <code>bulk_import.started</code>.</li>
  <li><strong>See <a href="#mq">Message Queue Deep Dive</a> for full details.</strong></li>
</ul>
</div>

<div class="card">
<h4>Lead Scoring Worker</h4>
<ul>
  <li><strong>Protocol:</strong> Consumes from Message Queue via TCP.</li>
  <li><strong>Purpose:</strong> Evaluates the lead against the org's configurable scoring rules (e.g., +20 for enterprise company size, +15 for matching industry, +10 for C-level title). Updates the <code>score</code> column on the lead record.</li>
  <li><strong>Interaction:</strong> Reads org scoring rules from cache (or SQL), then writes the score to <code>leads</code> SQL table.</li>
</ul>
</div>

<div class="card">
<h4>Search Index Updater</h4>
<ul>
  <li><strong>Protocol:</strong> Consumes from Message Queue via TCP; writes to Search Index via HTTP REST.</li>
  <li><strong>Purpose:</strong> Maintains the search index in sync with the database. Converts the lead record into a searchable document with fields like <code>name</code>, <code>email</code>, <code>company</code>, and indexes it. The index is eventually consistent with the source of truth (SQL).</li>
</ul>
</div>

<div class="card">
<h4>Notification Service</h4>
<ul>
  <li><strong>Protocol:</strong> Consumes from Message Queue via TCP; pushes to WebSocket Server via internal TCP.</li>
  <li><strong>Purpose:</strong> Determines which users should be notified (e.g., the assigned owner, followers of the account), creates notification records in NoSQL, and pushes real-time alerts to online users via the WebSocket Server.</li>
</ul>
</div>

<div class="card">
<h4>WebSocket Server</h4>
<ul>
  <li><strong>Protocol:</strong> WSS (WebSocket Secure) for client connections.</li>
  <li><strong>Purpose:</strong> Maintains persistent connections with logged-in users. Delivers real-time notifications. See <a href="#ws">WebSocket Deep Dive</a>.</li>
</ul>
</div>

<!-- ============================================================ -->
<h2 id="flow2">4. Flow 2 &mdash; Lead Conversion to Opportunity</h2>
<!-- ============================================================ -->
<p>When a sales rep qualifies a lead, they convert it into an Account, Contact, and Opportunity in one atomic action.</p>

<div class="mermaid">
graph LR
    REP["Sales Rep<br/>(Browser)"]
    LB["Load Balancer"]
    GW["API Gateway"]
    LS["Lead Service"]
    DB_SQL[("SQL Database")]
    MQ["Message Queue"]
    SI["Search Index<br/>Updater"]
    NS["Notification<br/>Service"]
    WE["Workflow<br/>Engine"]
    SEARCH[("Search Index")]

    REP -->|"HTTPS POST<br/>/api/v1/leads/{id}/convert"| LB
    LB --> GW
    GW -->|"gRPC ConvertLead()"| LS
    LS -->|"BEGIN TRANSACTION<br/>1. UPDATE lead SET status=converted<br/>2. INSERT account<br/>3. INSERT contact<br/>4. INSERT opportunity<br/>COMMIT"| DB_SQL
    LS -->|"Publish: lead.converted"| MQ
    MQ --> SI
    MQ --> NS
    MQ --> WE
    SI -->|"Index account, contact,<br/>opportunity docs"| SEARCH
    WE -->|"Evaluate conversion<br/>trigger rules"| DB_SQL
</div>

<h3>Examples</h3>

<div class="example">
<strong>Example 1 — Standard Conversion:</strong> Sales rep Bob opens lead "Jane Doe (BigCo)" with score 85 and clicks "Convert." The browser sends <code>HTTPS POST /api/v1/leads/{lead_id}/convert</code> with body <code>{account_name: "BigCo Inc.", opportunity_name: "BigCo — Enterprise License", opportunity_stage: "Qualification", opportunity_amount: 50000, close_date: "2026-06-30"}</code>. The Lead Service executes a SQL transaction: (1) updates the lead's status to <code>converted</code> and sets <code>converted_opportunity_id</code>, (2) inserts a new Account "BigCo Inc.", (3) inserts a new Contact "Jane Doe" linked to the account, (4) inserts a new Opportunity "BigCo — Enterprise License" at the "Qualification" stage linked to the account. On commit, a <code>lead.converted</code> event is published. The Search Index Updater indexes all three new records. The Workflow Engine checks for any automation rules triggered by lead conversion (e.g., "auto-create a follow-up task in 3 days") and executes them.
</div>

<div class="example">
<strong>Example 2 — Conversion with Existing Account:</strong> Bob converts lead "Tom Smith (BigCo)" but BigCo already exists as an account. The Lead Service detects the existing account (by matching <code>org_id</code> and <code>account_name</code>), skips account creation, and links the new Contact and Opportunity to the existing Account. The transaction only inserts a contact and opportunity, and updates the lead.
</div>

<h3>Component Deep Dive</h3>

<div class="card">
<h4>Lead Service — Conversion Endpoint</h4>
<ul>
  <li><strong>Protocol (External):</strong> HTTPS <code>POST /api/v1/leads/{id}/convert</code></li>
  <li><strong>Protocol (Internal):</strong> gRPC <code>ConvertLead(org_id, lead_id, conversion_data)</code></li>
  <li><strong>Input:</strong> <code>lead_id</code> (path param), conversion data JSON (account name, opportunity name/stage/amount/close_date, optional existing account_id).</li>
  <li><strong>Output:</strong> <code>200 OK</code> with JSON <code>{account_id, contact_id, opportunity_id}</code>.</li>
  <li><strong>Transaction:</strong> Uses a SQL transaction to ensure atomicity. If any step fails (e.g., unique constraint on contact email), the entire conversion is rolled back and a <code>409 Conflict</code> is returned.</li>
  <li><strong>Why POST, not PUT:</strong> Conversion is a non-idempotent action that creates new resources. POST is appropriate because it has side effects (creating account, contact, opportunity) and is not safely repeatable.</li>
</ul>
</div>

<div class="card">
<h4>Workflow Engine</h4>
<ul>
  <li><strong>Protocol:</strong> Consumes events from Message Queue via TCP; reads/writes SQL via TCP.</li>
  <li><strong>Purpose:</strong> Evaluates automation rules configured by the org admin. For a <code>lead.converted</code> event, it checks all active rules with trigger <code>object_type=lead, event=converted</code>. Actions can include: create a task, send an email, update a field, call a webhook.</li>
  <li><strong>Execution:</strong> Rules are evaluated synchronously within the worker. Actions are executed or queued depending on type (email sends are queued; field updates are inline).</li>
</ul>
</div>

<!-- ============================================================ -->
<h2 id="flow3">5. Flow 3 &mdash; Opportunity Pipeline Management</h2>
<!-- ============================================================ -->
<p>Sales reps move opportunities through pipeline stages as deals progress.</p>

<div class="mermaid">
graph LR
    REP["Sales Rep<br/>(Browser)"]
    LB["Load Balancer"]
    GW["API Gateway"]
    OS["Opportunity<br/>Service"]
    DB_SQL[("SQL Database")]
    CACHE[("In-Memory<br/>Cache")]
    MQ["Message Queue"]
    WE["Workflow<br/>Engine"]
    NS["Notification<br/>Service"]
    SI["Search Index<br/>Updater"]
    AUDIT[("NoSQL<br/>Audit Log")]

    REP -->|"HTTPS PATCH<br/>/api/v1/opportunities/{id}"| LB
    LB --> GW
    GW -->|"gRPC UpdateOpportunity()"| OS
    OS -->|"Read stage config"| CACHE
    OS -->|"UPDATE opportunity<br/>SET stage, probability"| DB_SQL
    OS -->|"INSERT audit record"| AUDIT
    OS -->|"Publish: opportunity.stage_changed"| MQ
    MQ --> WE
    MQ --> NS
    MQ --> SI
    WE -->|"Evaluate stage-change<br/>rules &amp; actions"| DB_SQL
</div>

<h3>Examples</h3>

<div class="example">
<strong>Example 1 — Stage Advancement:</strong> Bob moves opportunity "BigCo — Enterprise License" from "Qualification" to "Proposal" by selecting the new stage in the UI. The browser sends <code>HTTPS PATCH /api/v1/opportunities/{opp_id}</code> with body <code>{stage: "Proposal"}</code>. The Opportunity Service reads the org's stage configuration from the cache (stage "Proposal" has <code>probability: 50%</code>), updates the opportunity's stage and probability in SQL, writes an audit log entry to NoSQL (<code>{old_stage: "Qualification", new_stage: "Proposal", changed_by: "Bob", timestamp: "..."}</code>), and publishes an <code>opportunity.stage_changed</code> event. The Workflow Engine detects a rule: "When stage changes to Proposal, auto-create task: Send proposal document within 2 days" and creates the task. The Notification Service alerts the VP of Sales (who follows all enterprise deals) via WebSocket.
</div>

<div class="example">
<strong>Example 2 — Closed-Won:</strong> Bob moves the opportunity to "Closed-Won." The Workflow Engine fires a "closed-won" rule that: (1) sends a congratulatory email to the sales team, (2) creates an onboarding case for the customer success team, (3) triggers an outbound webhook to the billing system with deal details. The notification is sent to the entire account team.
</div>

<div class="example">
<strong>Example 3 — Closed-Lost:</strong> Bob marks the opportunity as "Closed-Lost" with a loss reason "Went with competitor." The Workflow Engine fires a rule that creates a follow-up task: "Schedule win-back call in 90 days." The audit log records the loss reason for reporting purposes.
</div>

<h3>Component Deep Dive</h3>

<div class="card">
<h4>Opportunity Service</h4>
<ul>
  <li><strong>Protocol (External):</strong> HTTPS</li>
  <li><strong>Endpoints:</strong>
    <ul>
      <li><code>POST /api/v1/opportunities</code> — Create a new opportunity. Input: opportunity JSON. Output: <code>201 Created</code> with opportunity JSON.</li>
      <li><code>GET /api/v1/opportunities/{id}</code> — Read a single opportunity. Output: <code>200 OK</code> with opportunity JSON including related account and contact info.</li>
      <li><code>PATCH /api/v1/opportunities/{id}</code> — Partial update (e.g., stage change). Input: JSON with fields to update. Output: <code>200 OK</code> with updated opportunity. <em>PATCH used instead of PUT because we are partially updating, not replacing the entire resource.</em></li>
      <li><code>GET /api/v1/opportunities?stage=Proposal&owner_id=bob&sort=close_date</code> — List with filters. Output: paginated list.</li>
      <li><code>DELETE /api/v1/opportunities/{id}</code> — Soft-delete. Output: <code>204 No Content</code>.</li>
    </ul>
  </li>
  <li><strong>Internal Protocol:</strong> gRPC <code>UpdateOpportunity(org_id, opp_id, fields)</code></li>
</ul>
</div>

<div class="card">
<h4>NoSQL Audit Log</h4>
<ul>
  <li><strong>Protocol:</strong> TCP (NoSQL wire protocol).</li>
  <li><strong>Purpose:</strong> Append-only log of all data mutations. NoSQL chosen because: (1) extremely high write volume (every CRM record change across all tenants), (2) simple access pattern — always partition by <code>org_id</code> and query by time range, (3) no joins needed, (4) horizontal scaling via partitioning.</li>
  <li><strong>Schema:</strong> Partition key = <code>org_id</code>, sort key = <code>timestamp</code>. Fields include <code>user_id</code>, <code>action</code>, <code>object_type</code>, <code>object_id</code>, <code>old_values_json</code>, <code>new_values_json</code>.</li>
</ul>
</div>

<!-- ============================================================ -->
<h2 id="flow4">6. Flow 4 &mdash; Global Search</h2>
<!-- ============================================================ -->
<p>Users search across all CRM entities within their org, scoped by their permissions.</p>

<div class="mermaid">
graph LR
    USER["User<br/>(Browser)"]
    LB["Load Balancer"]
    GW["API Gateway"]
    SS["Search Service"]
    SEARCH[("Search Index<br/>(Inverted Index)")]
    CACHE[("In-Memory Cache")]
    META["Metadata<br/>Service"]
    DB_SQL[("SQL Database")]

    USER -->|"HTTPS GET<br/>/api/v1/search?q=BigCo&type=all"| LB
    LB --> GW
    GW -->|"gRPC Search()"| SS
    SS -->|"Read user permissions"| CACHE
    SS -->|"Query inverted index<br/>with org_id filter"| SEARCH
    SEARCH -->|"Return matching<br/>doc IDs + snippets"| SS
    SS -->|"Filter by RBAC"| SS
    SS -->|"Hydrate records<br/>(optional)"| DB_SQL
    SS -->|"Return results"| USER
</div>

<h3>Examples</h3>

<div class="example">
<strong>Example 1 — Simple Global Search:</strong> User Alice types "BigCo" into the global search bar. The browser sends <code>HTTPS GET /api/v1/search?q=BigCo&type=all&limit=20</code>. The Search Service reads Alice's permissions from cache (she can see Accounts and Contacts but not Opportunities in the "Enterprise" division). It queries the search index with filters <code>org_id=acme AND query="BigCo"</code>, getting back matches: Account "BigCo Inc.", Contact "Jane Doe (BigCo)", Opportunity "BigCo — Enterprise License". The RBAC filter removes the Opportunity (Alice lacks access). The service returns the Account and Contact results with highlighted snippets.
</div>

<div class="example">
<strong>Example 2 — Filtered Search:</strong> Alice searches for contacts only: <code>GET /api/v1/search?q=Jane&type=contact&limit=10</code>. The Search Service adds an <code>entity_type=contact</code> filter to the index query, narrowing results to just contacts matching "Jane." Results come back faster because the index partition is smaller.
</div>

<div class="example">
<strong>Example 3 — No Results:</strong> Alice searches for "Acme Widget" but no records match in her org. The Search Service returns <code>200 OK</code> with an empty results array and a suggestion: "No results found. Try broadening your search or checking your spelling."
</div>

<h3>Component Deep Dive</h3>

<div class="card">
<h4>Search Service</h4>
<ul>
  <li><strong>Protocol (External):</strong> HTTPS <code>GET /api/v1/search?q={query}&type={entity_type}&limit={n}&offset={n}</code></li>
  <li><strong>Input:</strong> Query string, optional entity type filter, pagination params.</li>
  <li><strong>Output:</strong> <code>200 OK</code> with JSON array of search results: <code>[{entity_type, entity_id, title, snippet, score}]</code> plus pagination metadata.</li>
  <li><strong>Internal Protocol:</strong> gRPC <code>Search(org_id, query, filters, user_permissions)</code></li>
  <li><strong>RBAC enforcement:</strong> After getting results from the index, the service filters out any records the user cannot access based on their role, profile, and record-level sharing rules. This is applied in-memory because the search index does not store permission data (permissions change frequently and would require constant re-indexing).</li>
</ul>
</div>

<div class="card">
<h4>Search Index</h4>
<ul>
  <li><strong>Type:</strong> Inverted index (full-text search engine).</li>
  <li><strong>Protocol:</strong> HTTP REST for reads/writes.</li>
  <li><strong>Indexing:</strong> Documents are indexed per entity. Each document includes <code>org_id</code> (for tenant scoping), <code>entity_type</code>, <code>entity_id</code>, and searchable text fields. The inverted index maps tokens → document IDs for fast lookup.</li>
  <li><strong>Consistency:</strong> Eventually consistent with the SQL source of truth. Updates are pushed asynchronously via the Search Index Updater consuming from the Message Queue. Typical lag: &lt;2 seconds.</li>
  <li><strong>Sharding:</strong> Sharded by <code>org_id</code> to keep all of a tenant's search data on the same shard for efficient queries.</li>
</ul>
</div>

<div class="card">
<h4>Metadata Service</h4>
<ul>
  <li><strong>Protocol (Internal):</strong> gRPC.</li>
  <li><strong>Purpose:</strong> Serves custom object and field definitions for an org. When the Search Service needs to know what fields to search across (including custom fields), it queries the Metadata Service. Results are heavily cached since metadata changes infrequently.</li>
</ul>
</div>

<!-- ============================================================ -->
<h2 id="flow5">7. Flow 5 &mdash; Reporting &amp; Dashboards</h2>
<!-- ============================================================ -->
<p>Users build reports that aggregate data across CRM entities and view dashboards composed of multiple reports.</p>

<div class="mermaid">
graph LR
    USER["User<br/>(Browser)"]
    LB["Load Balancer"]
    GW["API Gateway"]
    RS["Reporting<br/>Service"]
    CACHE[("In-Memory<br/>Cache")]
    REPLICA[("SQL Read<br/>Replica")]
    MQ["Message Queue"]
    RW["Report<br/>Worker"]
    OBJ[("Object Storage<br/>(exported files)")]

    USER -->|"HTTPS POST<br/>/api/v1/reports/{id}/execute"| LB
    LB --> GW
    GW -->|"gRPC ExecuteReport()"| RS
    RS -->|"Check cache for<br/>recent results"| CACHE
    RS -->|"Cache MISS:<br/>Query with aggregations"| REPLICA
    REPLICA -->|"Aggregated results"| RS
    RS -->|"Store in cache"| CACHE
    RS -->|"Return report data"| USER

    USER -->|"HTTPS POST<br/>/api/v1/reports/{id}/export"| LB
    LB --> GW
    GW -->|"Queue export job"| MQ
    MQ --> RW
    RW -->|"Query data"| REPLICA
    RW -->|"Write CSV/PDF"| OBJ
    RW -->|"Notify user"| GW
</div>

<h3>Examples</h3>

<div class="example">
<strong>Example 1 — Pipeline Report:</strong> VP of Sales requests a report: "Opportunities by stage, filtered to Q2 close dates, grouped by owner." She clicks "Run Report" and the browser sends <code>HTTPS POST /api/v1/reports/{report_id}/execute</code>. The Reporting Service checks the cache for this report definition + filter combination. Cache miss. It queries the SQL Read Replica: <code>SELECT owner_name, stage, COUNT(*), SUM(amount) FROM opportunities WHERE org_id='acme' AND close_date BETWEEN '2026-04-01' AND '2026-06-30' GROUP BY owner_name, stage</code>. The aggregated results are cached for 5 minutes (in case she refreshes or another user runs the same report) and returned as a JSON payload rendered into a table and chart in the UI.
</div>

<div class="example">
<strong>Example 2 — Dashboard View:</strong> The VP opens her "Sales Dashboard" which has 4 widgets: Pipeline by Stage (bar chart), Revenue Forecast (line chart), Top Deals (table), Win/Loss Ratio (pie chart). The browser sends <code>HTTPS GET /api/v1/dashboards/{dashboard_id}</code>. The Reporting Service executes all 4 underlying reports in parallel. Three are cached (cache hit); one requires a fresh query. The dashboard is returned within 400ms.
</div>

<div class="example">
<strong>Example 3 — Scheduled Export:</strong> The VP schedules a weekly CSV export of the pipeline report. Every Monday at 8 AM, a cron-based scheduler publishes a <code>report.export_scheduled</code> event to the Message Queue. The Report Worker executes the query against the read replica, generates a CSV file, uploads it to Object Storage, and sends an email notification with a download link (a pre-signed URL pointing to the Object Storage via CDN).
</div>

<h3>Component Deep Dive</h3>

<div class="card">
<h4>Reporting Service</h4>
<ul>
  <li><strong>Protocol (External):</strong> HTTPS
    <ul>
      <li><code>POST /api/v1/reports</code> — Create report definition. Input: report config JSON (object type, filters, groupings, aggregations). Output: <code>201 Created</code>.</li>
      <li><code>POST /api/v1/reports/{id}/execute</code> — Execute report. POST (not GET) because the report execution may be complex with body parameters for runtime filters. Output: <code>200 OK</code> with results JSON.</li>
      <li><code>GET /api/v1/dashboards/{id}</code> — Get dashboard with all widget data. Output: <code>200 OK</code> with dashboard layout + report results.</li>
      <li><code>POST /api/v1/reports/{id}/export</code> — Trigger async export. Output: <code>202 Accepted</code> with job ID.</li>
    </ul>
  </li>
  <li><strong>Query construction:</strong> The service translates the report definition into SQL, respecting RBAC (adding WHERE clauses for record visibility). For custom fields, it joins the EAV table.</li>
</ul>
</div>

<div class="card">
<h4>SQL Read Replica</h4>
<ul>
  <li><strong>Purpose:</strong> Dedicated read replicas for reporting queries to avoid impacting transactional OLTP workloads. Reports can involve heavy aggregations (SUM, COUNT, GROUP BY) across thousands of records, which would degrade write performance if run on the primary.</li>
  <li><strong>Consistency:</strong> Slightly behind the primary (replication lag typically &lt;1 second). Acceptable for reports since users expect near-real-time, not real-time data in analytics.</li>
</ul>
</div>

<div class="card">
<h4>Object Storage</h4>
<ul>
  <li><strong>Purpose:</strong> Stores exported report files (CSV, PDF) and CRM file attachments. Highly durable, cost-effective for large binary objects.</li>
  <li><strong>Access:</strong> Pre-signed URLs generated by the service for secure, time-limited downloads. Served via CDN for performance.</li>
</ul>
</div>

<!-- ============================================================ -->
<h2 id="flow6">8. Flow 6 &mdash; Real-Time Notifications</h2>
<!-- ============================================================ -->
<p>Users receive real-time notifications when records they care about change.</p>

<div class="mermaid">
graph LR
    MQ["Message Queue<br/>(record change events)"]
    NS["Notification<br/>Service"]
    SUB_DB[("NoSQL<br/>Subscriptions")]
    NOTIF_DB[("NoSQL<br/>Notifications")]
    WSM["WebSocket<br/>Manager"]
    CONN[("Connection<br/>Store")]
    WSS1["WebSocket<br/>Server 1"]
    WSS2["WebSocket<br/>Server 2"]
    USER1["User (Online)<br/>Browser"]
    USER2["User (Offline)"]

    MQ -->|"Consume:<br/>opportunity.stage_changed"| NS
    NS -->|"Look up subscribers<br/>for this record"| SUB_DB
    NS -->|"Store notification<br/>for each subscriber"| NOTIF_DB
    NS -->|"Find online<br/>subscribers"| WSM
    WSM -->|"Look up connections"| CONN
    WSM -->|"Route to correct<br/>WebSocket server"| WSS1
    WSM -->|"Route to correct<br/>WebSocket server"| WSS2
    WSS1 -->|"Push notification"| USER1

    USER2 -->|"Later: HTTPS GET<br/>/api/v1/notifications"| NS
    NS -->|"Read from store"| NOTIF_DB
    NS -->|"Return unread<br/>notifications"| USER2
</div>

<h3>Examples</h3>

<div class="example">
<strong>Example 1 — Online User Gets Real-Time Push:</strong> Bob (online) is following opportunity "BigCo — Enterprise License." When Alice changes its stage from "Proposal" to "Negotiation," the <code>opportunity.stage_changed</code> event hits the Message Queue. The Notification Service consumes it, queries the Subscriptions NoSQL store, and finds Bob is a subscriber. It creates a notification record in the Notifications NoSQL store: <code>{user_id: "Bob", type: "stage_change", message: "Alice moved BigCo — Enterprise License to Negotiation", record_id: opp_123, is_read: false, timestamp: ...}</code>. It then queries the WebSocket Manager: "Is Bob online?" The Connection Store confirms Bob has an active WebSocket on Server 1. The WebSocket Manager routes the notification to Server 1, which pushes it to Bob's browser. Bob sees a toast notification instantly.
</div>

<div class="example">
<strong>Example 2 — Offline User Fetches Later:</strong> Charlie (offline) also follows the opportunity but was away. When he opens the CRM app the next morning, the browser re-establishes a WebSocket and fetches unread notifications via <code>HTTPS GET /api/v1/notifications?is_read=false</code>. The Notification Service queries the NoSQL store for Charlie's unread notifications and returns them. Charlie sees a badge "3 new notifications" and clicks to view.
</div>

<div class="example">
<strong>Example 3 — @Mention Notification:</strong> Alice adds a note on the opportunity: "Great progress! @Charlie please prepare the contract." The Activity Service publishes an <code>activity.mention</code> event to the Message Queue. The Notification Service creates a high-priority notification for Charlie. If Charlie is online, it's pushed immediately; if offline, it's stored and marked as high-priority so it appears at the top of his notification feed.
</div>

<h3>Component Deep Dive</h3>

<div class="card">
<h4>Notification Service</h4>
<ul>
  <li><strong>Protocol (External):</strong> HTTPS
    <ul>
      <li><code>GET /api/v1/notifications?is_read=false&limit=20</code> — Fetch notifications. Output: paginated list of notification objects.</li>
      <li><code>PATCH /api/v1/notifications/{id}</code> — Mark as read. Input: <code>{is_read: true}</code>. Output: <code>200 OK</code>.</li>
      <li><code>PUT /api/v1/notifications/read-all</code> — Mark all as read. Output: <code>200 OK</code>. PUT because it's idempotent — calling it twice has the same effect.</li>
    </ul>
  </li>
  <li><strong>Internal Protocol:</strong> Consumes from Message Queue; communicates with WebSocket Manager via gRPC.</li>
  <li><strong>Fan-out:</strong> For a single record change event, the service may fan out to 1–50 subscribers. It batches notifications for write efficiency to NoSQL.</li>
</ul>
</div>

<div class="card">
<h4>WebSocket Manager</h4>
<ul>
  <li><strong>Purpose:</strong> Tracks which users are connected to which WebSocket server instance. When a notification needs to be pushed, it looks up the user's connection in the Connection Store and routes the message to the correct server.</li>
  <li><strong>See <a href="#ws">WebSocket Deep Dive</a> for full connection lifecycle details.</strong></li>
</ul>
</div>

<div class="card">
<h4>Connection Store</h4>
<ul>
  <li><strong>Type:</strong> In-memory key-value cache with TTL.</li>
  <li><strong>Schema:</strong> Key = <code>user_id</code>, Value = <code>{server_id, connection_id, connected_at}</code>.</li>
  <li><strong>TTL:</strong> Entries expire if not refreshed by heartbeat (60 seconds). This ensures stale connections from crashed servers are auto-cleaned.</li>
</ul>
</div>

<!-- ============================================================ -->
<h2 id="combined">9. Combined Overall Architecture Diagram</h2>
<!-- ============================================================ -->
<p>This diagram unifies all flows into one view showing how the components interact across the system.</p>

<div class="mermaid">
graph TB
    subgraph Clients
        WEB["Web App<br/>(Browser)"]
        MOB["Mobile App"]
        EXT["External<br/>API Client"]
    end

    subgraph Edge Layer
        CDN["CDN<br/>(Static Assets,<br/>File Downloads)"]
        LB["Load Balancer"]
    end

    subgraph Gateway Layer
        GW["API Gateway<br/>(Auth, Rate Limit,<br/>Routing)"]
    end

    subgraph Application Services
        LS["Lead<br/>Service"]
        OS["Opportunity<br/>Service"]
        ACS["Account/Contact<br/>Service"]
        CS["Case<br/>Service"]
        AS["Activity<br/>Service"]
        SS["Search<br/>Service"]
        RS["Reporting<br/>Service"]
        NS["Notification<br/>Service"]
        FS["File<br/>Service"]
        META["Metadata<br/>Service"]
    end

    subgraph Async Workers
        LSS["Lead Scoring<br/>Worker"]
        WE["Workflow<br/>Engine"]
        SIU["Search Index<br/>Updater"]
        RW["Report<br/>Worker"]
    end

    subgraph Real-Time Layer
        WSM["WebSocket<br/>Manager"]
        WSS["WebSocket<br/>Servers"]
    end

    subgraph Message Layer
        MQ["Message Queue"]
    end

    subgraph Data Layer
        DB_PRIMARY[("SQL Primary<br/>(Accounts, Contacts,<br/>Leads, Opportunities,<br/>Cases, Activities,<br/>Custom Fields)")]
        DB_REPLICA[("SQL Read<br/>Replicas")]
        NOSQL[("NoSQL<br/>(Notifications,<br/>Audit Log,<br/>Subscriptions)")]
        SEARCH_IDX[("Search Index<br/>(Inverted Index)")]
        OBJ[("Object Storage<br/>(Files, Exports)")]
    end

    subgraph Cache Layer
        CACHE[("In-Memory Cache<br/>(Sessions, Metadata,<br/>Hot Records,<br/>Report Results)")]
    end

    WEB --> CDN
    WEB --> LB
    MOB --> LB
    EXT --> LB
    WEB -.->|"WSS"| WSS
    MOB -.->|"WSS"| WSS
    LB --> GW
    GW --> LS
    GW --> OS
    GW --> ACS
    GW --> CS
    GW --> AS
    GW --> SS
    GW --> RS
    GW --> NS
    GW --> FS

    LS --> DB_PRIMARY
    OS --> DB_PRIMARY
    ACS --> DB_PRIMARY
    CS --> DB_PRIMARY
    AS --> DB_PRIMARY
    META --> DB_PRIMARY
    FS --> OBJ

    LS --> MQ
    OS --> MQ
    ACS --> MQ
    CS --> MQ
    AS --> MQ

    MQ --> LSS
    MQ --> WE
    MQ --> SIU
    MQ --> NS
    MQ --> RW

    LSS --> DB_PRIMARY
    WE --> DB_PRIMARY
    SIU --> SEARCH_IDX
    RW --> DB_REPLICA
    RW --> OBJ

    SS --> SEARCH_IDX
    SS --> CACHE
    RS --> CACHE
    RS --> DB_REPLICA
    META --> CACHE
    NS --> NOSQL
    NS --> WSM
    WSM --> WSS

    OS --> CACHE
    LS --> CACHE
    ACS --> CACHE

    DB_PRIMARY --> DB_REPLICA
</div>

<h3>Examples (End-to-End)</h3>

<div class="example">
<strong>End-to-End Example — Full Sales Cycle:</strong><br/><br/>
<strong>Step 1 (Lead Capture):</strong> A prospect fills out a web form on Acme Corp's site. An <code>HTTPS POST /api/v1/leads</code> goes through the Load Balancer → API Gateway → Lead Service → SQL Primary. The Lead Scoring Worker scores the lead at 90. The Notification Service alerts sales rep Bob via WebSocket. The Search Index Updater indexes the new lead.<br/><br/>
<strong>Step 2 (Lead Conversion):</strong> Bob reviews the lead and clicks "Convert." <code>HTTPS POST /api/v1/leads/{id}/convert</code> flows through to the Lead Service, which atomically creates an Account, Contact, and Opportunity in SQL. Events propagate through the Message Queue to index all new records and trigger a workflow rule that creates a follow-up task.<br/><br/>
<strong>Step 3 (Pipeline Management):</strong> Over the next 3 weeks, Bob moves the opportunity through Qualification → Proposal → Negotiation using <code>HTTPS PATCH /api/v1/opportunities/{id}</code>. Each stage change is audited in NoSQL, indexed in the Search Index, and triggers notifications to the account team.<br/><br/>
<strong>Step 4 (Reporting):</strong> The VP of Sales runs a pipeline report via <code>POST /api/v1/reports/{id}/execute</code>. The Reporting Service queries the SQL Read Replica for aggregated data and returns the report. The VP also exports it as CSV; the Report Worker generates the file to Object Storage, and the VP downloads it via a CDN-served pre-signed URL.<br/><br/>
<strong>Step 5 (Closed-Won):</strong> Bob closes the deal. <code>PATCH /api/v1/opportunities/{id}</code> with <code>{stage: "Closed-Won"}</code>. The Workflow Engine triggers: send congratulations email, create onboarding case, webhook to billing. The entire team is notified in real time. The audit trail captures every change from lead capture to close.
</div>

<!-- ============================================================ -->
<h2 id="schema">10. Database Schema</h2>
<!-- ============================================================ -->

<h3>10.1 SQL Tables</h3>
<p>SQL is chosen for the core CRM entities because they are highly relational (accounts have contacts, contacts have opportunities, etc.), require ACID transactions (e.g., atomic lead conversion), and need complex queries for reporting (JOINs, GROUP BY, aggregations).</p>

<div class="info"><strong>Multi-Tenancy Strategy:</strong> All SQL tables include an <code>org_id</code> column and are <strong>sharded by <code>org_id</code></strong>. This ensures all data for a single tenant is co-located on the same shard, allowing efficient joins and queries within a tenant without cross-shard operations. The sharding key is a hash of <code>org_id</code> distributed across N shards. Very large tenants (enterprise customers with millions of records) can be assigned dedicated shards.</div>

<h4>Table: <code>tenants</code></h4>
<table>
<tr><th>Column</th><th>Type</th><th>Key</th><th>Notes</th></tr>
<tr><td>org_id</td><td>UUID</td><td>PK</td><td>Unique tenant identifier</td></tr>
<tr><td>name</td><td>VARCHAR(255)</td><td></td><td>Organization name</td></tr>
<tr><td>plan</td><td>ENUM</td><td></td><td>Subscription tier (free, professional, enterprise)</td></tr>
<tr><td>settings_json</td><td>JSONB</td><td></td><td>Org-level configurations (timezone, locale, etc.)</td></tr>
<tr><td>created_at</td><td>TIMESTAMP</td><td></td><td></td></tr>
<tr><td>updated_at</td><td>TIMESTAMP</td><td></td><td></td></tr>
</table>
<ul>
  <li><strong>Read:</strong> On user login (to load org settings), on API requests (to validate org exists and check plan limits).</li>
  <li><strong>Write:</strong> On org creation (signup), admin settings changes.</li>
  <li><strong>Index:</strong> PK on <code>org_id</code> (hash index for O(1) lookups).</li>
  <li><strong>Sharding:</strong> Not sharded — small table, replicated across all nodes.</li>
</ul>

<h4>Table: <code>users</code></h4>
<table>
<tr><th>Column</th><th>Type</th><th>Key</th><th>Notes</th></tr>
<tr><td>user_id</td><td>UUID</td><td>PK</td><td>Unique user identifier</td></tr>
<tr><td>org_id</td><td>UUID</td><td>FK → tenants</td><td>Tenant the user belongs to</td></tr>
<tr><td>email</td><td>VARCHAR(255)</td><td></td><td>Login email</td></tr>
<tr><td>name</td><td>VARCHAR(255)</td><td></td><td>Display name</td></tr>
<tr><td>role</td><td>VARCHAR(50)</td><td></td><td>e.g., admin, sales_rep, sales_manager</td></tr>
<tr><td>password_hash</td><td>VARCHAR(255)</td><td></td><td>Bcrypt hash</td></tr>
<tr><td>is_active</td><td>BOOLEAN</td><td></td><td>Soft-delete / deactivation flag</td></tr>
<tr><td>last_login</td><td>TIMESTAMP</td><td></td><td></td></tr>
<tr><td>created_at</td><td>TIMESTAMP</td><td></td><td></td></tr>
</table>
<ul>
  <li><strong>Read:</strong> On login (authenticate), on every API request (load permissions from cache or DB), on record assignment lookups.</li>
  <li><strong>Write:</strong> On user creation, profile updates, login (update <code>last_login</code>).</li>
  <li><strong>Indexes:</strong>
    <ul>
      <li>B-tree index on <code>(org_id, email)</code> — used for login (find user by email within org). B-tree chosen because emails are unique and we need equality lookups. Composite index ensures queries are scoped to a tenant.</li>
      <li>B-tree index on <code>(org_id, role)</code> — used for listing users by role (e.g., "all sales reps in this org").</li>
    </ul>
  </li>
  <li><strong>Sharding:</strong> Sharded by <code>org_id</code>.</li>
</ul>

<h4>Table: <code>accounts</code></h4>
<table>
<tr><th>Column</th><th>Type</th><th>Key</th><th>Notes</th></tr>
<tr><td>account_id</td><td>UUID</td><td>PK</td><td></td></tr>
<tr><td>org_id</td><td>UUID</td><td>FK → tenants</td><td></td></tr>
<tr><td>name</td><td>VARCHAR(255)</td><td></td><td>Company name</td></tr>
<tr><td>industry</td><td>VARCHAR(100)</td><td></td><td></td></tr>
<tr><td>website</td><td>VARCHAR(500)</td><td></td><td></td></tr>
<tr><td>phone</td><td>VARCHAR(20)</td><td></td><td></td></tr>
<tr><td>address_json</td><td>JSONB</td><td></td><td>Structured address</td></tr>
<tr><td>annual_revenue</td><td>DECIMAL(15,2)</td><td></td><td></td></tr>
<tr><td>employee_count</td><td>INTEGER</td><td></td><td></td></tr>
<tr><td>owner_id</td><td>UUID</td><td>FK → users</td><td>Assigned account owner</td></tr>
<tr><td>is_deleted</td><td>BOOLEAN</td><td></td><td>Soft-delete</td></tr>
<tr><td>created_at</td><td>TIMESTAMP</td><td></td><td></td></tr>
<tr><td>updated_at</td><td>TIMESTAMP</td><td></td><td></td></tr>
</table>
<ul>
  <li><strong>Read:</strong> When viewing an account detail page, when listing accounts, when searching, when viewing related contacts/opportunities, during reporting.</li>
  <li><strong>Write:</strong> On account creation (manual or via lead conversion), on updates (user edits fields), on soft-delete.</li>
  <li><strong>Indexes:</strong>
    <ul>
      <li>B-tree on <code>(org_id, name)</code> — accounts are frequently looked up by name within an org (e.g., duplicate detection during lead conversion).</li>
      <li>B-tree on <code>(org_id, owner_id)</code> — "show me all my accounts" is a very common query for sales reps.</li>
      <li>B-tree on <code>(org_id, industry)</code> — used for filtering accounts by industry in list views and reports.</li>
    </ul>
  </li>
  <li><strong>Sharding:</strong> Sharded by <code>org_id</code>.</li>
</ul>

<h4>Table: <code>contacts</code></h4>
<table>
<tr><th>Column</th><th>Type</th><th>Key</th><th>Notes</th></tr>
<tr><td>contact_id</td><td>UUID</td><td>PK</td><td></td></tr>
<tr><td>org_id</td><td>UUID</td><td>FK → tenants</td><td></td></tr>
<tr><td>account_id</td><td>UUID</td><td>FK → accounts</td><td>Parent account (many contacts per account)</td></tr>
<tr><td>first_name</td><td>VARCHAR(100)</td><td></td><td></td></tr>
<tr><td>last_name</td><td>VARCHAR(100)</td><td></td><td></td></tr>
<tr><td>email</td><td>VARCHAR(255)</td><td></td><td></td></tr>
<tr><td>phone</td><td>VARCHAR(20)</td><td></td><td></td></tr>
<tr><td>title</td><td>VARCHAR(100)</td><td></td><td>Job title</td></tr>
<tr><td>owner_id</td><td>UUID</td><td>FK → users</td><td></td></tr>
<tr><td>is_deleted</td><td>BOOLEAN</td><td></td><td></td></tr>
<tr><td>created_at</td><td>TIMESTAMP</td><td></td><td></td></tr>
<tr><td>updated_at</td><td>TIMESTAMP</td><td></td><td></td></tr>
</table>
<ul>
  <li><strong>Read:</strong> When viewing contact detail, listing contacts under an account, search, reporting.</li>
  <li><strong>Write:</strong> On manual creation, lead conversion, updates.</li>
  <li><strong>Indexes:</strong>
    <ul>
      <li>B-tree on <code>(org_id, account_id)</code> — "show all contacts for this account" is the most common contact query.</li>
      <li>B-tree on <code>(org_id, email)</code> — duplicate detection and email-based lookups.</li>
      <li>B-tree on <code>(org_id, owner_id)</code> — "show my contacts."</li>
    </ul>
  </li>
  <li><strong>Sharding:</strong> Sharded by <code>org_id</code>.</li>
</ul>

<h4>Table: <code>leads</code></h4>
<table>
<tr><th>Column</th><th>Type</th><th>Key</th><th>Notes</th></tr>
<tr><td>lead_id</td><td>UUID</td><td>PK</td><td></td></tr>
<tr><td>org_id</td><td>UUID</td><td>FK → tenants</td><td></td></tr>
<tr><td>first_name</td><td>VARCHAR(100)</td><td></td><td></td></tr>
<tr><td>last_name</td><td>VARCHAR(100)</td><td></td><td></td></tr>
<tr><td>email</td><td>VARCHAR(255)</td><td></td><td></td></tr>
<tr><td>phone</td><td>VARCHAR(20)</td><td></td><td></td></tr>
<tr><td>company</td><td>VARCHAR(255)</td><td></td><td></td></tr>
<tr><td>title</td><td>VARCHAR(100)</td><td></td><td></td></tr>
<tr><td>source</td><td>VARCHAR(50)</td><td></td><td>web_form, api, import, manual</td></tr>
<tr><td>status</td><td>VARCHAR(30)</td><td></td><td>new, contacted, qualified, converted, disqualified</td></tr>
<tr><td>score</td><td>INTEGER</td><td></td><td>0–100, set by scoring worker</td></tr>
<tr><td>owner_id</td><td>UUID</td><td>FK → users</td><td>Assigned sales rep</td></tr>
<tr><td>converted_account_id</td><td>UUID</td><td>FK → accounts, NULLABLE</td><td>Set on conversion</td></tr>
<tr><td>converted_contact_id</td><td>UUID</td><td>FK → contacts, NULLABLE</td><td>Set on conversion</td></tr>
<tr><td>converted_opportunity_id</td><td>UUID</td><td>FK → opportunities, NULLABLE</td><td>Set on conversion</td></tr>
<tr><td>is_deleted</td><td>BOOLEAN</td><td></td><td></td></tr>
<tr><td>created_at</td><td>TIMESTAMP</td><td></td><td></td></tr>
<tr><td>updated_at</td><td>TIMESTAMP</td><td></td><td></td></tr>
</table>
<ul>
  <li><strong>Read:</strong> When viewing lead detail, list views (filtered by status, score, owner), reporting, duplicate detection.</li>
  <li><strong>Write:</strong> On lead creation (web form, import), scoring updates, status changes, conversion.</li>
  <li><strong>Indexes:</strong>
    <ul>
      <li>B-tree on <code>(org_id, status, score DESC)</code> — the primary lead list view is "show new leads sorted by score." The composite index supports this query efficiently.</li>
      <li>B-tree on <code>(org_id, email)</code> — duplicate detection.</li>
      <li>B-tree on <code>(org_id, owner_id, status)</code> — "show my open leads."</li>
    </ul>
  </li>
  <li><strong>Sharding:</strong> Sharded by <code>org_id</code>.</li>
</ul>

<h4>Table: <code>opportunities</code></h4>
<table>
<tr><th>Column</th><th>Type</th><th>Key</th><th>Notes</th></tr>
<tr><td>opportunity_id</td><td>UUID</td><td>PK</td><td></td></tr>
<tr><td>org_id</td><td>UUID</td><td>FK → tenants</td><td></td></tr>
<tr><td>account_id</td><td>UUID</td><td>FK → accounts</td><td></td></tr>
<tr><td>name</td><td>VARCHAR(255)</td><td></td><td>Deal name</td></tr>
<tr><td>stage</td><td>VARCHAR(50)</td><td></td><td>Pipeline stage</td></tr>
<tr><td>amount</td><td>DECIMAL(15,2)</td><td></td><td>Deal value</td></tr>
<tr><td>probability</td><td>INTEGER</td><td></td><td>0–100%, auto-set based on stage config</td></tr>
<tr><td>close_date</td><td>DATE</td><td></td><td>Expected close date</td></tr>
<tr><td>owner_id</td><td>UUID</td><td>FK → users</td><td></td></tr>
<tr><td>loss_reason</td><td>VARCHAR(255)</td><td></td><td>Set when Closed-Lost</td></tr>
<tr><td>is_deleted</td><td>BOOLEAN</td><td></td><td></td></tr>
<tr><td>created_at</td><td>TIMESTAMP</td><td></td><td></td></tr>
<tr><td>updated_at</td><td>TIMESTAMP</td><td></td><td></td></tr>
</table>
<ul>
  <li><strong>Read:</strong> Pipeline views (grouped by stage), opportunity detail, account-level opportunity list, reporting (revenue forecasting, win/loss analysis).</li>
  <li><strong>Write:</strong> On creation (manual or lead conversion), stage changes, field updates, close.</li>
  <li><strong>Indexes:</strong>
    <ul>
      <li>B-tree on <code>(org_id, stage, close_date)</code> — the pipeline view groups by stage and sorts by close date. This composite index supports the most critical sales query.</li>
      <li>B-tree on <code>(org_id, account_id)</code> — "show all opportunities for this account."</li>
      <li>B-tree on <code>(org_id, owner_id, stage)</code> — "show my open opportunities."</li>
      <li>B-tree on <code>(org_id, close_date)</code> — date-range queries for reports (e.g., "Q2 pipeline").</li>
    </ul>
  </li>
  <li><strong>Sharding:</strong> Sharded by <code>org_id</code>.</li>
</ul>

<h4>Table: <code>cases</code></h4>
<table>
<tr><th>Column</th><th>Type</th><th>Key</th><th>Notes</th></tr>
<tr><td>case_id</td><td>UUID</td><td>PK</td><td></td></tr>
<tr><td>org_id</td><td>UUID</td><td>FK → tenants</td><td></td></tr>
<tr><td>contact_id</td><td>UUID</td><td>FK → contacts</td><td>Customer who raised the case</td></tr>
<tr><td>account_id</td><td>UUID</td><td>FK → accounts</td><td></td></tr>
<tr><td>subject</td><td>VARCHAR(500)</td><td></td><td></td></tr>
<tr><td>description</td><td>TEXT</td><td></td><td></td></tr>
<tr><td>status</td><td>VARCHAR(30)</td><td></td><td>new, in_progress, escalated, resolved, closed</td></tr>
<tr><td>priority</td><td>VARCHAR(10)</td><td></td><td>low, medium, high, critical</td></tr>
<tr><td>owner_id</td><td>UUID</td><td>FK → users</td><td>Assigned support agent</td></tr>
<tr><td>is_deleted</td><td>BOOLEAN</td><td></td><td></td></tr>
<tr><td>created_at</td><td>TIMESTAMP</td><td></td><td></td></tr>
<tr><td>updated_at</td><td>TIMESTAMP</td><td></td><td></td></tr>
</table>
<ul>
  <li><strong>Read:</strong> Case detail, case list (filtered by status/priority/owner), account-level case view, reporting.</li>
  <li><strong>Write:</strong> Case creation, status changes, reassignment.</li>
  <li><strong>Indexes:</strong>
    <ul>
      <li>B-tree on <code>(org_id, status, priority)</code> — support dashboards show cases grouped by status and priority.</li>
      <li>B-tree on <code>(org_id, account_id)</code> — "show all cases for this account."</li>
      <li>B-tree on <code>(org_id, owner_id, status)</code> — "show my open cases."</li>
    </ul>
  </li>
  <li><strong>Sharding:</strong> Sharded by <code>org_id</code>.</li>
</ul>

<h4>Table: <code>activities</code></h4>
<table>
<tr><th>Column</th><th>Type</th><th>Key</th><th>Notes</th></tr>
<tr><td>activity_id</td><td>UUID</td><td>PK</td><td></td></tr>
<tr><td>org_id</td><td>UUID</td><td>FK → tenants</td><td></td></tr>
<tr><td>type</td><td>VARCHAR(20)</td><td></td><td>call, email, meeting, task, note</td></tr>
<tr><td>subject</td><td>VARCHAR(500)</td><td></td><td></td></tr>
<tr><td>description</td><td>TEXT</td><td></td><td></td></tr>
<tr><td>related_to_type</td><td>VARCHAR(30)</td><td></td><td>account, contact, lead, opportunity, case (polymorphic)</td></tr>
<tr><td>related_to_id</td><td>UUID</td><td></td><td>ID of the related record</td></tr>
<tr><td>owner_id</td><td>UUID</td><td>FK → users</td><td></td></tr>
<tr><td>due_date</td><td>TIMESTAMP</td><td></td><td>For tasks</td></tr>
<tr><td>completed_at</td><td>TIMESTAMP</td><td></td><td>Null if not yet completed</td></tr>
<tr><td>created_at</td><td>TIMESTAMP</td><td></td><td></td></tr>
</table>
<ul>
  <li><strong>Read:</strong> On record detail pages (show activity timeline for an account/contact/etc.), "my tasks" view, reporting.</li>
  <li><strong>Write:</strong> When a user logs a call, sends an email, creates a task, or adds a note.</li>
  <li><strong>Indexes:</strong>
    <ul>
      <li>B-tree on <code>(org_id, related_to_type, related_to_id, created_at DESC)</code> — the activity timeline for a record is the most common query. The composite index allows efficient retrieval of "all activities for opportunity X, newest first." B-tree chosen for range scan on <code>created_at</code>.</li>
      <li>B-tree on <code>(org_id, owner_id, type, completed_at)</code> — "show my incomplete tasks."</li>
    </ul>
  </li>
  <li><strong>Polymorphic association note:</strong> <code>related_to_type</code> + <code>related_to_id</code> is used instead of separate foreign keys to each entity table. This is a denormalization trade-off — it avoids N nullable FK columns (one per entity type) at the cost of no FK constraint enforcement. Application-level validation ensures referential integrity.</li>
  <li><strong>Sharding:</strong> Sharded by <code>org_id</code>.</li>
</ul>

<h4>Table: <code>workflow_rules</code></h4>
<table>
<tr><th>Column</th><th>Type</th><th>Key</th><th>Notes</th></tr>
<tr><td>rule_id</td><td>UUID</td><td>PK</td><td></td></tr>
<tr><td>org_id</td><td>UUID</td><td>FK → tenants</td><td></td></tr>
<tr><td>name</td><td>VARCHAR(255)</td><td></td><td>Human-readable rule name</td></tr>
<tr><td>object_type</td><td>VARCHAR(30)</td><td></td><td>Entity this rule applies to</td></tr>
<tr><td>trigger_event</td><td>VARCHAR(30)</td><td></td><td>created, updated, deleted, field_changed</td></tr>
<tr><td>conditions_json</td><td>JSONB</td><td></td><td>Array of conditions (e.g., [{field: "stage", op: "eq", value: "Closed-Won"}])</td></tr>
<tr><td>actions_json</td><td>JSONB</td><td></td><td>Array of actions (e.g., [{type: "create_task", config: {...}}])</td></tr>
<tr><td>is_active</td><td>BOOLEAN</td><td></td><td></td></tr>
<tr><td>created_at</td><td>TIMESTAMP</td><td></td><td></td></tr>
<tr><td>updated_at</td><td>TIMESTAMP</td><td></td><td></td></tr>
</table>
<ul>
  <li><strong>Read:</strong> By the Workflow Engine on every record change event (to evaluate rules). Heavily cached to avoid DB hits on every event.</li>
  <li><strong>Write:</strong> When an admin creates/edits/deletes an automation rule. Infrequent writes (configuration data).</li>
  <li><strong>Indexes:</strong> B-tree on <code>(org_id, object_type, trigger_event, is_active)</code> — the Workflow Engine queries "all active rules for this object type and event." This 4-column index makes the lookup very efficient.</li>
  <li><strong>Sharding:</strong> Sharded by <code>org_id</code>.</li>
</ul>

<h4>Table: <code>custom_fields</code> (Metadata)</h4>
<table>
<tr><th>Column</th><th>Type</th><th>Key</th><th>Notes</th></tr>
<tr><td>field_id</td><td>UUID</td><td>PK</td><td></td></tr>
<tr><td>org_id</td><td>UUID</td><td>FK → tenants</td><td></td></tr>
<tr><td>object_type</td><td>VARCHAR(30)</td><td></td><td>Which entity this field extends</td></tr>
<tr><td>field_name</td><td>VARCHAR(100)</td><td></td><td>API name of the field</td></tr>
<tr><td>field_label</td><td>VARCHAR(255)</td><td></td><td>Display label</td></tr>
<tr><td>field_type</td><td>VARCHAR(20)</td><td></td><td>text, number, date, boolean, picklist, reference</td></tr>
<tr><td>is_required</td><td>BOOLEAN</td><td></td><td></td></tr>
<tr><td>default_value</td><td>VARCHAR(500)</td><td></td><td></td></tr>
<tr><td>picklist_values_json</td><td>JSONB</td><td></td><td>For picklist type: array of allowed values</td></tr>
<tr><td>created_at</td><td>TIMESTAMP</td><td></td><td></td></tr>
</table>
<ul>
  <li><strong>Read:</strong> On every page load that displays a record (to know which custom fields to render). Heavily cached.</li>
  <li><strong>Write:</strong> When admin creates/edits custom fields (very infrequent).</li>
  <li><strong>Indexes:</strong> B-tree on <code>(org_id, object_type)</code> — "get all custom fields for accounts in this org."</li>
  <li><strong>Sharding:</strong> Sharded by <code>org_id</code>.</li>
</ul>

<h4>Table: <code>custom_field_values</code> (EAV Pattern)</h4>
<table>
<tr><th>Column</th><th>Type</th><th>Key</th><th>Notes</th></tr>
<tr><td>value_id</td><td>UUID</td><td>PK</td><td></td></tr>
<tr><td>org_id</td><td>UUID</td><td>FK → tenants</td><td></td></tr>
<tr><td>object_type</td><td>VARCHAR(30)</td><td></td><td></td></tr>
<tr><td>object_id</td><td>UUID</td><td></td><td>ID of the record this value belongs to</td></tr>
<tr><td>field_id</td><td>UUID</td><td>FK → custom_fields</td><td></td></tr>
<tr><td>value_text</td><td>TEXT</td><td></td><td>Used for text/picklist values</td></tr>
<tr><td>value_number</td><td>DECIMAL(20,6)</td><td></td><td>Used for number values</td></tr>
<tr><td>value_date</td><td>TIMESTAMP</td><td></td><td>Used for date values</td></tr>
<tr><td>value_boolean</td><td>BOOLEAN</td><td></td><td>Used for boolean values</td></tr>
</table>
<div class="warn">
<strong>Denormalization Note (EAV Pattern):</strong> This is an Entity-Attribute-Value design, which trades query simplicity for flexibility. In a normalized design, each custom field would be a column on the entity table — but that would require DDL (ALTER TABLE) for every custom field, which is dangerous in a multi-tenant shared-schema database (DDL locks, millions of tenants adding fields constantly). The EAV pattern allows adding custom fields without any schema changes — just a row insert in <code>custom_fields</code>. The trade-off is that queries involving custom fields require JOINs to this table and value extraction from type-specific columns, making reporting on custom fields slower. We mitigate this with caching and denormalization into the search index.
</div>
<ul>
  <li><strong>Read:</strong> When displaying a record with custom fields (JOINed to the parent entity), during search (custom field values are indexed), during reporting.</li>
  <li><strong>Write:</strong> When a user sets or updates a custom field value on a record.</li>
  <li><strong>Indexes:</strong>
    <ul>
      <li>B-tree on <code>(org_id, object_type, object_id)</code> — "get all custom field values for this record." This is the primary access pattern.</li>
      <li>B-tree on <code>(org_id, field_id, value_text)</code> — for filtering records by a custom field value in list views (e.g., "show accounts where custom field 'Region' = 'West'").</li>
    </ul>
  </li>
  <li><strong>Sharding:</strong> Sharded by <code>org_id</code>.</li>
</ul>

<h4>Table: <code>file_attachments</code></h4>
<table>
<tr><th>Column</th><th>Type</th><th>Key</th><th>Notes</th></tr>
<tr><td>file_id</td><td>UUID</td><td>PK</td><td></td></tr>
<tr><td>org_id</td><td>UUID</td><td>FK → tenants</td><td></td></tr>
<tr><td>related_to_type</td><td>VARCHAR(30)</td><td></td><td>Polymorphic: which entity type</td></tr>
<tr><td>related_to_id</td><td>UUID</td><td></td><td>Which record</td></tr>
<tr><td>file_name</td><td>VARCHAR(500)</td><td></td><td>Original filename</td></tr>
<tr><td>file_size_bytes</td><td>BIGINT</td><td></td><td></td></tr>
<tr><td>content_type</td><td>VARCHAR(100)</td><td></td><td>MIME type</td></tr>
<tr><td>storage_path</td><td>VARCHAR(1000)</td><td></td><td>Path in object storage</td></tr>
<tr><td>uploaded_by</td><td>UUID</td><td>FK → users</td><td></td></tr>
<tr><td>created_at</td><td>TIMESTAMP</td><td></td><td></td></tr>
</table>
<ul>
  <li><strong>Read:</strong> When viewing attachments on a record detail page.</li>
  <li><strong>Write:</strong> When a user uploads a file to a record.</li>
  <li><strong>Indexes:</strong> B-tree on <code>(org_id, related_to_type, related_to_id)</code> — "show all files attached to this opportunity."</li>
  <li><strong>Note:</strong> File content is stored in Object Storage, not in the database. Only metadata is in SQL.</li>
  <li><strong>Sharding:</strong> Sharded by <code>org_id</code>.</li>
</ul>

<h3>10.2 NoSQL Tables</h3>
<p>NoSQL (wide-column / key-value store) is chosen for data with: high write volume, simple access patterns (no complex joins), time-series characteristics, and where horizontal scalability is paramount.</p>

<h4>Table: <code>notifications</code></h4>
<table>
<tr><th>Column</th><th>Type</th><th>Key</th><th>Notes</th></tr>
<tr><td>user_id</td><td>UUID</td><td>Partition Key</td><td>Notifications are always queried by user</td></tr>
<tr><td>timestamp</td><td>TIMESTAMP</td><td>Sort Key (DESC)</td><td>Newest first</td></tr>
<tr><td>notification_id</td><td>UUID</td><td></td><td></td></tr>
<tr><td>org_id</td><td>UUID</td><td></td><td></td></tr>
<tr><td>type</td><td>VARCHAR</td><td></td><td>record_change, mention, assignment, workflow</td></tr>
<tr><td>message</td><td>TEXT</td><td></td><td>Human-readable notification text</td></tr>
<tr><td>record_type</td><td>VARCHAR</td><td></td><td>Entity type of the related record</td></tr>
<tr><td>record_id</td><td>UUID</td><td></td><td></td></tr>
<tr><td>actor_id</td><td>UUID</td><td></td><td>User who triggered the notification</td></tr>
<tr><td>is_read</td><td>BOOLEAN</td><td></td><td></td></tr>
</table>
<ul>
  <li><strong>Why NoSQL:</strong> High write throughput (every record change can fan out to multiple users), simple access pattern (always query by <code>user_id</code> sorted by <code>timestamp</code>), no joins needed, eventual consistency is acceptable for notifications.</li>
  <li><strong>Read:</strong> When a user opens the notification panel or fetches via API.</li>
  <li><strong>Write:</strong> When the Notification Service processes a record change event and creates notifications for subscribers.</li>
  <li><strong>Sharding:</strong> Auto-partitioned by <code>user_id</code> (partition key). Ensures all notifications for one user are on the same partition for efficient retrieval.</li>
  <li><strong>TTL:</strong> Notifications older than 90 days are automatically expired/deleted to bound storage growth.</li>
</ul>

<h4>Table: <code>audit_log</code></h4>
<table>
<tr><th>Column</th><th>Type</th><th>Key</th><th>Notes</th></tr>
<tr><td>org_id</td><td>UUID</td><td>Partition Key</td><td></td></tr>
<tr><td>timestamp</td><td>TIMESTAMP</td><td>Sort Key (DESC)</td><td></td></tr>
<tr><td>audit_id</td><td>UUID</td><td></td><td></td></tr>
<tr><td>user_id</td><td>UUID</td><td></td><td>Who made the change</td></tr>
<tr><td>action</td><td>VARCHAR</td><td></td><td>create, update, delete</td></tr>
<tr><td>object_type</td><td>VARCHAR</td><td></td><td></td></tr>
<tr><td>object_id</td><td>UUID</td><td></td><td></td></tr>
<tr><td>old_values_json</td><td>JSON</td><td></td><td>Previous field values</td></tr>
<tr><td>new_values_json</td><td>JSON</td><td></td><td>New field values</td></tr>
<tr><td>ip_address</td><td>VARCHAR</td><td></td><td>Requester IP</td></tr>
</table>
<ul>
  <li><strong>Why NoSQL:</strong> Append-only, extremely high write volume (every single data mutation across all entities), simple access pattern (query by <code>org_id</code> + time range, optionally filter by <code>object_type</code>), no updates or deletes (immutable log), no joins needed.</li>
  <li><strong>Read:</strong> When an admin views the audit trail (e.g., "show me all changes to opportunities in the last 7 days"), during compliance reviews.</li>
  <li><strong>Write:</strong> On every CRM record create, update, or delete event.</li>
  <li><strong>Sharding:</strong> Auto-partitioned by <code>org_id</code>.</li>
  <li><strong>TTL:</strong> Configurable per org (e.g., 1 year for professional tier, 5 years for enterprise tier) to comply with data retention policies.</li>
</ul>

<h4>Table: <code>subscriptions</code></h4>
<table>
<tr><th>Column</th><th>Type</th><th>Key</th><th>Notes</th></tr>
<tr><td>record_key</td><td>VARCHAR</td><td>Partition Key</td><td>Composite: "{org_id}:{object_type}:{object_id}"</td></tr>
<tr><td>user_id</td><td>UUID</td><td>Sort Key</td><td></td></tr>
<tr><td>subscription_type</td><td>VARCHAR</td><td></td><td>owner, follower, team_member</td></tr>
<tr><td>created_at</td><td>TIMESTAMP</td><td></td><td></td></tr>
</table>
<ul>
  <li><strong>Why NoSQL:</strong> Simple key-value lookups ("who is subscribed to this record?"), no joins, high read frequency (checked on every record change event).</li>
  <li><strong>Read:</strong> By the Notification Service when determining who to notify about a record change.</li>
  <li><strong>Write:</strong> When a user follows a record, when a record owner is changed, when team members are added.</li>
  <li><strong>Sharding:</strong> Auto-partitioned by <code>record_key</code>.</li>
</ul>

<!-- ============================================================ -->
<h2 id="cdn">11. CDN Deep Dive</h2>
<!-- ============================================================ -->

<div class="card">
<h3>Why a CDN is Appropriate</h3>
<p>A CDN is appropriate for this system for two main reasons:</p>
<ol>
  <li><strong>Static Web Application Assets:</strong> The CRM web application consists of JavaScript bundles, CSS files, images, and fonts. These are the same for all tenants and all users. Serving them from edge locations worldwide reduces page load time significantly, especially for global enterprise customers with offices in multiple regions.</li>
  <li><strong>File Attachment Downloads:</strong> When users download file attachments (PDFs, documents, images) attached to CRM records, the File Service generates a pre-signed URL pointing to the CDN. The CDN caches these files at the edge, reducing latency and offloading bandwidth from the origin Object Storage.</li>
</ol>

<h3>What the CDN Does NOT Cache</h3>
<p>API responses (JSON data from the CRM services) are <strong>not</strong> cached at the CDN layer because:</p>
<ul>
  <li>CRM data is tenant-specific and permission-scoped — caching it at the CDN would risk serving one tenant's data to another.</li>
  <li>CRM data changes frequently (a record could be updated seconds after creation).</li>
  <li>API responses require authentication header validation which CDNs don't perform.</li>
</ul>

<h3>CDN Caching Strategy</h3>
<table>
<tr><th>Content Type</th><th>Cache-Control</th><th>TTL</th><th>Invalidation</th></tr>
<tr><td>JS/CSS bundles</td><td><code>public, max-age=31536000, immutable</code></td><td>1 year</td><td>Filename-based cache busting (content hash in filename, e.g., <code>app.a3f9c2.js</code>)</td></tr>
<tr><td>Images/fonts</td><td><code>public, max-age=2592000</code></td><td>30 days</td><td>Filename-based cache busting</td></tr>
<tr><td>File attachments</td><td><code>private, max-age=3600</code></td><td>1 hour</td><td>Pre-signed URLs expire; new URL = new cache key</td></tr>
<tr><td>Report exports (CSV/PDF)</td><td><code>private, max-age=86400</code></td><td>1 day</td><td>Pre-signed URLs expire</td></tr>
</table>
</div>

<!-- ============================================================ -->
<h2 id="cache">12. Cache Deep Dive</h2>
<!-- ============================================================ -->

<div class="card">
<h3>Why an In-Memory Cache is Appropriate</h3>
<p>An in-memory cache is critical for this system because:</p>
<ul>
  <li><strong>Metadata is read-heavy:</strong> Custom field definitions, workflow rules, stage configurations, and role permissions are read on nearly every API request but change very rarely. Without caching, every page load would require multiple DB queries just for metadata.</li>
  <li><strong>Hot records:</strong> Certain accounts and opportunities are accessed many times per day by multiple team members. Caching avoids redundant SQL queries.</li>
  <li><strong>Report results:</strong> Reports involve expensive aggregation queries. Multiple users may run the same or similar reports. Caching saves compute and reduces read replica load.</li>
  <li><strong>User sessions and permissions:</strong> Permission evaluation on every request must be fast (&lt;5ms). Caching the resolved permission set avoids the multi-table join needed to compute it.</li>
</ul>

<h3>Cache Strategy per Data Type</h3>
<table>
<tr><th>Data Type</th><th>Strategy</th><th>Eviction Policy</th><th>Expiration (TTL)</th><th>Population Trigger</th></tr>
<tr>
  <td><strong>Metadata</strong> (custom fields, stage configs, workflow rules)</td>
  <td><strong>Write-through</strong> — When an admin updates metadata, the service writes to both SQL and cache simultaneously. This ensures the cache is always fresh immediately after a write, avoiding stale metadata that could cause incorrect field rendering or workflow misfires.</td>
  <td>LRU (Least Recently Used) — inactive orgs' metadata is evicted first since active orgs will keep refreshing their entries.</td>
  <td>15 minutes — even with write-through, a TTL acts as a safety net against cache corruption. If the write-through fails, the stale entry expires within 15 minutes.</td>
  <td>First request for an org's metadata after a cache miss, OR an admin saving a configuration change.</td>
</tr>
<tr>
  <td><strong>User permissions</strong></td>
  <td><strong>Write-through</strong> — Permission changes are applied to cache immediately on save.</td>
  <td>LRU</td>
  <td>10 minutes</td>
  <td>User login, or first API request after cache miss.</td>
</tr>
<tr>
  <td><strong>Hot CRM records</strong> (accounts, contacts, opportunities)</td>
  <td><strong>Cache-aside (lazy loading)</strong> — Records are loaded into cache only when requested. On a cache miss, the service reads from SQL and populates the cache. On a write, the cache entry is invalidated (deleted), and the next read repopulates it. Cache-aside is chosen because CRM records are updated frequently and there's no need to keep every record in cache — only hot ones. Write-through would waste cache space on records that are written but never read again.</td>
  <td>LRU — records that haven't been accessed recently are evicted to make room for active records.</td>
  <td>5 minutes — short TTL because CRM data changes frequently and users expect to see updates quickly. A 5-minute window is acceptable because updates also invalidate the cache entry immediately; the TTL is just a safety net.</td>
  <td>First read of a record that's not in cache.</td>
</tr>
<tr>
  <td><strong>Report results</strong></td>
  <td><strong>Cache-aside</strong> — Reports are cached after execution. Same report with same filters = cache hit. Cache-aside because reports are expensive to compute and the same report may be run by multiple users.</td>
  <td>LRU</td>
  <td>5 minutes — reports should reflect reasonably recent data but don't need to be real-time. 5 minutes balances freshness with cost savings on the read replica.</td>
  <td>Report execution (cache miss triggers query; result is cached).</td>
</tr>
<tr>
  <td><strong>Search results</strong></td>
  <td>NOT cached — search results are too varied (different queries, filters, permissions) to cache effectively. The search index itself is optimized for fast reads.</td>
  <td>N/A</td>
  <td>N/A</td>
  <td>N/A</td>
</tr>
</table>

<h3>Why LRU Eviction?</h3>
<p>LRU is chosen because CRM usage follows a power-law distribution: a small percentage of records (key accounts, active deals) receive the vast majority of reads. LRU naturally keeps these hot records in cache while evicting cold records. Alternatives like LFU (Least Frequently Used) could also work but are more complex to implement and LRU performs nearly as well for this access pattern.</p>
</div>

<!-- ============================================================ -->
<h2 id="mq">13. Message Queue Deep Dive</h2>
<!-- ============================================================ -->

<div class="card">
<h3>Why a Message Queue?</h3>
<p>The message queue serves as the backbone for asynchronous processing in the CRM system. It decouples the synchronous API response from downstream operations that don't need to block the user:</p>
<ul>
  <li><strong>Lead scoring</strong> — Scoring can take 50–200ms depending on rule complexity. Running it synchronously would add latency to lead creation.</li>
  <li><strong>Search index updates</strong> — The search index is eventually consistent. Updating it asynchronously keeps write latency low.</li>
  <li><strong>Notification fan-out</strong> — A single record change may notify 1–50 users. This fan-out should not block the API response.</li>
  <li><strong>Workflow execution</strong> — Workflows may chain multiple actions (send email, create task, call webhook). These are inherently async.</li>
  <li><strong>Audit logging</strong> — Writing to the NoSQL audit log shouldn't slow down the primary write path.</li>
  <li><strong>Report exports</strong> — Generating CSV/PDF files can take seconds to minutes.</li>
</ul>

<h3>Why Not Alternatives?</h3>
<table>
<tr><th>Alternative</th><th>Why Not Chosen</th></tr>
<tr><td><strong>Synchronous processing</strong></td><td>Would add 200–500ms latency to every write operation. Users would perceive the CRM as slow. Not acceptable for a productivity tool.</td></tr>
<tr><td><strong>Pub/Sub</strong></td><td>Pub/Sub is designed for broadcasting messages to multiple independent subscribers with no message persistence guarantee (fire-and-forget). Our use case requires <strong>guaranteed delivery</strong> — if the scoring worker is temporarily down, we need the message to be retained and processed when it recovers. A message queue with at-least-once delivery semantics is a better fit. That said, the message queue supports topic-based routing (similar to pub/sub topics), so multiple consumers can subscribe to the same event type.</td></tr>
<tr><td><strong>Polling</strong></td><td>Workers polling a database table for new events would add latency (polling interval) and create unnecessary database load. A message queue provides push-based delivery with near-zero latency.</td></tr>
</table>

<h3>Message Queue Architecture</h3>
<ul>
  <li><strong>Protocol:</strong> TCP with a binary wire protocol (AMQP-like).</li>
  <li><strong>Topics:</strong> Each event type is a topic: <code>lead.created</code>, <code>lead.updated</code>, <code>lead.converted</code>, <code>opportunity.stage_changed</code>, <code>opportunity.closed</code>, <code>record.updated</code> (generic), <code>activity.created</code>, <code>activity.mention</code>, <code>report.export_scheduled</code>, <code>bulk_import.started</code>.</li>
  <li><strong>Consumer Groups:</strong> Each downstream system (scoring, search indexer, notifications, workflow engine, audit logger) is a separate consumer group. This means each event is processed independently by each consumer — the scoring worker consuming a <code>lead.created</code> event does not affect the notification service's consumption of the same event.</li>
  <li><strong>Delivery Guarantee:</strong> At-least-once. Consumers acknowledge (ack) messages after successful processing. If a consumer crashes before acking, the message is redelivered after a visibility timeout. This means consumers must be <strong>idempotent</strong> — processing the same event twice should produce the same result (e.g., the scoring worker checks if the score has already been updated before writing).</li>
  <li><strong>Message Format:</strong> JSON envelope: <code>{event_type, org_id, object_type, object_id, actor_id, timestamp, payload: {...}}</code>.</li>
  <li><strong>Ordering:</strong> Messages are ordered within a partition. Partitioning is by <code>org_id</code> to ensure events for the same tenant are processed in order (important for workflow evaluation — stage change from A→B must be processed before B→C).</li>
  <li><strong>Retention:</strong> Messages are retained for 7 days even after consumption, allowing replays for debugging or backfilling (e.g., if the search indexer was down and missed events, it can replay from the queue).</li>
</ul>

<h3>How Messages Flow</h3>
<ol>
  <li><strong>Enqueue:</strong> A service (e.g., Lead Service) publishes an event by calling the Message Queue's producer API over TCP. The message is written to the appropriate topic partition (determined by hashing <code>org_id</code>).</li>
  <li><strong>Distribute:</strong> The Message Queue broker delivers the message to all consumer groups subscribed to that topic. Each consumer group receives exactly one copy.</li>
  <li><strong>Process:</strong> A worker in each consumer group picks up the message, processes it (e.g., scores the lead), and sends an acknowledgment (ack) back to the broker.</li>
  <li><strong>Ack/Retry:</strong> On successful ack, the message offset is advanced for that consumer group. If no ack is received within 30 seconds (visibility timeout), the message is redelivered to another worker in the same consumer group.</li>
  <li><strong>Dead Letter Queue (DLQ):</strong> After 3 failed processing attempts, the message is moved to a dead letter queue for manual investigation. This prevents a poison message from blocking the entire queue.</li>
</ol>
</div>

<!-- ============================================================ -->
<h2 id="ws">14. WebSocket Deep Dive</h2>
<!-- ============================================================ -->

<div class="card">
<h3>Why WebSockets?</h3>
<p>Real-time notifications are a key feature of a CRM: when a deal is updated, a new lead is assigned, or a user is @mentioned, the notification should appear instantly without requiring a page refresh.</p>

<table>
<tr><th>Alternative</th><th>Why Not Chosen</th></tr>
<tr><td><strong>Short Polling</strong></td><td>Client periodically calls <code>GET /notifications</code> (e.g., every 5 seconds). Wastes bandwidth and server resources when there are no new notifications (majority of polls return empty). With ~100k concurrent users each polling every 5 seconds, that's 20k requests/sec just for notification checks — most returning empty 200s. Not efficient.</td></tr>
<tr><td><strong>Long Polling</strong></td><td>Client sends a request that the server holds open until a notification arrives (or timeout). Better than short polling but still creates a new HTTP connection for every notification. Not ideal when notifications are frequent and users are connected for hours.</td></tr>
<tr><td><strong>Server-Sent Events (SSE)</strong></td><td>SSE provides a one-directional server-to-client stream over HTTP. It could work for notifications, but WebSockets provide bidirectional communication which allows the client to send heartbeats, subscription preferences, and acknowledgments back without separate HTTP requests. WebSockets also have better support for binary payloads if needed in the future (e.g., real-time collaborative editing).</td></tr>
</table>

<h3>WebSocket Connection Lifecycle</h3>
<ol>
  <li><strong>Connection Establishment:</strong> When a user logs into the CRM web app, the browser initiates a WebSocket connection: <code>WSS wss://realtime.crm.example.com/ws?token={JWT}</code>. The request hits the Load Balancer (which supports WebSocket upgrade) and is routed to an available WebSocket Server.</li>
  <li><strong>Authentication:</strong> The WebSocket Server validates the JWT token in the query parameter (same auth as REST API). If valid, the connection is accepted. If invalid, the server returns a 401 and closes the connection.</li>
  <li><strong>Registration:</strong> On successful connection, the WebSocket Server registers the connection in the <strong>Connection Store</strong> (an in-memory key-value cache): Key = <code>user_id</code>, Value = <code>{server_id: "ws-server-03", connection_id: "conn-abc123", connected_at: "..."}</code>. This mapping is needed because the Notification Service doesn't know which WebSocket Server the user is connected to.</li>
  <li><strong>Heartbeat:</strong> The client sends a ping frame every 30 seconds. The server responds with a pong. If no ping is received within 60 seconds, the server considers the connection dead and removes it from the Connection Store.</li>
  <li><strong>Message Delivery:</strong> When the Notification Service needs to push to user "Bob":
    <ol>
      <li>It queries the Connection Store: "Where is Bob connected?"</li>
      <li>Gets back: <code>{server_id: "ws-server-03"}</code></li>
      <li>Sends the notification payload to <code>ws-server-03</code> via an internal pub/sub channel (the WebSocket Manager maintains an internal channel per WebSocket Server).</li>
      <li><code>ws-server-03</code> looks up the local connection for Bob and sends the notification as a WebSocket text frame (JSON payload).</li>
    </ol>
  </li>
  <li><strong>Disconnection:</strong> When the user closes the browser tab or navigates away, the browser closes the WebSocket connection. The server detects the close, removes the entry from the Connection Store, and cleans up resources.</li>
  <li><strong>Reconnection:</strong> If the connection drops (network issue), the client auto-reconnects with exponential backoff (1s, 2s, 4s, 8s, max 30s). On reconnect, it also fetches any missed notifications via the REST API <code>GET /api/v1/notifications?since={last_received_timestamp}</code> to fill any gap.</li>
</ol>

<h3>Connection Store Details</h3>
<ul>
  <li><strong>Type:</strong> In-memory key-value cache (same technology as the main application cache, but a separate logical namespace).</li>
  <li><strong>Key:</strong> <code>ws:conn:{user_id}</code></li>
  <li><strong>Value:</strong> JSON: <code>{server_id, connection_id, org_id, connected_at}</code></li>
  <li><strong>TTL:</strong> 90 seconds. The WebSocket Server refreshes this TTL on every heartbeat. If a WebSocket Server crashes, its users' entries will expire within 90 seconds, ensuring no stale connections linger.</li>
</ul>

<h3>How Other WebSocket Servers Are Found</h3>
<p>The WebSocket Manager uses the Connection Store as a directory. Since the Connection Store is a shared, centralized cache accessible by all WebSocket Server instances and the Notification Service, any component can look up any user's connection location. There is no need for WebSocket Servers to discover each other directly — the Connection Store mediates all routing.</p>

<h3>Scaling WebSocket Servers</h3>
<p>Each WebSocket Server can hold ~50,000 concurrent connections (limited by file descriptors and memory). For 500,000 concurrent users, we need ~10 WebSocket Server instances. The Load Balancer uses sticky sessions (based on a cookie) to ensure a user's WebSocket connection is routed to the same server for the duration of the session. New WebSocket Server instances can be added horizontally as concurrent user count grows.</p>
</div>

<!-- ============================================================ -->
<h2 id="scaling">15. Scaling Considerations</h2>
<!-- ============================================================ -->

<div class="card">
<h3>15.1 Load Balancers</h3>
<p>Load balancers are placed at two levels in the architecture:</p>

<h4>Level 1: External Load Balancer (Edge)</h4>
<ul>
  <li><strong>Position:</strong> Between clients (web/mobile) and the API Gateway instances.</li>
  <li><strong>Protocol:</strong> HTTPS termination (TLS 1.3). Forwards to API Gateway over HTTP (internal network).</li>
  <li><strong>Algorithm:</strong> Round-robin with health checks. Each API Gateway instance is stateless, so any instance can handle any request.</li>
  <li><strong>Features:</strong> DDoS protection, geographic routing (route users to the nearest data center), SSL offloading, connection draining for graceful deployments.</li>
  <li><strong>WebSocket support:</strong> Must support WebSocket upgrade (HTTP Upgrade header) and sticky sessions for WebSocket connections.</li>
  <li><strong>Scaling:</strong> Managed/auto-scaling; scales horizontally with traffic.</li>
</ul>

<h4>Level 2: Internal Load Balancers (Service Mesh)</h4>
<ul>
  <li><strong>Position:</strong> Between the API Gateway and each microservice (Lead Service, Opportunity Service, etc.).</li>
  <li><strong>Protocol:</strong> gRPC (HTTP/2 with load balancing).</li>
  <li><strong>Algorithm:</strong> Least-connections (some service calls take longer than others; this ensures even load distribution based on actual load rather than request count).</li>
  <li><strong>Service Discovery:</strong> Services register themselves with a service registry. The internal LB queries the registry to find healthy instances.</li>
  <li><strong>Health checks:</strong> gRPC health checking protocol. Unhealthy instances are removed from the pool within 10 seconds.</li>
</ul>

<h3>15.2 Horizontal Scaling by Component</h3>
<table>
<tr><th>Component</th><th>Scaling Approach</th><th>Notes</th></tr>
<tr><td>API Gateway</td><td>Horizontal — add more stateless instances behind the LB</td><td>Auto-scale based on CPU and request count</td></tr>
<tr><td>Application Services</td><td>Horizontal — each service scales independently</td><td>E.g., if report execution surges, scale Reporting Service without scaling Lead Service</td></tr>
<tr><td>SQL Database</td><td>Sharded by <code>org_id</code> + read replicas for reporting</td><td>Vertical scaling for write-heavy shards; add shards for more tenants</td></tr>
<tr><td>NoSQL Database</td><td>Auto-partitioned by partition key</td><td>Scales horizontally by adding nodes; data rebalances automatically</td></tr>
<tr><td>Search Index</td><td>Sharded by <code>org_id</code>; replicated per shard</td><td>Add shards for capacity, replicas for read throughput</td></tr>
<tr><td>Message Queue</td><td>Partitioned by <code>org_id</code>; add partitions for throughput</td><td>Consumer groups scale by adding workers</td></tr>
<tr><td>Cache</td><td>Clustered — add nodes to the cache cluster</td><td>Consistent hashing for key distribution; hot key splitting for very large tenants</td></tr>
<tr><td>WebSocket Servers</td><td>Horizontal — add more instances for more concurrent connections</td><td>~50k connections per instance</td></tr>
<tr><td>Workers (scoring, indexing, etc.)</td><td>Horizontal — add more worker instances per consumer group</td><td>Auto-scale based on queue depth</td></tr>
<tr><td>Object Storage</td><td>Inherently scalable (managed infrastructure)</td><td>No scaling needed</td></tr>
</table>

<h3>15.3 Data Partitioning Strategy</h3>
<div class="info">
<strong>Sharding Key: <code>org_id</code></strong> — All SQL and NoSQL tables are sharded/partitioned by <code>org_id</code>. This was chosen because:
<ul>
  <li>Almost every query in a CRM is scoped to a single tenant (users only see their own org's data).</li>
  <li>Co-locating all of a tenant's data on one shard enables efficient joins, transactions, and queries without cross-shard operations.</li>
  <li>It provides natural data isolation — a query bug that scans a full shard only exposes one tenant's data.</li>
</ul>
<strong>Hot shard mitigation:</strong> Very large enterprise customers (millions of records) may cause hot shards. These tenants are detected via monitoring and can be migrated to dedicated shards or split across sub-shards using a secondary key (e.g., <code>org_id + object_type</code>).
</div>

<h3>15.4 Rate Limiting</h3>
<ul>
  <li><strong>Per-tenant:</strong> API calls are rate-limited based on the tenant's subscription plan (e.g., 100 requests/sec for professional, 500 for enterprise). Prevents a single tenant from monopolizing shared resources (noisy neighbor).</li>
  <li><strong>Per-user:</strong> Individual users within a tenant are limited to 30 requests/sec to prevent runaway scripts.</li>
  <li><strong>Burst allowance:</strong> Token bucket algorithm allows short bursts above the steady-state limit (e.g., 2x the limit for 5 seconds) to accommodate legitimate burst patterns like page loads.</li>
  <li><strong>Implementation:</strong> Enforced at the API Gateway using a distributed counter in the cache layer. Response: <code>429 Too Many Requests</code> with <code>Retry-After</code> header.</li>
</ul>

<h3>15.5 Multi-Region Deployment</h3>
<ul>
  <li>For global availability, the system is deployed in multiple regions (e.g., US-East, EU-West, AP-Southeast).</li>
  <li>Each region has a full stack (API Gateway, services, databases, caches).</li>
  <li>Tenants are homed to a primary region based on data residency requirements (GDPR requires EU customer data in EU).</li>
  <li>The CDN serves static assets from all edge locations regardless of region.</li>
  <li>Cross-region replication is asynchronous for disaster recovery (active-passive DR per region).</li>
</ul>
</div>

<!-- ============================================================ -->
<h2 id="tradeoffs">16. Tradeoffs &amp; Deep Dives</h2>
<!-- ============================================================ -->

<div class="card">
<h3>16.1 Multi-Tenancy: Shared Schema vs. Schema-Per-Tenant</h3>
<p><strong>Chosen: Shared schema with <code>org_id</code> column.</strong></p>
<table>
<tr><th></th><th>Shared Schema (Chosen)</th><th>Schema-Per-Tenant</th></tr>
<tr><td><strong>Scalability</strong></td><td>Scales to millions of tenants — just rows in tables</td><td>Each tenant = separate DB/schema; managing millions of schemas is operationally infeasible</td></tr>
<tr><td><strong>Resource efficiency</strong></td><td>Connection pooling shared; efficient use of DB connections</td><td>Each tenant needs its own connections; connection explosion</td></tr>
<tr><td><strong>Query isolation</strong></td><td>Must enforce <code>org_id</code> filter on every query (risk of data leak if omitted)</td><td>Natural isolation — impossible to accidentally query another tenant</td></tr>
<tr><td><strong>Customization</strong></td><td>Custom fields via EAV pattern (slower queries)</td><td>Custom fields as actual columns (faster queries)</td></tr>
<tr><td><strong>Migrations</strong></td><td>One migration for all tenants (but must be backward-compatible)</td><td>Must run migration on each schema separately (slow, risky)</td></tr>
</table>
<p><strong>Decision rationale:</strong> Shared schema wins because the system must support millions of small-to-medium tenants (Salesforce has ~150,000 customers). Schema-per-tenant is only viable for dozens to hundreds of tenants. The risk of cross-tenant data leakage is mitigated by enforcing <code>org_id</code> filtering at the data access layer (ORM/query builder level) as a mandatory parameter — a "tenant context" is injected into every database query automatically.</p>

<h3>16.2 Custom Fields: EAV vs. JSON Columns vs. Sparse Columns</h3>
<p><strong>Chosen: EAV (Entity-Attribute-Value) pattern.</strong></p>
<table>
<tr><th></th><th>EAV (Chosen)</th><th>JSON Column</th><th>Sparse Columns</th></tr>
<tr><td><strong>Queryability</strong></td><td>JOINs required but indexed; slower for aggregation</td><td>JSON path queries; limited indexing support in some SQL engines</td><td>Regular column queries; fast</td></tr>
<tr><td><strong>Flexibility</strong></td><td>Unlimited custom fields without schema changes</td><td>Unlimited fields, but indexing/querying is limited</td><td>Limited to pre-allocated columns (~500 reserved columns per entity)</td></tr>
<tr><td><strong>Indexing</strong></td><td>Can index specific field values via the EAV table</td><td>Partial JSON indexes possible but complex</td><td>Standard B-tree indexes on each column</td></tr>
<tr><td><strong>Storage</strong></td><td>Efficient — only stores values that exist</td><td>Efficient</td><td>Wastes space on unused columns (NULLs)</td></tr>
<tr><td><strong>Schema changes</strong></td><td>None required</td><td>None required</td><td>Requires reserving columns upfront; eventual schema changes needed</td></tr>
</table>
<p><strong>Decision rationale:</strong> EAV is chosen because it provides the best balance of flexibility and queryability. JSON columns lack indexing for custom field filters and groupings in reports (a critical CRM feature). Sparse columns limit the number of custom fields and still require schema management. EAV's query performance trade-off is mitigated by caching field metadata and denormalizing custom field values into the search index for full-text search.</p>

<h3>16.3 Strong Consistency vs. Eventual Consistency</h3>
<table>
<tr><th>Data Type</th><th>Consistency</th><th>Rationale</th></tr>
<tr><td>CRM records (accounts, contacts, leads, opportunities, cases)</td><td><strong>Strong</strong></td><td>A user who updates a record and refreshes the page must see their change immediately. Stale reads on CRM data cause confusion and data integrity issues (e.g., two reps updating the same deal stage simultaneously).</td></tr>
<tr><td>Search index</td><td><strong>Eventual</strong> (~2s lag)</td><td>Acceptable because search is a discovery tool, not a source of truth. Users understand search results may lag slightly behind.</td></tr>
<tr><td>Notifications</td><td><strong>Eventual</strong></td><td>A notification arriving 1–2 seconds after a record change is acceptable. Notifications are informational, not transactional.</td></tr>
<tr><td>Audit log</td><td><strong>Eventual</strong></td><td>Audit logs are consumed asynchronously for compliance. A few seconds of delay is fine.</td></tr>
<tr><td>Reports</td><td><strong>Eventual</strong> (read replica lag ~1s)</td><td>Reports are analytical and users expect near-real-time, not real-time data.</td></tr>
</table>

<h3>16.4 Microservices vs. Monolith</h3>
<p><strong>Chosen: Microservices architecture.</strong></p>
<table>
<tr><th></th><th>Microservices (Chosen)</th><th>Monolith</th></tr>
<tr><td><strong>Independent scaling</strong></td><td>Each service scales independently based on its load (e.g., scale Reporting Service during end-of-quarter without scaling Lead Service)</td><td>Must scale entire application even if only one module is under load</td></tr>
<tr><td><strong>Deployment</strong></td><td>Independent deployments — a bug fix in the Notification Service doesn't require redeploying the Opportunity Service</td><td>Single deployment unit — any change requires full deployment, increasing risk</td></tr>
<tr><td><strong>Team autonomy</strong></td><td>Different teams own different services; can use different technologies if needed</td><td>All teams work in the same codebase; coordination overhead</td></tr>
<tr><td><strong>Complexity</strong></td><td>Higher operational complexity (service discovery, distributed tracing, network failures)</td><td>Simpler to develop, test, and debug initially</td></tr>
<tr><td><strong>Data consistency</strong></td><td>Distributed transactions are complex (saga pattern needed for cross-service operations like lead conversion)</td><td>Local transactions are simple and ACID-compliant</td></tr>
<tr><td><strong>Latency</strong></td><td>Inter-service calls add latency (mitigated by gRPC and caching)</td><td>In-process calls are near-zero latency</td></tr>
</table>
<p><strong>Decision rationale:</strong> Microservices are chosen because a CRM platform at scale has fundamentally different scaling profiles per feature: search is read-heavy, lead capture is write-heavy, reporting is compute-heavy, notifications are fan-out-heavy. Independent scaling saves significant infrastructure cost. The operational complexity is justified at this scale and managed through service mesh, distributed tracing, and standardized deployment pipelines.</p>

<h3>16.5 gRPC for Internal Communication</h3>
<p><strong>Why gRPC over REST for inter-service communication:</strong></p>
<ul>
  <li><strong>Performance:</strong> gRPC uses Protocol Buffers (binary serialization), which is 5–10x faster to serialize/deserialize than JSON. With millions of inter-service calls per second, this adds up.</li>
  <li><strong>HTTP/2:</strong> gRPC runs over HTTP/2, which supports multiplexing (multiple requests on one connection), header compression, and bidirectional streaming. This reduces connection overhead compared to HTTP/1.1 REST.</li>
  <li><strong>Strongly typed contracts:</strong> <code>.proto</code> files serve as the API contract between services. Code generation ensures type safety and prevents integration bugs that can occur with loosely typed JSON APIs.</li>
  <li><strong>Streaming:</strong> gRPC supports server-streaming (useful for large result sets) and bidirectional streaming (useful for the WebSocket Manager ↔ Notification Service channel).</li>
</ul>
<p><strong>Why REST is kept for external APIs:</strong> REST over HTTPS is chosen for the client-facing API because: (1) browser JavaScript has native support for REST/JSON but limited gRPC support, (2) REST is universally understood by third-party integrators, (3) REST is easier to debug with standard tools (curl, Postman), (4) REST API documentation (OpenAPI/Swagger) is more widely adopted.</p>
</div>

<!-- ============================================================ -->
<h2 id="alternatives">17. Alternative Approaches</h2>
<!-- ============================================================ -->

<div class="card">
<h3>17.1 Graph Database for CRM Relationships</h3>
<p><strong>Idea:</strong> Use a graph database to model the CRM's relationship-heavy data (accounts → contacts → opportunities → activities).</p>
<p><strong>Why not chosen:</strong></p>
<ul>
  <li>CRM relationships are relatively shallow (2–3 hops max). SQL with proper indexing handles these joins efficiently.</li>
  <li>Graph databases are less mature for ACID transactions, which are critical for operations like lead conversion.</li>
  <li>The team's expertise is in SQL, and graph query languages (e.g., Cypher, Gremlin) have a steeper learning curve.</li>
  <li>Reporting requirements (GROUP BY, SUM, COUNT, DATE filters) are better served by SQL aggregation functions than graph traversals.</li>
  <li>Graph databases excel when relationships are the primary query dimension (e.g., social networks, fraud detection). In a CRM, the primary query is "show me records owned by me, filtered by status" — a straightforward indexed SQL query.</li>
</ul>
<p><strong>When it would be chosen:</strong> If the CRM needed deep relationship analysis (e.g., "find all contacts within 3 degrees of connection to this decision-maker"), a graph database layered on top of the SQL primary store would be valuable.</p>

<h3>17.2 Event Sourcing for Audit Trail</h3>
<p><strong>Idea:</strong> Instead of storing the current state of records in SQL and a separate audit log in NoSQL, use event sourcing: store every state change as an immutable event, and derive the current state by replaying events.</p>
<p><strong>Why not chosen:</strong></p>
<ul>
  <li>Event sourcing adds significant complexity to reads: to get the current state of an opportunity, the system would need to replay all events from creation. For a record with 500 state changes, this is slow.</li>
  <li>Snapshots mitigate this, but add another layer of complexity (snapshot management, consistency between snapshots and events).</li>
  <li>CRM users expect simple CRUD operations. The mental model of "current state + change log" (our approach) is simpler than "event stream + projection."</li>
  <li>The separate audit log in NoSQL achieves the same compliance goals (full history of changes) without the read-path complexity.</li>
</ul>
<p><strong>When it would be chosen:</strong> If the CRM had complex undo/redo requirements, or if the business needed to replay history to compute alternative scenarios (e.g., "what would revenue look like if we hadn't lost these 10 deals?"), event sourcing would be more appropriate.</p>

<h3>17.3 CQRS (Command Query Responsibility Segregation)</h3>
<p><strong>Idea:</strong> Separate the write model (commands: create, update, delete) from the read model (queries: list, search, report). Each has its own database optimized for its access pattern.</p>
<p><strong>Why not chosen as the primary pattern:</strong></p>
<ul>
  <li>Adds significant infrastructure complexity (two databases per entity, synchronization between them).</li>
  <li>For most CRM operations, the read and write models are identical (the same SQL table serves both reads and writes well with proper indexing).</li>
  <li>The consistency gap between write and read stores can confuse users ("I just updated this record but the list view still shows the old value").</li>
</ul>
<p><strong>Where we partially adopt CQRS:</strong> We do use a mild form of CQRS in two places: (1) Reporting uses read replicas (separate read model optimized for analytical queries), and (2) Search uses a separate inverted index (optimized for full-text search). These are acceptable because users understand reports and search results may lag slightly.</p>

<h3>17.4 NoSQL for All CRM Data</h3>
<p><strong>Idea:</strong> Use a document database for all CRM entities (store each account, contact, opportunity as a JSON document).</p>
<p><strong>Why not chosen:</strong></p>
<ul>
  <li>CRM data is fundamentally relational: accounts have contacts, contacts have opportunities, opportunities have activities. SQL models these relationships naturally with foreign keys and joins.</li>
  <li>Reporting requires complex aggregations (SUM, AVG, GROUP BY) across entities with joins. SQL is purpose-built for this. Doing aggregations across denormalized documents in NoSQL requires application-level joins or map-reduce, which are slower and more complex.</li>
  <li>ACID transactions are needed for operations like lead conversion (create 3 records atomically). Most NoSQL databases offer limited transaction support.</li>
  <li>Custom field filtering and sorting in list views requires flexible querying that SQL handles well with indexes.</li>
</ul>
<p><strong>When it would be chosen:</strong> If each CRM entity were self-contained (no cross-entity queries), had highly variable schemas per tenant, and didn't require aggregation reporting, a document database would be simpler.</p>

<h3>17.5 Server-Sent Events Instead of WebSockets</h3>
<p><strong>Idea:</strong> Use SSE (Server-Sent Events) for real-time notifications instead of WebSockets.</p>
<p><strong>Why not chosen:</strong></p>
<ul>
  <li>SSE is unidirectional (server → client only). With WebSockets, the client can send heartbeats, typing indicators (for collaborative notes), and subscription changes without separate HTTP requests.</li>
  <li>SSE connections count as regular HTTP connections. Some corporate proxies and firewalls limit the number of concurrent HTTP connections, which can cause SSE connections to be dropped. WebSocket connections use a different protocol after upgrade and are generally better handled by proxies.</li>
  <li>WebSockets provide a foundation for future real-time collaboration features (co-editing records, live cursors) without requiring a protocol change.</li>
</ul>
<p><strong>When it would be chosen:</strong> If the system only needed unidirectional server-to-client notifications with no future plans for bidirectional communication, SSE would be simpler to implement and scale (no need for connection store or WebSocket Manager).</p>

<h3>17.6 Polling for Notification Delivery</h3>
<p><strong>Idea:</strong> Clients poll <code>GET /api/v1/notifications</code> every N seconds instead of maintaining WebSocket connections.</p>
<p><strong>Why not chosen:</strong> As discussed in the <a href="#ws">WebSocket Deep Dive</a>, polling wastes bandwidth and server resources. With 100k+ concurrent users, even 10-second polling intervals create 10k requests/sec of mostly empty responses. WebSockets eliminate this waste by pushing only when there's actual data.</p>
<p><strong>When it would be chosen:</strong> For a very small-scale CRM (<1000 concurrent users), polling is simpler to implement and doesn't require WebSocket infrastructure. It's also a viable fallback for environments where WebSockets are blocked.</p>
</div>

<!-- ============================================================ -->
<h2 id="additional">18. Additional Considerations</h2>
<!-- ============================================================ -->

<div class="card">
<h3>18.1 Security</h3>
<ul>
  <li><strong>Encryption at rest:</strong> All databases and object storage use AES-256 encryption at rest. Encryption keys are managed by a centralized key management service (KMS) with automatic key rotation every 90 days.</li>
  <li><strong>Encryption in transit:</strong> All communication uses TLS 1.3 — client to LB (HTTPS), inter-service (gRPC over TLS), service to database (TLS), WebSocket (WSS).</li>
  <li><strong>Authentication:</strong> OAuth 2.0 with JWT tokens. Tokens include <code>org_id</code>, <code>user_id</code>, <code>role</code>, and <code>permissions</code>. Short-lived access tokens (15 minutes) with refresh tokens (7 days).</li>
  <li><strong>API key authentication:</strong> For external API integrations (web forms, third-party apps). API keys are scoped to specific permissions and rate-limited separately.</li>
  <li><strong>Row-level security:</strong> Every database query is automatically scoped to the authenticated user's <code>org_id</code> by the data access layer. This is enforced at the ORM level, not at the application level, to prevent accidental omission.</li>
  <li><strong>Field-level security:</strong> The Metadata Service defines which fields each role can read/write. The API strips restricted fields from responses and rejects writes to restricted fields.</li>
  <li><strong>IP allowlisting:</strong> Enterprise tenants can restrict API access to specific IP ranges.</li>
  <li><strong>SSO:</strong> SAML 2.0 and OpenID Connect integration for enterprise single sign-on.</li>
</ul>

<h3>18.2 Observability</h3>
<ul>
  <li><strong>Distributed tracing:</strong> Every request gets a trace ID at the API Gateway, propagated through all services via gRPC metadata. This enables end-to-end latency analysis across the microservices.</li>
  <li><strong>Metrics:</strong> Each service exports metrics (request rate, error rate, latency percentiles, queue depth, cache hit rate) to a time-series database. Dashboards and alerts are configured for SLO monitoring.</li>
  <li><strong>Logging:</strong> Structured JSON logging with <code>org_id</code>, <code>user_id</code>, <code>request_id</code>, and <code>trace_id</code> in every log line. Logs are shipped to a centralized log aggregation system for search and analysis.</li>
  <li><strong>Alerting:</strong> Alerts on: error rate &gt; 1%, p95 latency &gt; 500ms, queue depth &gt; 10,000, cache hit rate &lt; 80%, disk usage &gt; 80%.</li>
</ul>

<h3>18.3 Data Migration &amp; Import</h3>
<ul>
  <li>New tenants onboarding from another CRM need bulk data import. This is handled asynchronously via the Message Queue (as described in Flow 1, Example 2).</li>
  <li>Imports are validated (schema, data types, duplicate detection) before insertion.</li>
  <li>Large imports (&gt;100k records) are processed in batches with progress tracking and the ability to pause/resume.</li>
</ul>

<h3>18.4 API Versioning</h3>
<ul>
  <li>The API uses URL-based versioning: <code>/api/v1/</code>, <code>/api/v2/</code>.</li>
  <li>Deprecated API versions are supported for 18 months after the next version is released, with deprecation headers in responses.</li>
  <li>Breaking changes (field removals, type changes) only happen in major versions. Non-breaking additions (new fields, new endpoints) can be added within a version.</li>
</ul>

<h3>18.5 Disaster Recovery</h3>
<ul>
  <li><strong>RPO (Recovery Point Objective):</strong> &lt;1 minute. SQL databases use synchronous replication within a region and asynchronous replication across regions.</li>
  <li><strong>RTO (Recovery Time Objective):</strong> &lt;15 minutes. Automated failover to read replicas promoted to primary. DNS-based traffic switching between regions.</li>
  <li><strong>Backup:</strong> Daily full backups + continuous WAL (Write-Ahead Log) archiving for SQL. NoSQL and Object Storage have built-in replication.</li>
  <li><strong>Chaos testing:</strong> Regular failure injection tests (kill a service instance, simulate network partition, DB failover drill) to validate recovery procedures.</li>
</ul>

<h3>18.6 Compliance</h3>
<ul>
  <li><strong>GDPR:</strong> Data deletion API (<code>DELETE /api/v1/contacts/{id}/gdpr</code>) permanently removes a contact's personal data from all stores (SQL, NoSQL, search index, object storage). This is a hard delete, not soft delete.</li>
  <li><strong>Data residency:</strong> Tenant data is stored in the region specified during onboarding. Cross-region data movement is prohibited without explicit tenant consent.</li>
  <li><strong>Audit log immutability:</strong> The NoSQL audit log is append-only. No API or internal process can modify or delete audit records.</li>
</ul>

<h3>18.7 Tenant Isolation &amp; Noisy Neighbor Prevention</h3>
<ul>
  <li><strong>Rate limiting per tenant</strong> (as described in Scaling section) prevents any single tenant from monopolizing API capacity.</li>
  <li><strong>Query timeout:</strong> Database queries are terminated after 30 seconds to prevent a single tenant's expensive query from degrading performance for co-tenants on the same shard.</li>
  <li><strong>Resource quotas:</strong> Each tenant has storage quotas (e.g., 10 GB for professional tier, unlimited for enterprise) and API quotas enforced at the API Gateway.</li>
  <li><strong>Background job isolation:</strong> Bulk operations (imports, exports) are processed in a separate worker pool from real-time operations to prevent bulk jobs from starving interactive requests.</li>
</ul>
</div>

<!-- ============================================================ -->
<h2 id="vendors">19. Vendor Section</h2>
<!-- ============================================================ -->

<div class="card">
<p>The architecture is designed to be vendor-agnostic. Below are potential vendors for each infrastructure component, with rationale for why they are suitable.</p>

<table>
<tr><th>Component</th><th>Vendor Options</th><th>Rationale</th></tr>
<tr>
  <td><strong>SQL Database</strong></td>
  <td>PostgreSQL, MySQL, Amazon Aurora, Google Cloud Spanner, CockroachDB</td>
  <td><strong>PostgreSQL</strong> is a strong default: mature, excellent JSONB support (for custom field values and settings), strong indexing (B-tree, GIN for JSON, full-text), ACID-compliant, and extensive ecosystem. <strong>Cloud Spanner / CockroachDB</strong> are alternatives if globally distributed strong consistency is needed without manual sharding — they handle sharding and replication automatically. <strong>Aurora</strong> offers managed PostgreSQL-compatible service with automatic storage scaling and fast read replicas.</td>
</tr>
<tr>
  <td><strong>NoSQL Database</strong></td>
  <td>Apache Cassandra, Amazon DynamoDB, Google Cloud Bigtable, ScyllaDB</td>
  <td><strong>Cassandra/ScyllaDB</strong> are ideal for the audit log and notifications: they support wide-column storage with partition + sort key access patterns, handle massive write throughput, and scale linearly by adding nodes. <strong>DynamoDB</strong> offers the same model with zero operational overhead (fully managed). <strong>Bigtable</strong> is similar with strong integration in GCP.</td>
</tr>
<tr>
  <td><strong>In-Memory Cache</strong></td>
  <td>Redis, Memcached, Amazon ElastiCache, Dragonfly</td>
  <td><strong>Redis</strong> is the top choice: supports rich data structures (hashes for metadata, sorted sets for leaderboards, pub/sub for WebSocket routing), built-in TTL expiration, LRU eviction, and clustering. <strong>Memcached</strong> is simpler (key-value only) and may be sufficient for caching CRM records but lacks the data structure versatility needed for the Connection Store and pub/sub channel. <strong>Dragonfly</strong> is a modern, Redis-compatible alternative with better multi-threaded performance.</td>
</tr>
<tr>
  <td><strong>Message Queue</strong></td>
  <td>Apache Kafka, Amazon SQS/SNS, RabbitMQ, Apache Pulsar, NATS</td>
  <td><strong>Kafka</strong> is ideal: supports topic-based partitioning (by <code>org_id</code>), consumer groups, message replay (7-day retention), at-least-once delivery, and scales to millions of messages/sec. Its log-based architecture fits our event-driven design. <strong>Pulsar</strong> is an alternative with multi-tenancy built-in. <strong>RabbitMQ</strong> is simpler but lacks Kafka's replay and partitioning capabilities. <strong>SQS/SNS</strong> is fully managed but has higher per-message latency.</td>
</tr>
<tr>
  <td><strong>Search Index</strong></td>
  <td>Elasticsearch, Apache Solr, OpenSearch, Typesense, Meilisearch</td>
  <td><strong>Elasticsearch/OpenSearch</strong> is the standard for full-text search at scale: distributed, supports inverted indexes, relevance scoring, aggregations, and tenant-scoped queries via index routing. <strong>Typesense/Meilisearch</strong> are simpler alternatives for smaller scale but lack the cluster management and advanced query capabilities needed for a multi-tenant CRM.</td>
</tr>
<tr>
  <td><strong>Object Storage</strong></td>
  <td>Amazon S3, Google Cloud Storage, Azure Blob Storage, MinIO</td>
  <td><strong>S3</strong> (or equivalent) is the industry standard: 99.999999999% durability, lifecycle policies for archival, pre-signed URLs for secure access, and integration with CDN for delivery. <strong>MinIO</strong> is an S3-compatible option for on-premise deployments.</td>
</tr>
<tr>
  <td><strong>CDN</strong></td>
  <td>Cloudflare, Amazon CloudFront, Fastly, Akamai</td>
  <td><strong>Cloudflare</strong> offers a global edge network with DDoS protection, WebSocket proxying, and edge caching — all important for a CRM. <strong>CloudFront</strong> integrates tightly with S3 for file delivery. <strong>Fastly</strong> offers advanced cache purging (instant invalidation) which is useful for frequently updated static assets.</td>
</tr>
<tr>
  <td><strong>Load Balancer</strong></td>
  <td>NGINX, HAProxy, AWS ALB/NLB, Envoy, Traefik</td>
  <td><strong>NGINX/Envoy</strong> for the external LB: both support HTTPS termination, WebSocket upgrade, and health checks. <strong>Envoy</strong> is also used as a sidecar proxy in the service mesh for internal load balancing with gRPC support, circuit breaking, and retry policies. <strong>AWS ALB</strong> is a managed alternative with native WebSocket and gRPC support.</td>
</tr>
<tr>
  <td><strong>Service Mesh / Discovery</strong></td>
  <td>Istio, Linkerd, Consul, Kubernetes Service Discovery</td>
  <td><strong>Istio</strong> (with Envoy sidecars) provides service discovery, mutual TLS, traffic management, and observability. <strong>Consul</strong> is lighter-weight for service discovery and health checking without the full mesh overhead. <strong>Kubernetes</strong> built-in service discovery is sufficient if running on K8s.</td>
</tr>
</table>
</div>

</body>
</html>
