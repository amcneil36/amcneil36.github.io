<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>System Design: LinkedIn</title>
    <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
    <style>
        :root {
            --primary: #0a66c2;
            --primary-dark: #004182;
            --bg: #f3f2ef;
            --card-bg: #ffffff;
            --text: #1d2226;
            --text-light: #666;
            --border: #e0dfdc;
            --accent-green: #057642;
            --accent-orange: #b24020;
            --accent-purple: #7c3aed;
            --code-bg: #1e1e1e;
        }
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, sans-serif;
            background: var(--bg);
            color: var(--text);
            line-height: 1.7;
        }
        .container {
            display: flex;
            max-width: 1400px;
            margin: 0 auto;
        }
        nav.sidebar {
            position: fixed;
            top: 0;
            left: 0;
            width: 280px;
            height: 100vh;
            overflow-y: auto;
            background: var(--card-bg);
            border-right: 1px solid var(--border);
            padding: 24px 16px;
            z-index: 100;
        }
        nav.sidebar h2 {
            color: var(--primary);
            font-size: 18px;
            margin-bottom: 16px;
            padding-bottom: 8px;
            border-bottom: 2px solid var(--primary);
        }
        nav.sidebar a {
            display: block;
            color: var(--text);
            text-decoration: none;
            padding: 5px 8px;
            font-size: 13px;
            border-radius: 4px;
            transition: background 0.2s;
        }
        nav.sidebar a:hover { background: var(--bg); }
        nav.sidebar a.indent { padding-left: 24px; color: var(--text-light); font-size: 12px; }
        main {
            margin-left: 280px;
            padding: 32px 48px;
            max-width: 1100px;
            width: 100%;
        }
        h1 {
            font-size: 36px;
            color: var(--primary-dark);
            margin-bottom: 8px;
        }
        h1 .subtitle {
            display: block;
            font-size: 16px;
            color: var(--text-light);
            font-weight: 400;
        }
        h2 {
            font-size: 26px;
            color: var(--primary-dark);
            margin-top: 48px;
            margin-bottom: 16px;
            padding-bottom: 8px;
            border-bottom: 2px solid var(--primary);
        }
        h3 {
            font-size: 20px;
            color: var(--text);
            margin-top: 28px;
            margin-bottom: 12px;
        }
        h4 {
            font-size: 16px;
            color: var(--primary);
            margin-top: 20px;
            margin-bottom: 8px;
        }
        p, li { margin-bottom: 8px; }
        ul, ol { padding-left: 24px; margin-bottom: 16px; }
        .card {
            background: var(--card-bg);
            border-radius: 8px;
            padding: 24px;
            margin: 16px 0;
            border: 1px solid var(--border);
            box-shadow: 0 1px 3px rgba(0,0,0,0.08);
        }
        .diagram-card {
            background: var(--card-bg);
            border-radius: 8px;
            padding: 24px;
            margin: 20px 0;
            border: 1px solid var(--border);
            box-shadow: 0 2px 8px rgba(0,0,0,0.06);
            overflow-x: auto;
        }
        .example-box {
            background: #eef6ff;
            border-left: 4px solid var(--primary);
            padding: 16px 20px;
            margin: 12px 0;
            border-radius: 0 8px 8px 0;
        }
        .example-box strong { color: var(--primary-dark); }
        .warn-box {
            background: #fff7ed;
            border-left: 4px solid var(--accent-orange);
            padding: 16px 20px;
            margin: 12px 0;
            border-radius: 0 8px 8px 0;
        }
        .info-box {
            background: #f0fdf4;
            border-left: 4px solid var(--accent-green);
            padding: 16px 20px;
            margin: 12px 0;
            border-radius: 0 8px 8px 0;
        }
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 16px 0;
            font-size: 14px;
        }
        th, td {
            padding: 10px 14px;
            text-align: left;
            border: 1px solid var(--border);
        }
        th {
            background: var(--primary);
            color: white;
            font-weight: 600;
        }
        tr:nth-child(even) { background: #f9f9f9; }
        code {
            background: #f0f0f0;
            padding: 2px 6px;
            border-radius: 4px;
            font-size: 13px;
            font-family: 'SF Mono', 'Fira Code', Consolas, monospace;
        }
        pre {
            background: var(--code-bg);
            color: #d4d4d4;
            padding: 16px;
            border-radius: 8px;
            overflow-x: auto;
            font-size: 13px;
            margin: 12px 0;
        }
        .tag {
            display: inline-block;
            padding: 2px 10px;
            border-radius: 12px;
            font-size: 12px;
            font-weight: 600;
            margin-right: 4px;
        }
        .tag-sql { background: #dbeafe; color: #1e40af; }
        .tag-nosql { background: #fce7f3; color: #9d174d; }
        .tag-graph { background: #ede9fe; color: #5b21b6; }
        .tag-cache { background: #fef3c7; color: #92400e; }
        .tag-http { background: #d1fae5; color: #065f46; }
        .tag-ws { background: #fef9c3; color: #854d0e; }
        .mermaid { text-align: center; margin: 16px 0; }
        .api-endpoint {
            background: #f8f8f8;
            border: 1px solid var(--border);
            border-radius: 6px;
            padding: 8px 14px;
            margin: 6px 0;
            font-family: 'SF Mono', Consolas, monospace;
            font-size: 13px;
        }
        .api-endpoint .method {
            font-weight: 700;
            margin-right: 8px;
            padding: 2px 8px;
            border-radius: 4px;
            color: white;
            font-size: 11px;
        }
        .method-get { background: #22c55e; }
        .method-post { background: #3b82f6; }
        .method-put { background: #f59e0b; }
        .method-patch { background: #a855f7; }
        .method-delete { background: #ef4444; }
        @media (max-width: 900px) {
            nav.sidebar { display: none; }
            main { margin-left: 0; padding: 16px; }
        }
    </style>
</head>
<body>

<nav class="sidebar">
    <h2>üìã Table of Contents</h2>
    <a href="#requirements">1. Requirements</a>
    <a href="#functional" class="indent">Functional</a>
    <a href="#non-functional" class="indent">Non-Functional</a>
    <a href="#capacity" class="indent">Capacity Estimates</a>
    <a href="#flow1">2. Flow 1 ‚Äî Profile Management</a>
    <a href="#flow2">3. Flow 2 ‚Äî Connection Management</a>
    <a href="#flow3">4. Flow 3 ‚Äî Post Creation</a>
    <a href="#flow4">5. Flow 4 ‚Äî Feed Viewing</a>
    <a href="#flow5">6. Flow 5 ‚Äî Messaging</a>
    <a href="#flow6">7. Flow 6 ‚Äî Job Posting & Application</a>
    <a href="#flow7">8. Flow 7 ‚Äî Notification Delivery</a>
    <a href="#flow8">9. Flow 8 ‚Äî Search</a>
    <a href="#combined">10. Combined Overall Diagram</a>
    <a href="#schema">11. Database Schema</a>
    <a href="#sql-tables" class="indent">SQL Tables</a>
    <a href="#nosql-tables" class="indent">NoSQL Tables</a>
    <a href="#graph-db" class="indent">Graph Database</a>
    <a href="#denormalization" class="indent">Denormalization</a>
    <a href="#indexes" class="indent">Indexes</a>
    <a href="#sharding" class="indent">Sharding</a>
    <a href="#cdn">12. CDN Deep Dive</a>
    <a href="#cache">13. Caching Deep Dive</a>
    <a href="#websocket">14. WebSocket Deep Dive</a>
    <a href="#mq">15. Message Queue Deep Dive</a>
    <a href="#lb">16. Load Balancer Deep Dive</a>
    <a href="#scaling">17. Scaling Considerations</a>
    <a href="#tradeoffs">18. Tradeoffs & Deep Dives</a>
    <a href="#alternatives">19. Alternative Approaches</a>
    <a href="#additional">20. Additional Information</a>
    <a href="#vendors">21. Vendor Section</a>
</nav>

<main>

<!-- ======================== HEADER ======================== -->
<h1>System Design: LinkedIn
    <span class="subtitle">Professional Networking Platform ‚Äî Comprehensive Architecture</span>
</h1>

<!-- ======================== REQUIREMENTS ======================== -->
<h2 id="requirements">1. Requirements</h2>

<h3 id="functional">Functional Requirements</h3>
<div class="card">
<ol>
    <li><strong>User Profiles:</strong> Users can create, view, and edit their professional profiles including work experience, education, skills, summary, and profile photo.</li>
    <li><strong>Connections:</strong> Users can send, accept, reject, and withdraw connection requests. Users can view their 1st, 2nd, and 3rd-degree connections.</li>
    <li><strong>Feed &amp; Posts:</strong> Users can create posts (text, images, articles, videos). Users can view a personalized, ranked feed of content from their network.</li>
    <li><strong>Engagement:</strong> Users can like, comment on, and share posts.</li>
    <li><strong>Messaging:</strong> Users can send and receive real-time direct messages to/from their connections.</li>
    <li><strong>Jobs:</strong> Companies can post job listings. Users can search for jobs and submit applications.</li>
    <li><strong>Notifications:</strong> Users receive notifications for connection requests, likes, comments, messages, job alerts, and profile views.</li>
    <li><strong>Search:</strong> Users can search for people, companies, jobs, and posts using keywords and filters.</li>
</ol>
</div>

<h3 id="non-functional">Non-Functional Requirements</h3>
<div class="card">
<ol>
    <li><strong>High Availability:</strong> 99.99% uptime ‚Äî the platform should be accessible at all times.</li>
    <li><strong>Low Latency:</strong> Feed loads in &lt;200ms, messages delivered in &lt;100ms, search results in &lt;300ms.</li>
    <li><strong>Scalability:</strong> Support 900M+ registered users, ~310M monthly active users, ~160M daily active users.</li>
    <li><strong>Eventual Consistency:</strong> Acceptable for feed, notifications, like/comment counts. Strong consistency required for messaging, connection status, and job applications.</li>
    <li><strong>Data Durability:</strong> Zero data loss for user profiles, messages, and job applications.</li>
    <li><strong>Fault Tolerance:</strong> Graceful degradation under partial failures ‚Äî e.g., feed may show slightly stale data if ranking service is down.</li>
    <li><strong>Security:</strong> End-to-end encryption for messages, OAuth 2.0 authentication, rate limiting, data privacy compliance (GDPR, CCPA).</li>
</ol>
</div>

<h3 id="capacity">Capacity Estimates</h3>
<div class="card">
<table>
    <tr><th>Metric</th><th>Estimate</th></tr>
    <tr><td>Registered users</td><td>~900M</td></tr>
    <tr><td>Monthly active users (MAU)</td><td>~310M</td></tr>
    <tr><td>Daily active users (DAU)</td><td>~160M</td></tr>
    <tr><td>Posts created per day</td><td>~3M</td></tr>
    <tr><td>Feed reads per day</td><td>~2B (each DAU loads feed ~12 times)</td></tr>
    <tr><td>Messages sent per day</td><td>~300M</td></tr>
    <tr><td>Connection requests per day</td><td>~50M</td></tr>
    <tr><td>Job postings active at any time</td><td>~15M</td></tr>
    <tr><td>Search queries per day</td><td>~500M</td></tr>
    <tr><td>Average connections per user</td><td>~500</td></tr>
    <tr><td>Read:Write ratio (feed)</td><td>~100:1</td></tr>
</table>
</div>

<!-- ======================== FLOW 1: PROFILE ======================== -->
<h2 id="flow1">2. Flow 1 ‚Äî Profile Management</h2>

<div class="diagram-card">
<div class="mermaid">
graph LR
    C["üë§ Client App"] -->|HTTP| LB["‚öñÔ∏è Load Balancer"]
    LB -->|HTTP| PS["üîß Profile Service"]
    PS -->|Read/Write| UDB[("üë• User DB\n(SQL)")]
    PS -->|Upload Media| OS["üì¶ Object Storage"]
    OS --> CDN["üåê CDN"]
    PS -->|Invalidate/Update| PC["‚ö° Profile Cache"]
    PS -->|Publish Event| MQ["üì® Message Queue"]
    MQ -->|Consume| SIS["üîç Search Index\nService"]
    SIS -->|Update| SI[("üìá Search Index")]
</div>
</div>

<h3>Examples</h3>

<div class="example-box">
<strong>Example 1 ‚Äî Creating a Profile:</strong> Jane signs up for LinkedIn and fills out her profile ‚Äî name, headline "Software Engineer at FAANG", summary, and uploads a profile photo. The client app sends an <code>HTTP POST</code> to the Profile Service via the Load Balancer. The Profile Service writes the new user record to the User DB (SQL), uploads the profile photo to Object Storage (which is then served via the CDN), writes the profile to the Profile Cache (write-through), and publishes a <code>profile_created</code> event to the Message Queue. The Search Index Service consumes the event and indexes Jane's profile so she appears in search results.
</div>

<div class="example-box">
<strong>Example 2 ‚Äî Editing a Profile:</strong> Jane gets promoted and updates her title to "Senior Software Engineer." The client sends an <code>HTTP PUT</code> to the Profile Service. The service updates the record in the User DB, updates the Profile Cache (write-through), and publishes a <code>profile_updated</code> event to the Message Queue. The Search Index Service re-indexes her profile. Jane's connections may later see a "Jane updated her profile" notification (handled by Flow 7).
</div>

<div class="example-box">
<strong>Example 3 ‚Äî Viewing a Profile:</strong> Bob visits Jane's profile. The client sends an <code>HTTP GET</code> to the Profile Service. The service first checks the Profile Cache ‚Äî if the profile is cached, it returns it immediately. If not, it reads from the User DB, populates the cache, and returns the data. The profile photo URL points to the CDN for fast delivery. A <code>profile_viewed</code> event is published to the Message Queue for generating a "Someone viewed your profile" notification.
</div>

<h3>Component Deep Dive</h3>

<div class="card">
<h4>Client App</h4>
<p>Web browser (React SPA) or native mobile app (iOS/Android). Communicates with backend exclusively over HTTPS. Handles local form validation, image compression before upload, and caching of recently viewed profiles.</p>

<h4>Load Balancer</h4>
<p>Layer 7 (HTTP/HTTPS) load balancer. Terminates TLS, routes requests to healthy Profile Service instances using round-robin or least-connections algorithm. Performs health checks every 10 seconds.</p>

<h4>Profile Service <span class="tag tag-http">HTTP REST</span></h4>
<p>Stateless microservice handling all profile CRUD operations. Protocol: HTTP/1.1 REST.</p>
<div class="api-endpoint"><span class="method method-post">POST</span> <code>/api/v1/users</code> ‚Äî Create profile. Input: <code>{name, email, headline, summary}</code>. Output: <code>{user_id, profile}</code></div>
<div class="api-endpoint"><span class="method method-get">GET</span> <code>/api/v1/users/{user_id}</code> ‚Äî Get profile. Input: <code>user_id (path)</code>. Output: <code>{profile object}</code></div>
<div class="api-endpoint"><span class="method method-put">PUT</span> <code>/api/v1/users/{user_id}</code> ‚Äî Update profile. Input: <code>{fields to update}</code>. Output: <code>{updated profile}</code></div>
<div class="api-endpoint"><span class="method method-post">POST</span> <code>/api/v1/users/{user_id}/experience</code> ‚Äî Add experience. Input: <code>{company, title, dates, description}</code>. Output: <code>{experience_id}</code></div>
<div class="api-endpoint"><span class="method method-post">POST</span> <code>/api/v1/users/{user_id}/education</code> ‚Äî Add education. Input: <code>{school, degree, field, dates}</code>. Output: <code>{education_id}</code></div>
<div class="api-endpoint"><span class="method method-post">POST</span> <code>/api/v1/users/{user_id}/photo</code> ‚Äî Upload photo. Input: <code>multipart/form-data image</code>. Output: <code>{photo_url}</code></div>

<h4>User DB <span class="tag tag-sql">SQL</span></h4>
<p>Relational database storing user profile data. SQL chosen for ACID transactions (ensuring profile updates are atomic), structured schema, and relational queries (joining users with experiences, education, skills). Details in the Schema section.</p>

<h4>Object Storage</h4>
<p>Blob storage for profile photos and uploaded media. Stores original and resized versions. Returns a URL that is served via the CDN.</p>

<h4>CDN</h4>
<p>Content Delivery Network caches and serves profile photos and static assets from edge locations close to users. Reduces latency and offloads origin traffic. Pull-based: content is cached on first request.</p>

<h4>Profile Cache <span class="tag tag-cache">Cache</span></h4>
<p>In-memory key-value cache storing serialized profile data. Strategy: write-through (updates go to cache and DB simultaneously). Eviction: LRU. TTL: 24 hours. See the Caching Deep Dive section for more.</p>

<h4>Message Queue</h4>
<p>Asynchronous message broker. Profile events (<code>profile_created</code>, <code>profile_updated</code>, <code>profile_viewed</code>) are published here for downstream consumers like the Search Index Service and Notification Service. See Message Queue Deep Dive.</p>

<h4>Search Index Service</h4>
<p>Consumes events from the Message Queue and updates the Search Index. Maintains an inverted index on profile fields (name, headline, skills, company). See Flow 8 (Search) for more detail.</p>
</div>

<!-- ======================== FLOW 2: CONNECTIONS ======================== -->
<h2 id="flow2">3. Flow 2 ‚Äî Connection Management</h2>

<div class="diagram-card">
<div class="mermaid">
graph LR
    C["üë§ Client App"] -->|HTTP| LB["‚öñÔ∏è Load Balancer"]
    LB -->|HTTP| CS["ü§ù Connection\nService"]
    CS -->|Read/Write| CDB[("üîó Connection DB\n(SQL)")]
    CS -->|Update Graph| GDB[("üï∏Ô∏è Graph DB")]
    CS -->|Invalidate/Update| CC["‚ö° Connection\nCache"]
    CS -->|Publish Event| MQ["üì® Message Queue"]
    MQ -->|Consume| NS["üîî Notification\nService"]
    CS -->|Check Profile| PC["‚ö° Profile Cache"]
</div>
</div>

<h3>Examples</h3>

<div class="example-box">
<strong>Example 1 ‚Äî Sending a Connection Request:</strong> Bob finds Jane's profile and clicks "Connect." The client sends an <code>HTTP POST</code> to the Connection Service via the Load Balancer. The service first validates that no existing connection or pending request exists (checks Connection DB), then inserts a new row with <code>status=pending</code> in the Connection DB. A <code>connection_requested</code> event is published to the Message Queue, which the Notification Service consumes to send Jane a push notification and in-app notification: "Bob wants to connect."
</div>

<div class="example-box">
<strong>Example 2 ‚Äî Accepting a Connection Request:</strong> Jane opens her notifications and clicks "Accept" on Bob's connection request. The client sends an <code>HTTP PATCH</code> to the Connection Service. The service updates the row in Connection DB to <code>status=accepted</code>, adds the bidirectional edge in the Graph DB (Bob‚ÜîJane), updates the Connection Cache (incrementing connection counts for both users), and publishes a <code>connection_accepted</code> event to the Message Queue. Bob later receives a notification: "Jane accepted your connection request."
</div>

<div class="example-box">
<strong>Example 3 ‚Äî Rejecting a Connection Request:</strong> Jane clicks "Ignore" on a connection request from a stranger. The client sends an <code>HTTP PATCH</code> to the Connection Service with <code>status=rejected</code>. The row is updated in Connection DB. No graph edge is added. No notification is sent to the requester (by design, to avoid awkwardness).
</div>

<div class="example-box">
<strong>Example 4 ‚Äî Viewing 2nd-Degree Connections (People You May Know):</strong> Bob opens the "My Network" page. The client sends an <code>HTTP GET</code> to the Connection Service for suggestions. The service queries the Graph DB to find 2nd-degree connections (friends of friends), filters out existing connections and pending requests, scores candidates by mutual connections, shared companies, and shared skills, and returns the top suggestions.
</div>

<h3>Component Deep Dive</h3>

<div class="card">
<h4>Connection Service <span class="tag tag-http">HTTP REST</span></h4>
<p>Handles connection lifecycle. Protocol: HTTP/1.1 REST.</p>
<div class="api-endpoint"><span class="method method-post">POST</span> <code>/api/v1/connections</code> ‚Äî Send request. Input: <code>{requester_id, receiver_id, message?}</code>. Output: <code>{connection_id, status: "pending"}</code></div>
<div class="api-endpoint"><span class="method method-patch">PATCH</span> <code>/api/v1/connections/{connection_id}</code> ‚Äî Accept/Reject. Input: <code>{status: "accepted"|"rejected"}</code>. Output: <code>{updated connection}</code></div>
<div class="api-endpoint"><span class="method method-get">GET</span> <code>/api/v1/users/{user_id}/connections</code> ‚Äî List connections. Input: <code>user_id, cursor, limit</code>. Output: <code>{connections[], next_cursor}</code></div>
<div class="api-endpoint"><span class="method method-get">GET</span> <code>/api/v1/users/{user_id}/suggestions</code> ‚Äî People You May Know. Input: <code>user_id, limit</code>. Output: <code>{suggestions[]}</code></div>
<div class="api-endpoint"><span class="method method-delete">DELETE</span> <code>/api/v1/connections/{connection_id}</code> ‚Äî Remove connection. Input: <code>connection_id</code>. Output: <code>{success}</code></div>

<h4>Connection DB <span class="tag tag-sql">SQL</span></h4>
<p>Stores connection request records with statuses. SQL chosen for ACID transactions ‚Äî crucial because connection state changes must be atomic (e.g., transitioning from <code>pending</code> to <code>accepted</code> must not result in duplicates or race conditions).</p>

<h4>Graph DB <span class="tag tag-graph">Graph</span></h4>
<p>Stores the social graph as nodes (users) and edges (connections). Optimized for graph traversal queries like "find all 2nd-degree connections" and "find mutual connections between Bob and Jane." These traversal queries would be extremely expensive in a relational database (requiring recursive joins), but graph databases handle them natively with O(1) per edge traversal.</p>

<h4>Connection Cache <span class="tag tag-cache">Cache</span></h4>
<p>Caches connection counts per user and recent connection lists. Updated on every connection status change (write-through). Used to display "500+ connections" on profile and to quickly check if two users are connected (for displaying "Connect" vs "Message" buttons). Eviction: LRU. TTL: 12 hours.</p>
</div>

<!-- ======================== FLOW 3: POST CREATION ======================== -->
<h2 id="flow3">4. Flow 3 ‚Äî Post Creation</h2>

<div class="diagram-card">
<div class="mermaid">
graph LR
    C["üë§ Client App"] -->|HTTP| LB["‚öñÔ∏è Load Balancer"]
    LB -->|HTTP| PostS["üìù Post Service"]
    PostS -->|Write| PDB[("üì∞ Post DB\n(NoSQL)")]
    PostS -->|Upload Media| OS["üì¶ Object Storage"]
    OS --> CDN["üåê CDN"]
    PostS -->|Publish| MQ["üì® Message Queue"]
    MQ -->|Consume| FOS["üì§ Fan-out Service"]
    FOS -->|Write| FDB[("üìã Feed DB\n(NoSQL)")]
    FOS -->|Write| FC["‚ö° Feed Cache"]
    MQ -->|Consume| NS["üîî Notification Service"]
    MQ -->|Consume| SIS["üîç Search Index\nService"]
</div>
</div>

<h3>Examples</h3>

<div class="example-box">
<strong>Example 1 ‚Äî Text Post:</strong> Jane writes "Excited to start my new role as Senior Engineer!" and clicks "Post." The client sends an <code>HTTP POST</code> to the Post Service via the Load Balancer. The service writes the post to the Post DB (NoSQL), publishes a <code>post_created</code> event to the Message Queue. The Fan-out Service consumes the event, looks up Jane's connections (~500 people), and writes a feed entry into each connection's feed in the Feed DB and Feed Cache. The Search Index Service indexes the post content. Connections who have notifications enabled for Jane receive a notification via the Notification Service.
</div>

<div class="example-box">
<strong>Example 2 ‚Äî Image Post:</strong> Bob shares a photo from a conference with a caption. The client first uploads the image via a pre-signed URL to Object Storage, receives back the media URL (served via CDN), then sends an <code>HTTP POST</code> to the Post Service with the text and media URL. The rest of the flow is identical to Example 1. The CDN ensures the image loads quickly for all of Bob's connections regardless of geography.
</div>

<div class="example-box">
<strong>Example 3 ‚Äî Influencer Post (Hybrid Fan-out Edge Case):</strong> A LinkedIn influencer with 5M followers writes a post. The Post Service writes to Post DB and publishes the event. However, the Fan-out Service detects this user has followers exceeding the fan-out threshold (e.g., >10,000 connections). Instead of writing 5M feed entries (which would be extremely expensive and slow), the service marks this post as "pull-at-read" in the Post DB. When followers load their feed (Flow 4), the Feed Service will pull this influencer's recent posts at read time and merge them with the precomputed feed. This is the <strong>hybrid fan-out</strong> strategy.
</div>

<div class="example-box">
<strong>Example 4 ‚Äî Liking a Post:</strong> Bob sees Jane's post in his feed and clicks the like button. The client sends an <code>HTTP POST</code> to <code>/api/v1/posts/{post_id}/likes</code>. The Post Service atomically increments the like count in the Post DB and writes a like record. A <code>post_liked</code> event is published to the Message Queue, which triggers a notification to Jane: "Bob liked your post."
</div>

<div class="example-box">
<strong>Example 5 ‚Äî Commenting on a Post:</strong> Alice comments "Congrats Jane!" on Jane's post. The client sends an <code>HTTP POST</code> to <code>/api/v1/posts/{post_id}/comments</code>. The Post Service writes the comment to the Post DB (as a sub-document or in a separate comments collection), increments the comment count, and publishes a <code>post_commented</code> event. Jane gets notified: "Alice commented on your post."
</div>

<h3>Component Deep Dive</h3>

<div class="card">
<h4>Post Service <span class="tag tag-http">HTTP REST</span></h4>
<p>Manages post lifecycle and engagement (likes, comments, shares). Protocol: HTTP/1.1 REST.</p>
<div class="api-endpoint"><span class="method method-post">POST</span> <code>/api/v1/posts</code> ‚Äî Create post. Input: <code>{author_id, content, media_urls[], post_type}</code>. Output: <code>{post_id, post}</code></div>
<div class="api-endpoint"><span class="method method-get">GET</span> <code>/api/v1/posts/{post_id}</code> ‚Äî Get post. Input: <code>post_id</code>. Output: <code>{post object with like/comment counts}</code></div>
<div class="api-endpoint"><span class="method method-delete">DELETE</span> <code>/api/v1/posts/{post_id}</code> ‚Äî Delete post. Input: <code>post_id</code>. Output: <code>{success}</code></div>
<div class="api-endpoint"><span class="method method-post">POST</span> <code>/api/v1/posts/{post_id}/likes</code> ‚Äî Like post. Input: <code>{user_id}</code>. Output: <code>{success, new_like_count}</code></div>
<div class="api-endpoint"><span class="method method-delete">DELETE</span> <code>/api/v1/posts/{post_id}/likes</code> ‚Äî Unlike. Input: <code>{user_id}</code>. Output: <code>{success, new_like_count}</code></div>
<div class="api-endpoint"><span class="method method-post">POST</span> <code>/api/v1/posts/{post_id}/comments</code> ‚Äî Comment. Input: <code>{user_id, content}</code>. Output: <code>{comment_id, comment}</code></div>
<div class="api-endpoint"><span class="method method-get">GET</span> <code>/api/v1/posts/{post_id}/comments?cursor={c}&limit={n}</code> ‚Äî List comments. Output: <code>{comments[], next_cursor}</code></div>

<h4>Post DB <span class="tag tag-nosql">NoSQL</span></h4>
<p>Stores post documents. NoSQL chosen for high write throughput (~3M posts/day plus millions of likes/comments), flexible schema (different post types ‚Äî text, image, video, article ‚Äî have different fields), and horizontal scalability. Documents include denormalized author info (name, photo URL) to avoid joins at read time.</p>

<h4>Fan-out Service</h4>
<p>Consumes <code>post_created</code> events from the Message Queue. For regular users (&lt;10K connections), performs <strong>fan-out on write</strong>: writes a feed entry to each connection's feed partition in the Feed DB and Feed Cache. For influencers (&gt;10K connections), skips fan-out and marks the post for <strong>pull-at-read</strong>. This is the core of the hybrid fan-out strategy that balances write cost vs. read latency.</p>

<h4>Feed DB <span class="tag tag-nosql">NoSQL</span></h4>
<p>Stores precomputed feed entries per user. Partition key: <code>user_id</code>, sort key: <code>created_at</code> (descending). Each entry is a lightweight pointer: <code>{post_id, author_id, created_at, post_snippet}</code>. NoSQL chosen for extreme read throughput (~2B feed reads/day) and key-value access pattern.</p>

<h4>Feed Cache <span class="tag tag-cache">Cache</span></h4>
<p>Caches the top N (~200) feed entries per user. Populated by the Fan-out Service on write and by the Feed Service on read (cache-aside for cold caches). Eviction: LRU with per-user size limit. TTL: 1 hour.</p>
</div>

<!-- ======================== FLOW 4: FEED VIEWING ======================== -->
<h2 id="flow4">5. Flow 4 ‚Äî Feed Viewing</h2>

<div class="diagram-card">
<div class="mermaid">
graph LR
    C["üë§ Client App"] -->|HTTP GET| LB["‚öñÔ∏è Load Balancer"]
    LB -->|HTTP| FS["üìñ Feed Service"]
    FS -->|Read| FC["‚ö° Feed Cache"]
    FS -->|Read if miss| FDB[("üìã Feed DB\n(NoSQL)")]
    FS -->|Pull influencer posts| PDB[("üì∞ Post DB\n(NoSQL)")]
    FS -->|Rank| RS["üìä Ranking Service"]
    RS -->|Query social proximity| GDB[("üï∏Ô∏è Graph DB")]
    FS -->|Hydrate author info| PC["‚ö° Profile Cache"]
    CDN["üåê CDN"] -->|Serve media| C
</div>
</div>

<h3>Examples</h3>

<div class="example-box">
<strong>Example 1 ‚Äî Standard Feed Load (Cache Hit):</strong> Bob opens LinkedIn and scrolls through his feed. The client sends an <code>HTTP GET</code> to <code>/api/v1/feed?cursor=&limit=20</code> via the Load Balancer. The Feed Service checks the Feed Cache for Bob's precomputed feed entries ‚Äî cache hit! It retrieves the top 20 entries, pulls any recent influencer posts from the Post DB (pull-at-read for influencers), merges them, sends the combined list to the Ranking Service for scoring (based on recency, engagement, social proximity from Graph DB, and Bob's past behavior), hydrates any missing author details from the Profile Cache, and returns the ranked feed to Bob. Media in posts is served via CDN.
</div>

<div class="example-box">
<strong>Example 2 ‚Äî Feed Load (Cache Miss / Cold Start):</strong> A user who hasn't opened LinkedIn in weeks loads their feed. The Feed Cache doesn't have their entries (expired or evicted). The Feed Service reads from the Feed DB instead, fetches precomputed entries, pulls influencer posts, ranks them, populates the Feed Cache for subsequent requests (cache-aside), and returns the ranked feed. This path has slightly higher latency (~100ms more) due to the DB read.
</div>

<div class="example-box">
<strong>Example 3 ‚Äî Infinite Scroll / Pagination:</strong> Bob scrolls to the bottom of the first 20 posts and the client auto-fetches the next page: <code>HTTP GET /api/v1/feed?cursor=eyJ0IjoiMjAyNS0wMS4..&limit=20</code>. The Feed Service uses the cursor (an opaque encoded timestamp) to fetch the next batch of entries, rank them, and return them. Cursor-based pagination ensures consistency even as new posts are being added in real-time.
</div>

<h3>Component Deep Dive</h3>

<div class="card">
<h4>Feed Service <span class="tag tag-http">HTTP REST</span></h4>
<p>Assembles personalized, ranked feeds. Stateless. Protocol: HTTP/1.1 REST.</p>
<div class="api-endpoint"><span class="method method-get">GET</span> <code>/api/v1/feed?cursor={cursor}&limit={limit}</code> ‚Äî Get feed. Input: <code>cursor (opaque string), limit (int, default 20)</code>. Output: <code>{posts[], next_cursor, has_more}</code></div>
<p>The Feed Service performs the following steps in order: (1) check Feed Cache, (2) fallback to Feed DB, (3) merge pull-at-read influencer posts, (4) call Ranking Service, (5) hydrate author details, (6) return.</p>

<h4>Ranking Service</h4>
<p>Internal microservice (called via gRPC for low-latency inter-service communication). Takes a list of candidate post entries and returns them scored and ranked. Ranking factors include: recency, engagement velocity (likes/comments in first hour), social proximity (1st vs 2nd degree from Graph DB), content type preference (does this user engage more with images or articles?), and creator affinity (how often has this user engaged with this author before?). Uses an ML model retrained periodically.</p>

<h4>Graph DB (for social proximity) <span class="tag tag-graph">Graph</span></h4>
<p>The Ranking Service queries the Graph DB to determine the degree of connection between the feed viewer and each post author. 1st-degree connections get a higher ranking boost than 2nd-degree.</p>
</div>

<!-- ======================== FLOW 5: MESSAGING ======================== -->
<h2 id="flow5">6. Flow 5 ‚Äî Messaging</h2>

<div class="diagram-card">
<div class="mermaid">
graph LR
    CA["üë§ Client A\n(Sender)"] <-->|"WebSocket"| WSS1["üîå WebSocket\nServer 1"]
    WSS1 -->|HTTP| MS["üí¨ Message Service"]
    MS -->|Write| MDB[("üí¨ Message DB\n(NoSQL)")]
    MS -->|Lookup recipient WS| CM["üó∫Ô∏è Connection\nManager\n(Cache)"]
    CM -->|Route| WSS2["üîå WebSocket\nServer 2"]
    WSS2 <-->|"WebSocket"| CB["üë§ Client B\n(Receiver)"]
    MS -->|Publish| MQ["üì® Message Queue"]
    MQ -->|Consume| NS["üîî Notification\nService"]
</div>
</div>

<h3>Examples</h3>

<div class="example-box">
<strong>Example 1 ‚Äî Real-time Message (Both Users Online):</strong> Alice opens a conversation with Bob and types "Hey Bob, saw your post ‚Äî congrats!" and hits send. Alice's client sends the message over her existing WebSocket connection to WebSocket Server 1. The server forwards it to the Message Service, which persists it to the Message DB with <code>conversation_id</code>, <code>sender_id=Alice</code>, <code>content</code>, and <code>created_at</code>. The Message Service then queries the Connection Manager (cache) to look up Bob's active WebSocket connection and finds he's connected to WebSocket Server 2. The Message Service routes the message to WebSocket Server 2, which pushes it to Bob's client in real-time. Bob sees the message appear instantly. A delivery receipt is sent back through the same path.
</div>

<div class="example-box">
<strong>Example 2 ‚Äî Offline Message (Recipient Offline):</strong> Alice sends a message to Carol, but Carol is not currently online. The Message Service persists the message to the Message DB and queries the Connection Manager ‚Äî Carol has no active WebSocket connection. The service publishes a <code>message_received</code> event to the Message Queue. The Notification Service consumes it and sends a push notification to Carol's mobile device (via APNs for iOS or FCM for Android). When Carol later opens LinkedIn, her client establishes a WebSocket connection, and the client fetches missed messages via <code>HTTP GET /api/v1/conversations/{id}/messages?since={last_seen_timestamp}</code>.
</div>

<div class="example-box">
<strong>Example 3 ‚Äî Fetching Message History:</strong> Alice opens an old conversation with Bob and scrolls up to see past messages. The client sends an <code>HTTP GET</code> to the Message Service with cursor-based pagination. The service reads from the Message DB (partitioned by <code>conversation_id</code>, sorted by <code>created_at</code> descending) and returns the messages page by page. Recent messages may also be served from a message cache layer.
</div>

<h3>Component Deep Dive</h3>

<div class="card">
<h4>WebSocket Servers <span class="tag tag-ws">WebSocket</span></h4>
<p>Maintain persistent, bidirectional TCP connections with clients. Each server handles thousands of concurrent WebSocket connections. Connection establishment: Client initiates an HTTP upgrade request (<code>GET /ws</code> with <code>Upgrade: websocket</code> header). After the handshake, the connection is upgraded to WebSocket (RFC 6455) over TCP. Heartbeat pings are sent every 30 seconds to detect dead connections. See the WebSocket Deep Dive section for more.</p>

<h4>Message Service <span class="tag tag-http">HTTP REST</span> + Internal</h4>
<p>Processes incoming messages and handles conversation management. Receives messages from WebSocket Servers via internal HTTP calls, persists to DB, and routes to recipient.</p>
<div class="api-endpoint"><span class="method method-get">GET</span> <code>/api/v1/conversations</code> ‚Äî List conversations. Input: <code>user_id, cursor, limit</code>. Output: <code>{conversations[] with last_message preview}</code></div>
<div class="api-endpoint"><span class="method method-get">GET</span> <code>/api/v1/conversations/{conversation_id}/messages</code> ‚Äî Message history. Input: <code>cursor, limit</code>. Output: <code>{messages[], next_cursor}</code></div>
<div class="api-endpoint"><span class="method method-patch">PATCH</span> <code>/api/v1/messages/{message_id}/read</code> ‚Äî Mark as read. Input: <code>message_id</code>. Output: <code>{success}</code></div>

<h4>Connection Manager <span class="tag tag-cache">Cache</span></h4>
<p>In-memory distributed cache that maps <code>user_id ‚Üí {websocket_server_id, connection_id}</code>. When a user establishes a WebSocket connection, the WebSocket Server registers the mapping in the Connection Manager. When the connection drops, the mapping is removed. This enables the Message Service to route messages to the correct WebSocket Server without broadcasting to all servers. Uses a distributed hash table for high availability.</p>

<h4>Message DB <span class="tag tag-nosql">NoSQL</span></h4>
<p>Stores message documents. Partition key: <code>conversation_id</code>, sort key: <code>message_id</code> (time-ordered). NoSQL chosen for high write throughput (~300M messages/day), time-series access pattern (messages are almost always read in chronological order within a conversation), and horizontal scalability. Messages include <code>sender_id</code>, <code>content</code>, <code>created_at</code>, <code>read_at</code>, and <code>message_type</code> (text, image, file).</p>
</div>

<!-- ======================== FLOW 6: JOBS ======================== -->
<h2 id="flow6">7. Flow 6 ‚Äî Job Posting &amp; Application</h2>

<div class="diagram-card">
<div class="mermaid">
graph LR
    CO["üè¢ Company Admin"] -->|HTTP POST| LB["‚öñÔ∏è Load Balancer"]
    LB -->|HTTP| JS["üíº Job Service"]
    JS -->|Write| JDB[("üíº Job DB\n(SQL)")]
    JS -->|Publish| MQ["üì® Message Queue"]
    MQ -->|Consume| SIS["üîç Search Index\nService"]
    SIS -->|Update| SI[("üìá Search Index")]
    MQ -->|Consume| NS["üîî Notification\nService"]

    U["üë§ Applicant"] -->|HTTP POST| LB2["‚öñÔ∏è Load Balancer"]
    LB2 -->|HTTP| AS["üìÑ Application\nService"]
    AS -->|Write| ADB[("üìÑ Application DB\n(SQL)")]
    AS -->|Upload Resume| OS["üì¶ Object Storage"]
    AS -->|Publish| MQ2["üì® Message Queue"]
    MQ2 -->|Consume| NS
</div>
</div>

<h3>Examples</h3>

<div class="example-box">
<strong>Example 1 ‚Äî Posting a Job:</strong> A recruiter at Acme Corp creates a job listing for "Senior Backend Engineer" with description, requirements, location (San Francisco), and salary range. The client sends an <code>HTTP POST</code> to the Job Service. The service writes the job to the Job DB (SQL), publishes a <code>job_created</code> event to the Message Queue. The Search Index Service indexes the job so it appears in search results. The Notification Service identifies users who have job alerts matching "Backend Engineer in San Francisco" and sends them notifications: "New job matching your alert at Acme Corp."
</div>

<div class="example-box">
<strong>Example 2 ‚Äî Searching for Jobs:</strong> Jane searches "Machine Learning Engineer Remote." The client sends an <code>HTTP GET</code> to <code>/api/v1/jobs/search?q=Machine+Learning+Engineer&location=Remote</code>. The Job Service queries the Search Index, which performs a full-text search on job titles and descriptions, filters by location, ranks by relevance and recency, and returns paginated results with job previews (title, company, location, posted date).
</div>

<div class="example-box">
<strong>Example 3 ‚Äî Applying to a Job:</strong> Jane finds the perfect job and clicks "Easy Apply." She uploads her resume (which goes to Object Storage) and submits the application. The client sends an <code>HTTP POST</code> to the Application Service. The service writes the application record to the Application DB (SQL) with <code>status=submitted</code>, associates the resume URL, and publishes a <code>job_application_submitted</code> event. The recruiter at Acme Corp receives a notification: "Jane applied for Senior Backend Engineer."
</div>

<div class="example-box">
<strong>Example 4 ‚Äî Duplicate Application Prevention:</strong> Jane accidentally clicks "Apply" again on the same job. The Application Service checks the Application DB for an existing record with <code>(job_id, user_id)</code> ‚Äî finds a duplicate ‚Äî and returns a <code>409 Conflict</code> response: "You have already applied to this position." The unique composite index on <code>(job_id, user_id)</code> enforces this at the database level.
</div>

<h3>Component Deep Dive</h3>

<div class="card">
<h4>Job Service <span class="tag tag-http">HTTP REST</span></h4>
<div class="api-endpoint"><span class="method method-post">POST</span> <code>/api/v1/jobs</code> ‚Äî Create job. Input: <code>{company_id, title, description, location, salary_range, requirements}</code>. Output: <code>{job_id, job}</code></div>
<div class="api-endpoint"><span class="method method-get">GET</span> <code>/api/v1/jobs/{job_id}</code> ‚Äî Get job details. Output: <code>{job with application_count}</code></div>
<div class="api-endpoint"><span class="method method-get">GET</span> <code>/api/v1/jobs/search?q={query}&location={loc}&cursor={c}&limit={n}</code> ‚Äî Search jobs. Output: <code>{jobs[], next_cursor}</code></div>
<div class="api-endpoint"><span class="method method-put">PUT</span> <code>/api/v1/jobs/{job_id}</code> ‚Äî Update job. Input: <code>{fields to update}</code>. Output: <code>{updated job}</code></div>
<div class="api-endpoint"><span class="method method-patch">PATCH</span> <code>/api/v1/jobs/{job_id}/close</code> ‚Äî Close job listing. Output: <code>{success}</code></div>

<h4>Application Service <span class="tag tag-http">HTTP REST</span></h4>
<div class="api-endpoint"><span class="method method-post">POST</span> <code>/api/v1/jobs/{job_id}/apply</code> ‚Äî Apply. Input: <code>{user_id, resume (file), cover_letter?}</code>. Output: <code>{application_id, status: "submitted"}</code></div>
<div class="api-endpoint"><span class="method method-get">GET</span> <code>/api/v1/users/{user_id}/applications</code> ‚Äî My applications. Output: <code>{applications[] with job preview and status}</code></div>

<h4>Job DB <span class="tag tag-sql">SQL</span></h4>
<p>SQL chosen because job postings have a well-defined, structured schema, require relational queries (join with companies table), and benefit from ACID transactions (closing a job listing should atomically update status and stop accepting applications).</p>

<h4>Application DB <span class="tag tag-sql">SQL</span></h4>
<p>SQL chosen for ACID transactions on application state transitions (submitted ‚Üí reviewed ‚Üí interview ‚Üí offer ‚Üí hired/rejected) and to enforce the unique constraint on <code>(job_id, user_id)</code> preventing duplicate applications.</p>
</div>

<!-- ======================== FLOW 7: NOTIFICATIONS ======================== -->
<h2 id="flow7">8. Flow 7 ‚Äî Notification Delivery</h2>

<div class="diagram-card">
<div class="mermaid">
graph LR
    MQ["üì® Message Queue\n(Events from\nall services)"] -->|Consume| NS["üîî Notification\nService"]
    NS -->|Write| NDB[("üîî Notification DB\n(NoSQL)")]
    NS -->|Real-time push| WSS["üîå WebSocket\nServer"]
    WSS <-->|WebSocket| C["üë§ Client App"]
    NS -->|Mobile push| PNS["üì± Push Gateway\n(APNs / FCM)"]
    PNS --> MOBILE["üì± Mobile Device"]

    C2["üë§ Client App\n(Notification Feed)"] -->|HTTP GET| LB["‚öñÔ∏è Load Balancer"]
    LB -->|HTTP| NRS["üîî Notification\nRead Service"]
    NRS -->|Read| NC["‚ö° Notification\nCache"]
    NRS -->|Read if miss| NDB
</div>
</div>

<h3>Examples</h3>

<div class="example-box">
<strong>Example 1 ‚Äî Connection Request Notification:</strong> Bob sends Jane a connection request (Flow 2). The Connection Service publishes a <code>connection_requested</code> event to the Message Queue. The Notification Service consumes the event, creates a notification record in the Notification DB: <code>{user_id: Jane, type: "connection_request", source_user: Bob, entity_id: connection_id}</code>. If Jane is online (has active WebSocket), the notification is pushed in real-time via the WebSocket Server ‚Äî Jane sees a red badge on the notification bell. If Jane is offline, a push notification is sent via APNs/FCM to her phone: "Bob wants to connect with you."
</div>

<div class="example-box">
<strong>Example 2 ‚Äî Post Like Notification:</strong> Bob likes Jane's post (Flow 3). A <code>post_liked</code> event is consumed by the Notification Service. It creates a notification and, to avoid notification spam, applies <strong>aggregation logic</strong>: if multiple people liked Jane's post within a short window, the notifications are batched into "Bob and 4 others liked your post" instead of 5 separate notifications.
</div>

<div class="example-box">
<strong>Example 3 ‚Äî Viewing Notification Feed:</strong> Jane clicks the notification bell icon. The client sends an <code>HTTP GET</code> to <code>/api/v1/notifications?cursor=&limit=20</code>. The Notification Read Service checks the Notification Cache first (returns cached notifications if available), otherwise reads from the Notification DB (partitioned by <code>user_id</code>, sorted by <code>created_at</code> descending). Jane sees a list: "Bob liked your post," "Carol accepted your connection request," "New job matching your alert at Acme Corp."
</div>

<div class="example-box">
<strong>Example 4 ‚Äî Marking Notifications as Read:</strong> Jane taps on a notification. The client sends an <code>HTTP PATCH</code> to <code>/api/v1/notifications/{notification_id}</code> with <code>{read: true}</code>. The Notification Read Service updates the record in the Notification DB and invalidates the cached notification. The unread count badge decrements.
</div>

<h3>Component Deep Dive</h3>

<div class="card">
<h4>Notification Service (Consumer)</h4>
<p>Consumes events from the Message Queue. Responsible for: (1) creating notification records, (2) aggregation/batching logic, (3) delivery routing (in-app via WebSocket, mobile push via APNs/FCM), (4) respecting user notification preferences (e.g., "Don't notify me about post likes"). Runs as multiple consumer instances for high throughput.</p>

<h4>Notification Read Service <span class="tag tag-http">HTTP REST</span></h4>
<div class="api-endpoint"><span class="method method-get">GET</span> <code>/api/v1/notifications?cursor={c}&limit={n}</code> ‚Äî Get notifications. Output: <code>{notifications[], unread_count, next_cursor}</code></div>
<div class="api-endpoint"><span class="method method-patch">PATCH</span> <code>/api/v1/notifications/{notification_id}</code> ‚Äî Mark read. Input: <code>{read: true}</code>. Output: <code>{success}</code></div>
<div class="api-endpoint"><span class="method method-patch">PATCH</span> <code>/api/v1/notifications/mark-all-read</code> ‚Äî Mark all read. Output: <code>{success}</code></div>

<h4>Notification DB <span class="tag tag-nosql">NoSQL</span></h4>
<p>Stores notification documents. Partition key: <code>user_id</code>, sort key: <code>created_at</code> (descending). NoSQL chosen for high write throughput (every like, comment, connection request, job alert generates notifications) and simple key-value access pattern (always queried by user_id sorted by recency).</p>

<h4>Notification Cache <span class="tag tag-cache">Cache</span></h4>
<p>Caches the most recent ~50 notifications per user plus unread count. Populated on first read (cache-aside) and updated when new notifications arrive (write-through from Notification Service). Eviction: LRU. TTL: 6 hours.</p>

<h4>Push Gateway</h4>
<p>Abstracts APNs (Apple Push Notification service) and FCM (Firebase Cloud Messaging) for mobile push. Handles device token management, platform-specific payload formatting, and retry logic for transient failures.</p>
</div>

<!-- ======================== FLOW 8: SEARCH ======================== -->
<h2 id="flow8">9. Flow 8 ‚Äî Search</h2>

<div class="diagram-card">
<div class="mermaid">
graph LR
    C["üë§ Client App"] -->|HTTP GET| LB["‚öñÔ∏è Load Balancer"]
    LB -->|HTTP| SS["üîç Search Service"]
    SS -->|Query| SI[("üìá Search Index\n(Inverted Index)")]
    SS -->|Social ranking| GDB[("üï∏Ô∏è Graph DB")]
    SS -->|Hydrate results| PC["‚ö° Profile Cache"]
    SS -->|Hydrate results| JC["‚ö° Job Cache"]

    SIS["üîç Search Index\nService"] -->|Consume events| MQ["üì® Message Queue"]
    SIS -->|Update index| SI
</div>
</div>

<h3>Examples</h3>

<div class="example-box">
<strong>Example 1 ‚Äî People Search:</strong> Bob types "Jane Doe Software Engineer" in the search bar and selects "People" filter. The client sends an <code>HTTP GET</code> to <code>/api/v1/search?q=Jane+Doe+Software+Engineer&type=people</code>. The Search Service queries the Search Index (inverted index on name, headline, skills, company fields). Results are ranked by: text relevance (TF-IDF / BM25 score), social proximity (1st-degree connections ranked highest, queried from Graph DB), mutual connections count, and profile completeness. The top results are hydrated with full profile data from the Profile Cache and returned. Bob sees "Jane Doe ‚Äî Senior Software Engineer at Acme Corp ‚Äî 12 mutual connections" at the top.
</div>

<div class="example-box">
<strong>Example 2 ‚Äî Job Search:</strong> Jane searches "Remote Machine Learning Engineer $150k+" with location filter "Remote." The Search Service queries the Search Index on job fields (title, description, company), applies filters (location=Remote, salary ‚â• 150k), ranks by relevance and recency, and returns paginated results.
</div>

<div class="example-box">
<strong>Example 3 ‚Äî Typeahead / Autocomplete:</strong> As Bob types "Jan" in the search bar, the client sends an <code>HTTP GET</code> to <code>/api/v1/search/typeahead?prefix=Jan&limit=5</code>. The Search Service queries a prefix trie or n-gram index for fast prefix matching. Returns suggestions: "Jane Doe," "January Conference 2025," "Jan Smith." This must return in &lt;50ms for a responsive UX.
</div>

<h3>Component Deep Dive</h3>

<div class="card">
<h4>Search Service <span class="tag tag-http">HTTP REST</span></h4>
<div class="api-endpoint"><span class="method method-get">GET</span> <code>/api/v1/search?q={query}&type={people|jobs|posts|companies}&cursor={c}&limit={n}</code> ‚Äî Unified search. Input: query string, type filter, pagination. Output: <code>{results[], next_cursor, total_count}</code></div>
<div class="api-endpoint"><span class="method method-get">GET</span> <code>/api/v1/search/typeahead?prefix={p}&limit={n}</code> ‚Äî Autocomplete. Output: <code>{suggestions[]}</code></div>
<p>The Search Service is stateless and handles: query parsing, search index query construction, result ranking (combining text relevance with social signals), result hydration (enriching index results with full objects from caches), and pagination.</p>

<h4>Search Index</h4>
<p>Distributed inverted index supporting full-text search across multiple entity types. Maintains separate indexes for: people (name, headline, skills, company, location), jobs (title, description, company, location), posts (content, hashtags), and companies (name, industry, description). Updated asynchronously by the Search Index Service consuming events from the Message Queue. Supports BM25 scoring, faceted search, and prefix matching for typeahead.</p>

<h4>Search Index Service</h4>
<p>Consumer that listens to the Message Queue for events: <code>profile_created</code>, <code>profile_updated</code>, <code>job_created</code>, <code>job_updated</code>, <code>post_created</code>, <code>company_updated</code>. Transforms the event data into index documents and upserts them into the Search Index. Handles deletions (e.g., when a job is closed). Near-real-time indexing with a typical delay of &lt;5 seconds.</p>
</div>

<!-- ======================== COMBINED DIAGRAM ======================== -->
<h2 id="combined">10. Combined Overall Diagram</h2>

<div class="diagram-card">
<div class="mermaid">
graph TB
    subgraph Clients
        WEB["üåê Web App"]
        IOS["üì± iOS App"]
        AND["üì± Android App"]
    end

    subgraph Edge
        CDN["üåê CDN\n(Static assets, media)"]
        LB["‚öñÔ∏è Load Balancer\n/ API Gateway"]
        WSLB["‚öñÔ∏è WebSocket\nLoad Balancer"]
    end

    subgraph Services
        PS["üîß Profile\nService"]
        CS["ü§ù Connection\nService"]
        PostS["üìù Post\nService"]
        FS["üìñ Feed\nService"]
        MS["üí¨ Message\nService"]
        JS["üíº Job\nService"]
        AS["üìÑ Application\nService"]
        SS["üîç Search\nService"]
        NRS["üîî Notification\nRead Service"]
        RS["üìä Ranking\nService"]
    end

    subgraph AsyncWorkers["Async Workers"]
        FOS["üì§ Fan-out\nService"]
        NS["üîî Notification\nService"]
        SIS["üîç Search Index\nService"]
    end

    subgraph RealTime["Real-Time Layer"]
        WSS["üîå WebSocket\nServers"]
        CM["üó∫Ô∏è Connection\nManager"]
    end

    subgraph DataStores["Data Stores"]
        UDB[("üë• User DB\nSQL")]
        CDB[("üîó Connection DB\nSQL")]
        JDB[("üíº Job DB\nSQL")]
        ADB[("üìÑ Application DB\nSQL")]
        PDB[("üì∞ Post DB\nNoSQL")]
        FDB[("üìã Feed DB\nNoSQL")]
        MDB[("üí¨ Message DB\nNoSQL")]
        NDB[("üîî Notification DB\nNoSQL")]
        GDB[("üï∏Ô∏è Graph DB")]
        SI[("üìá Search Index")]
    end

    subgraph Caching["Cache Layer"]
        CACHE["‚ö° In-Memory Cache\n(Profile, Feed,\nConnection, Notification)"]
    end

    subgraph Storage
        OS["üì¶ Object Storage"]
        MQ["üì® Message Queue"]
    end

    subgraph External["External Services"]
        PNS["üì± Push Gateway\n(APNs / FCM)"]
    end

    WEB & IOS & AND --> CDN
    WEB & IOS & AND --> LB
    WEB & IOS & AND <-->|WebSocket| WSLB

    LB --> PS & CS & PostS & FS & JS & AS & SS & NRS
    WSLB --> WSS

    PS --> UDB & OS & CACHE & MQ
    CS --> CDB & GDB & CACHE & MQ
    PostS --> PDB & OS & MQ
    FS --> CACHE & FDB & PDB & RS
    RS --> GDB
    MS --> MDB & CM & MQ
    JS --> JDB & MQ
    AS --> ADB & OS & MQ
    SS --> SI & GDB & CACHE
    NRS --> NDB & CACHE

    WSS --> MS & CM
    NS --> WSS & PNS

    MQ --> FOS & NS & SIS
    FOS --> FDB & CACHE
    SIS --> SI
    NS --> NDB

    OS --> CDN
</div>
</div>

<h3>Combined Flow Examples</h3>

<div class="example-box">
<strong>Example 1 ‚Äî Full Post-to-Notification Flow:</strong> Jane opens LinkedIn on her iPhone (connects via WebSocket LB to a WebSocket Server and registers in Connection Manager). She creates a post: "Thrilled to share I've been promoted!" with a photo. The HTTP POST hits the Load Balancer ‚Üí Post Service ‚Üí writes to Post DB, uploads photo to Object Storage (served via CDN). A <code>post_created</code> event enters the Message Queue. Three consumers act in parallel: (1) Fan-out Service pushes feed entries to Jane's 500 connections in the Feed DB + Feed Cache; (2) Search Index Service indexes the post; (3) Notification Service creates notifications for those who opted in. Bob, who is online, receives a real-time notification via his WebSocket connection: "Jane shared a post." He clicks it, triggering an HTTP GET for the full post. Alice, who is offline, gets a mobile push notification via APNs. She opens the app later, establishes a WebSocket connection, loads her feed (Feed Service reads from Feed Cache, merges influencer posts, ranks via Ranking Service using social proximity from Graph DB), and sees Jane's post in her feed.
</div>

<div class="example-box">
<strong>Example 2 ‚Äî Full Job Search & Apply Flow:</strong> A recruiter at Acme Corp posts a "Staff ML Engineer" role via HTTP POST ‚Üí Load Balancer ‚Üí Job Service ‚Üí Job DB. The event flows through the Message Queue to the Search Index Service (indexes the job) and Notification Service (alerts matching users). Jane receives a job alert notification via WebSocket push. She clicks it, opening the job detail page (HTTP GET ‚Üí Job Service ‚Üí Job DB). She clicks "Easy Apply," uploads her resume to Object Storage, and submits (HTTP POST ‚Üí Application Service ‚Üí Application DB). A <code>job_application_submitted</code> event goes to the Message Queue ‚Üí Notification Service ‚Üí the recruiter gets a notification: "Jane applied for Staff ML Engineer." The recruiter reviews applications via a dashboard (HTTP GET ‚Üí Application Service ‚Üí Application DB with joins to User DB for applicant profiles).
</div>

<div class="example-box">
<strong>Example 3 ‚Äî Full Messaging Flow:</strong> Bob searches for "Jane Doe" (HTTP GET ‚Üí Search Service ‚Üí Search Index ranked by social proximity from Graph DB). He clicks her profile (HTTP GET ‚Üí Profile Service ‚Üí Profile Cache hit ‚Üí returns profile with CDN-served photo). He clicks "Message" and types "Hi Jane!" The message travels over WebSocket to WebSocket Server 1 ‚Üí Message Service ‚Üí persisted in Message DB ‚Üí Connection Manager lookup finds Jane on WebSocket Server 3 ‚Üí message routed and delivered in real-time. Jane replies "Hey Bob!" ‚Äî same path in reverse. Both see messages appear instantly with &lt;100ms latency.
</div>

<!-- ======================== SCHEMA ======================== -->
<h2 id="schema">11. Database Schema Design</h2>

<h3 id="sql-tables">SQL Tables</h3>

<div class="card">
<h4>Users Table</h4>
<span class="tag tag-sql">SQL</span>
<table>
    <tr><th>Column</th><th>Type</th><th>Key</th><th>Description</th></tr>
    <tr><td>user_id</td><td>UUID</td><td>üîë Primary Key</td><td>Unique user identifier</td></tr>
    <tr><td>email</td><td>VARCHAR(255)</td><td>Unique Index</td><td>Login email</td></tr>
    <tr><td>password_hash</td><td>VARCHAR(255)</td><td></td><td>Bcrypt hashed password</td></tr>
    <tr><td>name</td><td>VARCHAR(100)</td><td></td><td>Display name</td></tr>
    <tr><td>headline</td><td>VARCHAR(255)</td><td></td><td>"Software Engineer at Acme"</td></tr>
    <tr><td>summary</td><td>TEXT</td><td></td><td>Profile summary / about</td></tr>
    <tr><td>profile_photo_url</td><td>VARCHAR(500)</td><td></td><td>CDN URL for profile photo</td></tr>
    <tr><td>location</td><td>VARCHAR(100)</td><td></td><td>City, Country</td></tr>
    <tr><td>industry</td><td>VARCHAR(100)</td><td></td><td>Technology, Finance, etc.</td></tr>
    <tr><td>connection_count</td><td>INT</td><td></td><td>Denormalized connection count</td></tr>
    <tr><td>created_at</td><td>TIMESTAMP</td><td></td><td>Account creation time</td></tr>
    <tr><td>updated_at</td><td>TIMESTAMP</td><td></td><td>Last profile update</td></tr>
</table>
<p><strong>Why SQL:</strong> User profile data is highly structured with a fixed schema. ACID transactions ensure that profile updates (e.g., changing email + name simultaneously) are atomic. Relational queries are needed to join users with experiences, education, and skills. User profiles are a core entity referenced by nearly every other table via foreign keys.</p>
<p><strong>Read events:</strong> When any user views a profile, when search results are hydrated, when connection suggestions are generated, when messages are displayed (sender name/photo).</p>
<p><strong>Write events:</strong> When a user creates an account, edits their profile, uploads a new photo, or when connection_count is updated (on connection accept/remove).</p>

<h4>Indexes on Users Table</h4>
<ul>
    <li><strong><code>email</code> ‚Äî Hash Index:</strong> Exact-match lookups during login authentication. Hash index provides O(1) lookup time for equality queries. Used every time a user logs in.</li>
    <li><strong><code>name, headline, summary</code> ‚Äî Inverted Index (Full-Text):</strong> Powers the people search functionality. An inverted index maps each word/token to the list of user_ids containing that word, enabling fast full-text search with BM25 scoring. This is maintained in the Search Index rather than in the SQL DB itself.</li>
    <li><strong><code>location, industry</code> ‚Äî B-Tree Composite Index:</strong> Supports filtered searches like "Engineers in San Francisco" with range and equality queries.</li>
</ul>

<h4>Sharding Strategy for Users Table</h4>
<p><strong>Hash-based sharding on <code>user_id</code></strong>. The user_id (UUID) is hashed to determine the shard. This ensures even distribution of users across shards (no hotspots since UUIDs are random). Chosen over range-based sharding because user_ids have no natural ordering that would benefit from range queries. All queries for a specific user hit exactly one shard. At ~900M users with ~2KB per row, total data is ~1.8TB ‚Äî distributed across shards for parallel read/write capacity.</p>
</div>

<div class="card">
<h4>Experiences Table</h4>
<span class="tag tag-sql">SQL</span>
<table>
    <tr><th>Column</th><th>Type</th><th>Key</th><th>Description</th></tr>
    <tr><td>experience_id</td><td>UUID</td><td>üîë Primary Key</td><td>Unique experience identifier</td></tr>
    <tr><td>user_id</td><td>UUID</td><td>üîó Foreign Key ‚Üí Users</td><td>Owner of this experience</td></tr>
    <tr><td>company_name</td><td>VARCHAR(200)</td><td></td><td>Company name</td></tr>
    <tr><td>company_id</td><td>UUID</td><td>üîó Foreign Key ‚Üí Companies</td><td>Link to company page (nullable)</td></tr>
    <tr><td>title</td><td>VARCHAR(200)</td><td></td><td>Job title</td></tr>
    <tr><td>start_date</td><td>DATE</td><td></td><td>Start date</td></tr>
    <tr><td>end_date</td><td>DATE</td><td></td><td>End date (null = current)</td></tr>
    <tr><td>description</td><td>TEXT</td><td></td><td>Role description</td></tr>
</table>
<p><strong>Why SQL:</strong> Normalized from Users table to avoid update anomalies (a user can have multiple experiences). Structured schema with well-defined types. Queried relationally: "Get all experiences for user X sorted by date."</p>
<p><strong>Indexes:</strong> <code>user_id</code> ‚Äî B-Tree Index. Supports the most common query: "Get all experiences for a given user." B-Tree chosen because results are often sorted by <code>start_date</code>, and B-Tree indexes support ordered retrieval efficiently.</p>
<p><strong>Read events:</strong> When a profile is viewed. <strong>Write events:</strong> When a user adds/edits/removes an experience.</p>
</div>

<div class="card">
<h4>Education Table</h4>
<span class="tag tag-sql">SQL</span>
<table>
    <tr><th>Column</th><th>Type</th><th>Key</th><th>Description</th></tr>
    <tr><td>education_id</td><td>UUID</td><td>üîë Primary Key</td><td>Unique education identifier</td></tr>
    <tr><td>user_id</td><td>UUID</td><td>üîó Foreign Key ‚Üí Users</td><td>Owner</td></tr>
    <tr><td>school_name</td><td>VARCHAR(200)</td><td></td><td>School name</td></tr>
    <tr><td>degree</td><td>VARCHAR(100)</td><td></td><td>BS, MS, PhD, etc.</td></tr>
    <tr><td>field_of_study</td><td>VARCHAR(200)</td><td></td><td>Computer Science, etc.</td></tr>
    <tr><td>start_date</td><td>DATE</td><td></td><td></td></tr>
    <tr><td>end_date</td><td>DATE</td><td></td><td></td></tr>
</table>
<p><strong>Why SQL:</strong> Same rationale as Experiences ‚Äî normalized, structured, relational. A user can have multiple education entries.</p>
<p><strong>Indexes:</strong> <code>user_id</code> ‚Äî B-Tree Index. Same rationale as Experiences.</p>
<p><strong>Read events:</strong> Profile view. <strong>Write events:</strong> User adds/edits education.</p>
</div>

<div class="card">
<h4>Skills Table</h4>
<span class="tag tag-sql">SQL</span>
<table>
    <tr><th>Column</th><th>Type</th><th>Key</th><th>Description</th></tr>
    <tr><td>skill_id</td><td>UUID</td><td>üîë Primary Key</td><td></td></tr>
    <tr><td>user_id</td><td>UUID</td><td>üîó Foreign Key ‚Üí Users</td><td></td></tr>
    <tr><td>skill_name</td><td>VARCHAR(100)</td><td></td><td>"Python", "Machine Learning"</td></tr>
    <tr><td>endorsement_count</td><td>INT</td><td></td><td>Denormalized count</td></tr>
</table>
<p><strong>Why SQL:</strong> Normalized. Skills are queried relationally with the user and used in search indexes.</p>
<p><strong>Indexes:</strong> <code>user_id</code> ‚Äî B-Tree Index. <code>skill_name</code> ‚Äî B-Tree Index (for queries like "find all users with Python skill").</p>
<p><strong>Read events:</strong> Profile view, search. <strong>Write events:</strong> User adds skill, someone endorses a skill.</p>
</div>

<div class="card">
<h4>Connections Table</h4>
<span class="tag tag-sql">SQL</span>
<table>
    <tr><th>Column</th><th>Type</th><th>Key</th><th>Description</th></tr>
    <tr><td>connection_id</td><td>UUID</td><td>üîë Primary Key</td><td></td></tr>
    <tr><td>requester_id</td><td>UUID</td><td>üîó Foreign Key ‚Üí Users</td><td>Who sent the request</td></tr>
    <tr><td>receiver_id</td><td>UUID</td><td>üîó Foreign Key ‚Üí Users</td><td>Who received the request</td></tr>
    <tr><td>status</td><td>ENUM</td><td></td><td>'pending', 'accepted', 'rejected'</td></tr>
    <tr><td>created_at</td><td>TIMESTAMP</td><td></td><td>When request was sent</td></tr>
    <tr><td>updated_at</td><td>TIMESTAMP</td><td></td><td>When status changed</td></tr>
</table>
<p><strong>Why SQL:</strong> ACID transactions are critical ‚Äî connection state changes (pending‚Üíaccepted) must be atomic to prevent race conditions (e.g., simultaneously accepting and rejecting). The unique constraint on <code>(requester_id, receiver_id)</code> prevents duplicate requests at the DB level.</p>
<p><strong>Indexes:</strong></p>
<ul>
    <li><strong><code>(requester_id, status)</code> ‚Äî B-Tree Composite Index:</strong> Efficiently query "all pending requests I sent" or "all my accepted connections." B-Tree supports equality on <code>requester_id</code> and filtering on <code>status</code>.</li>
    <li><strong><code>(receiver_id, status)</code> ‚Äî B-Tree Composite Index:</strong> Efficiently query "all pending requests I received." Critical for the notification badge count ("3 pending requests").</li>
    <li><strong><code>(requester_id, receiver_id)</code> ‚Äî Unique B-Tree Composite Index:</strong> Prevents duplicate connection requests and enables O(1) lookup for "are these two users connected?"</li>
</ul>
<p><strong>Sharding Strategy:</strong> This is tricky because queries go both ways (by requester_id and by receiver_id). Strategy: <strong>dual-write sharding</strong> ‚Äî each connection is written to two shards: one sharded by requester_id and one sharded by receiver_id. This doubles write volume but ensures all queries for a given user hit a single shard. The alternative (sharding by connection_id) would require scatter-gather for user-centric queries, which is unacceptable at scale.</p>
<p><strong>Read events:</strong> Profile view (show "Connected" button), People You May Know, checking connection status. <strong>Write events:</strong> Sending, accepting, rejecting, withdrawing a connection request.</p>
</div>

<div class="card">
<h4>Companies Table</h4>
<span class="tag tag-sql">SQL</span>
<table>
    <tr><th>Column</th><th>Type</th><th>Key</th><th>Description</th></tr>
    <tr><td>company_id</td><td>UUID</td><td>üîë Primary Key</td><td></td></tr>
    <tr><td>name</td><td>VARCHAR(200)</td><td></td><td>Company name</td></tr>
    <tr><td>industry</td><td>VARCHAR(100)</td><td></td><td></td></tr>
    <tr><td>description</td><td>TEXT</td><td></td><td></td></tr>
    <tr><td>logo_url</td><td>VARCHAR(500)</td><td></td><td>CDN URL</td></tr>
    <tr><td>website</td><td>VARCHAR(500)</td><td></td><td></td></tr>
    <tr><td>employee_count</td><td>INT</td><td></td><td>Denormalized</td></tr>
    <tr><td>created_at</td><td>TIMESTAMP</td><td></td><td></td></tr>
</table>
<p><strong>Why SQL:</strong> Structured entity, referenced by Jobs and Experiences tables. Relatively small dataset (~50M companies).</p>
<p><strong>Indexes:</strong> <code>name</code> ‚Äî B-Tree Index (for company search). Full-text inverted index on <code>name, description</code> in the Search Index.</p>
<p><strong>Read events:</strong> Company page view, job listings, profile experiences. <strong>Write events:</strong> Company admin creates/updates company page.</p>
</div>

<div class="card">
<h4>Jobs Table</h4>
<span class="tag tag-sql">SQL</span>
<table>
    <tr><th>Column</th><th>Type</th><th>Key</th><th>Description</th></tr>
    <tr><td>job_id</td><td>UUID</td><td>üîë Primary Key</td><td></td></tr>
    <tr><td>company_id</td><td>UUID</td><td>üîó Foreign Key ‚Üí Companies</td><td>Posting company</td></tr>
    <tr><td>posted_by</td><td>UUID</td><td>üîó Foreign Key ‚Üí Users</td><td>Recruiter who posted</td></tr>
    <tr><td>title</td><td>VARCHAR(200)</td><td></td><td>Job title</td></tr>
    <tr><td>description</td><td>TEXT</td><td></td><td>Full job description</td></tr>
    <tr><td>location</td><td>VARCHAR(200)</td><td></td><td>"San Francisco, CA" or "Remote"</td></tr>
    <tr><td>salary_min</td><td>INT</td><td></td><td>Min salary (nullable)</td></tr>
    <tr><td>salary_max</td><td>INT</td><td></td><td>Max salary (nullable)</td></tr>
    <tr><td>employment_type</td><td>ENUM</td><td></td><td>'full_time', 'part_time', 'contract', 'internship'</td></tr>
    <tr><td>status</td><td>ENUM</td><td></td><td>'active', 'closed', 'draft'</td></tr>
    <tr><td>application_count</td><td>INT</td><td></td><td>Denormalized</td></tr>
    <tr><td>posted_at</td><td>TIMESTAMP</td><td></td><td></td></tr>
    <tr><td>updated_at</td><td>TIMESTAMP</td><td></td><td></td></tr>
</table>
<p><strong>Why SQL:</strong> Structured schema, relational queries (join with Companies), ACID for status transitions (active‚Üíclosed must atomically stop accepting applications).</p>
<p><strong>Indexes:</strong></p>
<ul>
    <li><strong><code>company_id</code> ‚Äî B-Tree Index:</strong> "All jobs by this company." Common query on company pages.</li>
    <li><strong><code>(status, posted_at)</code> ‚Äî B-Tree Composite Index:</strong> "Active jobs sorted by recency." Needed for the default job browse view.</li>
    <li><strong><code>(location, employment_type, status)</code> ‚Äî B-Tree Composite Index:</strong> Filtered job search queries.</li>
    <li><strong><code>title, description</code> ‚Äî Inverted Index (Full-Text) in Search Index:</strong> Powers job keyword search.</li>
</ul>
<p><strong>Sharding Strategy:</strong> <strong>Hash-based sharding on <code>job_id</code></strong>. Jobs are most commonly accessed individually (job detail page) or via the Search Index (which returns job_ids). Company-centric queries ("all jobs by Acme Corp") go through the Search Index rather than the Jobs table directly, so sharding by job_id is optimal.</p>
<p><strong>Read events:</strong> Job search results, job detail page, company page. <strong>Write events:</strong> Recruiter creates/updates/closes a job, application_count increment.</p>
</div>

<div class="card">
<h4>Applications Table</h4>
<span class="tag tag-sql">SQL</span>
<table>
    <tr><th>Column</th><th>Type</th><th>Key</th><th>Description</th></tr>
    <tr><td>application_id</td><td>UUID</td><td>üîë Primary Key</td><td></td></tr>
    <tr><td>job_id</td><td>UUID</td><td>üîó Foreign Key ‚Üí Jobs</td><td></td></tr>
    <tr><td>user_id</td><td>UUID</td><td>üîó Foreign Key ‚Üí Users</td><td>Applicant</td></tr>
    <tr><td>resume_url</td><td>VARCHAR(500)</td><td></td><td>Object Storage URL</td></tr>
    <tr><td>cover_letter</td><td>TEXT</td><td></td><td>Optional</td></tr>
    <tr><td>status</td><td>ENUM</td><td></td><td>'submitted', 'reviewed', 'interview', 'offer', 'rejected', 'withdrawn'</td></tr>
    <tr><td>applied_at</td><td>TIMESTAMP</td><td></td><td></td></tr>
    <tr><td>updated_at</td><td>TIMESTAMP</td><td></td><td></td></tr>
</table>
<p><strong>Why SQL:</strong> ACID for application state transitions (submitted‚Üíinterview‚Üíoffer). Unique constraint on <code>(job_id, user_id)</code> prevents duplicate applications at the DB level.</p>
<p><strong>Indexes:</strong></p>
<ul>
    <li><strong><code>(job_id, user_id)</code> ‚Äî Unique B-Tree Composite Index:</strong> Prevents duplicates and enables O(1) lookup for "has this user applied to this job?"</li>
    <li><strong><code>user_id</code> ‚Äî B-Tree Index:</strong> "My applications" query.</li>
    <li><strong><code>(job_id, status)</code> ‚Äî B-Tree Composite Index:</strong> Recruiter dashboard: "all submitted applications for this job."</li>
</ul>
<p><strong>Read events:</strong> "My applications" page, recruiter application review dashboard. <strong>Write events:</strong> User applies, recruiter updates application status.</p>
</div>

<h3 id="nosql-tables">NoSQL Tables</h3>

<div class="card">
<h4>Posts Collection</h4>
<span class="tag tag-nosql">NoSQL (Document)</span>
<table>
    <tr><th>Field</th><th>Type</th><th>Key</th><th>Description</th></tr>
    <tr><td>post_id</td><td>UUID</td><td>üîë Partition Key</td><td></td></tr>
    <tr><td>author_id</td><td>UUID</td><td></td><td>Post creator</td></tr>
    <tr><td>author_name</td><td>STRING</td><td></td><td>Denormalized from Users</td></tr>
    <tr><td>author_photo_url</td><td>STRING</td><td></td><td>Denormalized from Users</td></tr>
    <tr><td>author_headline</td><td>STRING</td><td></td><td>Denormalized from Users</td></tr>
    <tr><td>content</td><td>TEXT</td><td></td><td>Post text</td></tr>
    <tr><td>media_urls</td><td>LIST&lt;STRING&gt;</td><td></td><td>CDN URLs for images/videos</td></tr>
    <tr><td>post_type</td><td>STRING</td><td></td><td>'text', 'image', 'video', 'article'</td></tr>
    <tr><td>like_count</td><td>INT</td><td></td><td>Denormalized</td></tr>
    <tr><td>comment_count</td><td>INT</td><td></td><td>Denormalized</td></tr>
    <tr><td>share_count</td><td>INT</td><td></td><td>Denormalized</td></tr>
    <tr><td>is_influencer_post</td><td>BOOLEAN</td><td></td><td>Flag for pull-at-read</td></tr>
    <tr><td>created_at</td><td>TIMESTAMP</td><td></td><td></td></tr>
</table>
<p><strong>Why NoSQL (Document):</strong> High write throughput (~3M posts/day + millions of engagement updates). Flexible schema accommodates different post types with different fields (video posts have duration, article posts have link metadata). Document model naturally groups all post data together for single-read retrieval. Horizontal scalability is essential at this write volume.</p>
<p><strong>Denormalization explained:</strong> <code>author_name</code>, <code>author_photo_url</code>, and <code>author_headline</code> are denormalized from the Users table. This avoids a cross-database join (NoSQL post ‚Üí SQL user) on every feed render, which would be prohibitively expensive at 2B feed reads/day. The tradeoff: when a user updates their name or photo, we must asynchronously propagate the change to their posts. This is acceptable because (a) profile updates are infrequent compared to post reads (write-once-read-many pattern), and (b) brief staleness of author info on posts is tolerable (eventual consistency).</p>
<p><strong>Denormalization of <code>like_count</code>, <code>comment_count</code>, <code>share_count</code>:</strong> These counters are stored directly on the post document to avoid counting queries. Atomically incremented/decremented on each like/comment/share event. This eliminates the need for a <code>SELECT COUNT(*)</code> query on a likes table every time a post is rendered.</p>
<p><strong>Global Secondary Index:</strong> <code>author_id + created_at</code> ‚Äî enables the query "get all posts by user X sorted by newest first" (needed for the "Posts" tab on a user's profile). This is a GSI in the NoSQL store.</p>
<p><strong>Sharding Strategy:</strong> <strong>Hash-based sharding on <code>post_id</code></strong>. Posts are most frequently accessed individually (feed rendering fetches specific post_ids). Sharding by post_id ensures even distribution and single-shard lookups.</p>
<p><strong>Read events:</strong> Feed rendering, post detail view, profile "Posts" tab. <strong>Write events:</strong> Post creation, like/unlike, comment, share.</p>
</div>

<div class="card">
<h4>Feed Table</h4>
<span class="tag tag-nosql">NoSQL (Wide-Column)</span>
<table>
    <tr><th>Field</th><th>Type</th><th>Key</th><th>Description</th></tr>
    <tr><td>user_id</td><td>UUID</td><td>üîë Partition Key</td><td>Feed owner</td></tr>
    <tr><td>created_at</td><td>TIMESTAMP</td><td>üîë Sort Key (DESC)</td><td>For chronological ordering</td></tr>
    <tr><td>post_id</td><td>UUID</td><td></td><td>Reference to post</td></tr>
    <tr><td>author_id</td><td>UUID</td><td></td><td>Post author</td></tr>
    <tr><td>post_snippet</td><td>STRING</td><td></td><td>First 200 chars (denormalized)</td></tr>
    <tr><td>post_type</td><td>STRING</td><td></td><td>Denormalized for filtering</td></tr>
</table>
<p><strong>Why NoSQL (Wide-Column):</strong> Extreme read throughput (~2B reads/day). Simple access pattern: always queried by <code>user_id</code> with range scan on <code>created_at</code> (descending). Wide-column stores excel at this partition-key + sort-key pattern. Each user's feed is a single partition, enabling efficient sequential reads.</p>
<p><strong>Denormalization explained:</strong> <code>post_snippet</code> and <code>post_type</code> are denormalized into feed entries so that the Feed Service can render feed previews without fetching the full post from the Post DB. Only when a user clicks to expand a post does the full post document get fetched. This reduces the number of cross-table reads by ~80% for typical feed loads.</p>
<p><strong>Sharding Strategy:</strong> <strong>Hash-based sharding on <code>user_id</code></strong>. All feed entries for a user are co-located on the same shard, enabling efficient range scans. Feed reads always query by user_id, so this sharding key aligns perfectly with the access pattern.</p>
<p><strong>Read events:</strong> User opens feed (paginated). <strong>Write events:</strong> Fan-out Service writes entries when a connection posts.</p>
</div>

<div class="card">
<h4>Messages Table</h4>
<span class="tag tag-nosql">NoSQL (Wide-Column)</span>
<table>
    <tr><th>Field</th><th>Type</th><th>Key</th><th>Description</th></tr>
    <tr><td>conversation_id</td><td>UUID</td><td>üîë Partition Key</td><td></td></tr>
    <tr><td>message_id</td><td>UUID (ULID)</td><td>üîë Sort Key</td><td>Time-ordered unique ID</td></tr>
    <tr><td>sender_id</td><td>UUID</td><td></td><td></td></tr>
    <tr><td>content</td><td>TEXT</td><td></td><td>Message body</td></tr>
    <tr><td>message_type</td><td>STRING</td><td></td><td>'text', 'image', 'file'</td></tr>
    <tr><td>media_url</td><td>STRING</td><td></td><td>Optional media attachment</td></tr>
    <tr><td>created_at</td><td>TIMESTAMP</td><td></td><td></td></tr>
    <tr><td>read_at</td><td>TIMESTAMP</td><td></td><td>Null if unread</td></tr>
</table>
<p><strong>Why NoSQL (Wide-Column):</strong> High write throughput (~300M messages/day). Time-series access pattern ‚Äî messages are always read in chronological order within a conversation. The partition-key (conversation_id) + sort-key (message_id) pattern is ideal for wide-column stores. Each conversation's messages are co-located for efficient sequential reads.</p>
<p><strong>Sharding Strategy:</strong> <strong>Hash-based sharding on <code>conversation_id</code></strong>. All messages in a conversation are on the same shard, enabling efficient range scans for message history. Conversations are the natural unit of access.</p>
<p><strong>Read events:</strong> User opens a conversation, scrolls through message history. <strong>Write events:</strong> User sends a message, marks a message as read.</p>
</div>

<div class="card">
<h4>Conversations Table</h4>
<span class="tag tag-nosql">NoSQL (Document)</span>
<table>
    <tr><th>Field</th><th>Type</th><th>Key</th><th>Description</th></tr>
    <tr><td>conversation_id</td><td>UUID</td><td>üîë Partition Key</td><td></td></tr>
    <tr><td>participant_ids</td><td>LIST&lt;UUID&gt;</td><td></td><td>User IDs in the conversation</td></tr>
    <tr><td>last_message_snippet</td><td>STRING</td><td></td><td>Denormalized preview</td></tr>
    <tr><td>last_message_at</td><td>TIMESTAMP</td><td></td><td>For sorting conversation list</td></tr>
    <tr><td>unread_counts</td><td>MAP&lt;UUID, INT&gt;</td><td></td><td>Per-participant unread count</td></tr>
    <tr><td>created_at</td><td>TIMESTAMP</td><td></td><td></td></tr>
</table>
<p><strong>Why NoSQL:</strong> Flexible schema (variable number of participants, map of unread counts). High update frequency (every message updates <code>last_message_snippet</code> and <code>unread_counts</code>).</p>
<p><strong>Global Secondary Index:</strong> Each participant_id maps to their conversation_ids (for the "my conversations" query). This is a GSI on <code>participant_ids</code>.</p>
<p><strong>Denormalization explained:</strong> <code>last_message_snippet</code> is denormalized so the conversations list view ("inbox") can render previews without fetching the actual messages table for each conversation.</p>
<p><strong>Read events:</strong> User opens messaging inbox. <strong>Write events:</strong> New message updates last_message fields.</p>
</div>

<div class="card">
<h4>Notifications Table</h4>
<span class="tag tag-nosql">NoSQL (Wide-Column)</span>
<table>
    <tr><th>Field</th><th>Type</th><th>Key</th><th>Description</th></tr>
    <tr><td>user_id</td><td>UUID</td><td>üîë Partition Key</td><td>Notification recipient</td></tr>
    <tr><td>notification_id</td><td>UUID (ULID)</td><td>üîë Sort Key (DESC)</td><td>Time-ordered</td></tr>
    <tr><td>type</td><td>STRING</td><td></td><td>'connection_request', 'post_like', 'comment', 'job_alert', etc.</td></tr>
    <tr><td>source_user_id</td><td>UUID</td><td></td><td>Who triggered it</td></tr>
    <tr><td>source_user_name</td><td>STRING</td><td></td><td>Denormalized</td></tr>
    <tr><td>source_user_photo</td><td>STRING</td><td></td><td>Denormalized</td></tr>
    <tr><td>entity_id</td><td>UUID</td><td></td><td>post_id, connection_id, job_id, etc.</td></tr>
    <tr><td>content</td><td>STRING</td><td></td><td>"Bob liked your post"</td></tr>
    <tr><td>read</td><td>BOOLEAN</td><td></td><td></td></tr>
    <tr><td>created_at</td><td>TIMESTAMP</td><td></td><td></td></tr>
</table>
<p><strong>Why NoSQL (Wide-Column):</strong> Very high write throughput (a single popular post can generate millions of notifications). Simple access pattern: always queried by <code>user_id</code> sorted by recency. Wide-column is ideal.</p>
<p><strong>Denormalization explained:</strong> <code>source_user_name</code> and <code>source_user_photo</code> are denormalized to avoid N+1 queries when rendering the notification feed. Without denormalization, loading 20 notifications would require 20 additional queries to the Users table ‚Äî unacceptable at scale.</p>
<p><strong>Sharding Strategy:</strong> <strong>Hash-based sharding on <code>user_id</code></strong>. Same rationale as Feed table ‚Äî all notifications for a user are co-located.</p>
<p><strong>Read events:</strong> User clicks notification bell. <strong>Write events:</strong> Any event that generates a notification (like, comment, connection request, job alert, message).</p>
</div>

<div class="card">
<h4>Likes Table</h4>
<span class="tag tag-nosql">NoSQL (Wide-Column)</span>
<table>
    <tr><th>Field</th><th>Type</th><th>Key</th><th>Description</th></tr>
    <tr><td>post_id</td><td>UUID</td><td>üîë Partition Key</td><td></td></tr>
    <tr><td>user_id</td><td>UUID</td><td>üîë Sort Key</td><td>Who liked</td></tr>
    <tr><td>created_at</td><td>TIMESTAMP</td><td></td><td></td></tr>
</table>
<p><strong>Why NoSQL:</strong> High write throughput. Simple access pattern: check if user X liked post Y, list who liked post Y.</p>
<p><strong>Sharding Strategy:</strong> Hash-based on <code>post_id</code>. All likes for a post are co-located.</p>
<p><strong>Read events:</strong> Check if current user liked a post (for filled/unfilled heart icon), "View all likes" list. <strong>Write events:</strong> User likes/unlikes a post.</p>
</div>

<div class="card">
<h4>Comments Table</h4>
<span class="tag tag-nosql">NoSQL (Wide-Column)</span>
<table>
    <tr><th>Field</th><th>Type</th><th>Key</th><th>Description</th></tr>
    <tr><td>post_id</td><td>UUID</td><td>üîë Partition Key</td><td></td></tr>
    <tr><td>comment_id</td><td>UUID (ULID)</td><td>üîë Sort Key</td><td>Time-ordered</td></tr>
    <tr><td>user_id</td><td>UUID</td><td></td><td>Commenter</td></tr>
    <tr><td>user_name</td><td>STRING</td><td></td><td>Denormalized</td></tr>
    <tr><td>user_photo_url</td><td>STRING</td><td></td><td>Denormalized</td></tr>
    <tr><td>content</td><td>TEXT</td><td></td><td></td></tr>
    <tr><td>created_at</td><td>TIMESTAMP</td><td></td><td></td></tr>
</table>
<p><strong>Why NoSQL:</strong> High write throughput. Time-series access pattern. All comments for a post are co-located for efficient pagination.</p>
<p><strong>Denormalization explained:</strong> <code>user_name</code> and <code>user_photo_url</code> avoid N+1 queries when rendering a comment thread (loading 50 comments without denormalization = 50 extra user lookups).</p>
<p><strong>Sharding Strategy:</strong> Hash-based on <code>post_id</code>. Comments are always queried per-post.</p>
<p><strong>Read events:</strong> Viewing comments on a post. <strong>Write events:</strong> User posts a comment.</p>
</div>

<h3 id="graph-db">Graph Database</h3>

<div class="card">
<h4>Social Graph</h4>
<span class="tag tag-graph">Graph</span>
<p><strong>Nodes:</strong> Users (with properties: <code>user_id</code>, <code>name</code>, <code>industry</code>, <code>location</code>)</p>
<p><strong>Edges:</strong> <code>CONNECTED_TO</code> (bidirectional, with properties: <code>connected_since</code>)</p>
<p><strong>Additional edges:</strong> <code>FOLLOWS</code> (unidirectional, for following without connecting), <code>WORKS_AT</code> (user‚Üícompany), <code>STUDIED_AT</code> (user‚Üíschool)</p>
<p><strong>Why Graph DB:</strong> The social graph is the core of LinkedIn. Queries like "find 2nd-degree connections," "find mutual connections," "People You May Know," and social proximity ranking for search all require graph traversal. In a relational DB, finding 2nd-degree connections for a user with 500 connections requires a self-join returning up to 250,000 rows, then deduplication ‚Äî this is O(n¬≤) and extremely slow. In a graph DB, it's a 2-hop traversal from the starting node, visiting at most 500 √ó 500 = 250K nodes with O(1) per edge traversal. The graph DB maintains adjacency lists that make this efficient.</p>
<p><strong>Sharding Strategy:</strong> <strong>Hash-based sharding on <code>user_id</code></strong> for nodes. Edges are co-located with their source node for efficient outbound traversals. Cross-shard traversals (needed for 2nd-degree queries) use a routing layer that parallelizes sub-queries across shards.</p>
<p><strong>Read events:</strong> People You May Know, mutual connections, search ranking by social proximity, connection degree badge on profiles. <strong>Write events:</strong> Connection accepted (add edge), connection removed (remove edge).</p>
</div>

<h3 id="denormalization">Denormalization Summary</h3>

<div class="card">
<table>
    <tr><th>Denormalized Field</th><th>Source</th><th>Target</th><th>Reason</th></tr>
    <tr><td>author_name, author_photo, author_headline</td><td>Users (SQL)</td><td>Posts (NoSQL)</td><td>Avoid cross-DB join on 2B daily feed reads. Profile changes are rare (1:1000 ratio vs reads). Eventual consistency acceptable.</td></tr>
    <tr><td>post_snippet, post_type</td><td>Posts (NoSQL)</td><td>Feed (NoSQL)</td><td>Render feed previews without fetching full post documents. Reduces cross-table reads by ~80%.</td></tr>
    <tr><td>source_user_name, source_user_photo</td><td>Users (SQL)</td><td>Notifications (NoSQL)</td><td>Avoid N+1 queries rendering notification feed. Loading 20 notifications would otherwise require 20 user lookups.</td></tr>
    <tr><td>user_name, user_photo_url</td><td>Users (SQL)</td><td>Comments (NoSQL)</td><td>Avoid N+1 queries rendering comment threads.</td></tr>
    <tr><td>last_message_snippet</td><td>Messages (NoSQL)</td><td>Conversations (NoSQL)</td><td>Render inbox preview without fetching latest message per conversation.</td></tr>
    <tr><td>connection_count</td><td>Connections (SQL)</td><td>Users (SQL)</td><td>Display "500+ connections" without COUNT query on Connections table.</td></tr>
    <tr><td>like_count, comment_count, share_count</td><td>Likes, Comments (NoSQL)</td><td>Posts (NoSQL)</td><td>Display engagement counts without COUNT queries on separate tables.</td></tr>
    <tr><td>endorsement_count</td><td>Endorsements</td><td>Skills (SQL)</td><td>Display endorsement count on skills without join to endorsements table.</td></tr>
    <tr><td>application_count</td><td>Applications (SQL)</td><td>Jobs (SQL)</td><td>Display "200 applicants" on job listing without COUNT query.</td></tr>
    <tr><td>employee_count</td><td>Experiences/Users</td><td>Companies (SQL)</td><td>Display company size without aggregation query.</td></tr>
</table>

<h4>Normalization Decisions</h4>
<p>The following data is <strong>normalized</strong> (kept in separate tables) intentionally:</p>
<ul>
    <li><strong>Experiences, Education, Skills</strong> are separate tables from Users. Reason: a user can have multiple of each (1:N relationship). Embedding them in the Users row would create variable-width rows in SQL and complicate updates (adding/removing a single experience shouldn't require rewriting the entire user record).</li>
    <li><strong>Applications</strong> are separate from Jobs. Reason: applications have their own lifecycle (status transitions), are queried from both the applicant's perspective ("my applications") and the recruiter's perspective ("applicants for this job"), and require their own ACID guarantees.</li>
</ul>
</div>

<h3 id="indexes">Index Summary</h3>

<div class="card">
<table>
    <tr><th>Table</th><th>Column(s)</th><th>Index Type</th><th>Why</th></tr>
    <tr><td>Users</td><td>email</td><td>Hash</td><td>O(1) exact-match lookup for login authentication</td></tr>
    <tr><td>Users</td><td>name, headline, summary</td><td>Inverted (Full-Text) in Search Index</td><td>People search with BM25 ranking</td></tr>
    <tr><td>Users</td><td>(location, industry)</td><td>B-Tree Composite</td><td>Filtered people search</td></tr>
    <tr><td>Experiences</td><td>user_id</td><td>B-Tree</td><td>Ordered retrieval of user's experiences</td></tr>
    <tr><td>Education</td><td>user_id</td><td>B-Tree</td><td>Ordered retrieval of user's education</td></tr>
    <tr><td>Skills</td><td>user_id</td><td>B-Tree</td><td>Retrieve user's skills</td></tr>
    <tr><td>Skills</td><td>skill_name</td><td>B-Tree</td><td>Find users by skill</td></tr>
    <tr><td>Connections</td><td>(requester_id, status)</td><td>B-Tree Composite</td><td>Sent requests filtered by status</td></tr>
    <tr><td>Connections</td><td>(receiver_id, status)</td><td>B-Tree Composite</td><td>Received requests filtered by status</td></tr>
    <tr><td>Connections</td><td>(requester_id, receiver_id)</td><td>B-Tree Unique Composite</td><td>Prevent duplicates + connection check</td></tr>
    <tr><td>Jobs</td><td>company_id</td><td>B-Tree</td><td>Jobs by company</td></tr>
    <tr><td>Jobs</td><td>(status, posted_at)</td><td>B-Tree Composite</td><td>Active jobs by recency</td></tr>
    <tr><td>Jobs</td><td>(location, employment_type, status)</td><td>B-Tree Composite</td><td>Filtered job browse</td></tr>
    <tr><td>Jobs</td><td>title, description</td><td>Inverted (Full-Text) in Search Index</td><td>Job keyword search</td></tr>
    <tr><td>Applications</td><td>(job_id, user_id)</td><td>B-Tree Unique Composite</td><td>Prevent duplicate applications</td></tr>
    <tr><td>Applications</td><td>user_id</td><td>B-Tree</td><td>"My applications" query</td></tr>
    <tr><td>Applications</td><td>(job_id, status)</td><td>B-Tree Composite</td><td>Recruiter: applicants by status</td></tr>
    <tr><td>Posts (NoSQL)</td><td>author_id + created_at</td><td>GSI (B-Tree equivalent)</td><td>User's posts sorted by date</td></tr>
</table>
</div>

<h3 id="sharding">Sharding Summary</h3>

<div class="card">
<table>
    <tr><th>Table</th><th>Shard Key</th><th>Strategy</th><th>Rationale</th></tr>
    <tr><td>Users</td><td>user_id</td><td>Hash</td><td>Even distribution (UUIDs are random). All user queries hit one shard.</td></tr>
    <tr><td>Connections</td><td>requester_id & receiver_id (dual-write)</td><td>Hash (dual-write)</td><td>Queries go both ways. Dual-write ensures single-shard reads for either user.</td></tr>
    <tr><td>Posts</td><td>post_id</td><td>Hash</td><td>Posts accessed individually via feed. Even distribution.</td></tr>
    <tr><td>Feed</td><td>user_id</td><td>Hash</td><td>Feed always queried per-user. Co-locates all entries for efficient range scan.</td></tr>
    <tr><td>Messages</td><td>conversation_id</td><td>Hash</td><td>Messages always queried per-conversation.</td></tr>
    <tr><td>Notifications</td><td>user_id</td><td>Hash</td><td>Always queried per-user.</td></tr>
    <tr><td>Jobs</td><td>job_id</td><td>Hash</td><td>Individual access via search index results.</td></tr>
    <tr><td>Graph DB</td><td>user_id (node)</td><td>Hash</td><td>Co-locate edges with source node for fast traversals.</td></tr>
</table>
</div>

<!-- ======================== CDN DEEP DIVE ======================== -->
<h2 id="cdn">12. CDN Deep Dive</h2>

<div class="card">
<h3>Why CDN is Appropriate</h3>
<p>LinkedIn is a global platform with 900M+ users across every continent. Static and media content constitutes a significant portion of all data transferred. A CDN is essential because:</p>
<ul>
    <li><strong>Profile photos:</strong> ~900M users, each with a profile photo. Profile pages are viewed billions of times daily. Without a CDN, the Object Storage origin would be overwhelmed.</li>
    <li><strong>Post media:</strong> Images and videos in posts are viewed by hundreds or thousands of users each. CDN caching means the origin serves each piece of media only once per edge location.</li>
    <li><strong>Static assets:</strong> JavaScript bundles, CSS, fonts, and icons for the web app. Served from edge locations for fast initial page loads.</li>
    <li><strong>Geographic distribution:</strong> Users in Tokyo should not have to fetch media from a US data center (200ms+ latency). CDN edge locations serve content within 10-30ms.</li>
</ul>

<h3>CDN Strategy</h3>
<ul>
    <li><strong>Pull-based CDN:</strong> Content is cached on the edge server upon first request. Subsequent requests for the same content are served from the edge cache. This is preferred over push-based because LinkedIn has hundreds of millions of media files ‚Äî proactively pushing all of them to all edge locations would be wasteful since many profile photos are rarely viewed.</li>
    <li><strong>TTL (Time-to-Live):</strong> Static assets (JS/CSS): 1 year with content-hash in filename for cache busting on deploys. Profile photos: 24 hours (users may update their photo). Post media: 30 days (immutable once posted). Company logos: 7 days.</li>
    <li><strong>Cache Invalidation:</strong> When a user uploads a new profile photo, the Object Storage URL changes (new filename with content hash), so the CDN naturally serves the new version. For cases where the URL stays the same, explicit cache invalidation is sent to the CDN.</li>
    <li><strong>Image Optimization:</strong> The CDN serves responsive images ‚Äî different sizes/formats (WebP, AVIF) based on the client's <code>Accept</code> header and viewport size. Original high-res images are stored in Object Storage; the CDN or an image processing service generates resized variants on-the-fly and caches them.</li>
</ul>
</div>

<!-- ======================== CACHE DEEP DIVE ======================== -->
<h2 id="cache">13. Caching Deep Dive</h2>

<div class="card">
<h3>Why In-Memory Cache is Appropriate</h3>
<p>LinkedIn is extremely read-heavy (feed reads alone: ~2B/day). Without caching, the databases would need to handle this load directly, requiring massive horizontal scaling and still resulting in higher latency. An in-memory cache reduces database load by 80-95% for hot data and reduces read latency from ~5-10ms (DB) to ~0.5ms (cache).</p>

<h3>Cache Instances</h3>

<h4>1. Profile Cache</h4>
<table>
    <tr><th>Property</th><th>Value</th><th>Rationale</th></tr>
    <tr><td>Strategy</td><td>Write-Through</td><td>When a profile is updated, the cache is updated synchronously with the DB write. This ensures the cache is always consistent with the DB. Chosen because profile updates are infrequent (a user updates their profile maybe a few times a year), so the write overhead is minimal, but reads are extremely frequent (profile viewed millions of times/day for popular users). Cache-aside would risk serving stale data after an update.</td></tr>
    <tr><td>Populated When</td><td>Cache-aside on first read (if not already populated by a write). Write-through on every profile update.</td><td>First read populates the cache; subsequent reads are cache hits. Updates flow through the cache to maintain consistency.</td></tr>
    <tr><td>Eviction Policy</td><td>LRU (Least Recently Used)</td><td>Profiles follow a power-law access pattern (a small fraction of profiles ‚Äî recruiters, influencers, job seekers ‚Äî account for the majority of views). LRU naturally retains the hottest profiles and evicts cold ones. Simpler and more effective than LFU for this workload because access patterns change over time (a user who goes viral temporarily should be cached, even if their long-term frequency is low).</td></tr>
    <tr><td>Expiration Policy</td><td>24-hour TTL</td><td>Even with write-through, a TTL serves as a safety net against cache-DB divergence from edge cases (failed write-through, network partition). 24 hours balances freshness with hit rate ‚Äî most active profiles are accessed multiple times within 24 hours, so the TTL rarely triggers for hot data.</td></tr>
    <tr><td>Key Format</td><td><code>profile:{user_id}</code></td><td></td></tr>
    <tr><td>Value</td><td>Serialized profile JSON (including experiences, education, skills)</td><td></td></tr>
</table>

<h4>2. Feed Cache</h4>
<table>
    <tr><th>Property</th><th>Value</th><th>Rationale</th></tr>
    <tr><td>Strategy</td><td>Write-Behind (Write-Back) + Cache-Aside</td><td>The Fan-out Service writes new feed entries directly to the cache (and asynchronously to the Feed DB). This minimizes write latency during fan-out (writing to cache is &lt;1ms vs. 5-10ms for DB). The Feed DB is updated in batches. Cache-aside is used for cold reads (users who haven't been active recently and whose feed cache has expired). Write-behind is chosen over write-through because feed fan-out involves massive burst writes (a post by someone with 10K connections = 10K cache writes) and the DB doesn't need to be synchronously updated.</td></tr>
    <tr><td>Populated When</td><td>Fan-out Service writes new entries on post creation. Cache-aside populates on first feed read if cache is cold.</td><td></td></tr>
    <tr><td>Eviction Policy</td><td>LRU with per-user size limit (top 200 entries)</td><td>Users only browse recent feed items. Storing more than 200 entries per user wastes cache memory. LRU ensures that if memory pressure is high, feeds of inactive users are evicted first. The 200-entry limit per user prevents a single highly-active user from consuming disproportionate cache space.</td></tr>
    <tr><td>Expiration Policy</td><td>1-hour TTL</td><td>Feed freshness matters ‚Äî a 1-hour TTL ensures that even without new writes (e.g., a user with no posting connections), the feed is periodically refreshed to incorporate ranking changes and new influencer posts. Short TTL is acceptable because feed cache misses are handled gracefully (fallback to Feed DB).</td></tr>
    <tr><td>Key Format</td><td><code>feed:{user_id}</code></td><td></td></tr>
    <tr><td>Value</td><td>Sorted list of feed entry objects (post_id, author_id, snippet, created_at)</td><td></td></tr>
</table>

<h4>3. Connection Cache</h4>
<table>
    <tr><th>Property</th><th>Value</th><th>Rationale</th></tr>
    <tr><td>Strategy</td><td>Write-Through</td><td>Connection status is critical for UX (showing "Connect" vs "Message" vs "Pending" button). Must be consistent. Write-through ensures immediate consistency after accepting/rejecting a connection.</td></tr>
    <tr><td>Populated When</td><td>Cache-aside on first read. Write-through on connection status changes.</td><td></td></tr>
    <tr><td>Eviction Policy</td><td>LRU</td><td>Active users' connection data stays cached. Inactive users' data is evicted.</td></tr>
    <tr><td>Expiration Policy</td><td>12-hour TTL</td><td>Connections change infrequently. 12-hour TTL provides a safety net. Connection checks happen on every profile view and message send, so this data is very hot.</td></tr>
    <tr><td>Key Format</td><td><code>conn:{user_id}:count</code>, <code>conn:{user_id_a}:{user_id_b}:status</code></td><td></td></tr>
</table>

<h4>4. Notification Cache</h4>
<table>
    <tr><th>Property</th><th>Value</th><th>Rationale</th></tr>
    <tr><td>Strategy</td><td>Write-Through (from Notification Service) + Cache-Aside (on read)</td><td>New notifications are written through to the cache so they appear immediately when the user checks their notification feed. Cache-aside handles initial population for users who haven't checked notifications recently.</td></tr>
    <tr><td>Populated When</td><td>Notification Service writes through on each new notification. Cache-aside on first notification feed read.</td><td></td></tr>
    <tr><td>Eviction Policy</td><td>LRU with per-user limit (top 50 notifications)</td><td>Users rarely scroll past 50 notifications. LRU evicts inactive users' data under memory pressure.</td></tr>
    <tr><td>Expiration Policy</td><td>6-hour TTL</td><td>Notifications are time-sensitive. 6-hour TTL balances freshness with cache hit rate. Most users check notifications multiple times per day.</td></tr>
    <tr><td>Key Format</td><td><code>notif:{user_id}</code>, <code>notif:{user_id}:unread_count</code></td><td></td></tr>
</table>

<h4>5. Session / Auth Cache</h4>
<table>
    <tr><th>Property</th><th>Value</th><th>Rationale</th></tr>
    <tr><td>Strategy</td><td>Write-Through</td><td>Session tokens are written to cache on login and validated on every API request. Must be immediately consistent ‚Äî if a session is revoked, the cache must reflect this instantly.</td></tr>
    <tr><td>Populated When</td><td>User logs in (writes session token). Validated on every API request.</td><td></td></tr>
    <tr><td>Eviction Policy</td><td>LRU</td><td></td></tr>
    <tr><td>Expiration Policy</td><td>30-day TTL (matches session duration)</td><td>LinkedIn sessions persist for ~30 days. TTL automatically expires stale sessions without requiring explicit cleanup.</td></tr>
    <tr><td>Key Format</td><td><code>session:{token}</code></td><td></td></tr>
</table>
</div>

<!-- ======================== WEBSOCKET DEEP DIVE ======================== -->
<h2 id="websocket">14. WebSocket Deep Dive</h2>

<div class="card">
<h3>Why WebSocket</h3>
<p>LinkedIn messaging requires real-time, bidirectional communication. WebSocket is chosen over the alternatives:</p>

<table>
    <tr><th>Approach</th><th>Why Not Used</th></tr>
    <tr><td><strong>HTTP Polling</strong></td><td>Client would poll the server every N seconds for new messages. At 160M DAU, even polling every 5 seconds = 32M requests/second of unnecessary traffic (most polls return empty). Extremely wasteful of server resources and battery on mobile devices. Latency is limited by the poll interval (up to 5 seconds delay).</td></tr>
    <tr><td><strong>HTTP Long Polling</strong></td><td>Better than polling ‚Äî client holds an open HTTP connection until the server has data or a timeout occurs. However, each "push" requires a new HTTP connection (TCP handshake + TLS handshake = ~100ms overhead). For a chat session with rapid back-and-forth messages, this overhead compounds. Also consumes server threads/connections waiting on idle clients.</td></tr>
    <tr><td><strong>Server-Sent Events (SSE)</strong></td><td>Unidirectional ‚Äî server can push to client, but client cannot send to server over the same connection. Messaging requires bidirectional communication (sending AND receiving messages). Would need a separate HTTP POST channel for sends, adding complexity.</td></tr>
    <tr><td><strong>WebSocket ‚úÖ</strong></td><td><strong>Selected.</strong> Full-duplex, bidirectional communication over a single persistent TCP connection. After the initial HTTP upgrade handshake, messages flow in both directions with minimal overhead (2-10 bytes per frame header vs. ~800 bytes for HTTP headers). Ideal for chat. Sub-100ms latency for message delivery.</td></tr>
</table>

<h3>Connection Establishment</h3>
<ol>
    <li><strong>Client initiates:</strong> When the user opens LinkedIn (or the messaging tab), the client sends an HTTP/1.1 request with <code>Upgrade: websocket</code> and <code>Connection: Upgrade</code> headers to the WebSocket Load Balancer endpoint (e.g., <code>wss://realtime.linkedin.com/ws</code>). The request includes the user's JWT token in the <code>Authorization</code> header or as a query parameter.</li>
    <li><strong>Authentication:</strong> The WebSocket Load Balancer routes the upgrade request to a WebSocket Server. The server validates the JWT token against the Session/Auth Cache. If invalid, it returns <code>401 Unauthorized</code>.</li>
    <li><strong>Upgrade:</strong> If authenticated, the server responds with <code>101 Switching Protocols</code>, and the connection is upgraded from HTTP to WebSocket over the same TCP connection.</li>
    <li><strong>Registration:</strong> The WebSocket Server registers the mapping <code>{user_id ‚Üí websocket_server_id, connection_id}</code> in the Connection Manager (distributed in-memory cache). This enables other services to route messages to this user.</li>
    <li><strong>Heartbeat:</strong> Every 30 seconds, the server sends a WebSocket <code>ping</code> frame. The client must respond with a <code>pong</code> frame within 10 seconds. If no pong is received, the server closes the connection and removes the mapping from the Connection Manager.</li>
</ol>

<h3>Message Routing</h3>
<ol>
    <li>Client A sends a message over their WebSocket connection to WebSocket Server 1.</li>
    <li>WebSocket Server 1 forwards the message to the Message Service (internal HTTP call).</li>
    <li>Message Service persists the message to Message DB.</li>
    <li>Message Service queries the Connection Manager: "Which WebSocket Server is Client B connected to?"</li>
    <li><strong>If Client B is online:</strong> Connection Manager returns <code>{server: WebSocket Server 3, connection: conn_xyz}</code>. Message Service sends the message to WebSocket Server 3 (internal routing via a pub/sub channel between servers or direct HTTP call). WebSocket Server 3 pushes the message to Client B's WebSocket connection.</li>
    <li><strong>If Client B is offline:</strong> Connection Manager returns no entry. Message Service publishes a <code>message_received</code> event to the Message Queue for the Notification Service to send a push notification.</li>
</ol>

<h3>Connection Manager Details</h3>
<p>The Connection Manager is a distributed in-memory cache (hash table) storing:</p>
<pre>{
  "user_12345": {
    "ws_server": "ws-server-07",
    "connection_id": "conn_abc123",
    "connected_at": "2025-01-15T10:30:00Z",
    "device_type": "mobile"
  }
}</pre>
<p>A user can have multiple active connections (e.g., web + mobile). The Connection Manager stores all active connections per user as a list. When routing a message, the Message Service sends to ALL active connections so the message appears on all devices simultaneously.</p>

<h3>Scaling WebSocket Servers</h3>
<ul>
    <li>Each WebSocket Server maintains ~100K concurrent connections (limited by memory and file descriptors).</li>
    <li>At 160M DAU with ~30% concurrent, that's ~48M concurrent WebSocket connections requiring ~480 WebSocket Servers.</li>
    <li>The WebSocket Load Balancer uses <strong>sticky sessions</strong> (connection affinity) ‚Äî once a client connects to a server, it stays on that server for the duration of the connection. This is implemented via the <code>Connection</code> header or IP-based affinity.</li>
    <li>When a WebSocket Server goes down, all its connections drop. Clients reconnect (with exponential backoff) and are assigned to a healthy server. The Connection Manager entries for the downed server are cleaned up via TTL (30-second expiry) or explicit cleanup by a health monitor.</li>
</ul>

<h3>Protocol Details</h3>
<p>WebSocket runs over TCP (not UDP) because:</p>
<ul>
    <li>Message delivery must be reliable (no dropped messages in a chat).</li>
    <li>Message ordering must be preserved within a conversation.</li>
    <li>TCP provides both guarantees. UDP would require implementing reliability at the application layer, adding complexity for no benefit in a text chat use case.</li>
    <li>TLS (WSS) encrypts all WebSocket traffic in transit.</li>
</ul>
</div>

<!-- ======================== MESSAGE QUEUE DEEP DIVE ======================== -->
<h2 id="mq">15. Message Queue Deep Dive</h2>

<div class="card">
<h3>Why Message Queue</h3>
<p>Multiple flows in LinkedIn involve asynchronous processing that shouldn't block the user-facing request:</p>
<ul>
    <li><strong>Feed fan-out:</strong> When a user posts, fanning out to 500+ connections' feeds is too slow to do synchronously (would add seconds to the "Post" API response time).</li>
    <li><strong>Notifications:</strong> Generating and delivering notifications (in-app + push) shouldn't block the action that triggered them (liking a post should return in &lt;100ms, not wait for notification delivery).</li>
    <li><strong>Search indexing:</strong> Updating the search index after profile/post/job changes is not time-critical and can happen asynchronously (5-second delay is fine).</li>
    <li><strong>Analytics:</strong> Logging events for analytics (profile views, job clicks, engagement) is fire-and-forget.</li>
</ul>

<h3>Why Not Alternatives</h3>
<table>
    <tr><th>Approach</th><th>Why Not Used</th></tr>
    <tr><td><strong>Synchronous Service-to-Service HTTP Calls</strong></td><td>Creates tight coupling between services. If the Notification Service is down, the Post Service would fail too. No buffering ‚Äî if the consumer is temporarily overloaded, events are lost. No retry mechanism. Blocks the user-facing request until all downstream processing completes.</td></tr>
    <tr><td><strong>Pure Pub/Sub</strong></td><td>Pub/Sub (fan-out to all subscribers) is useful for broadcasting but doesn't provide message persistence, ordering guarantees, or consumer group semantics. If a subscriber is temporarily down, it misses messages. A message queue provides durability (messages persisted to disk), at-least-once delivery, and consumer groups with offset tracking.</td></tr>
    <tr><td><strong>Message Queue ‚úÖ</strong></td><td><strong>Selected.</strong> Provides: durable persistence, at-least-once delivery, consumer groups (multiple consumers process different event types in parallel), ordering within a partition, backpressure handling (producers are throttled if queue is full), dead letter queue for failed messages, replay capability.</td></tr>
</table>

<h3>How Messages Flow</h3>
<ol>
    <li><strong>Producer publishes:</strong> A service (e.g., Post Service) publishes an event to a specific topic/queue (e.g., <code>post-events</code>) with a message payload:
<pre>{
  "event_type": "post_created",
  "post_id": "post_abc123",
  "author_id": "user_12345",
  "author_connection_count": 500,
  "created_at": "2025-01-15T10:30:00Z"
}</pre>
    The message is assigned to a partition based on the partition key (e.g., <code>author_id</code>) to ensure ordering per author.</li>
    <li><strong>Queue persists:</strong> The message is durably written to disk (replicated across multiple brokers for fault tolerance). The producer receives an acknowledgment.</li>
    <li><strong>Consumer groups process:</strong> Multiple consumer groups subscribe to the same topic:
        <ul>
            <li><code>fan-out-consumer-group</code>: Fan-out Service instances consume <code>post_created</code> events and write feed entries.</li>
            <li><code>notification-consumer-group</code>: Notification Service instances consume all event types and generate notifications.</li>
            <li><code>search-index-consumer-group</code>: Search Index Service instances consume events and update the search index.</li>
        </ul>
        Each consumer group independently tracks its offset (position in the queue). Each message is processed by exactly one consumer within each group (but by all groups).
    </li>
    <li><strong>Consumer acknowledges:</strong> After successfully processing a message, the consumer commits its offset. If the consumer crashes before committing, the message is redelivered to another consumer in the group (at-least-once delivery).</li>
    <li><strong>Idempotency:</strong> Because of at-least-once delivery, consumers must be idempotent. For example, the Fan-out Service uses the <code>post_id</code> as a deduplication key ‚Äî if a feed entry for that post_id already exists, it skips the write.</li>
    <li><strong>Dead Letter Queue (DLQ):</strong> If a message fails processing after 3 retries (e.g., due to a bug in the consumer), it's moved to a Dead Letter Queue for manual investigation. This prevents a single bad message from blocking the entire queue.</li>
</ol>

<h3>Topics</h3>
<table>
    <tr><th>Topic</th><th>Producers</th><th>Consumers</th><th>Partition Key</th></tr>
    <tr><td><code>post-events</code></td><td>Post Service</td><td>Fan-out Service, Notification Service, Search Index Service</td><td>author_id</td></tr>
    <tr><td><code>connection-events</code></td><td>Connection Service</td><td>Notification Service, Graph DB Sync</td><td>requester_id</td></tr>
    <tr><td><code>profile-events</code></td><td>Profile Service</td><td>Search Index Service, Notification Service</td><td>user_id</td></tr>
    <tr><td><code>job-events</code></td><td>Job Service</td><td>Search Index Service, Notification Service (job alerts)</td><td>company_id</td></tr>
    <tr><td><code>application-events</code></td><td>Application Service</td><td>Notification Service</td><td>job_id</td></tr>
    <tr><td><code>message-events</code></td><td>Message Service</td><td>Notification Service (for offline push)</td><td>conversation_id</td></tr>
</table>
</div>

<!-- ======================== LOAD BALANCER DEEP DIVE ======================== -->
<h2 id="lb">16. Load Balancer Deep Dive</h2>

<div class="card">
<h3>Load Balancer Placement</h3>
<p>Load balancers are placed at three layers in the architecture:</p>

<h4>1. Edge Load Balancer / API Gateway</h4>
<ul>
    <li><strong>Position:</strong> Between clients (web/mobile) and backend services.</li>
    <li><strong>Type:</strong> Layer 7 (Application layer / HTTP-aware).</li>
    <li><strong>Algorithm:</strong> Least-connections with health checks. Least-connections is preferred over round-robin because some API requests (e.g., feed generation) take longer than others (e.g., profile read), so distributing by active connection count prevents overloading slow-processing servers.</li>
    <li><strong>Responsibilities:</strong>
        <ul>
            <li><strong>TLS/SSL Termination:</strong> Decrypts HTTPS at the load balancer, forwards plain HTTP to services (within the trusted internal network). Reduces TLS overhead on application servers.</li>
            <li><strong>Request Routing:</strong> Routes requests to the correct service based on URL path: <code>/api/v1/users/*</code> ‚Üí Profile Service, <code>/api/v1/feed/*</code> ‚Üí Feed Service, <code>/api/v1/posts/*</code> ‚Üí Post Service, etc.</li>
            <li><strong>Rate Limiting:</strong> Token bucket algorithm per user/IP. Prevents abuse and protects backend services from traffic spikes. E.g., max 100 requests/minute per user.</li>
            <li><strong>Authentication:</strong> Validates JWT tokens on every request (using the Session/Auth Cache). Returns 401 for invalid tokens before the request reaches any service.</li>
            <li><strong>Health Checks:</strong> Pings each service instance every 10 seconds on <code>/health</code>. Removes unhealthy instances from the rotation. Adds them back after 3 consecutive successful health checks.</li>
        </ul>
    </li>
</ul>

<h4>2. WebSocket Load Balancer</h4>
<ul>
    <li><strong>Position:</strong> Between clients and WebSocket Servers.</li>
    <li><strong>Type:</strong> Layer 4 (Transport layer / TCP-aware). Layer 4 is chosen because WebSocket connections are long-lived TCP connections ‚Äî Layer 7 inspection is unnecessary after the initial handshake, and Layer 4 is more efficient for passing through TCP streams.</li>
    <li><strong>Algorithm:</strong> Least-connections with <strong>sticky sessions</strong>. Once a WebSocket connection is established with a specific server, the load balancer maintains affinity (all frames for that connection go to the same server). Sticky sessions are essential because WebSocket state (the connection context) lives on the specific server.</li>
    <li><strong>Health Checks:</strong> TCP-level health checks every 5 seconds. If a WebSocket Server becomes unresponsive, new connections are routed to healthy servers. Existing connections on the failed server drop, and clients reconnect (with exponential backoff).</li>
</ul>

<h4>3. Internal Service Load Balancers</h4>
<ul>
    <li><strong>Position:</strong> Between services for inter-service communication (e.g., Feed Service ‚Üí Ranking Service, Message Service ‚Üí WebSocket Servers).</li>
    <li><strong>Type:</strong> Layer 7 (gRPC/HTTP-aware for internal calls).</li>
    <li><strong>Algorithm:</strong> Round-robin (internal services have more uniform request processing times).</li>
    <li><strong>Implementation:</strong> Client-side load balancing (service mesh pattern) is preferred for internal traffic. Each service instance has a sidecar proxy that handles service discovery, load balancing, and retries. This eliminates the need for dedicated hardware/software load balancers between every pair of services.</li>
</ul>
</div>

<!-- ======================== SCALING ======================== -->
<h2 id="scaling">17. Scaling Considerations</h2>

<div class="card">
<h3>Horizontal Scaling of Stateless Services</h3>
<p>All backend services (Profile, Connection, Post, Feed, Job, Application, Search, Notification, Message, Ranking, Fan-out, Search Index) are <strong>stateless</strong> ‚Äî they store no session data locally. This means any request can be handled by any instance, enabling trivial horizontal scaling by adding more instances behind the load balancer. Auto-scaling rules trigger based on CPU utilization (&gt;70%), request queue depth, or p99 latency thresholds.</p>

<h3>Database Scaling</h3>
<ul>
    <li><strong>SQL Read Replicas:</strong> User DB, Connection DB, Job DB, and Application DB each have 3-5 read replicas. Read-heavy queries (profile views, job search, connection checks) are routed to replicas, while writes go to the primary. Replication lag is &lt;100ms (acceptable for eventual consistency on reads).</li>
    <li><strong>NoSQL Horizontal Sharding:</strong> Post DB, Feed DB, Message DB, and Notification DB are horizontally sharded (see Sharding section). Adding more shards increases both storage capacity and throughput linearly.</li>
    <li><strong>Graph DB Partitioning:</strong> The social graph is partitioned by user_id with cross-partition traversal support. As the graph grows, new partitions are added.</li>
</ul>

<h3>Cache Scaling</h3>
<ul>
    <li>Cache cluster is horizontally scaled using consistent hashing. Adding a new cache node redistributes only ~1/N of the keys (minimal cache invalidation).</li>
    <li>Estimated cache size: Profile Cache (~50GB for 25M hot profiles), Feed Cache (~100GB for 50M active feed caches), Connection Cache (~20GB), Notification Cache (~30GB), Session Cache (~10GB). Total: ~210GB distributed across a cache cluster.</li>
</ul>

<h3>WebSocket Server Scaling</h3>
<p>~480 WebSocket Servers for 48M concurrent connections. Auto-scaling adds servers during peak hours (9 AM - 6 PM in each timezone). The Connection Manager (cache) is the coordination point ‚Äî new servers simply register their connections there.</p>

<h3>Message Queue Scaling</h3>
<ul>
    <li>Topics are partitioned (e.g., <code>post-events</code> with 256 partitions). Adding partitions increases throughput.</li>
    <li>Consumer groups scale by adding more consumer instances (up to the number of partitions).</li>
    <li>During peak traffic (e.g., a viral post), the Fan-out consumer group auto-scales to handle the burst.</li>
</ul>

<h3>Search Index Scaling</h3>
<p>The search index is distributed across multiple shards. Each shard handles a subset of the data. Query routing sends the search query to all shards in parallel, and results are merged. Adding shards increases both index capacity and query throughput.</p>

<h3>CDN Scaling</h3>
<p>CDN scaling is handled by the CDN provider ‚Äî edge locations are added based on traffic patterns. No manual intervention needed.</p>

<h3>Geographic Distribution</h3>
<p>For a global user base, the system is deployed in multiple regions (US-East, US-West, EU, Asia-Pacific). Each region has a full stack of services, databases (with cross-region replication), and cache clusters. DNS-based routing (GeoDNS) directs users to the nearest region. Cross-region replication for SQL uses semi-synchronous replication with a primary region per user (determined by the user's signup location).</p>

<h3>Load Balancer Placement for Scale</h3>
<ul>
    <li><strong>Edge LB / API Gateway:</strong> Deployed in each region, scales horizontally with traffic. Multiple LB instances behind a DNS round-robin or Anycast IP.</li>
    <li><strong>WebSocket LB:</strong> One per region, Layer 4 with sticky sessions, scales by adding more LB instances.</li>
    <li><strong>Internal LBs:</strong> Client-side (service mesh sidecar) ‚Äî scales automatically with services.</li>
</ul>
</div>

<!-- ======================== TRADEOFFS ======================== -->
<h2 id="tradeoffs">18. Tradeoffs &amp; Deep Dives</h2>

<div class="card">
<h3>Tradeoff 1: Fan-out on Write vs. Fan-out on Read (Feed)</h3>
<table>
    <tr><th></th><th>Fan-out on Write (Push)</th><th>Fan-out on Read (Pull)</th></tr>
    <tr><td><strong>How</strong></td><td>When a user posts, write the post reference to all their connections' feeds immediately.</td><td>When a user opens their feed, query all their connections' recent posts and merge/rank on the fly.</td></tr>
    <tr><td><strong>Write cost</strong></td><td>High (1 post ‚Üí N writes, where N = connection count)</td><td>Low (1 post ‚Üí 1 write to Post DB)</td></tr>
    <tr><td><strong>Read cost</strong></td><td>Low (feed is precomputed, just read from Feed DB/cache)</td><td>High (must query N connections' posts, merge, rank ‚Äî O(N log N))</td></tr>
    <tr><td><strong>Latency</strong></td><td>Fast reads (&lt;50ms)</td><td>Slow reads (100-500ms depending on N)</td></tr>
    <tr><td><strong>Storage</strong></td><td>High (duplicated feed entries)</td><td>Low (no duplication)</td></tr>
    <tr><td><strong>Stale data</strong></td><td>Possible ‚Äî if a post is deleted, all fan-out entries must be cleaned up</td><td>Always fresh ‚Äî reads the source of truth</td></tr>
</table>
<p><strong>Our choice: Hybrid.</strong> Fan-out on write for regular users (&lt;10K connections, which is 99%+ of users). Fan-out on read for influencers (&gt;10K connections/followers). This gives us fast reads for the vast majority while avoiding the write amplification problem for influencers (a post by someone with 5M followers would require 5M writes ‚Äî too expensive and slow).</p>

<h3>Tradeoff 2: Eventual Consistency vs. Strong Consistency</h3>
<ul>
    <li><strong>Strong consistency chosen for:</strong> Messaging (messages must not be lost or reordered), connection status (must not show "Connect" after already connected), job applications (must prevent duplicates), authentication (session must be valid/invalid immediately).</li>
    <li><strong>Eventual consistency chosen for:</strong> Feed (a new post appearing with a 1-2 second delay is acceptable), like/comment counts (slight inaccuracy is fine ‚Äî "~1,200 likes" vs "exactly 1,203 likes"), notifications (1-2 second delay is acceptable), search index (5-second indexing delay is imperceptible).</li>
    <li><strong>Rationale:</strong> Strong consistency requires synchronous writes to all replicas (higher latency, lower availability under network partitions ‚Äî per CAP theorem). For read-heavy, latency-sensitive features like feed and notifications, we trade consistency for availability and performance.</li>
</ul>

<h3>Tradeoff 3: Denormalization vs. Storage/Consistency Cost</h3>
<p>We chose to denormalize author info into posts, comments, notifications, and feed entries. The tradeoff:</p>
<ul>
    <li><strong>Pro:</strong> Eliminates expensive cross-table joins at read time. At 2B+ daily reads, even a 5ms join per read saves 10B ms = ~2,800 hours of compute time per day.</li>
    <li><strong>Con:</strong> When a user updates their name or photo, we must propagate the change to all their posts, comments, and notifications. This is done asynchronously via the Message Queue and may take minutes for users with thousands of posts.</li>
    <li><strong>Why acceptable:</strong> Profile updates are rare (~0.01% of users update per day) vs. post/feed reads (~100% of active users read per day). The read optimization dwarfs the write overhead.</li>
</ul>

<h3>Tradeoff 4: SQL vs. NoSQL Selection</h3>
<ul>
    <li><strong>SQL chosen for entities with:</strong> Well-defined schemas that rarely change, ACID requirements (profile updates, connection state, job applications), complex relational queries (join user with experiences/education), moderate write volume.</li>
    <li><strong>NoSQL chosen for entities with:</strong> Extreme write throughput (posts, likes, comments, notifications, messages, feed), simple access patterns (key-value or key-range), flexible/evolving schemas (different post types), horizontal scalability as the primary concern.</li>
</ul>

<h3>Tradeoff 5: Graph DB vs. SQL for Social Graph</h3>
<p>The social graph could theoretically be stored in a SQL table with <code>(user_a, user_b)</code> rows. However:</p>
<ul>
    <li>A 2nd-degree connection query in SQL requires a self-join: <code>SELECT DISTINCT c2.user_b FROM connections c1 JOIN connections c2 ON c1.user_b = c2.user_a WHERE c1.user_a = ?</code>. For a user with 500 connections, this join produces up to 250,000 intermediate rows. For 3rd-degree: up to 125,000,000 rows. This is O(n^k) where k is the degree.</li>
    <li>In a graph DB, this is a k-hop breadth-first traversal visiting at most n^k nodes, but with O(1) per edge traversal using adjacency lists. The graph DB is purpose-built for this access pattern and can be 100-1000x faster for multi-hop queries.</li>
    <li><strong>Tradeoff:</strong> We now maintain two representations of connections ‚Äî SQL (for ACID state management) and Graph DB (for traversal). This adds complexity and requires keeping them in sync via the Message Queue (eventual consistency between the two is acceptable since graph queries are for suggestions/ranking, not transactional decisions).</li>
</ul>
</div>

<!-- ======================== ALTERNATIVES ======================== -->
<h2 id="alternatives">19. Alternative Approaches</h2>

<div class="card">
<h3>Alternative 1: GraphQL Instead of REST</h3>
<p><strong>Description:</strong> Use GraphQL as the API layer instead of REST, allowing clients to specify exactly which fields they need in a single query.</p>
<p><strong>Why not chosen:</strong> While GraphQL reduces over-fetching and under-fetching (especially useful for LinkedIn's complex profile pages with nested experiences, education, and skills), it introduces complexity: query cost analysis to prevent expensive queries, caching is harder (REST responses are easily cached by URL; GraphQL POST bodies are not), and it requires schema stitching across microservices. For a system of this scale, the simplicity and cache-friendliness of REST (especially with CDN caching for GET requests) is preferred. GraphQL could be adopted incrementally for specific client-heavy use cases (e.g., the mobile app's profile page) without replacing REST entirely.</p>

<h3>Alternative 2: Event Sourcing for Feed</h3>
<p><strong>Description:</strong> Instead of storing the current state of the feed (a list of post references), store an immutable log of all events (post_created, post_liked, post_deleted) and reconstruct the feed by replaying events.</p>
<p><strong>Why not chosen:</strong> Event sourcing provides perfect auditability and enables time-travel queries ("what did my feed look like yesterday?"), but feed reconstruction from events is computationally expensive at 2B daily feed reads. The precomputed feed (fan-out on write with materialized views in Feed DB) is far more efficient for reads. Event sourcing also increases storage requirements dramatically (storing every event forever). The tradeoff favors the simpler precomputed model for LinkedIn's read-heavy feed workload.</p>

<h3>Alternative 3: CQRS (Command Query Responsibility Segregation)</h3>
<p><strong>Description:</strong> Completely separate the write model (optimized for writes) from the read model (optimized for reads) with separate databases for each.</p>
<p><strong>Why not chosen (as a system-wide pattern):</strong> We actually do use CQRS implicitly in specific places: the Post DB (write-optimized) and Feed DB (read-optimized) are separate stores with the Message Queue synchronizing them. However, applying CQRS system-wide (e.g., separate read/write models for profiles, connections, jobs) would add significant complexity with marginal benefit. The existing SQL read replicas provide sufficient read scaling for those entities without full CQRS separation.</p>

<h3>Alternative 4: Peer-to-Peer Messaging (WebRTC Data Channels)</h3>
<p><strong>Description:</strong> Use WebRTC data channels for direct peer-to-peer messaging, bypassing the server for message relay.</p>
<p><strong>Why not chosen:</strong> P2P messaging doesn't work when one party is offline (no server-side persistence). It also makes compliance, content moderation, and message history across devices impossible. LinkedIn needs server-side message storage for multi-device sync, legal compliance (regulated industries), and search within messages. WebRTC is better suited for real-time video/audio calls (which LinkedIn does use for video calls, but not for text messaging).</p>

<h3>Alternative 5: Pure Fan-out on Read for Feed</h3>
<p><strong>Description:</strong> Do not precompute feeds at all. When a user opens their feed, query all their connections' recent posts in real-time.</p>
<p><strong>Why not chosen:</strong> At 500 average connections per user and 2B daily feed reads, this would require ~1 trillion cross-table lookups per day. The latency would be 200-500ms per feed load (vs. 50ms with precomputed feeds). The database load would be 10-50x higher. While simpler architecturally (no Fan-out Service, no Feed DB), the performance and cost tradeoffs are unacceptable at LinkedIn's scale.</p>

<h3>Alternative 6: Server-Sent Events (SSE) Instead of WebSocket</h3>
<p><strong>Description:</strong> Use SSE for server-to-client push (notifications, incoming messages) and standard HTTP POST for client-to-server communication (sending messages).</p>
<p><strong>Why not chosen:</strong> SSE is unidirectional (server‚Üíclient only), requiring a separate HTTP channel for sends. This means each message send incurs a full HTTP request overhead (headers, TCP handshake if connection is reused). For a rapid chat conversation, this adds latency and overhead. WebSocket's bidirectional nature over a single persistent connection is more efficient for messaging. However, SSE could be a viable alternative for notification-only push (where the client rarely sends data).</p>

<h3>Alternative 7: Single Monolithic Database</h3>
<p><strong>Description:</strong> Use a single SQL database for everything instead of the polyglot persistence approach (SQL + NoSQL + Graph + Search Index).</p>
<p><strong>Why not chosen:</strong> A single SQL database cannot handle: ~2B feed reads/day (NoSQL horizontal scalability needed), ~300M messages/day (time-series access pattern better served by wide-column NoSQL), multi-hop graph traversals (SQL self-joins are O(n^k)), and full-text search with BM25 ranking (purpose-built search index needed). Polyglot persistence lets each data store be optimized for its specific access pattern, which is critical at LinkedIn's scale. The tradeoff is increased operational complexity (managing multiple database technologies), but the performance gains justify it.</p>
</div>

<!-- ======================== ADDITIONAL INFO ======================== -->
<h2 id="additional">20. Additional Information</h2>

<div class="card">
<h3>Authentication &amp; Authorization</h3>
<ul>
    <li><strong>OAuth 2.0:</strong> Used for user authentication. Supports SSO (Sign in with Google/Apple) and LinkedIn's own email/password login.</li>
    <li><strong>JWT Tokens:</strong> Short-lived access tokens (15 minutes) + long-lived refresh tokens (30 days). Access tokens are validated against the Session Cache on every API request. Refresh tokens are stored in the Auth DB and used to issue new access tokens.</li>
    <li><strong>API Gateway validates authentication</strong> before routing to any service, preventing unauthenticated traffic from reaching backend services.</li>
</ul>

<h3>Rate Limiting</h3>
<ul>
    <li><strong>Algorithm:</strong> Token bucket per user per endpoint. Example limits: Feed GET: 60 requests/minute, Post creation: 10 posts/hour, Connection requests: 100/week, Search: 30 queries/minute, Message send: 100 messages/minute.</li>
    <li><strong>Implementation:</strong> Enforced at the API Gateway layer using a distributed counter in the cache. Returns <code>429 Too Many Requests</code> with <code>Retry-After</code> header when the limit is exceeded.</li>
</ul>

<h3>Media Processing Pipeline</h3>
<ul>
    <li>When a user uploads an image or video as part of a post, the media goes through a processing pipeline:
        <ol>
            <li>Client uploads to Object Storage via a pre-signed URL (bypasses application servers for efficiency).</li>
            <li>An event triggers the Media Processing Service (via Message Queue).</li>
            <li>For images: generate thumbnails (150px, 400px, 800px), convert to WebP/AVIF for modern browsers, strip EXIF metadata (privacy), run content moderation (detect inappropriate content via ML model).</li>
            <li>For videos: transcode to multiple resolutions (360p, 720p, 1080p) and formats (H.264, H.265), generate HLS (HTTP Live Streaming) segments for adaptive bitrate streaming, extract a thumbnail frame.</li>
            <li>Processed media is stored back in Object Storage and served via CDN.</li>
        </ol>
    </li>
</ul>

<h3>Content Moderation</h3>
<p>Posts, comments, and messages are scanned for policy violations (spam, hate speech, misinformation) using a combination of ML classifiers and keyword filters. This runs asynchronously via the Message Queue. Flagged content is reviewed by human moderators or automatically hidden, depending on confidence scores.</p>

<h3>Data Privacy &amp; Compliance</h3>
<ul>
    <li><strong>GDPR/CCPA:</strong> Users can request data export (all their profile, posts, messages) and account deletion. Deletion triggers a cascade across all databases and the search index.</li>
    <li><strong>Encryption:</strong> Data encrypted at rest (AES-256) in all databases and Object Storage. Data encrypted in transit (TLS 1.3) for all HTTP and WebSocket connections.</li>
    <li><strong>Data Residency:</strong> EU user data is stored in EU data centers to comply with GDPR data residency requirements.</li>
</ul>

<h3>Monitoring &amp; Observability</h3>
<ul>
    <li><strong>Metrics:</strong> Each service emits metrics (request count, latency p50/p95/p99, error rate, cache hit rate). Dashboards track system health.</li>
    <li><strong>Distributed Tracing:</strong> Each request is assigned a trace ID that propagates across all service calls, enabling end-to-end latency debugging.</li>
    <li><strong>Alerting:</strong> Automated alerts for: p99 latency &gt; 500ms, error rate &gt; 1%, cache hit rate &lt; 80%, message queue lag &gt; 10K messages, WebSocket connection drops &gt; 5%.</li>
</ul>

<h3>Disaster Recovery</h3>
<ul>
    <li><strong>Database backups:</strong> Continuous backup with point-in-time recovery. SQL databases have synchronous replication to a standby in the same region + asynchronous replication to another region.</li>
    <li><strong>Multi-region failover:</strong> If an entire region goes down, DNS failover routes traffic to the secondary region within 30 seconds. The secondary region has warm standby databases with &lt;5-minute replication lag.</li>
    <li><strong>Message Queue durability:</strong> Messages are replicated across 3 brokers with synchronous acknowledgment. Broker failure doesn't lose messages.</li>
</ul>

<h3>A/B Testing &amp; Feature Flags</h3>
<p>LinkedIn uses feature flags to gradually roll out new features (e.g., a new feed ranking algorithm). Traffic is split between control and treatment groups. The Feed Service checks the user's experiment assignment and applies the appropriate algorithm. This allows safe deployment and rollback of changes.</p>
</div>

<!-- ======================== VENDORS ======================== -->
<h2 id="vendors">21. Vendor Section</h2>

<div class="card">
<p>The architecture above is vendor-agnostic. Below are potential vendor choices for each component, with rationale:</p>

<table>
    <tr><th>Component</th><th>Potential Vendors</th><th>Rationale</th></tr>
    <tr>
        <td><strong>SQL Database</strong></td>
        <td>PostgreSQL, MySQL, CockroachDB, Amazon Aurora</td>
        <td><strong>PostgreSQL</strong> is preferred for its advanced features (full-text search, JSONB columns, excellent index support), strong community, and battle-tested at scale. <strong>CockroachDB</strong> is a strong choice if geo-distributed SQL with automatic sharding is needed (reduces operational overhead). <strong>Aurora</strong> provides MySQL/PostgreSQL compatibility with automatic storage scaling up to 128TB.</td>
    </tr>
    <tr>
        <td><strong>NoSQL Database</strong></td>
        <td>Apache Cassandra, Amazon DynamoDB, ScyllaDB</td>
        <td><strong>Cassandra</strong> is preferred for its write-heavy performance, wide-column model (perfect for feed, messages, notifications), tunable consistency, and linear scalability. LinkedIn actually uses Cassandra-derived <em>Espresso</em> internally. <strong>ScyllaDB</strong> is a drop-in Cassandra replacement written in C++ with lower latency. <strong>DynamoDB</strong> is managed and serverless but locks you into AWS.</td>
    </tr>
    <tr>
        <td><strong>Graph Database</strong></td>
        <td>Neo4j, Amazon Neptune, JanusGraph, TigerGraph</td>
        <td><strong>Neo4j</strong> is the most mature graph DB with the Cypher query language, excellent for social graph traversals. <strong>TigerGraph</strong> offers better performance for deep-link graph analytics (3rd-degree connections at scale). <strong>JanusGraph</strong> is open-source and can run on Cassandra as the storage backend, reducing operational complexity if Cassandra is already in use.</td>
    </tr>
    <tr>
        <td><strong>Search Index</strong></td>
        <td>Elasticsearch, Apache Solr, Typesense, Meilisearch</td>
        <td><strong>Elasticsearch</strong> is the industry standard for full-text search at scale, with BM25 scoring, faceted search, aggregations, and a mature distributed architecture. LinkedIn uses a custom search system internally, but Elasticsearch is the closest widely-available equivalent. <strong>Typesense</strong> is simpler and faster for typeahead/autocomplete but less feature-rich for complex queries.</td>
    </tr>
    <tr>
        <td><strong>Message Queue</strong></td>
        <td>Apache Kafka, Apache Pulsar, Amazon SQS/SNS, RabbitMQ</td>
        <td><strong>Apache Kafka</strong> is the preferred choice ‚Äî it was literally built by LinkedIn for this exact use case. It provides durable, partitioned, ordered, high-throughput messaging with consumer groups and log compaction. <strong>Pulsar</strong> offers similar capabilities with native multi-tenancy and geo-replication. <strong>RabbitMQ</strong> is simpler but doesn't scale as well for the throughput needed here.</td>
    </tr>
    <tr>
        <td><strong>In-Memory Cache</strong></td>
        <td>Redis, Memcached, Hazelcast, Apache Ignite</td>
        <td><strong>Redis</strong> is preferred for its rich data structures (sorted sets for feeds, hash maps for profiles, pub/sub for WebSocket inter-server communication), clustering support, and persistence options. <strong>Memcached</strong> is simpler and slightly faster for pure key-value caching but lacks Redis's data structure versatility. LinkedIn uses a custom caching system called <em>Couchbase</em>-based solution internally.</td>
    </tr>
    <tr>
        <td><strong>Object Storage</strong></td>
        <td>Amazon S3, Google Cloud Storage, Azure Blob Storage, MinIO</td>
        <td><strong>Amazon S3</strong> is the industry standard with 99.999999999% (11 nines) durability. <strong>MinIO</strong> provides S3-compatible storage for on-premises/multi-cloud deployments. Any of these work ‚Äî the choice depends on the primary cloud provider.</td>
    </tr>
    <tr>
        <td><strong>CDN</strong></td>
        <td>Cloudflare, Akamai, Amazon CloudFront, Fastly</td>
        <td><strong>Cloudflare</strong> offers the largest global network (300+ cities) with competitive pricing. <strong>Akamai</strong> has the most edge locations and mature enterprise features. <strong>Fastly</strong> offers real-time cache purging (sub-second), useful for profile photo updates.</td>
    </tr>
    <tr>
        <td><strong>Load Balancer</strong></td>
        <td>NGINX, HAProxy, Envoy, AWS ALB/NLB</td>
        <td><strong>NGINX</strong> is widely used for Layer 7 load balancing with excellent HTTP handling. <strong>Envoy</strong> is preferred for the service mesh sidecar pattern (internal load balancing), with native gRPC support and observability. <strong>HAProxy</strong> is preferred for WebSocket load balancing (Layer 4) due to its efficient TCP proxying.</td>
    </tr>
    <tr>
        <td><strong>Push Notification Gateway</strong></td>
        <td>Firebase Cloud Messaging (FCM), Apple Push Notification service (APNs)</td>
        <td>These are not interchangeable ‚Äî <strong>APNs</strong> is required for iOS devices and <strong>FCM</strong> is required for Android devices. Both must be integrated. A notification abstraction service wraps both APIs.</td>
    </tr>
</table>
</div>

<!-- ======================== FOOTER ======================== -->
<div style="margin-top: 60px; padding: 24px; background: var(--primary-dark); color: white; border-radius: 8px; text-align: center;">
    <p style="margin: 0; font-size: 14px;">LinkedIn System Design ‚Äî Comprehensive Architecture Document</p>
    <p style="margin: 4px 0 0; font-size: 12px; opacity: 0.7;">Generated February 2026</p>
</div>

</main>

<script>
    mermaid.initialize({
        startOnLoad: true,
        theme: 'default',
        flowchart: {
            useMaxWidth: true,
            htmlLabels: true,
            curve: 'basis'
        }
    });
</script>

</body>
</html>
