<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>System Design: Indeed</title>
<script src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script>
<style>
  body { font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; max-width: 1100px; margin: 40px auto; padding: 0 20px; background: #f9fafb; color: #1a1a2e; line-height: 1.7; }
  h1 { font-size: 2.4em; border-bottom: 4px solid #2563eb; padding-bottom: 12px; color: #1e293b; }
  h2 { font-size: 1.7em; color: #2563eb; margin-top: 50px; border-left: 5px solid #2563eb; padding-left: 14px; }
  h3 { font-size: 1.3em; color: #334155; margin-top: 30px; }
  h4 { font-size: 1.1em; color: #475569; margin-top: 20px; }
  table { border-collapse: collapse; width: 100%; margin: 16px 0; }
  th, td { border: 1px solid #cbd5e1; padding: 10px 14px; text-align: left; }
  th { background: #2563eb; color: #fff; }
  tr:nth-child(even) { background: #f1f5f9; }
  code { background: #e2e8f0; padding: 2px 6px; border-radius: 4px; font-size: 0.95em; }
  pre { background: #1e293b; color: #e2e8f0; padding: 16px; border-radius: 8px; overflow-x: auto; }
  .example-box { background: #eff6ff; border-left: 4px solid #3b82f6; padding: 14px 18px; margin: 14px 0; border-radius: 0 8px 8px 0; }
  .warn-box { background: #fefce8; border-left: 4px solid #eab308; padding: 14px 18px; margin: 14px 0; border-radius: 0 8px 8px 0; }
  .info-box { background: #f0fdf4; border-left: 4px solid #22c55e; padding: 14px 18px; margin: 14px 0; border-radius: 0 8px 8px 0; }
  .mermaid { background: #fff; padding: 20px; border-radius: 10px; box-shadow: 0 2px 8px rgba(0,0,0,0.08); margin: 20px 0; }
  ul, ol { margin: 8px 0; }
  li { margin-bottom: 6px; }
  hr { border: none; border-top: 2px solid #e2e8f0; margin: 40px 0; }
</style>
</head>
<body>
<script>mermaid.initialize({startOnLoad:true, theme:'base', themeVariables:{primaryColor:'#dbeafe', primaryTextColor:'#1e293b', primaryBorderColor:'#2563eb', lineColor:'#64748b', secondaryColor:'#f1f5f9', tertiaryColor:'#fff'}});</script>

<h1>System Design: Indeed (Job Search Platform)</h1>
<p><em>A comprehensive system design for a large-scale job search and application platform akin to Indeed, supporting job posting by employers, job searching by seekers, application management, resume management, and job alerts.</em></p>

<hr>

<!-- ==================== FUNCTIONAL REQUIREMENTS ==================== -->
<h2>1. Functional Requirements</h2>
<ol>
  <li><strong>Job Posting (Employer):</strong> Employers can create, edit, deactivate, and delete job postings. Each posting includes title, description, requirements, salary range, location, job type (full-time, part-time, contract, internship), and remote policy.</li>
  <li><strong>Job Search (Job Seeker):</strong> Job seekers can search for jobs using keywords, location (including radius/geo), salary range, job type, company name, date posted, and other filters. Results are ranked by relevance.</li>
  <li><strong>Job Application (Job Seeker):</strong> Job seekers can apply to a job by submitting a resume (uploaded file) and an optional cover letter. A seeker cannot apply to the same job twice.</li>
  <li><strong>Resume Management (Job Seeker):</strong> Job seekers can upload, replace, and manage multiple resumes. One resume is marked as "primary."</li>
  <li><strong>Employer Application Dashboard:</strong> Employers can view all applications for a given job posting, filter by status (submitted, reviewed, shortlisted, rejected, hired), and update application statuses.</li>
  <li><strong>Saved Jobs (Job Seeker):</strong> Job seekers can bookmark/save jobs and retrieve their saved list.</li>
  <li><strong>Job Alerts (Job Seeker):</strong> Job seekers can create alerts with keyword/location/job-type criteria. When matching new jobs are posted, they receive push notifications or emails at their chosen frequency (instant, daily, weekly).</li>
  <li><strong>Company Pages:</strong> Employers maintain a company profile (logo, description, industry, size, website). Job seekers can browse company profiles.</li>
</ol>

<hr>

<!-- ==================== NON-FUNCTIONAL REQUIREMENTS ==================== -->
<h2>2. Non-Functional Requirements</h2>
<table>
  <tr><th>Requirement</th><th>Target</th><th>Rationale</th></tr>
  <tr><td>Search Latency</td><td>&lt; 200 ms (p95)</td><td>Job seekers expect near-instant results; slow search drives users to competitors.</td></tr>
  <tr><td>Availability</td><td>99.9% uptime</td><td>Revenue depends on continuous availability for both employers and seekers.</td></tr>
  <tr><td>Scalability</td><td>~300M+ job listings, ~250M+ registered users</td><td>Global platform must handle massive catalog and user base.</td></tr>
  <tr><td>Throughput</td><td>~50K search queries/sec peak</td><td>Search is the dominant read path.</td></tr>
  <tr><td>Consistency</td><td>Strong for applications; eventual for search index</td><td>A submitted application must never be lost. Search index lag of a few seconds is acceptable.</td></tr>
  <tr><td>Durability</td><td>99.999999999% for resumes &amp; applications</td><td>Legal/compliance requirement; loss of resumes or applications is unacceptable.</td></tr>
  <tr><td>Data Privacy</td><td>GDPR, CCPA compliant</td><td>Handles PII (resumes, contact info).</td></tr>
  <tr><td>Global Reach</td><td>Multi-region deployment</td><td>Indeed operates in 60+ countries.</td></tr>
</table>

<hr>

<!-- ==================== FLOW 1: EMPLOYER POSTS A JOB ==================== -->
<h2>3. Flow 1 â€” Employer Posts a Job</h2>

<div class="mermaid">
graph LR
    A["ðŸ‘¤ Employer<br/>(Browser/App)"] -->|"POST /jobs"| B["Load Balancer"]
    B --> C["API Gateway"]
    C -->|"Auth + Route"| D["Job Service"]
    D -->|"Write job"| E[("Job DB<br/>(SQL)")]
    D -->|"Publish: job_created"| F["Message Queue"]
    F -->|"Consume"| G["Search Indexer<br/>Service"]
    G -->|"Index document"| H[("Search Index<br/>(Inverted Index)")]
    F -->|"Consume"| I["Job Alert<br/>Service"]
    I -->|"Read alerts"| J[("Alert DB<br/>(SQL)")]
    I -->|"Send notification"| K["Notification<br/>Service"]
    K -->|"Push / Email"| L["ðŸ“± Job Seekers"]

    style E fill:#fef3c7,stroke:#d97706
    style H fill:#fef3c7,stroke:#d97706
    style J fill:#fef3c7,stroke:#d97706
</div>

<h3>Flow 1 â€” Examples</h3>

<div class="example-box">
<strong>Example 1 (Happy Path):</strong> An employer at Acme Corp logs in and clicks "Post a Job." They fill out the form: title = "Senior Software Engineer," location = "San Francisco, CA," salary = "$150Kâ€“$200K," type = "Full-Time," remote = "Hybrid." They click "Submit." The browser sends an <code>HTTP POST /jobs</code> to the Load Balancer, which forwards it through the API Gateway to the Job Service. The Job Service validates the payload, writes the new job row to the Job DB (SQL), and publishes a <code>job_created</code> event onto the Message Queue. Two consumers pick up the event: (1) the Search Indexer Service indexes the job into the Search Index so it becomes searchable within seconds, and (2) the Job Alert Service queries the Alert DB for any seekers whose saved alert criteria match "software engineer in San Francisco." It finds 340 matching alerts and dispatches notifications via the Notification Service â€” some as instant push notifications and others batched for daily/weekly email digests.
</div>

<div class="example-box">
<strong>Example 2 (Job Edit):</strong> The employer realizes they forgot to add "Kubernetes experience required." They edit the posting via <code>HTTP PUT /jobs/{jobId}</code>. The Job Service updates the row in the Job DB and publishes a <code>job_updated</code> event. The Search Indexer re-indexes the document with the updated description. The Job Alert Service does <em>not</em> re-trigger alerts for edits (only new postings trigger alerts), preventing notification spam.
</div>

<div class="example-box">
<strong>Example 3 (Job Deactivation):</strong> The position is filled. The employer clicks "Close Posting," which sends <code>HTTP PATCH /jobs/{jobId}</code> with <code>{"status": "closed"}</code>. The Job Service updates the status in the Job DB and publishes <code>job_closed</code>. The Search Indexer removes the document from the active search index so it no longer appears in results.
</div>

<h3>Flow 1 â€” Component Deep Dive</h3>

<h4>Load Balancer</h4>
<p>Layer-7 (HTTP/HTTPS) load balancer distributing requests across API Gateway instances using round-robin with health checks. Terminates TLS.</p>

<h4>API Gateway</h4>
<p>Handles authentication (JWT token validation), rate limiting, request routing, and protocol translation. Routes <code>/jobs/*</code> endpoints to the Job Service.</p>

<h4>Job Service</h4>
<table>
  <tr><th>Endpoint</th><th>Method</th><th>Input</th><th>Output</th><th>Description</th></tr>
  <tr><td><code>/jobs</code></td><td>POST</td><td>JSON body: title, description, requirements, salary_min, salary_max, location, job_type, remote_policy, company_id</td><td>201 Created â€” <code>{job_id, status, created_at}</code></td><td>Creates a new job posting. Publishes <code>job_created</code> event.</td></tr>
  <tr><td><code>/jobs/{jobId}</code></td><td>GET</td><td>Path param: jobId</td><td>200 OK â€” full job object</td><td>Retrieves a single job's details.</td></tr>
  <tr><td><code>/jobs/{jobId}</code></td><td>PUT</td><td>Path param: jobId; JSON body with updated fields</td><td>200 OK â€” updated job object</td><td>Full update of a job posting. Publishes <code>job_updated</code> event.</td></tr>
  <tr><td><code>/jobs/{jobId}</code></td><td>PATCH</td><td>Path param: jobId; JSON body e.g. <code>{"status":"closed"}</code></td><td>200 OK â€” updated job object</td><td>Partial update (e.g., close a posting). Publishes relevant event.</td></tr>
  <tr><td><code>/jobs/{jobId}</code></td><td>DELETE</td><td>Path param: jobId</td><td>204 No Content</td><td>Soft-deletes a job. Publishes <code>job_deleted</code> event.</td></tr>
  <tr><td><code>/companies/{companyId}/jobs</code></td><td>GET</td><td>Path param: companyId; Query params: status, page, limit</td><td>200 OK â€” paginated list of jobs</td><td>Lists all jobs for a company (employer dashboard).</td></tr>
</table>
<p><strong>Protocol:</strong> HTTP/HTTPS (REST). Stateless. Communicates with Job DB via SQL queries and publishes events to the Message Queue.</p>

<h4>Job DB (SQL)</h4>
<p>Relational database storing structured job, company, and user data. SQL chosen because job postings have a well-defined schema, require ACID transactions (e.g., ensuring a job is created atomically), and benefit from relational joins (jobs â†” companies â†” users). See Schema section for table details.</p>

<h4>Message Queue</h4>
<p>Asynchronous, durable message queue decoupling the Job Service from downstream consumers. The Job Service publishes events (<code>job_created</code>, <code>job_updated</code>, <code>job_closed</code>, <code>job_deleted</code>) as messages. Consumers (Search Indexer, Job Alert Service) subscribe to relevant topics. Messages are persisted until acknowledged, ensuring at-least-once delivery. This prevents the Job Service from being blocked by slow indexing or notification delivery and provides fault tolerance â€” if the Search Indexer is temporarily down, messages queue up and are processed when it recovers.</p>
<p><strong>Why not synchronous calls?</strong> Synchronous indexing would increase POST latency and create tight coupling. If the search index were unavailable, job posting would fail entirely. The message queue decouples these concerns.</p>
<p><strong>Why not pub/sub?</strong> A message queue with topic-based subscriptions achieves the same fan-out. A pure pub/sub system would also work here. The key requirement is durable delivery with at-least-once semantics and the ability to replay messages on consumer failure. A message queue with topic subscriptions (pub/sub semantics) satisfies both.</p>

<h4>Search Indexer Service</h4>
<p>Consumes <code>job_created</code> / <code>job_updated</code> / <code>job_closed</code> events from the Message Queue. Transforms the job data into an index document (denormalizing company name, location coordinates, etc.) and upserts it into the Search Index. On <code>job_closed</code> or <code>job_deleted</code>, removes the document.</p>

<h4>Search Index (Inverted Index)</h4>
<p>A full-text search engine with an inverted index. Stores denormalized job documents optimized for keyword search, geo-proximity queries, and faceted filtering. Supports relevance scoring (TF-IDF / BM25). Not a traditional database â€” it is an index layer derived from the Job DB as the source of truth.</p>

<h4>Job Alert Service</h4>
<p>Consumes <code>job_created</code> events. For each new job, queries the Alert DB for active alerts whose keyword/location/job_type criteria match the job. Uses a matching algorithm (keyword overlap + geo radius). For matched alerts, enqueues notifications to the Notification Service. For "instant" frequency alerts, sends immediately. For "daily"/"weekly," batches into a digest.</p>

<h4>Notification Service</h4>
<p>Responsible for delivering push notifications (via APNs for iOS, FCM for Android) and transactional emails. Receives notification requests from the Job Alert Service (and later, the Application Service). Handles retry logic and delivery status tracking.</p>

<hr>

<!-- ==================== FLOW 2: JOB SEEKER SEARCHES FOR JOBS ==================== -->
<h2>4. Flow 2 â€” Job Seeker Searches for Jobs</h2>

<div class="mermaid">
graph LR
    A["ðŸ‘¤ Job Seeker<br/>(Browser/App)"] -->|"GET /search?q=...&loc=..."| B["Load Balancer"]
    B --> C["API Gateway"]
    C -->|"Route"| D["Search Service"]
    D -->|"Check cache"| E[("Cache<br/>(In-Memory)")]
    E -.->|"Cache miss"| F[("Search Index<br/>(Inverted Index)")]
    D -->|"Query"| F
    F -->|"Return job IDs + snippets"| D
    D -->|"Fetch full details<br/>on click"| G[("Job DB<br/>(SQL)")]
    D -->|"Response"| A
    H["CDN"] -->|"Serve static assets<br/>(logos, images)"| A

    style E fill:#dbeafe,stroke:#2563eb
    style F fill:#fef3c7,stroke:#d97706
    style G fill:#fef3c7,stroke:#d97706
</div>

<h3>Flow 2 â€” Examples</h3>

<div class="example-box">
<strong>Example 1 (Keyword + Location Search):</strong> A job seeker types "data scientist" in the search bar, sets location to "New York, NY" with a 25-mile radius, and clicks Search. The browser sends <code>HTTP GET /search?q=data+scientist&location=New+York,NY&radius=25mi&page=1</code> to the Load Balancer â†’ API Gateway â†’ Search Service. The Search Service first checks the in-memory Cache for this exact query hash. Cache miss. It queries the Search Index with a full-text match on "data scientist," a geo-distance filter for 25 miles around NYC coordinates, and filters for <code>status=active</code>. The Search Index returns the top 20 matching job IDs with relevance scores, titles, company names, and salary snippets. The Search Service caches this result set (TTL: 5 minutes) and returns it to the client. The client renders the search results page. Company logos are served from the CDN.
</div>

<div class="example-box">
<strong>Example 2 (Cache Hit):</strong> Another seeker searches "data scientist" + "New York, NY" + 25-mile radius 30 seconds later. The Search Service hashes the query, checks the cache, and finds a hit. It returns the cached results immediately â€” eliminating the Search Index query entirely. Latency drops from ~120ms to ~15ms.
</div>

<div class="example-box">
<strong>Example 3 (Filtered Search with Pagination):</strong> A seeker searches "python developer" in "Remote" with salary â‰¥ $120K, job type = "Full-Time," posted within the last 7 days, page 2. <code>GET /search?q=python+developer&remote=true&salary_min=120000&job_type=full-time&posted_within=7d&page=2&limit=20</code>. The Search Service applies all filters against the Search Index, skips the first 20 results, and returns results 21â€“40.
</div>

<div class="example-box">
<strong>Example 4 (Click into Job Detail):</strong> The seeker clicks on a specific job in the search results. The client sends <code>GET /jobs/{jobId}</code>. The Job Service checks the cache for this job ID. On a cache miss, it queries the Job DB for the full job record (including full description, requirements, benefits, etc.) and caches it (TTL: 1 hour). Returns the complete job detail to the client.
</div>

<h3>Flow 2 â€” Component Deep Dive</h3>

<h4>Search Service</h4>
<table>
  <tr><th>Endpoint</th><th>Method</th><th>Input</th><th>Output</th><th>Description</th></tr>
  <tr><td><code>/search</code></td><td>GET</td><td>Query params: q (keywords), location, radius, salary_min, salary_max, job_type, remote, company, posted_within, sort_by, page, limit</td><td>200 OK â€” <code>{results: [{job_id, title, company_name, location, salary_range, posted_at, snippet}], total_count, page, limit}</code></td><td>Full-text search with filters, geo queries, and pagination. Returns denormalized summaries from the Search Index.</td></tr>
</table>
<p><strong>Protocol:</strong> HTTP/HTTPS (REST). Stateless. Reads from Cache first, then Search Index on miss. Does <em>not</em> write to the Search Index (that is the Search Indexer's job).</p>
<p>The Search Service also performs query parsing, spell correction / "did you mean" suggestions, synonym expansion (e.g., "SWE" â†’ "software engineer"), and relevance tuning.</p>

<h4>Cache (In-Memory)</h4>
<p>An in-memory key-value cache sits in front of the Search Index. See the dedicated Cache Deep Dive section below for full details on caching strategy, eviction, and expiration.</p>

<h4>CDN</h4>
<p>Serves static assets: company logos, UI assets (JS/CSS bundles), and images. See the dedicated CDN Deep Dive section below.</p>

<hr>

<!-- ==================== FLOW 3: JOB SEEKER APPLIES TO A JOB ==================== -->
<h2>5. Flow 3 â€” Job Seeker Applies to a Job</h2>

<div class="mermaid">
graph LR
    A["ðŸ‘¤ Job Seeker<br/>(Browser/App)"] -->|"POST /jobs/{jobId}/apply"| B["Load Balancer"]
    B --> C["API Gateway"]
    C -->|"Auth + Route"| D["Application<br/>Service"]
    D -->|"Upload resume"| E[("Object Storage")]
    D -->|"Write application"| F[("Application DB<br/>(SQL)")]
    D -->|"Publish: application_submitted"| G["Message Queue"]
    G -->|"Consume"| H["Notification<br/>Service"]
    H -->|"Email / Push"| I["ðŸ‘¤ Employer"]
    D -->|"201 Created"| A

    style E fill:#d1fae5,stroke:#059669
    style F fill:#fef3c7,stroke:#d97706
</div>

<h3>Flow 3 â€” Examples</h3>

<div class="example-box">
<strong>Example 1 (Happy Path â€” Apply with Existing Resume):</strong> A job seeker viewing the "Senior Software Engineer" posting at Acme Corp clicks "Apply Now." They select their primary resume (already uploaded) and type a short cover letter. The client sends <code>HTTP POST /jobs/{jobId}/apply</code> with <code>{resume_id: "abc-123", cover_letter: "I'm excited about..."}</code>. The Application Service checks that (a) the job exists and is active, (b) the seeker hasn't already applied. It fetches the resume URL from the Resumes table, writes a new row to the Application DB with <code>status = "submitted"</code>, and publishes an <code>application_submitted</code> event to the Message Queue. The Notification Service consumes the event and sends the employer a push notification: "New application received for Senior Software Engineer from Jane Doe." The seeker receives a <code>201 Created</code> response and sees a confirmation screen.
</div>

<div class="example-box">
<strong>Example 2 (Apply with New Resume Upload):</strong> The seeker wants to submit a custom resume for this role. They click "Upload New Resume" and select a PDF from their device. The client sends a multipart <code>POST /jobs/{jobId}/apply</code> with the resume file and cover letter. The Application Service first uploads the resume file to Object Storage, receiving back a storage URL. It then stores a new Resumes row, creates the Application row referencing the new resume URL, and publishes the event. The employer is notified as above.
</div>

<div class="example-box">
<strong>Example 3 (Duplicate Application â€” Rejected):</strong> The seeker accidentally clicks "Apply Now" again for the same job. The Application Service checks the <code>UNIQUE(job_id, user_id)</code> constraint. It finds an existing application and returns <code>409 Conflict</code>: "You have already applied to this job." No duplicate row is created.
</div>

<div class="example-box">
<strong>Example 4 (Closed Job â€” Rejected):</strong> A seeker tries to apply to a posting that was closed 10 minutes ago (they had the page cached). The Application Service looks up the job and finds <code>status = "closed"</code>. It returns <code>400 Bad Request</code>: "This job posting is no longer accepting applications."
</div>

<h3>Flow 3 â€” Component Deep Dive</h3>

<h4>Application Service</h4>
<table>
  <tr><th>Endpoint</th><th>Method</th><th>Input</th><th>Output</th><th>Description</th></tr>
  <tr><td><code>/jobs/{jobId}/apply</code></td><td>POST</td><td>Path: jobId; Body: resume_id (or multipart file), cover_letter (optional)</td><td>201 Created â€” <code>{application_id, status, created_at}</code></td><td>Submit an application. Validates job is active and no duplicate exists.</td></tr>
  <tr><td><code>/users/{userId}/applications</code></td><td>GET</td><td>Path: userId; Query: status, page, limit</td><td>200 OK â€” paginated list of applications with job title, company, status</td><td>Seeker views their submitted applications.</td></tr>
  <tr><td><code>/jobs/{jobId}/applications</code></td><td>GET</td><td>Path: jobId; Query: status, page, limit</td><td>200 OK â€” paginated list of applications with seeker name, resume link, status</td><td>Employer views applications for a specific posting.</td></tr>
  <tr><td><code>/applications/{appId}/status</code></td><td>PATCH</td><td>Path: appId; Body: <code>{"status": "shortlisted"}</code></td><td>200 OK â€” updated application</td><td>Employer updates application status. Publishes <code>application_status_changed</code> event to notify the seeker.</td></tr>
</table>
<p><strong>Protocol:</strong> HTTP/HTTPS (REST). Stateless. Writes to Application DB (SQL) and Object Storage.</p>

<h4>Object Storage</h4>
<p>Stores binary files: resumes (PDF, DOCX), cover letters. Provides pre-signed URLs for secure, time-limited downloads. Highly durable (11 nines). Files are keyed by <code>resumes/{user_id}/{resume_id}.{ext}</code>.</p>
<p><strong>Why Object Storage for resumes?</strong> Resumes are unstructured binary blobs ranging from 50KB to 10MB. Object storage is purpose-built for this: it scales horizontally, provides high durability, and supports direct browser upload/download via pre-signed URLs â€” offloading bandwidth from the Application Service.</p>

<h4>Application DB (SQL)</h4>
<p>Relational database storing application records. SQL chosen for ACID guarantees â€” an application must be atomically created with its unique constraint enforced. Also supports complex queries (filter by status, join with job/user data for employer dashboard).</p>

<hr>

<!-- ==================== FLOW 4: EMPLOYER REVIEWS APPLICATIONS ==================== -->
<h2>6. Flow 4 â€” Employer Reviews Applications</h2>

<div class="mermaid">
graph LR
    A["ðŸ‘¤ Employer<br/>(Browser/App)"] -->|"GET /jobs/{jobId}/applications"| B["Load Balancer"]
    B --> C["API Gateway"]
    C -->|"Auth + Route"| D["Application<br/>Service"]
    D -->|"Query applications"| E[("Application DB<br/>(SQL)")]
    D -->|"Return list"| A

    A -->|"Click 'View Resume'"| B
    B --> C
    C --> D
    D -->|"Generate pre-signed URL"| F[("Object Storage")]
    D -->|"Return URL"| A
    A -->|"Download resume"| F

    A -->|"PATCH /applications/{appId}/status"| B
    B --> C
    C --> D
    D -->|"Update status"| E
    D -->|"Publish: status_changed"| G["Message Queue"]
    G --> H["Notification<br/>Service"]
    H -->|"Notify seeker"| I["ðŸ“± Job Seeker"]

    style E fill:#fef3c7,stroke:#d97706
    style F fill:#d1fae5,stroke:#059669
</div>

<h3>Flow 4 â€” Examples</h3>

<div class="example-box">
<strong>Example 1 (View Applications):</strong> An employer at Acme Corp navigates to their "Senior Software Engineer" posting and clicks "View Applications." The client sends <code>GET /jobs/{jobId}/applications?status=submitted&page=1&limit=25</code>. The Application Service queries the Application DB for all applications with <code>job_id = {jobId}</code> and <code>status = "submitted"</code>, ordered by <code>created_at DESC</code>. It returns a paginated list: applicant name, applied date, and status. The employer sees 73 applications.
</div>

<div class="example-box">
<strong>Example 2 (View Resume):</strong> The employer clicks "View Resume" next to Jane Doe's application. The client sends <code>GET /applications/{appId}/resume</code>. The Application Service looks up the resume's Object Storage key, generates a pre-signed URL (valid for 15 minutes), and returns it. The browser directly downloads the PDF from Object Storage using the pre-signed URL â€” the Application Service never proxies the file bytes.
</div>

<div class="example-box">
<strong>Example 3 (Shortlist a Candidate):</strong> The employer reads Jane's resume, likes it, and clicks "Shortlist." The client sends <code>PATCH /applications/{appId}/status</code> with <code>{"status": "shortlisted"}</code>. The Application Service updates the application row in the Application DB and publishes an <code>application_status_changed</code> event. The Notification Service sends Jane a push notification: "Your application for Senior Software Engineer at Acme Corp has been shortlisted!"
</div>

<h3>Flow 4 â€” Component Deep Dive</h3>
<p>All components in this flow (Application Service, Application DB, Object Storage, Message Queue, Notification Service) have been described in previous flows. The key addition here is the <strong>pre-signed URL pattern</strong> for resume download: the Application Service never serves file bytes directly. Instead, it generates a temporary, signed URL that grants the employer time-limited read access to the file in Object Storage. This offloads bandwidth and keeps the service stateless.</p>

<hr>

<!-- ==================== FLOW 5: JOB ALERTS ==================== -->
<h2>7. Flow 5 â€” Job Seeker Sets Up &amp; Receives Job Alerts</h2>

<div class="mermaid">
graph LR
    subgraph "Alert Setup"
        A["ðŸ‘¤ Job Seeker"] -->|"POST /alerts"| B["Load Balancer"]
        B --> C["API Gateway"]
        C --> D["Alert Service"]
        D -->|"Write alert"| E[("Alert DB<br/>(SQL)")]
    end

    subgraph "Alert Trigger (on new job)"
        F["Message Queue<br/>(job_created)"] -->|"Consume"| G["Job Alert<br/>Service"]
        G -->|"Match against alerts"| E
        G -->|"Send notification"| H["Notification<br/>Service"]
        H -->|"Push / Email"| I["ðŸ“± Job Seekers"]
    end

    style E fill:#fef3c7,stroke:#d97706
</div>

<h3>Flow 5 â€” Examples</h3>

<div class="example-box">
<strong>Example 1 (Create Alert):</strong> A job seeker wants to be notified whenever a "Machine Learning Engineer" role is posted in "Austin, TX." They go to "Job Alerts" and create an alert: keywords = "machine learning engineer," location = "Austin, TX," job_type = any, frequency = "instant." The client sends <code>POST /alerts</code> with the criteria. The Alert Service writes the alert to the Alert DB with <code>is_active = true</code>. Returns <code>201 Created</code>.
</div>

<div class="example-box">
<strong>Example 2 (Instant Alert Triggered):</strong> An hour later, a startup posts a "Senior ML Engineer" role in Austin. The Job Service publishes a <code>job_created</code> event. The Job Alert Service consumes it, queries the Alert DB for active alerts where keywords overlap with the job title/description and location is within the alert's radius. It matches the seeker's alert. Since the frequency is "instant," it immediately dispatches a push notification and email via the Notification Service: "New job matching your alert: Senior ML Engineer at StartupX in Austin, TX."
</div>

<div class="example-box">
<strong>Example 3 (Daily Digest):</strong> Another seeker has a "daily" alert for "Product Manager" in "Remote." Over the course of the day, 12 new matching jobs are posted. Each time, the Job Alert Service identifies the match but does not send immediately. Instead, it queues the matches into a batch. At the scheduled digest time (e.g., 8:00 AM in the seeker's timezone), a scheduled job in the Job Alert Service compiles all 12 matches into a single digest email and sends it via the Notification Service.
</div>

<h3>Flow 5 â€” Component Deep Dive</h3>

<h4>Alert Service</h4>
<table>
  <tr><th>Endpoint</th><th>Method</th><th>Input</th><th>Output</th><th>Description</th></tr>
  <tr><td><code>/alerts</code></td><td>POST</td><td>Body: keywords, location, job_type, salary_min, frequency</td><td>201 Created â€” <code>{alert_id}</code></td><td>Creates a new job alert.</td></tr>
  <tr><td><code>/alerts</code></td><td>GET</td><td>Query: user_id</td><td>200 OK â€” list of user's alerts</td><td>Retrieves a seeker's saved alerts.</td></tr>
  <tr><td><code>/alerts/{alertId}</code></td><td>PUT</td><td>Path: alertId; Body: updated criteria</td><td>200 OK</td><td>Updates alert criteria or frequency.</td></tr>
  <tr><td><code>/alerts/{alertId}</code></td><td>DELETE</td><td>Path: alertId</td><td>204 No Content</td><td>Deactivates/deletes an alert.</td></tr>
</table>
<p><strong>Protocol:</strong> HTTP/HTTPS (REST).</p>

<h4>Job Alert Service (Consumer)</h4>
<p>This is the background consumer (distinct from the Alert CRUD Service above). It consumes <code>job_created</code> events and performs matching against the Alert DB. For scalability, matching can be done by pre-computing an inverted index of alert keywords â†’ alert_ids in memory, allowing O(1) lookup per keyword rather than scanning all alerts per job. For geo-matching, alerts are partitioned by region (coarse-grained buckets) to reduce the candidate set.</p>

<h4>Alert DB (SQL)</h4>
<p>Stores alert criteria. SQL chosen because alerts have a fixed schema, are relatively low-volume compared to jobs/applications, and benefit from indexed queries on <code>(is_active, location, keywords)</code>.</p>

<hr>

<!-- ==================== OVERALL COMBINED FLOW ==================== -->
<h2>8. Overall Combined Flow</h2>

<div class="mermaid">
graph TB
    subgraph "Clients"
        SEEKER["ðŸ‘¤ Job Seeker<br/>(Browser / Mobile App)"]
        EMPLOYER["ðŸ‘¤ Employer<br/>(Browser / Mobile App)"]
    end

    CDN["CDN<br/>(Static Assets,<br/>Company Logos)"]

    LB["Load Balancer<br/>(L7, TLS Termination)"]

    GW["API Gateway<br/>(Auth, Rate Limit, Routing)"]

    subgraph "Application Services"
        JS["Job Service"]
        SS["Search Service"]
        AS["Application Service"]
        ALS["Alert Service"]
    end

    subgraph "Async Processing"
        MQ["Message Queue"]
        SIS["Search Indexer Service"]
        JAS["Job Alert Service<br/>(Consumer)"]
        NS["Notification Service"]
    end

    subgraph "Data Stores"
        JDB[("Job DB<br/>(SQL)")]
        ADB[("Application DB<br/>(SQL)")]
        ALDB[("Alert DB<br/>(SQL)")]
        SI[("Search Index<br/>(Inverted Index)")]
        OS[("Object Storage<br/>(Resumes)")]
        CACHE[("Cache<br/>(In-Memory)")]
    end

    SEEKER --> CDN
    EMPLOYER --> CDN
    SEEKER --> LB
    EMPLOYER --> LB
    LB --> GW
    GW --> JS
    GW --> SS
    GW --> AS
    GW --> ALS

    JS --> JDB
    JS --> MQ
    JS --> CACHE

    SS --> CACHE
    SS --> SI
    SS --> JDB

    AS --> ADB
    AS --> OS
    AS --> MQ

    ALS --> ALDB

    MQ --> SIS
    MQ --> JAS
    MQ --> NS

    SIS --> SI
    JAS --> ALDB
    JAS --> NS

    NS -->|"Push / Email"| SEEKER
    NS -->|"Push / Email"| EMPLOYER

    style JDB fill:#fef3c7,stroke:#d97706
    style ADB fill:#fef3c7,stroke:#d97706
    style ALDB fill:#fef3c7,stroke:#d97706
    style SI fill:#fef3c7,stroke:#d97706
    style OS fill:#d1fae5,stroke:#059669
    style CACHE fill:#dbeafe,stroke:#2563eb
    style CDN fill:#ede9fe,stroke:#7c3aed
</div>

<h3>Overall Flow â€” Examples</h3>

<div class="example-box">
<strong>End-to-End Example 1 (Full Lifecycle â€” Job Posting â†’ Search â†’ Apply â†’ Review):</strong><br><br>
<strong>Step 1:</strong> An employer at Acme Corp creates a "Senior Backend Engineer" posting in San Francisco via <code>POST /jobs</code>. The Job Service writes it to the Job DB and publishes <code>job_created</code> to the Message Queue. The Search Indexer adds it to the Search Index. The Job Alert Service matches it to 200 alerts and sends instant notifications.<br><br>
<strong>Step 2:</strong> A job seeker named Alex, who received one of those instant push notifications, opens Indeed and taps the notification â€” which deep-links to <code>GET /jobs/{jobId}</code>. The Job Service returns the full posting from cache (populated when the employer created the job).<br><br>
<strong>Step 3:</strong> Alex clicks "Apply Now" and selects their primary resume. <code>POST /jobs/{jobId}/apply</code> creates an application in the Application DB and publishes <code>application_submitted</code> to the Message Queue. The Notification Service emails the Acme employer: "New application from Alex Chen."<br><br>
<strong>Step 4:</strong> The Acme hiring manager opens their dashboard (<code>GET /jobs/{jobId}/applications</code>), sees Alex's application, clicks "View Resume" (which generates a pre-signed Object Storage URL), reads the resume, and clicks "Shortlist" (<code>PATCH /applications/{appId}/status</code>). Alex receives a push notification: "You've been shortlisted!"
</div>

<div class="example-box">
<strong>End-to-End Example 2 (Search-Driven Discovery):</strong><br><br>
A job seeker named Priya visits Indeed.com and searches "product designer remote $100K+" (<code>GET /search?q=product+designer&remote=true&salary_min=100000</code>). The Search Service checks cache (miss), queries the Search Index, returns 15 results per page. Priya scrolls, clicks on a listing at DesignCo, reads the details (served from cache), saves it for later (<code>POST /users/{userId}/saved-jobs/{jobId}</code>), and sets up a "daily" job alert for the same criteria (<code>POST /alerts</code>). The next day at 8 AM, she receives a digest email with 5 new matching postings.
</div>

<hr>

<!-- ==================== SCHEMA ==================== -->
<h2>9. Database Schema</h2>

<h3>SQL Tables</h3>

<h4>9.1. <code>users</code> Table</h4>
<table>
  <tr><th>Column</th><th>Type</th><th>Constraints</th><th>Description</th></tr>
  <tr><td><code>user_id</code></td><td>UUID</td><td><strong>PRIMARY KEY</strong></td><td>Unique user identifier.</td></tr>
  <tr><td><code>email</code></td><td>VARCHAR(255)</td><td>UNIQUE, NOT NULL</td><td>Login email.</td></tr>
  <tr><td><code>password_hash</code></td><td>VARCHAR(255)</td><td>NOT NULL</td><td>Bcrypt-hashed password.</td></tr>
  <tr><td><code>name</code></td><td>VARCHAR(255)</td><td>NOT NULL</td><td>Display name.</td></tr>
  <tr><td><code>user_type</code></td><td>ENUM('job_seeker','employer')</td><td>NOT NULL</td><td>Role.</td></tr>
  <tr><td><code>location</code></td><td>VARCHAR(255)</td><td></td><td>User's city/region (for seekers).</td></tr>
  <tr><td><code>created_at</code></td><td>TIMESTAMP</td><td>NOT NULL</td><td>Account creation time.</td></tr>
  <tr><td><code>updated_at</code></td><td>TIMESTAMP</td><td>NOT NULL</td><td>Last profile update.</td></tr>
</table>
<p><strong>Why SQL:</strong> User data is highly structured, requires uniqueness constraints (email), and is involved in relational joins (users â†” applications, users â†” jobs). ACID properties ensure account creation and updates are atomic.</p>
<p><strong>Indexes:</strong></p>
<ul>
  <li><code>UNIQUE B-tree index on (email)</code> â€” Ensures uniqueness and enables fast login lookups by email. B-tree chosen because lookups are exact-match and it also supports range scans for admin queries.</li>
</ul>
<p><strong>Read/Write events:</strong></p>
<ul>
  <li><strong>Write:</strong> User registers, user updates profile.</li>
  <li><strong>Read:</strong> User logs in (lookup by email), Application Service fetches user name for employer dashboard, profile page views.</li>
</ul>

<h4>9.2. <code>companies</code> Table</h4>
<table>
  <tr><th>Column</th><th>Type</th><th>Constraints</th><th>Description</th></tr>
  <tr><td><code>company_id</code></td><td>UUID</td><td><strong>PRIMARY KEY</strong></td><td>Unique company identifier.</td></tr>
  <tr><td><code>name</code></td><td>VARCHAR(255)</td><td>NOT NULL</td><td>Company name.</td></tr>
  <tr><td><code>description</code></td><td>TEXT</td><td></td><td>Company overview.</td></tr>
  <tr><td><code>logo_url</code></td><td>VARCHAR(512)</td><td></td><td>URL to logo in Object Storage/CDN.</td></tr>
  <tr><td><code>website</code></td><td>VARCHAR(512)</td><td></td><td>Company website.</td></tr>
  <tr><td><code>industry</code></td><td>VARCHAR(100)</td><td></td><td>Industry category.</td></tr>
  <tr><td><code>size</code></td><td>VARCHAR(50)</td><td></td><td>Company size bracket (e.g., "1001-5000").</td></tr>
  <tr><td><code>headquarters_location</code></td><td>VARCHAR(255)</td><td></td><td>HQ location.</td></tr>
  <tr><td><code>created_by</code></td><td>UUID</td><td><strong>FOREIGN KEY â†’ users.user_id</strong></td><td>Employer who registered the company.</td></tr>
  <tr><td><code>created_at</code></td><td>TIMESTAMP</td><td>NOT NULL</td><td></td></tr>
  <tr><td><code>updated_at</code></td><td>TIMESTAMP</td><td>NOT NULL</td><td></td></tr>
</table>
<p><strong>Why SQL:</strong> Fixed schema, low volume (~millions of companies vs. hundreds of millions of jobs), relational join to jobs. ACID for profile updates.</p>
<p><strong>Indexes:</strong></p>
<ul>
  <li><code>B-tree index on (name)</code> â€” Enables fast company name search/autocomplete for seekers browsing company pages.</li>
  <li><code>B-tree index on (created_by)</code> â€” Fast lookup of companies managed by a specific employer user.</li>
</ul>
<p><strong>Read/Write events:</strong></p>
<ul>
  <li><strong>Write:</strong> Employer creates/updates company profile.</li>
  <li><strong>Read:</strong> Seeker views company page, Search Indexer denormalizes company name into job documents, employer dashboard.</li>
</ul>

<h4>9.3. <code>jobs</code> Table</h4>
<table>
  <tr><th>Column</th><th>Type</th><th>Constraints</th><th>Description</th></tr>
  <tr><td><code>job_id</code></td><td>UUID</td><td><strong>PRIMARY KEY</strong></td><td>Unique job identifier.</td></tr>
  <tr><td><code>company_id</code></td><td>UUID</td><td><strong>FOREIGN KEY â†’ companies.company_id</strong></td><td>Posting company.</td></tr>
  <tr><td><code>posted_by</code></td><td>UUID</td><td><strong>FOREIGN KEY â†’ users.user_id</strong></td><td>Employer who posted.</td></tr>
  <tr><td><code>title</code></td><td>VARCHAR(255)</td><td>NOT NULL</td><td>Job title.</td></tr>
  <tr><td><code>description</code></td><td>TEXT</td><td>NOT NULL</td><td>Full job description (may include HTML/Markdown).</td></tr>
  <tr><td><code>requirements</code></td><td>TEXT</td><td></td><td>Qualification requirements.</td></tr>
  <tr><td><code>salary_min</code></td><td>INTEGER</td><td></td><td>Minimum salary (annual, USD).</td></tr>
  <tr><td><code>salary_max</code></td><td>INTEGER</td><td></td><td>Maximum salary.</td></tr>
  <tr><td><code>location</code></td><td>VARCHAR(255)</td><td></td><td>Job location text (e.g., "San Francisco, CA").</td></tr>
  <tr><td><code>latitude</code></td><td>DECIMAL(9,6)</td><td></td><td>Geocoded latitude.</td></tr>
  <tr><td><code>longitude</code></td><td>DECIMAL(9,6)</td><td></td><td>Geocoded longitude.</td></tr>
  <tr><td><code>job_type</code></td><td>ENUM('full_time','part_time','contract','internship','temporary')</td><td>NOT NULL</td><td></td></tr>
  <tr><td><code>remote_policy</code></td><td>ENUM('remote','hybrid','on_site')</td><td>NOT NULL</td><td></td></tr>
  <tr><td><code>status</code></td><td>ENUM('active','closed','draft','expired')</td><td>NOT NULL, DEFAULT 'draft'</td><td></td></tr>
  <tr><td><code>created_at</code></td><td>TIMESTAMP</td><td>NOT NULL</td><td></td></tr>
  <tr><td><code>updated_at</code></td><td>TIMESTAMP</td><td>NOT NULL</td><td></td></tr>
  <tr><td><code>expires_at</code></td><td>TIMESTAMP</td><td></td><td>Auto-close date.</td></tr>
</table>
<p><strong>Why SQL:</strong> Jobs have a well-defined relational schema (foreign keys to companies and users). Employers frequently update job status and details, requiring ACID transactional guarantees. Complex filtered queries on the employer dashboard (filter by status, sort by date) are well-served by SQL.</p>
<p><strong>Indexes:</strong></p>
<ul>
  <li><code>B-tree composite index on (company_id, status, created_at DESC)</code> â€” Employer dashboard query: "Show me all active jobs for my company, newest first." The composite index covers this query entirely.</li>
  <li><code>B-tree composite index on (status, created_at DESC)</code> â€” Used by the Search Indexer to find recently updated active jobs for re-indexing.</li>
  <li><code>B-tree index on (posted_by)</code> â€” Lookup jobs by the employer who posted them.</li>
  <li><code>B-tree index on (expires_at)</code> â€” A background job periodically scans for expired postings (<code>WHERE expires_at < NOW() AND status = 'active'</code>) and marks them expired.</li>
</ul>
<p><strong>Sharding Strategy:</strong> The jobs table will grow to hundreds of millions of rows. Shard by <code>company_id</code> (hash-based sharding). This co-locates all jobs from the same company on the same shard, which is optimal because the most common query pattern from the employer side is "all jobs for my company." Cross-company queries (e.g., search) go through the Search Index, not the SQL database, so cross-shard queries are rare.</p>
<p><strong>Read/Write events:</strong></p>
<ul>
  <li><strong>Write:</strong> Employer creates/edits/closes a job posting.</li>
  <li><strong>Read:</strong> Seeker views a job detail page, employer views their job listings, Search Indexer reads for indexing, Application Service validates job is active before accepting an application.</li>
</ul>

<h4>9.4. <code>applications</code> Table</h4>
<table>
  <tr><th>Column</th><th>Type</th><th>Constraints</th><th>Description</th></tr>
  <tr><td><code>application_id</code></td><td>UUID</td><td><strong>PRIMARY KEY</strong></td><td>Unique application identifier.</td></tr>
  <tr><td><code>job_id</code></td><td>UUID</td><td><strong>FOREIGN KEY â†’ jobs.job_id</strong></td><td>The job applied to.</td></tr>
  <tr><td><code>user_id</code></td><td>UUID</td><td><strong>FOREIGN KEY â†’ users.user_id</strong></td><td>The applying seeker.</td></tr>
  <tr><td><code>resume_url</code></td><td>VARCHAR(512)</td><td>NOT NULL</td><td>Object Storage URL of the resume used.</td></tr>
  <tr><td><code>cover_letter</code></td><td>TEXT</td><td></td><td>Optional cover letter text.</td></tr>
  <tr><td><code>status</code></td><td>ENUM('submitted','viewed','shortlisted','rejected','hired')</td><td>NOT NULL, DEFAULT 'submitted'</td><td>Application pipeline status.</td></tr>
  <tr><td><code>created_at</code></td><td>TIMESTAMP</td><td>NOT NULL</td><td>Submission time.</td></tr>
  <tr><td><code>updated_at</code></td><td>TIMESTAMP</td><td>NOT NULL</td><td>Last status change.</td></tr>
</table>
<p><strong>Constraints:</strong> <code>UNIQUE(job_id, user_id)</code> â€” prevents duplicate applications.</p>
<p><strong>Why SQL:</strong> Applications are transactional â€” they must be atomically created with duplicate detection (unique constraint). Employers need complex queries (filter by status, paginate, join with user data). The data is inherently relational (application â†” job â†” user).</p>
<p><strong>Indexes:</strong></p>
<ul>
  <li><code>UNIQUE B-tree composite index on (job_id, user_id)</code> â€” Enforces no-duplicate-application constraint and enables fast "has this user applied to this job?" lookups.</li>
  <li><code>B-tree composite index on (job_id, status, created_at DESC)</code> â€” Employer query: "Show me all submitted applications for this job, newest first."</li>
  <li><code>B-tree composite index on (user_id, created_at DESC)</code> â€” Seeker query: "Show me all my applications, newest first."</li>
</ul>
<p><strong>Sharding Strategy:</strong> Shard by <code>job_id</code> (hash-based). This co-locates all applications for the same job on one shard, which is optimal for the employer's primary query pattern (view all applications for a job). The seeker's "my applications" query (<code>WHERE user_id = ?</code>) becomes a cross-shard scatter query, but this is acceptable because: (a) it is a lower-frequency query, (b) seekers typically have few applications (dozens, not thousands), and (c) results can be cached.</p>
<p><strong>Read/Write events:</strong></p>
<ul>
  <li><strong>Write:</strong> Seeker applies to a job, employer updates application status.</li>
  <li><strong>Read:</strong> Employer views applications for a job, seeker views their application history.</li>
</ul>

<h4>9.5. <code>resumes</code> Table</h4>
<table>
  <tr><th>Column</th><th>Type</th><th>Constraints</th><th>Description</th></tr>
  <tr><td><code>resume_id</code></td><td>UUID</td><td><strong>PRIMARY KEY</strong></td><td></td></tr>
  <tr><td><code>user_id</code></td><td>UUID</td><td><strong>FOREIGN KEY â†’ users.user_id</strong></td><td>Owning seeker.</td></tr>
  <tr><td><code>file_url</code></td><td>VARCHAR(512)</td><td>NOT NULL</td><td>Object Storage URL.</td></tr>
  <tr><td><code>file_name</code></td><td>VARCHAR(255)</td><td>NOT NULL</td><td>Original file name for display.</td></tr>
  <tr><td><code>file_size_bytes</code></td><td>INTEGER</td><td></td><td>File size for UI display.</td></tr>
  <tr><td><code>is_primary</code></td><td>BOOLEAN</td><td>DEFAULT false</td><td>Whether this is the default resume.</td></tr>
  <tr><td><code>created_at</code></td><td>TIMESTAMP</td><td>NOT NULL</td><td></td></tr>
</table>
<p><strong>Why SQL:</strong> Low volume per user (a seeker typically has 1-5 resumes). Needs foreign key to users. Simple queries.</p>
<p><strong>Indexes:</strong></p>
<ul>
  <li><code>B-tree index on (user_id)</code> â€” Lookup all resumes for a given user.</li>
</ul>
<p><strong>Read/Write events:</strong></p>
<ul>
  <li><strong>Write:</strong> Seeker uploads a resume, sets a resume as primary.</li>
  <li><strong>Read:</strong> Seeker views their resumes (to select one for an application), Application Service resolves resume URL.</li>
</ul>

<h4>9.6. <code>saved_jobs</code> Table</h4>
<table>
  <tr><th>Column</th><th>Type</th><th>Constraints</th><th>Description</th></tr>
  <tr><td><code>user_id</code></td><td>UUID</td><td><strong>PRIMARY KEY (part 1), FOREIGN KEY â†’ users.user_id</strong></td><td></td></tr>
  <tr><td><code>job_id</code></td><td>UUID</td><td><strong>PRIMARY KEY (part 2), FOREIGN KEY â†’ jobs.job_id</strong></td><td></td></tr>
  <tr><td><code>created_at</code></td><td>TIMESTAMP</td><td>NOT NULL</td><td>When the job was saved.</td></tr>
</table>
<p><strong>Composite Primary Key:</strong> <code>(user_id, job_id)</code> â€” ensures a user can't save the same job twice.</p>
<p><strong>Why SQL:</strong> Simple join table with relational integrity (FK to users and jobs). Low complexity.</p>
<p><strong>Indexes:</strong></p>
<ul>
  <li>The composite PK itself serves as the index for "all saved jobs for a user" (since <code>user_id</code> is the leading column).</li>
  <li><code>B-tree index on (job_id)</code> â€” If we need to count how many users saved a job (for "X people saved this job" UI feature).</li>
</ul>
<p><strong>Read/Write events:</strong></p>
<ul>
  <li><strong>Write:</strong> Seeker saves/unsaves a job.</li>
  <li><strong>Read:</strong> Seeker views their saved jobs list.</li>
</ul>

<h4>9.7. <code>job_alerts</code> Table</h4>
<table>
  <tr><th>Column</th><th>Type</th><th>Constraints</th><th>Description</th></tr>
  <tr><td><code>alert_id</code></td><td>UUID</td><td><strong>PRIMARY KEY</strong></td><td></td></tr>
  <tr><td><code>user_id</code></td><td>UUID</td><td><strong>FOREIGN KEY â†’ users.user_id</strong></td><td></td></tr>
  <tr><td><code>keywords</code></td><td>VARCHAR(500)</td><td>NOT NULL</td><td>Comma-separated keywords or phrase.</td></tr>
  <tr><td><code>location</code></td><td>VARCHAR(255)</td><td></td><td>Target location (nullable for "anywhere").</td></tr>
  <tr><td><code>latitude</code></td><td>DECIMAL(9,6)</td><td></td><td>Geocoded location for radius matching.</td></tr>
  <tr><td><code>longitude</code></td><td>DECIMAL(9,6)</td><td></td><td></td></tr>
  <tr><td><code>radius_miles</code></td><td>INTEGER</td><td>DEFAULT 25</td><td></td></tr>
  <tr><td><code>job_type</code></td><td>VARCHAR(50)</td><td></td><td>Filter (nullable = any type).</td></tr>
  <tr><td><code>salary_min</code></td><td>INTEGER</td><td></td><td>Minimum salary filter.</td></tr>
  <tr><td><code>frequency</code></td><td>ENUM('instant','daily','weekly')</td><td>NOT NULL, DEFAULT 'daily'</td><td>Notification frequency.</td></tr>
  <tr><td><code>is_active</code></td><td>BOOLEAN</td><td>DEFAULT true</td><td></td></tr>
  <tr><td><code>created_at</code></td><td>TIMESTAMP</td><td>NOT NULL</td><td></td></tr>
</table>
<p><strong>Why SQL:</strong> Structured criteria fields, relational FK to users, and complex matching queries (WHERE keyword LIKE ... AND location within radius AND ...). Moderate volume (~tens of millions of alerts).</p>
<p><strong>Indexes:</strong></p>
<ul>
  <li><code>B-tree index on (user_id)</code> â€” User views their alerts.</li>
  <li><code>B-tree composite index on (is_active, frequency)</code> â€” Job Alert Service queries for all active instant alerts to match against incoming jobs.</li>
  <li><code>Full-text / inverted index on (keywords)</code> â€” Enables efficient keyword matching against incoming job titles. This is critical for the alert matching process. An inverted index maps each keyword token to the set of alert_ids containing it, enabling fast intersection-based matching.</li>
</ul>
<p><strong>Read/Write events:</strong></p>
<ul>
  <li><strong>Write:</strong> Seeker creates/updates/deletes an alert.</li>
  <li><strong>Read:</strong> Seeker views their alerts. Job Alert Service matches incoming jobs against active alerts.</li>
</ul>

<h3>Search Index (Inverted Index â€” Not a traditional DB)</h3>

<h4>9.8. Search Index Document Schema</h4>
<table>
  <tr><th>Field</th><th>Type</th><th>Source</th><th>Description</th></tr>
  <tr><td><code>job_id</code></td><td>keyword (exact match)</td><td>jobs.job_id</td><td>Document ID.</td></tr>
  <tr><td><code>title</code></td><td>text (analyzed, full-text)</td><td>jobs.title</td><td>Tokenized and analyzed for full-text search.</td></tr>
  <tr><td><code>description</code></td><td>text (analyzed, full-text)</td><td>jobs.description</td><td>Full-text searchable.</td></tr>
  <tr><td><code>requirements</code></td><td>text (analyzed, full-text)</td><td>jobs.requirements</td><td></td></tr>
  <tr><td><code>company_name</code></td><td>text + keyword</td><td><strong>companies.name</strong> (denormalized)</td><td>Denormalized from companies table for search without join.</td></tr>
  <tr><td><code>company_id</code></td><td>keyword</td><td>jobs.company_id</td><td>For filtering by company.</td></tr>
  <tr><td><code>location_text</code></td><td>text</td><td>jobs.location</td><td>Human-readable location for display.</td></tr>
  <tr><td><code>geo_point</code></td><td>geo_point</td><td>jobs.latitude, jobs.longitude</td><td>For geo-distance queries (e.g., within 25 miles).</td></tr>
  <tr><td><code>salary_min</code></td><td>integer</td><td>jobs.salary_min</td><td>Range filter.</td></tr>
  <tr><td><code>salary_max</code></td><td>integer</td><td>jobs.salary_max</td><td>Range filter.</td></tr>
  <tr><td><code>job_type</code></td><td>keyword</td><td>jobs.job_type</td><td>Exact-match facet filter.</td></tr>
  <tr><td><code>remote_policy</code></td><td>keyword</td><td>jobs.remote_policy</td><td>Facet filter.</td></tr>
  <tr><td><code>posted_at</code></td><td>date</td><td>jobs.created_at</td><td>Recency ranking and "posted within" filter.</td></tr>
  <tr><td><code>status</code></td><td>keyword</td><td>jobs.status</td><td>Only <code>active</code> jobs are indexed; this field allows the indexer to filter.</td></tr>
</table>

<div class="info-box">
<strong>Denormalization Explanation:</strong> <code>company_name</code> is denormalized from the <code>companies</code> table into the Search Index document. This is done because the Search Index does not support joins. Every search result must display the company name alongside the job title, and performing a separate DB lookup for each of 20 results would add unacceptable latency. Since company names change extremely rarely, the cost of denormalization (re-indexing affected jobs when a company name changes) is negligible compared to the read performance benefit.
</div>

<p><strong>Read/Write events:</strong></p>
<ul>
  <li><strong>Write:</strong> Search Indexer Service consumes message queue events and upserts/deletes documents.</li>
  <li><strong>Read:</strong> Search Service queries the index on every search request.</li>
</ul>

<hr>

<!-- ==================== CDN DEEP DIVE ==================== -->
<h2>10. CDN Deep Dive</h2>

<h3>Why a CDN is appropriate</h3>
<p>Indeed serves a globally distributed user base. A CDN is highly appropriate for:</p>
<ul>
  <li><strong>Static assets:</strong> JavaScript bundles, CSS files, images, fonts â€” these are identical for all users and benefit enormously from edge caching.</li>
  <li><strong>Company logos:</strong> Displayed on every search result. A popular search showing 20 results loads 20 logos. Serving these from edge PoPs (Points of Presence) close to users drastically reduces latency and offloads origin servers.</li>
  <li><strong>Job listing pages (optional):</strong> For very popular job postings viewed thousands of times, the CDN can cache the rendered HTML or API JSON response at the edge.</li>
</ul>

<h3>What the CDN does NOT cache</h3>
<ul>
  <li><strong>Search results:</strong> Personalized and parameterized â€” too many unique combinations to cache at the CDN level. Cached at the application-level in-memory cache instead.</li>
  <li><strong>Resumes:</strong> Private, sensitive documents. Must be accessed via pre-signed URLs with short TTLs, not publicly cached on a CDN.</li>
  <li><strong>Application data:</strong> Private, personalized.</li>
</ul>

<h3>CDN Configuration</h3>
<table>
  <tr><th>Aspect</th><th>Setting</th><th>Rationale</th></tr>
  <tr><td>TTL for static assets</td><td>1 year (with cache-busting via content hashing in filenames)</td><td>Immutable assets like <code>app.a1b2c3.js</code> can be cached forever; new deploys produce new filenames.</td></tr>
  <tr><td>TTL for company logos</td><td>24 hours</td><td>Logos change infrequently. 24-hour TTL balances freshness with cache hit rate.</td></tr>
  <tr><td>Cache invalidation</td><td>Purge API on logo upload</td><td>When an employer updates their logo, the origin sends a purge request to the CDN.</td></tr>
  <tr><td>Origin</td><td>Object Storage for logos; build artifact server for static assets</td><td>Separation of concerns.</td></tr>
</table>

<hr>

<!-- ==================== CACHE DEEP DIVE ==================== -->
<h2>11. In-Memory Cache Deep Dive</h2>

<h3>Why an In-Memory Cache is appropriate</h3>
<p>Indeed's dominant workload is <strong>read-heavy</strong>. Millions of search queries per minute hit the same or similar queries. Without caching, every search would query the Search Index â€” adding latency and load. An in-memory cache dramatically reduces Search Index load and improves p95 latency.</p>

<h3>What is cached</h3>
<table>
  <tr><th>Cache Namespace</th><th>Key</th><th>Value</th><th>TTL</th><th>Rationale</th></tr>
  <tr><td>Search Results</td><td>Hash of (query, filters, sort, page)</td><td>Serialized search results (job IDs, titles, snippets)</td><td>5 minutes</td><td>Popular queries like "software engineer New York" are repeated thousands of times per minute. 5-minute TTL balances freshness (new jobs appear within 5 min) vs. cache hit rate (~80%+ for popular queries).</td></tr>
  <tr><td>Job Details</td><td><code>job:{job_id}</code></td><td>Full job JSON</td><td>1 hour</td><td>Job details rarely change. Popular postings get viewed thousands of times. 1-hour TTL because updates (edits, closures) should reflect within an hour; cache is also invalidated on <code>job_updated</code>/<code>job_closed</code> events.</td></tr>
  <tr><td>Company Profiles</td><td><code>company:{company_id}</code></td><td>Company JSON</td><td>6 hours</td><td>Company profiles change very rarely. High reuse across all jobs by that company.</td></tr>
</table>

<h3>Caching Strategy: Cache-Aside (Lazy Loading)</h3>
<p>The <strong>cache-aside</strong> pattern is used:</p>
<ol>
  <li>On request, the service checks the cache for the key.</li>
  <li>On <strong>cache hit</strong>: return cached data directly.</li>
  <li>On <strong>cache miss</strong>: query the Search Index (or Job DB), populate the cache with the result, and return the data.</li>
</ol>
<p><strong>Why cache-aside instead of write-through?</strong> Write-through would populate the cache on every write (job creation/update). However, most jobs are only viewed a handful of times â€” many niche postings receive very few views. Cache-aside ensures only <em>actually requested</em> data enters the cache, avoiding cache pollution with rarely-accessed items. This is especially important for search result caching where the key space (unique query combinations) is enormous.</p>
<p><strong>Exception â€” Active Invalidation:</strong> When the Job Service publishes <code>job_updated</code> or <code>job_closed</code> events, a cache invalidation consumer deletes the corresponding <code>job:{job_id}</code> key from the cache. This ensures stale data (e.g., a closed job still showing as active) is evicted promptly rather than waiting for TTL expiry. This hybrid approach (cache-aside for population + event-driven invalidation for freshness) provides the best balance.</p>

<h3>Eviction Policy: LRU (Least Recently Used)</h3>
<p>LRU eviction is chosen because:</p>
<ul>
  <li>Job search traffic follows a power-law distribution: a small percentage of queries (popular job titles + major metro locations) account for a large percentage of traffic.</li>
  <li>LRU keeps "hot" queries in cache and evicts long-tail queries that haven't been accessed recently.</li>
  <li>Alternative considered: LFU (Least Frequently Used) â€” could work well too but is more complex to implement and can be slow to adapt to changing trends (e.g., sudden spike in "AI engineer" searches). LRU adapts faster.</li>
</ul>

<h3>Expiration Policy (TTL)</h3>
<ul>
  <li><strong>Search results: 5 minutes.</strong> Short TTL ensures new job postings appear in results within minutes. Acceptable trade-off: a seeker might see a just-closed job in results and get a "job closed" message on click.</li>
  <li><strong>Job details: 1 hour</strong> + event-driven invalidation. The combination ensures that under normal operation, stale data is evicted within seconds of a change (via the event), while the TTL serves as a safety net if event delivery is delayed.</li>
  <li><strong>Company profiles: 6 hours.</strong> Very low change frequency justifies a longer TTL for higher cache hit rates.</li>
</ul>

<hr>

<!-- ==================== MESSAGE QUEUE DEEP DIVE ==================== -->
<h2>12. Message Queue Deep Dive</h2>

<h3>Why a Message Queue</h3>
<p>The system has several operations that should be <strong>asynchronous</strong>: search indexing, job alert matching, and notification delivery. These should not block the synchronous request path (e.g., an employer shouldn't wait for search indexing to complete before getting a "job created" response).</p>

<h3>How it works</h3>
<ul>
  <li><strong>Producers:</strong> Job Service (publishes <code>job_created</code>, <code>job_updated</code>, <code>job_closed</code>), Application Service (publishes <code>application_submitted</code>, <code>application_status_changed</code>).</li>
  <li><strong>Topics:</strong> Messages are published to named topics (e.g., <code>jobs</code> topic, <code>applications</code> topic). Each topic can have multiple consumer groups.</li>
  <li><strong>Consumer Groups:</strong>
    <ul>
      <li>The Search Indexer subscribes to the <code>jobs</code> topic.</li>
      <li>The Job Alert Service subscribes to the <code>jobs</code> topic (separate consumer group â€” both get every message).</li>
      <li>The Notification Service subscribes to the <code>applications</code> topic.</li>
      <li>The Cache Invalidation consumer subscribes to the <code>jobs</code> topic to invalidate stale cache entries.</li>
    </ul>
  </li>
  <li><strong>Delivery semantics:</strong> At-least-once. Consumers acknowledge messages after processing. If a consumer crashes before ack, the message is redelivered. Consumers must be idempotent (e.g., re-indexing a job is safe; re-sending a notification uses deduplication by event ID).</li>
  <li><strong>Ordering:</strong> Messages within a partition are ordered by publish time. Jobs are partitioned by <code>job_id</code>, so all events for the same job are processed in order (preventing a <code>job_updated</code> from being processed before <code>job_created</code>).</li>
  <li><strong>Durability:</strong> Messages are persisted to disk with replication. If all consumers are down, messages queue up and are not lost.</li>
</ul>

<h3>Why not polling?</h3>
<p>Polling (e.g., Search Indexer polls the Job DB every 5 seconds for new jobs) would introduce unnecessary load on the Job DB, add latency (up to 5 seconds delay), and scale poorly. The message queue provides real-time push-based notification with no extra DB load.</p>

<h3>Why not WebSockets?</h3>
<p>WebSockets are a client â†” server communication pattern. The message queue connects backend services, not clients. WebSockets are not applicable for service-to-service async communication.</p>

<h3>Why not direct pub/sub?</h3>
<p>A pure pub/sub system could work, but the message queue provides stronger durability guarantees (messages are not lost if no subscriber is online) and consumer group semantics (multiple instances of the same service share the workload). The chosen message queue supports pub/sub-style fan-out via topics, so it provides the best of both worlds.</p>

<hr>

<!-- ==================== SCALING CONSIDERATIONS ==================== -->
<h2>13. Scaling Considerations</h2>

<h3>Load Balancers</h3>
<p><strong>Where:</strong></p>
<ul>
  <li><strong>LB1 â€” Between Clients and API Gateway:</strong> Layer 7 (HTTP/S) load balancer. Terminates TLS, distributes traffic across API Gateway instances. Uses round-robin with health checks. Can also perform geographic routing (route European users to EU API Gateway instances).</li>
  <li><strong>LB2 â€” Between API Gateway and Services:</strong> Internal load balancer (or service mesh / service discovery) distributing requests from the API Gateway to multiple instances of Job Service, Search Service, Application Service, Alert Service. Uses least-connections algorithm to avoid overloading a single instance.</li>
</ul>

<h4>Load Balancer Deep Dive</h4>
<table>
  <tr><th>Aspect</th><th>LB1 (External)</th><th>LB2 (Internal)</th></tr>
  <tr><td>Layer</td><td>Layer 7 (HTTP/S)</td><td>Layer 7 (HTTP) or Layer 4 (TCP)</td></tr>
  <tr><td>TLS Termination</td><td>Yes</td><td>No (internal traffic is plain HTTP or mTLS)</td></tr>
  <tr><td>Algorithm</td><td>Round-robin with weighted health checks</td><td>Least connections</td></tr>
  <tr><td>Health Checks</td><td>HTTP GET /health every 10s, 3 failures = unhealthy</td><td>Same</td></tr>
  <tr><td>Session Affinity</td><td>Not needed (stateless services)</td><td>Not needed</td></tr>
  <tr><td>Scaling</td><td>Auto-scaled across multiple availability zones</td><td>Auto-scaled</td></tr>
</table>

<h3>Horizontal Scaling</h3>
<table>
  <tr><th>Component</th><th>Scaling Approach</th></tr>
  <tr><td>API Gateway</td><td>Horizontal scaling behind LB1. Stateless â€” add more instances to handle increased traffic.</td></tr>
  <tr><td>Job Service</td><td>Horizontal scaling behind LB2. Stateless.</td></tr>
  <tr><td>Search Service</td><td>Horizontal scaling behind LB2. This is the most heavily loaded service (read-heavy). Auto-scale based on CPU/request latency.</td></tr>
  <tr><td>Application Service</td><td>Horizontal scaling behind LB2. Moderate load.</td></tr>
  <tr><td>Search Index</td><td>Shard by geography or job_id hash. Add replicas for read throughput. Each shard handles a subset of documents; queries fan out to all shards and results are merged.</td></tr>
  <tr><td>Job DB</td><td>Vertical scaling initially. Then read replicas for read-heavy queries (job detail pages). Then shard by company_id for write scaling.</td></tr>
  <tr><td>Application DB</td><td>Read replicas. Shard by job_id for write scaling as volume grows.</td></tr>
  <tr><td>Cache</td><td>Cluster mode with hash-based partitioning. Add nodes as memory pressure increases.</td></tr>
  <tr><td>Message Queue</td><td>Add partitions per topic for parallelism. Add consumer instances (up to partition count).</td></tr>
  <tr><td>Object Storage</td><td>Inherently scalable. No manual intervention needed.</td></tr>
</table>

<h3>Multi-Region Deployment</h3>
<p>For global availability and low latency:</p>
<ul>
  <li>Deploy API Gateway, services, and caches in multiple regions (US, EU, APAC).</li>
  <li>The Job DB and Application DB use a primary-replica topology: primary region for writes, async replication to read replicas in other regions. Writes are always routed to the primary region.</li>
  <li>The Search Index is replicated per region â€” each region has a full copy of the index for low-latency search.</li>
  <li>CDN PoPs serve static assets globally.</li>
</ul>

<h3>Auto-Scaling Policies</h3>
<ul>
  <li>Scale services based on CPU utilization (target: 60-70%) and request latency (p95 &lt; SLA).</li>
  <li>Scale message queue consumers based on consumer lag (if messages queue up, add more consumers).</li>
  <li>Scale cache cluster based on memory utilization (target: &lt; 75%) and eviction rate.</li>
</ul>

<hr>

<!-- ==================== TRADEOFFS AND DEEP DIVES ==================== -->
<h2>14. Tradeoffs and Deep Dives</h2>

<h3>Search Index Freshness vs. Complexity</h3>
<p><strong>Tradeoff:</strong> Using a separate Search Index (inverted index) means job data exists in two places: the Job DB (source of truth) and the Search Index (derived). This introduces eventual consistency â€” a newly posted job may not appear in search for a few seconds.</p>
<p><strong>Why accepted:</strong> The alternative (querying the SQL database directly for full-text search with geo-filtering) does not scale. SQL LIKE queries are O(n), cannot rank by relevance, and cannot perform geo-distance calculations efficiently. A dedicated search index with inverted indexes, BM25 scoring, and geo-spatial indexes provides sub-200ms search across 300M+ documents.</p>

<h3>SQL vs. NoSQL for Core Tables</h3>
<p><strong>Tradeoff:</strong> SQL introduces more complex sharding compared to NoSQL. NoSQL databases provide built-in horizontal sharding.</p>
<p><strong>Why SQL:</strong> The core data (users, jobs, applications) is highly relational with foreign key constraints, unique constraints, and complex filtered/sorted queries. Application submission requires ACID (check job is active + check no duplicate + insert, atomically). NoSQL would push these guarantees to the application layer, increasing complexity and risk of bugs. The sharding overhead is manageable with hash-based sharding on carefully chosen keys.</p>

<h3>Denormalization in Search Index</h3>
<p><strong>Tradeoff:</strong> Denormalizing <code>company_name</code> into the Search Index means if a company renames, all their job documents must be re-indexed.</p>
<p><strong>Why accepted:</strong> Company renames are extremely rare (maybe a few per day globally). Each rename triggers re-indexing of that company's jobs (maybe a few hundred documents) â€” trivial. The alternative (looking up company names at search time via joins) would require a network call per search result, adding 50-100ms latency per search.</p>

<h3>Sharding Applications by job_id vs. user_id</h3>
<p><strong>Tradeoff:</strong> Sharding by <code>job_id</code> optimizes the employer's "view all applications for my job" query but makes the seeker's "view my applications" query a scatter-gather across shards.</p>
<p><strong>Why job_id:</strong> The employer's query is more latency-sensitive and higher-volume (employers review applications frequently and need fast pagination). The seeker's query returns a small result set (a person applies to maybe 50 jobs) and can tolerate slightly higher latency. Additionally, the seeker's applications can be cached in the application-level cache for fast retrieval.</p>

<h3>Cache-Aside vs. Write-Through</h3>
<p><strong>Tradeoff:</strong> Cache-aside may serve stale data on first request after TTL expiry. Write-through ensures cache is always fresh.</p>
<p><strong>Why cache-aside:</strong> The key space for search queries is enormous (millions of unique query combinations). Write-through would need to precompute which cache keys are affected by a new job â€” essentially running every possible search query for every new job. This is infeasible. Cache-aside only caches data that is actually requested, keeping cache utilization efficient.</p>

<hr>

<!-- ==================== ALTERNATIVE APPROACHES ==================== -->
<h2>15. Alternative Approaches</h2>

<h3>Alternative 1: Skip the Search Index â€” Use SQL Full-Text Search</h3>
<p><strong>Approach:</strong> Use SQL's built-in full-text search capabilities (e.g., GIN/tsvector indexes) directly on the <code>jobs</code> table instead of maintaining a separate Search Index.</p>
<p><strong>Why rejected:</strong></p>
<ul>
  <li>SQL full-text search does not scale well beyond ~10M rows for complex queries with geo-filtering, faceted aggregations, and relevance scoring.</li>
  <li>No built-in geo-distance queries â€” would need a separate solution.</li>
  <li>Cannot support features like "did you mean," synonym expansion, or fuzzy matching without significant custom code.</li>
  <li>Tight coupling between search reads and transactional writes on the same database would cause contention.</li>
</ul>

<h3>Alternative 2: Use NoSQL for Everything</h3>
<p><strong>Approach:</strong> Use a wide-column NoSQL database for all storage (users, jobs, applications) with denormalized data models.</p>
<p><strong>Why rejected:</strong></p>
<ul>
  <li>Applications require ACID transactions (atomic check-and-insert with unique constraint). NoSQL databases typically offer only per-row atomicity.</li>
  <li>Employers need complex filtered and sorted queries on the dashboard (e.g., "show me all shortlisted applications for my job, sorted by date"). NoSQL requires denormalizing into separate query-specific tables, significantly increasing write complexity and storage.</li>
  <li>The data is inherently relational (jobs belong to companies, applications reference jobs and users). Abandoning relations pushes referential integrity to the application layer.</li>
</ul>

<h3>Alternative 3: Real-Time Notifications via WebSockets</h3>
<p><strong>Approach:</strong> Instead of push notifications + email for job alerts, maintain persistent WebSocket connections to deliver real-time alerts in-app.</p>
<p><strong>Why rejected:</strong></p>
<ul>
  <li>Indeed is <strong>not a real-time application</strong>. Job seekers don't sit on the app 24/7 waiting for jobs. They check periodically.</li>
  <li>Maintaining WebSocket connections for 250M+ users would require enormous infrastructure for persistent connection management.</li>
  <li>Most alert delivery is via email (users check job alerts from their inbox). WebSockets only help for in-app delivery to users who happen to be online.</li>
  <li>Push notifications (APNs/FCM) handle the "user has app installed but is not currently using it" case much more efficiently.</li>
</ul>

<h3>Alternative 4: Monolithic Architecture</h3>
<p><strong>Approach:</strong> Build all functionality (job service, search, applications, alerts) as a single monolithic application.</p>
<p><strong>Why rejected:</strong></p>
<ul>
  <li>Search and application submission have vastly different scaling needs. Search is 100x higher volume than applications. A monolith cannot scale these independently.</li>
  <li>The search team, job posting team, and application team would step on each other with every deployment in a monolith.</li>
  <li>A bug in the alert system should not bring down job search. Microservices provide fault isolation.</li>
</ul>

<h3>Alternative 5: Polling Instead of Message Queue</h3>
<p><strong>Approach:</strong> Have the Search Indexer poll the Job DB every few seconds for new/changed jobs instead of using a message queue.</p>
<p><strong>Why rejected:</strong></p>
<ul>
  <li>Polling adds load to the Job DB proportional to poll frequency Ã— number of consumers.</li>
  <li>Higher latency: if you poll every 5 seconds, a new job won't be indexed for up to 5 seconds. The message queue provides near-instant delivery.</li>
  <li>No replay capability. If the indexer misses a cycle, those changes are lost. The message queue retains messages until acknowledged.</li>
  <li>Cannot fan out to multiple consumers without each one independently polling and tracking cursors.</li>
</ul>

<hr>

<!-- ==================== ADDITIONAL CONSIDERATIONS ==================== -->
<h2>16. Additional Considerations</h2>

<h3>Resume Parsing &amp; Structured Data</h3>
<p>After a resume is uploaded to Object Storage, an asynchronous resume parsing pipeline (triggered via the message queue) can extract structured data: skills, work history, education. This parsed data powers features like "skill-based job matching," auto-fill on the application form, and employer candidate search. The parsed data is stored in a separate <code>resume_parsed_data</code> table (or a NoSQL document store, since the schema varies per resume).</p>

<h3>Fraud &amp; Spam Detection</h3>
<p>Employers posting fake jobs and seekers submitting spam applications are real problems. A fraud detection service (ML-based) can analyze job postings for red flags (e.g., unrealistic salary, suspicious company) and flag applications that appear automated. This runs asynchronously via the message queue.</p>

<h3>Rate Limiting</h3>
<p>The API Gateway enforces rate limits per user (e.g., 100 searches/min, 10 applications/hour) to prevent abuse and ensure fair usage. Rate limit counters are stored in the in-memory cache with TTLs.</p>

<h3>Analytics &amp; Tracking</h3>
<p>Every search query, job view, and application event is logged to an analytics pipeline (separate from the main data path) for business intelligence: which keywords are trending, which postings get the most views, conversion rate from view to application. This data informs job ranking algorithms and employer-facing analytics dashboards.</p>

<h3>Job Expiration</h3>
<p>A scheduled background job runs daily, scanning for jobs where <code>expires_at < NOW() AND status = 'active'</code>. It updates their status to <code>expired</code> and publishes <code>job_closed</code> events so the Search Indexer removes them from results.</p>

<h3>Data Retention &amp; GDPR</h3>
<p>Users can request deletion of their data (GDPR "right to be forgotten"). The system must cascade-delete: user account â†’ their applications â†’ their resumes (from Object Storage) â†’ their alerts â†’ their saved jobs. A dedicated data deletion pipeline handles this asynchronously to avoid blocking the main request path.</p>

<h3>Idempotency</h3>
<p>Application submission includes an idempotency key (client-generated UUID) in the request header. If the client retries a failed submission (e.g., network timeout), the server detects the duplicate key and returns the existing application rather than creating a duplicate â€” even if the DB unique constraint check hasn't committed yet.</p>

<hr>

<!-- ==================== VENDOR SECTION ==================== -->
<h2>17. Potential Vendor Mapping</h2>

<div class="warn-box">
<strong>Note:</strong> The design above is vendor-agnostic. The following vendor suggestions are for reference only and can be swapped without architectural changes.
</div>

<table>
  <tr><th>Component</th><th>Potential Vendors</th><th>Rationale</th></tr>
  <tr><td>SQL Database</td><td>PostgreSQL, MySQL, Amazon Aurora, Google Cloud SQL</td><td>PostgreSQL is preferred for its rich feature set (GIN indexes, JSONB support, strong community). Aurora provides managed PostgreSQL-compatible with auto-scaling storage.</td></tr>
  <tr><td>Search Index</td><td>Elasticsearch, Apache Solr, OpenSearch</td><td>Elasticsearch is the de facto standard for full-text search with geo-spatial queries, faceted filters, and BM25 relevance scoring. OpenSearch is the open-source fork.</td></tr>
  <tr><td>In-Memory Cache</td><td>Redis, Memcached, KeyDB</td><td>Redis offers rich data structures (sorted sets for ranking, hash maps for structured data), TTL support, and cluster mode for scaling. Memcached is simpler but lacks persistence and advanced features.</td></tr>
  <tr><td>Message Queue</td><td>Apache Kafka, RabbitMQ, Amazon SQS/SNS, Apache Pulsar</td><td>Kafka excels at high-throughput event streaming with durable, ordered, partitioned topics â€” ideal for the multi-consumer pattern here (Search Indexer, Alert Service, Cache Invalidation all consume from the same topic). Pulsar is an alternative with native multi-tenancy.</td></tr>
  <tr><td>Object Storage</td><td>Amazon S3, Google Cloud Storage, Azure Blob Storage, MinIO</td><td>S3 is the industry standard with 11 nines durability, pre-signed URL support, and lifecycle policies. MinIO for on-prem.</td></tr>
  <tr><td>CDN</td><td>Cloudflare, Amazon CloudFront, Fastly, Akamai</td><td>Cloudflare offers global edge network, DDoS protection, and Worker scripts for edge compute. CloudFront integrates tightly with S3 origins.</td></tr>
  <tr><td>Load Balancer</td><td>AWS ALB/NLB, Google Cloud Load Balancer, NGINX, HAProxy, Envoy</td><td>AWS ALB for L7 HTTP load balancing with path-based routing. NGINX/Envoy for self-managed or service mesh scenarios.</td></tr>
  <tr><td>Push Notifications</td><td>APNs (iOS), FCM (Android), OneSignal</td><td>APNs and FCM are required for native iOS/Android push. OneSignal provides a unified abstraction.</td></tr>
  <tr><td>Email Delivery</td><td>SendGrid, Amazon SES, Mailgun, Postmark</td><td>SendGrid for transactional + marketing email with high deliverability rates and analytics.</td></tr>
</table>

<hr>

<p style="text-align: center; color: #64748b; margin-top: 60px;"><em>End of System Design: Indeed</em></p>

</body>
</html>
