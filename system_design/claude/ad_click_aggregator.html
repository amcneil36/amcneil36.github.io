<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>System Design: Ad Click Aggregator</title>
    <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
    <style>
        :root {
            --bg: #0f1117;
            --surface: #1a1d27;
            --border: #2d3148;
            --text: #e0e0e0;
            --heading: #ffffff;
            --accent: #6c9fff;
            --accent2: #a78bfa;
            --accent3: #34d399;
            --warn: #fbbf24;
            --code-bg: #161822;
            --example-bg: #14251e;
            --example-border: #22543d;
            --alt-bg: #1e1a2a;
            --alt-border: #3b2e6e;
        }
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body {
            font-family: 'Segoe UI', system-ui, -apple-system, sans-serif;
            background: var(--bg);
            color: var(--text);
            line-height: 1.7;
            padding: 2rem;
            max-width: 1200px;
            margin: 0 auto;
        }
        h1 {
            color: var(--heading);
            font-size: 2.4rem;
            border-bottom: 3px solid var(--accent);
            padding-bottom: 0.6rem;
            margin-bottom: 2rem;
        }
        h2 {
            color: var(--accent);
            font-size: 1.7rem;
            margin-top: 3rem;
            margin-bottom: 1rem;
            border-left: 4px solid var(--accent);
            padding-left: 0.8rem;
        }
        h3 {
            color: var(--accent2);
            font-size: 1.3rem;
            margin-top: 2rem;
            margin-bottom: 0.7rem;
        }
        h4 {
            color: var(--accent3);
            font-size: 1.1rem;
            margin-top: 1.5rem;
            margin-bottom: 0.5rem;
        }
        p { margin-bottom: 1rem; }
        ul, ol { margin-left: 1.5rem; margin-bottom: 1rem; }
        li { margin-bottom: 0.4rem; }
        code {
            background: var(--code-bg);
            padding: 0.15rem 0.4rem;
            border-radius: 4px;
            font-size: 0.92em;
            color: var(--accent3);
        }
        .section {
            background: var(--surface);
            border: 1px solid var(--border);
            border-radius: 10px;
            padding: 1.5rem 2rem;
            margin-bottom: 2rem;
        }
        .diagram-container {
            background: #ffffff;
            border-radius: 8px;
            padding: 1.5rem;
            margin: 1.5rem 0;
            text-align: center;
        }
        .example-box {
            background: var(--example-bg);
            border: 1px solid var(--example-border);
            border-radius: 8px;
            padding: 1.2rem 1.5rem;
            margin: 1rem 0;
        }
        .example-box strong {
            color: var(--accent3);
        }
        .alt-box {
            background: var(--alt-bg);
            border: 1px solid var(--alt-border);
            border-radius: 8px;
            padding: 1.2rem 1.5rem;
            margin: 1rem 0;
        }
        .alt-box strong {
            color: var(--accent2);
        }
        .warn-box {
            background: #1e1a0e;
            border: 1px solid #6b5a10;
            border-radius: 8px;
            padding: 1.2rem 1.5rem;
            margin: 1rem 0;
        }
        .warn-box strong {
            color: var(--warn);
        }
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 1rem 0 1.5rem 0;
        }
        th, td {
            border: 1px solid var(--border);
            padding: 0.6rem 1rem;
            text-align: left;
        }
        th {
            background: var(--code-bg);
            color: var(--accent);
            font-weight: 600;
        }
        td { color: var(--text); }
        .tag {
            display: inline-block;
            padding: 0.1rem 0.5rem;
            border-radius: 4px;
            font-size: 0.82em;
            font-weight: 600;
        }
        .pk { background: #1e3a5f; color: #6cb4ff; }
        .fk { background: #3b1e5f; color: #b48aff; }
        .idx { background: #1e4a3a; color: #4ad9a4; }
        .toc {
            background: var(--surface);
            border: 1px solid var(--border);
            border-radius: 10px;
            padding: 1.5rem 2rem;
            margin-bottom: 2rem;
        }
        .toc a {
            color: var(--accent);
            text-decoration: none;
        }
        .toc a:hover { text-decoration: underline; }
        .toc ol { margin-left: 1.2rem; }
        .toc li { margin-bottom: 0.3rem; }
    </style>
</head>
<body>

<h1>&#128202; System Design: Ad Click Aggregator</h1>

<nav class="toc">
    <h3 style="margin-top:0;">Table of Contents</h3>
    <ol>
        <li><a href="#fr">Functional Requirements</a></li>
        <li><a href="#nfr">Non-Functional Requirements</a></li>
        <li><a href="#flow1">Flow 1 &mdash; Click Ingestion</a></li>
        <li><a href="#flow2">Flow 2 &mdash; Click Aggregation &amp; Processing</a></li>
        <li><a href="#flow3">Flow 3 &mdash; Query Aggregated Data</a></li>
        <li><a href="#overall">Overall Combined Diagram</a></li>
        <li><a href="#schema">Database Schema</a></li>
        <li><a href="#cdn-cache">CDN &amp; Cache Deep Dive</a></li>
        <li><a href="#scaling">Scaling Considerations</a></li>
        <li><a href="#tradeoffs">Tradeoffs &amp; Deep Dives</a></li>
        <li><a href="#alternatives">Alternative Approaches</a></li>
        <li><a href="#additional">Additional Considerations</a></li>
        <li><a href="#vendors">Vendor Recommendations</a></li>
    </ol>
</nav>

<!-- ============================================================ -->
<div class="section" id="fr">
<h2 style="margin-top:0;">1. Functional Requirements</h2>
<ol>
    <li><strong>Record ad click events</strong> &mdash; When a user clicks an advertisement, the system must capture the click with metadata (ad ID, campaign ID, user ID, timestamp, IP, user-agent, referrer).</li>
    <li><strong>Redirect the user</strong> &mdash; After recording the click, the user must be seamlessly redirected to the advertiser&rsquo;s landing page with minimal delay.</li>
    <li><strong>Deduplicate clicks</strong> &mdash; The same user clicking the same ad within a short window (e.g., 1 minute) should count as a single click.</li>
    <li><strong>Detect and filter fraudulent clicks</strong> &mdash; Identify bot traffic, click farms, and other invalid clicks so they are excluded from billing aggregates.</li>
    <li><strong>Aggregate click counts</strong> &mdash; Pre-compute click counts at multiple granularities (per-minute, per-hour, per-day) grouped by ad, campaign, and advertiser.</li>
    <li><strong>Serve aggregated data via query API</strong> &mdash; Advertisers and internal dashboards must be able to query click aggregates for any ad, campaign, or advertiser over arbitrary time ranges.</li>
    <li><strong>Persist raw click logs</strong> &mdash; Store every valid click event in an append-only raw log for auditing, billing reconciliation, and reprocessing.</li>
    <li><strong>Support billing reconciliation</strong> &mdash; Aggregated counts must be accurate enough for cost-per-click (CPC) billing with a periodic batch reconciliation path.</li>
</ol>
</div>

<!-- ============================================================ -->
<div class="section" id="nfr">
<h2 style="margin-top:0;">2. Non-Functional Requirements</h2>
<ol>
    <li><strong>High write throughput</strong> &mdash; Support 10K&ndash;100K+ clicks per second with the ability to absorb traffic spikes (e.g., Super Bowl ads).</li>
    <li><strong>Low ingestion latency</strong> &mdash; Click recording + redirect must complete in &lt;&thinsp;100&thinsp;ms so the user experience is unaffected.</li>
    <li><strong>Exactly-once counting semantics</strong> &mdash; Achieved via at-least-once delivery from the message queue combined with idempotent consumers and deduplication.</li>
    <li><strong>High availability (99.99%)</strong> &mdash; Losing click events directly loses revenue for both the ad platform and advertisers.</li>
    <li><strong>Fault tolerance / durability</strong> &mdash; No click data loss. The message queue and raw click log must persist data to disk with replication.</li>
    <li><strong>Horizontal scalability</strong> &mdash; Every stateless component must scale out; stateful stores must support sharding.</li>
    <li><strong>Query latency &lt;&thinsp;500&thinsp;ms</strong> &mdash; Aggregated data queries should return within 500&thinsp;ms for dashboard and API consumers.</li>
    <li><strong>Data accuracy for billing</strong> &mdash; Aggregate counts used for CPC billing must be reconciled against raw logs with &lt;&thinsp;0.1% error.</li>
    <li><strong>Idempotency</strong> &mdash; Retries and reprocessing must not inflate counts.</li>
</ol>
</div>

<!-- ============================================================ -->
<div class="section" id="flow1">
<h2 style="margin-top:0;">3. Flow 1 &mdash; Click Ingestion</h2>
<p>This flow covers what happens from the moment a user clicks an ad until the click event is safely enqueued for downstream processing and the user is redirected.</p>

<div class="diagram-container">
    <pre class="mermaid">
graph LR
    subgraph Client
        U["&#128100; User / Browser"]
    end

    U -->|"1&#46; GET /click?ad_id=X&amp;redirect=Y"| LB["&#9878; Load Balancer"]
    LB -->|"2&#46; Route request"| CTS["Click Tracker<br/>Service"]
    CTS -->|"3&#46; Publish click event"| MQ[["&#128233; Message Queue"]]
    CTS -->|"4&#46; HTTP 302 Redirect<br/>to advertiser URL"| U
    </pre>
</div>

<h3>Examples</h3>

<div class="example-box">
<strong>Example 1 &mdash; Normal Click:</strong><br/>
User <em>alice</em> is browsing a news site and sees a banner ad for &ldquo;Acme Shoes&rdquo; (ad_id=<code>ad_9001</code>, campaign_id=<code>camp_42</code>). She clicks the ad, which issues <code>GET https://click.adplatform.com/click?ad_id=ad_9001&amp;campaign_id=camp_42&amp;user_id=alice_123&amp;redirect=https://acmeshoes.com/sale</code>. The Load Balancer forwards the request to an available Click Tracker Service instance. The Click Tracker publishes a click event <code>{click_id: "clk_abc", ad_id: "ad_9001", campaign_id: "camp_42", user_id: "alice_123", ts: 1707840000, ip: "203.0.113.50", ua: "Mozilla/5.0...", referrer: "https://news.com/article"}</code> to the Message Queue, then immediately responds with <code>HTTP 302 Location: https://acmeshoes.com/sale</code>. Alice&rsquo;s browser follows the redirect and she lands on the Acme Shoes page. Total latency perceived by Alice: ~50&thinsp;ms.
</div>

<div class="example-box">
<strong>Example 2 &mdash; Duplicate Click (rapid double-click):</strong><br/>
User <em>bob</em> double-clicks the same ad within 200&thinsp;ms. Two <code>GET /click</code> requests hit the Click Tracker Service. The Click Tracker does <strong>not</strong> deduplicate at this stage&mdash;it is designed for speed. Both click events are published to the Message Queue with distinct <code>click_id</code> values, and both receive 302 redirects. Deduplication happens downstream in Flow 2 when the Aggregation Workers check the Dedup Cache.
</div>

<div class="example-box">
<strong>Example 3 &mdash; Click Tracker unavailable (failure):</strong><br/>
If a Click Tracker instance crashes, the Load Balancer detects it via health checks and stops routing to it. Subsequent clicks are routed to healthy instances. Clicks in-flight at the time of the crash (after the user sent the request but before the event was published) are lost at the ingestion layer. To mitigate, the Click Tracker can write-ahead to a local disk buffer before publishing. However, accepting some minimal loss at this layer is a practical tradeoff for the ultra-low-latency requirement.
</div>

<h3>Component Deep Dive</h3>

<h4>User / Browser (Client)</h4>
<p>The ad creative (banner, video, native) rendered in the user&rsquo;s browser or app contains an ad tracking URL as the click-through href. When the user clicks, the browser issues a standard HTTP GET to the tracking URL. No JavaScript SDK is required for click tracking&mdash;the redirect-based approach works universally across browsers and apps.</p>

<h4>Load Balancer</h4>
<ul>
    <li><strong>Protocol:</strong> HTTP/HTTPS (Layer 7).</li>
    <li><strong>Algorithm:</strong> Round-robin or least-connections. No session affinity is needed because the Click Tracker Service is stateless.</li>
    <li><strong>Health checks:</strong> Periodic HTTP GET to <code>/health</code> on each Click Tracker instance; unhealthy instances are removed from the pool.</li>
    <li><strong>TLS termination:</strong> Terminates TLS at the load balancer; internal traffic between load balancer and Click Tracker uses plaintext HTTP for lower latency.</li>
</ul>

<h4>Click Tracker Service</h4>
<ul>
    <li><strong>Protocol:</strong> HTTP &mdash; <code>GET /click</code></li>
    <li><strong>Input (query parameters):</strong>
        <ul>
            <li><code>ad_id</code> (string) &mdash; the ad being clicked</li>
            <li><code>campaign_id</code> (string) &mdash; the parent campaign</li>
            <li><code>user_id</code> (string, from cookie or auth token) &mdash; the clicking user</li>
            <li><code>redirect</code> (URL-encoded string) &mdash; the advertiser landing page</li>
        </ul>
    </li>
    <li><strong>Output:</strong> <code>HTTP 302 Found</code> with <code>Location: {redirect URL}</code>.</li>
    <li><strong>Responsibilities:</strong>
        <ol>
            <li>Generate a globally unique <code>click_id</code> (e.g., UUID or Snowflake ID).</li>
            <li>Enrich the event with server-side metadata: timestamp, client IP (from <code>X-Forwarded-For</code>), user-agent, referrer.</li>
            <li>Publish the click event to the Message Queue asynchronously (fire-and-forget with an internal ack).</li>
            <li>Return 302 redirect immediately.</li>
        </ol>
    </li>
    <li><strong>Stateless:</strong> No local state; all persistence is delegated to the Message Queue.</li>
    <li><strong>Why GET and not POST:</strong> Ad click-through URLs are embedded as hyperlink <code>href</code> attributes. Clicking a link triggers a GET request natively in all browsers. A POST would require JavaScript interception, which is fragile (ad blockers, disabled JS).</li>
</ul>

<h4>Message Queue</h4>
<ul>
    <li><strong>Role:</strong> Decouples the high-throughput ingestion path from the slower processing path. Absorbs traffic spikes (burst buffering).</li>
    <li><strong>Topic:</strong> <code>raw-click-events</code>.</li>
    <li><strong>Partitioning:</strong> Partitioned by <code>ad_id</code> so that all clicks for the same ad go to the same partition, enabling ordered processing per-ad and efficient batching for counter increments.</li>
    <li><strong>Delivery guarantee:</strong> At-least-once. Combined with idempotent consumers (dedup in Flow 2), this yields effectively exactly-once counting.</li>
    <li><strong>Replication:</strong> Each partition replicated to &ge;3 brokers for durability.</li>
    <li><strong>Retention:</strong> 7 days of raw events for reprocessing.</li>
    <li><strong>Protocol:</strong> TCP-based binary protocol between producers/consumers and brokers (reliable, ordered delivery required).</li>
    <li><strong>Why a message queue vs. direct database writes:</strong> Direct writes to the aggregation database would create write hotspots under spike traffic (e.g., viral ad), overwhelm the DB, and couple the user-facing latency to DB write latency. The queue absorbs spikes, allows back-pressure, and enables multiple independent consumers (aggregation, fraud detection, billing).</li>
</ul>
</div>

<!-- ============================================================ -->
<div class="section" id="flow2">
<h2 style="margin-top:0;">4. Flow 2 &mdash; Click Aggregation &amp; Processing</h2>
<p>This flow covers how raw click events are consumed from the message queue, deduplicated, fraud-checked, persisted as raw logs, and aggregated into pre-computed counters.</p>

<div class="diagram-container">
    <pre class="mermaid">
graph TD
    MQ[["&#128233; Message Queue<br/>(raw-click-events)"]] -->|"1&#46; Consume batch<br/>of click events"| AW["Aggregation<br/>Workers"]
    AW -->|"2&#46; Lookup click fingerprint"| DC[("&#128275; Dedup Cache")]
    DC -->|"Duplicate &#8594; discard"| DISCARD["&#9940; Discard<br/>Duplicate"]
    DC -->|"3&#46; Not duplicate"| FD["Fraud Detection<br/>Service"]
    FD -->|"Invalid &#8594; flag &amp; log"| FLAG["&#9888;&#65039; Flag &amp;<br/>Quarantine"]
    FD -->|"4&#46; Valid click"| RCL[("&#128451; Raw Click Log<br/>(NoSQL / Object Store)")]
    FD -->|"5&#46; Increment counters"| ADB[("&#128202; Aggregation DB<br/>(NoSQL)")]
    AW -->|"6&#46; Update hot aggregates"| QC[("&#9889; Query Cache")]
    </pre>
</div>

<h3>Examples</h3>

<div class="example-box">
<strong>Example 1 &mdash; Normal aggregation (happy path):</strong><br/>
The Aggregation Worker pulls a batch of 500 click events from the Message Queue partition for <code>ad_9001</code>. For each event, it computes a dedup fingerprint <code>hash(user_id + ad_id + floor(timestamp / 60s))</code> and checks the Dedup Cache. For <em>alice</em>&rsquo;s click <code>clk_abc</code>, no entry is found &rarr; it writes the fingerprint to the Dedup Cache with a 120-second TTL. Next, the Fraud Detection Service checks: IP <code>203.0.113.50</code> has only 1 click in the last minute and the user-agent is a real browser &rarr; <strong>valid</strong>. The click is written to the Raw Click Log (append), and the counters in the Aggregation DB are atomically incremented: <code>ad_9001:min:2024-02-13T17:34</code> += 1, <code>camp_42:hour:2024-02-13T17</code> += 1, <code>adv_7:day:2024-02-13</code> += 1. The hot aggregate for <code>ad_9001</code> in the Query Cache is also updated.
</div>

<div class="example-box">
<strong>Example 2 &mdash; Duplicate discarded:</strong><br/>
<em>Bob</em>&rsquo;s double-click produced two events <code>clk_def</code> and <code>clk_ghi</code> within 200&thinsp;ms for the same ad. The Aggregation Worker processes <code>clk_def</code> first, writes the fingerprint <code>hash(bob_456 + ad_9001 + 1707840000_minute_bucket)</code> to the Dedup Cache. When <code>clk_ghi</code> arrives, the same fingerprint is found in the cache &rarr; the event is discarded. The message is acknowledged (committed) on the queue so it is not redelivered. Only one click is counted.
</div>

<div class="example-box">
<strong>Example 3 &mdash; Fraudulent click detected:</strong><br/>
IP address <code>198.51.100.12</code> has generated 500 clicks across different ads in the last 60 seconds. The Fraud Detection Service flags this IP as suspicious. A click from this IP for <code>ad_5050</code> is marked <code>is_valid=false</code>, written to the Raw Click Log for auditing (but <strong>not</strong> counted in the Aggregation DB). The click is quarantined for manual review. Aggregation counters remain unchanged.
</div>

<div class="example-box">
<strong>Example 4 &mdash; Worker failure mid-processing:</strong><br/>
An Aggregation Worker crashes after incrementing the counter in the Aggregation DB but <em>before</em> acknowledging the message on the queue. The message queue redelivers the batch to another worker. The new worker reprocesses the events: since the dedup fingerprint was already written to the cache (with TTL), the duplicate events are discarded. If the cache entry expired during the downtime, the Raw Click Log is checked as a fallback. This ensures no double-counting even under failure.
</div>

<h3>Component Deep Dive</h3>

<h4>Aggregation Workers</h4>
<ul>
    <li><strong>Type:</strong> Stateless stream processing consumers running in a consumer group.</li>
    <li><strong>Parallelism:</strong> One worker per message queue partition. If there are 256 partitions, up to 256 workers can run concurrently.</li>
    <li><strong>Batch processing:</strong> Each worker pulls a micro-batch of events (e.g., 500 events or 1 second worth), processes them, and commits the offset.</li>
    <li><strong>Processing pipeline per event:</strong>
        <ol>
            <li>Compute dedup fingerprint &rarr; check Dedup Cache.</li>
            <li>If not duplicate, call Fraud Detection Service.</li>
            <li>If valid, write to Raw Click Log and atomically increment Aggregation DB counters.</li>
            <li>Update Query Cache with the latest aggregate value.</li>
        </ol>
    </li>
    <li><strong>Idempotency:</strong> Counter increments use <code>click_id</code>-based idempotency keys. The Aggregation DB ignores duplicate <code>click_id</code> increments.</li>
</ul>

<h4>Dedup Cache (In-Memory Cache)</h4>
<ul>
    <li><strong>Purpose:</strong> Fast O(1) lookup to filter duplicate clicks before they hit the fraud detection and aggregation path.</li>
    <li><strong>Key:</strong> <code>hash(user_id + ad_id + floor(timestamp / 60))</code> &mdash; groups clicks by user, ad, and 1-minute window.</li>
    <li><strong>Value:</strong> <code>click_id</code> of the first click in the window (for audit trail).</li>
    <li><strong>TTL:</strong> 120 seconds (2&times; the dedup window to cover clock skew).</li>
    <li><strong>Eviction policy:</strong> TTL-based expiration. No LRU needed because entries naturally expire.</li>
    <li><strong>Caching strategy:</strong> Write-through &mdash; every new (non-duplicate) click writes its fingerprint to the cache immediately before proceeding to fraud detection.</li>
    <li><strong>Failure mode:</strong> If the cache is unavailable, the worker falls back to checking the Raw Click Log (slower but correct). This is a rare degraded path.</li>
    <li><strong>Populated by:</strong> Aggregation Workers upon processing each new click event.</li>
</ul>

<h4>Fraud Detection Service</h4>
<ul>
    <li><strong>Protocol:</strong> Internal RPC (gRPC or HTTP POST).</li>
    <li><strong>Input:</strong> Click event object (IP, user-agent, user_id, ad_id, timestamp).</li>
    <li><strong>Output:</strong> <code>{is_valid: boolean, reason: string}</code>.</li>
    <li><strong>Rules applied:</strong>
        <ul>
            <li>Click rate per IP per minute (threshold: e.g., &gt;50 clicks &rarr; flag).</li>
            <li>Click rate per user per minute (threshold: e.g., &gt;20 clicks &rarr; flag).</li>
            <li>Known bot user-agents (regex blacklist).</li>
            <li>Geo-IP mismatch with user profile country.</li>
            <li>Click-to-conversion time anomalies.</li>
        </ul>
    </li>
    <li><strong>State:</strong> Maintains sliding-window counters per IP and per user in an in-memory cache. These counters are approximate (probabilistic data structures like Count-Min Sketch can be used for memory efficiency).</li>
</ul>

<h4>Raw Click Log (NoSQL / Object Storage)</h4>
<ul>
    <li><strong>Purpose:</strong> Immutable, append-only store of every valid click event for auditing, billing reconciliation, reprocessing, and analytics.</li>
    <li><strong>Storage model:</strong> NoSQL wide-column store or object storage (for batch processing).</li>
    <li><strong>Write pattern:</strong> Append-only, very high throughput.</li>
    <li><strong>Read pattern:</strong> Batch reads for reconciliation (nightly job) and ad-hoc reads for audit queries.</li>
    <li><strong>Retention:</strong> 1&ndash;3 years (regulatory/billing requirements).</li>
    <li><strong>Partitioned by:</strong> Time (daily partitions) with a secondary sort by <code>ad_id</code>.</li>
</ul>

<h4>Aggregation DB (NoSQL)</h4>
<ul>
    <li><strong>Purpose:</strong> Stores pre-computed click aggregates at multiple time granularities.</li>
    <li><strong>Storage model:</strong> NoSQL wide-column store optimized for fast atomic increments and range scans.</li>
    <li><strong>Write pattern:</strong> High-frequency atomic counter increments.</li>
    <li><strong>Read pattern:</strong> Range scans by (entity_id, time_window_start, time_window_end) for query API.</li>
    <li><strong>Why NoSQL:</strong> The access pattern is simple key-range lookups with very high write throughput. No complex joins or transactions needed. NoSQL wide-column stores excel at this workload.</li>
</ul>

<h4>Query Cache</h4>
<ul>
    <li>Detailed in <a href="#cdn-cache">Section 8: CDN &amp; Cache Deep Dive</a>.</li>
</ul>
</div>

<!-- ============================================================ -->
<div class="section" id="flow3">
<h2 style="margin-top:0;">5. Flow 3 &mdash; Query Aggregated Data</h2>
<p>This flow covers how advertisers and internal dashboards retrieve aggregated click data.</p>

<div class="diagram-container">
    <pre class="mermaid">
graph LR
    ADV["&#128188; Advertiser /<br/>Dashboard"] -->|"1&#46; GET /api/v1/aggregates<br/>?ad_id=X&amp;start=T1&amp;end=T2&amp;granularity=hour"| LB2["&#9878; Load Balancer"]
    LB2 -->|"2&#46; Route request"| QS["Query<br/>Service"]
    QS -->|"3&#46; Check cache"| QC[("&#9889; Query Cache")]
    QC -->|"4a&#46; Cache Hit"| QS
    QS -->|"4b&#46; Cache Miss"| ADB[("&#128202; Aggregation DB")]
    ADB -->|"5&#46; Return rows"| QS
    QS -->|"6&#46; Populate cache<br/>+ return JSON"| ADV
    </pre>
</div>

<h3>Examples</h3>

<div class="example-box">
<strong>Example 1 &mdash; Cache hit (hot ad, frequently queried):</strong><br/>
An advertiser&rsquo;s dashboard auto-refreshes every 30 seconds for campaign <code>camp_42</code>. The dashboard calls <code>GET /api/v1/aggregates?campaign_id=camp_42&amp;start=2024-02-13T00:00&amp;end=2024-02-13T23:59&amp;granularity=hour</code>. The Load Balancer routes to a Query Service instance. The Query Service computes the cache key <code>agg:camp_42:hour:2024-02-13</code> and finds a hit in the Query Cache. The cached JSON is returned directly: <code>[{hour: "2024-02-13T00:00", clicks: 12045, unique_clicks: 9821}, {hour: "2024-02-13T01:00", clicks: 8923, unique_clicks: 7102}, ...]</code>. Latency: ~15&thinsp;ms.
</div>

<div class="example-box">
<strong>Example 2 &mdash; Cache miss (cold or historical query):</strong><br/>
An analyst queries aggregates for an ad that ran 3 months ago: <code>GET /api/v1/aggregates?ad_id=ad_2001&amp;start=2023-11-01&amp;end=2023-11-30&amp;granularity=day</code>. The Query Cache has no entry (TTL long expired). The Query Service queries the Aggregation DB: <code>SELECT * FROM day_aggregates WHERE entity_id = 'ad_2001' AND time_window BETWEEN '2023-11-01' AND '2023-11-30'</code>. The DB returns 30 rows. The Query Service populates the cache (with a 1-hour TTL for historical data) and returns the JSON to the analyst. Latency: ~200&thinsp;ms.
</div>

<div class="example-box">
<strong>Example 3 &mdash; Aggregation by advertiser (roll-up query):</strong><br/>
An advertiser wants total clicks across <em>all</em> their campaigns today: <code>GET /api/v1/aggregates?advertiser_id=adv_7&amp;start=2024-02-13&amp;end=2024-02-13&amp;granularity=day</code>. The Query Service checks the cache for <code>agg:adv_7:day:2024-02-13</code>. On a miss, it queries the Aggregation DB&rsquo;s advertiser-level daily table, which stores pre-aggregated totals. No expensive multi-table join is needed because of <strong>denormalization</strong> (advertiser-level counters are maintained separately). Latency: ~80&thinsp;ms.
</div>

<h3>Component Deep Dive</h3>

<h4>Load Balancer (Query Path)</h4>
<ul>
    <li><strong>Protocol:</strong> HTTPS (Layer 7).</li>
    <li><strong>Algorithm:</strong> Least-connections (query processing time varies by query complexity).</li>
    <li><strong>Rate limiting:</strong> Per-advertiser rate limiting to prevent API abuse. Configured at the load balancer or an API gateway in front of it.</li>
    <li><strong>Authentication:</strong> API key or OAuth token validated at the load balancer / API gateway layer.</li>
</ul>

<h4>Query Service</h4>
<ul>
    <li><strong>Protocol:</strong> HTTP REST.</li>
    <li><strong>Endpoint:</strong> <code>GET /api/v1/aggregates</code></li>
    <li><strong>Input (query parameters):</strong>
        <ul>
            <li><code>ad_id</code> or <code>campaign_id</code> or <code>advertiser_id</code> (at least one required)</li>
            <li><code>start</code> (ISO 8601 timestamp) &mdash; range start</li>
            <li><code>end</code> (ISO 8601 timestamp) &mdash; range end</li>
            <li><code>granularity</code> (enum: <code>minute</code>, <code>hour</code>, <code>day</code>) &mdash; time bucket size</li>
        </ul>
    </li>
    <li><strong>Output (JSON):</strong>
        <pre><code>[
  {"time_window": "2024-02-13T17:00", "click_count": 4521, "unique_click_count": 3890},
  {"time_window": "2024-02-13T18:00", "click_count": 5102, "unique_click_count": 4200}
]</code></pre>
    </li>
    <li><strong>Stateless:</strong> Reads from Query Cache and Aggregation DB. Horizontally scalable.</li>
    <li><strong>Pagination:</strong> For large time ranges, results are paginated using cursor-based pagination (<code>next_cursor</code> token in response).</li>
</ul>

<h4>Query Cache</h4>
<ul>
    <li>Detailed in <a href="#cdn-cache">Section 8: CDN &amp; Cache Deep Dive</a>.</li>
</ul>

<h4>Aggregation DB (Read Path)</h4>
<ul>
    <li>Same database as in Flow 2 (write path). The read pattern is range scans on <code>(entity_id, time_window)</code> composite keys, which NoSQL wide-column stores handle efficiently.</li>
</ul>
</div>

<!-- ============================================================ -->
<div class="section" id="overall">
<h2 style="margin-top:0;">6. Overall Combined Diagram</h2>
<p>This diagram combines all three flows into a single end-to-end view of the system.</p>

<div class="diagram-container">
    <pre class="mermaid">
graph TD
    subgraph Ingestion
        U["&#128100; User / Browser"] -->|"GET /click"| LB1["&#9878; Load Balancer<br/>(Ingestion)"]
        LB1 --> CTS["Click Tracker<br/>Service"]
        CTS -->|"302 Redirect"| U
    end

    subgraph Messaging
        CTS -->|"Publish"| MQ[["&#128233; Message Queue<br/>(raw-click-events)"]]
    end

    subgraph Processing
        MQ -->|"Consume batches"| AW["Aggregation<br/>Workers"]
        AW -->|"Check fingerprint"| DC[("&#128275; Dedup<br/>Cache")]
        AW -->|"Validate"| FD["Fraud Detection<br/>Service"]
        AW -->|"Append raw event"| RCL[("&#128451; Raw Click Log")]
        AW -->|"Increment counters"| ADB[("&#128202; Aggregation DB<br/>(NoSQL)")]
        AW -->|"Update hot data"| QC[("&#9889; Query Cache")]
    end

    subgraph Query
        ADV["&#128188; Advertiser /<br/>Dashboard"] -->|"GET /api/v1/aggregates"| LB2["&#9878; Load Balancer<br/>(Query)"]
        LB2 --> QSV["Query<br/>Service"]
        QSV --> QC
        QSV --> ADB
        QSV -->|"JSON response"| ADV
    end

    subgraph Reconciliation
        RCL -->|"Nightly batch job"| RECON["Batch<br/>Reconciliation"]
        RECON -->|"Correct drift"| ADB
    end
    </pre>
</div>

<h3>End-to-End Examples</h3>

<div class="example-box">
<strong>Example 1 &mdash; Full lifecycle of a single click:</strong><br/>
1. User <em>alice</em> clicks an ad for &ldquo;Acme Shoes&rdquo; &rarr; <code>GET /click?ad_id=ad_9001&amp;redirect=https://acmeshoes.com</code> hits the <strong>Ingestion Load Balancer</strong>.<br/>
2. The <strong>Click Tracker Service</strong> generates <code>click_id=clk_abc</code>, enriches the event, publishes it to the <strong>Message Queue</strong>, and returns <code>HTTP 302</code> to Alice.<br/>
3. An <strong>Aggregation Worker</strong> consumes the event. It checks the <strong>Dedup Cache</strong> &mdash; no fingerprint found &mdash; and writes the fingerprint. It calls the <strong>Fraud Detection Service</strong> &mdash; click is valid.<br/>
4. The worker appends the click to the <strong>Raw Click Log</strong> and increments counters in the <strong>Aggregation DB</strong>: <code>ad_9001:min:17:34</code> += 1, <code>camp_42:hour:17</code> += 1, <code>adv_7:day:2024-02-13</code> += 1.<br/>
5. The worker updates the <strong>Query Cache</strong> with the latest minute-level count for <code>ad_9001</code>.<br/>
6. Later, the advertiser opens their dashboard and calls <code>GET /api/v1/aggregates?campaign_id=camp_42&amp;granularity=hour&amp;start=today&amp;end=today</code>. The <strong>Query Load Balancer</strong> routes to the <strong>Query Service</strong>, which finds the data in the <strong>Query Cache</strong> and returns it in &lt;20&thinsp;ms.<br/>
7. At 2&thinsp;AM, the <strong>Batch Reconciliation</strong> job reads all of today&rsquo;s events from the <strong>Raw Click Log</strong>, recomputes exact aggregates, and corrects any drift in the <strong>Aggregation DB</strong>.
</div>

<div class="example-box">
<strong>Example 2 &mdash; Spike traffic during Super Bowl ad:</strong><br/>
A Super Bowl ad generates 500K clicks/second for 30 seconds. The <strong>Click Tracker Service</strong> fleet (behind the load balancer) handles the HTTP requests and publishes them to the <strong>Message Queue</strong>. The queue absorbs the burst; its partitions buffer events while <strong>Aggregation Workers</strong> process at their maximum throughput. Consumer lag grows temporarily (events are processed seconds late) but no data is lost. The workers catch up within minutes after the spike subsides. The <strong>Query Cache</strong> reflects slightly delayed counts during the spike, but the <strong>Batch Reconciliation</strong> job ensures final accuracy.
</div>

<div class="example-box">
<strong>Example 3 &mdash; Billing reconciliation dispute:</strong><br/>
An advertiser disputes their CPC bill, claiming they were charged for 1.2M clicks but their analytics show only 1.1M. The billing team runs a reconciliation query against the <strong>Raw Click Log</strong> for the advertiser&rsquo;s ads over the billing period, filtering out <code>is_valid=false</code> events. The raw log shows 1,198,500 valid clicks. The <strong>Aggregation DB</strong> shows 1,200,002 (0.01% drift due to a brief dedup cache failure). The Batch Reconciliation job corrects the aggregation DB, and the bill is adjusted to 1,198,500 clicks.
</div>
</div>

<!-- ============================================================ -->
<div class="section" id="schema">
<h2 style="margin-top:0;">7. Database Schema</h2>

<!-- SQL Tables -->
<h3>SQL Tables</h3>

<h4>Table: <code>ads</code> (SQL &mdash; Relational)</h4>
<table>
    <tr><th>Column</th><th>Type</th><th>Constraints</th><th>Description</th></tr>
    <tr><td><code>ad_id</code></td><td>VARCHAR(64)</td><td><span class="tag pk">PK</span></td><td>Unique ad identifier</td></tr>
    <tr><td><code>campaign_id</code></td><td>VARCHAR(64)</td><td><span class="tag fk">FK → campaigns</span></td><td>Parent campaign</td></tr>
    <tr><td><code>advertiser_id</code></td><td>VARCHAR(64)</td><td><span class="tag fk">FK → advertisers</span></td><td>Owning advertiser</td></tr>
    <tr><td><code>ad_type</code></td><td>ENUM</td><td>NOT NULL</td><td>banner, video, native, etc.</td></tr>
    <tr><td><code>redirect_url</code></td><td>TEXT</td><td>NOT NULL</td><td>Advertiser landing page</td></tr>
    <tr><td><code>status</code></td><td>ENUM</td><td>NOT NULL</td><td>active, paused, archived</td></tr>
    <tr><td><code>created_at</code></td><td>TIMESTAMP</td><td></td><td>Creation time</td></tr>
</table>
<p><strong>Why SQL:</strong> Ads have complex relationships with campaigns and advertisers. ACID transactions are needed when creating/updating ad configurations that affect billing. Low write volume (ads are created infrequently). Index on <code>campaign_id</code> (B-tree) for lookups by campaign. Index on <code>advertiser_id</code> (B-tree) for lookups by advertiser.</p>
<p><strong>Read events:</strong> When an ad is served (to construct the tracking URL), when the dashboard loads ad details.</p>
<p><strong>Write events:</strong> When an advertiser creates or updates an ad via the ad management UI.</p>

<h4>Table: <code>campaigns</code> (SQL &mdash; Relational)</h4>
<table>
    <tr><th>Column</th><th>Type</th><th>Constraints</th><th>Description</th></tr>
    <tr><td><code>campaign_id</code></td><td>VARCHAR(64)</td><td><span class="tag pk">PK</span></td><td>Unique campaign identifier</td></tr>
    <tr><td><code>advertiser_id</code></td><td>VARCHAR(64)</td><td><span class="tag fk">FK → advertisers</span></td><td>Owning advertiser</td></tr>
    <tr><td><code>name</code></td><td>VARCHAR(256)</td><td>NOT NULL</td><td>Campaign name</td></tr>
    <tr><td><code>budget_cents</code></td><td>BIGINT</td><td>NOT NULL</td><td>Total budget in cents</td></tr>
    <tr><td><code>cpc_bid_cents</code></td><td>INT</td><td>NOT NULL</td><td>Cost-per-click bid in cents</td></tr>
    <tr><td><code>start_date</code></td><td>DATE</td><td></td><td>Campaign start</td></tr>
    <tr><td><code>end_date</code></td><td>DATE</td><td></td><td>Campaign end</td></tr>
    <tr><td><code>status</code></td><td>ENUM</td><td>NOT NULL</td><td>active, paused, completed</td></tr>
</table>
<p><strong>Why SQL:</strong> Same reasoning as <code>ads</code>. Budget management requires ACID guarantees. Index on <code>advertiser_id</code> (B-tree). Index on <code>(status, end_date)</code> (composite B-tree) for finding active campaigns.</p>
<p><strong>Read events:</strong> Dashboard loads, billing calculations.</p>
<p><strong>Write events:</strong> Advertiser creates/updates campaign settings.</p>

<h4>Table: <code>advertisers</code> (SQL &mdash; Relational)</h4>
<table>
    <tr><th>Column</th><th>Type</th><th>Constraints</th><th>Description</th></tr>
    <tr><td><code>advertiser_id</code></td><td>VARCHAR(64)</td><td><span class="tag pk">PK</span></td><td>Unique advertiser identifier</td></tr>
    <tr><td><code>name</code></td><td>VARCHAR(256)</td><td>NOT NULL</td><td>Company name</td></tr>
    <tr><td><code>email</code></td><td>VARCHAR(256)</td><td>UNIQUE</td><td>Contact email</td></tr>
    <tr><td><code>billing_account_id</code></td><td>VARCHAR(64)</td><td></td><td>External billing reference</td></tr>
    <tr><td><code>created_at</code></td><td>TIMESTAMP</td><td></td><td>Registration time</td></tr>
</table>
<p><strong>Why SQL:</strong> Low cardinality, relational data, needs ACID for account management. No special indexes beyond the PK.</p>
<p><strong>Read events:</strong> Login, dashboard, billing.</p>
<p><strong>Write events:</strong> Account creation/update.</p>

<hr/>

<!-- NoSQL Tables -->
<h3>NoSQL Tables</h3>

<h4>Table: <code>raw_clicks</code> (NoSQL &mdash; Wide-Column Store)</h4>
<table>
    <tr><th>Column</th><th>Type</th><th>Constraints</th><th>Description</th></tr>
    <tr><td><code>click_id</code></td><td>VARCHAR(64)</td><td><span class="tag pk">PK (partition key)</span></td><td>Globally unique click ID</td></tr>
    <tr><td><code>ad_id</code></td><td>VARCHAR(64)</td><td><span class="tag idx">Indexed</span></td><td>Clicked ad</td></tr>
    <tr><td><code>campaign_id</code></td><td>VARCHAR(64)</td><td></td><td>Parent campaign (denormalized)</td></tr>
    <tr><td><code>advertiser_id</code></td><td>VARCHAR(64)</td><td></td><td>Advertiser (denormalized)</td></tr>
    <tr><td><code>user_id</code></td><td>VARCHAR(64)</td><td></td><td>Clicking user</td></tr>
    <tr><td><code>timestamp</code></td><td>BIGINT</td><td><span class="tag idx">Indexed</span></td><td>Unix epoch millis</td></tr>
    <tr><td><code>ip_address</code></td><td>VARCHAR(45)</td><td></td><td>Client IP</td></tr>
    <tr><td><code>user_agent</code></td><td>TEXT</td><td></td><td>Browser user-agent string</td></tr>
    <tr><td><code>referrer_url</code></td><td>TEXT</td><td></td><td>Page where the ad was shown</td></tr>
    <tr><td><code>country</code></td><td>VARCHAR(2)</td><td></td><td>Geo-IP country code</td></tr>
    <tr><td><code>is_valid</code></td><td>BOOLEAN</td><td></td><td>Fraud detection result</td></tr>
</table>
<p><strong>Denormalization note:</strong> <code>campaign_id</code> and <code>advertiser_id</code> are denormalized from the <code>ads</code> SQL table into every raw click row. This avoids expensive joins at query time when reconciliation jobs need to filter/group by campaign or advertiser. The tradeoff is slightly increased storage (a few extra bytes per row), which is negligible given the row size. Since ad-to-campaign mappings rarely change, stale data risk is minimal.</p>
<p><strong>Why NoSQL:</strong> Write-heavy workload (10K&ndash;100K+ inserts/second), append-only access pattern, no complex queries needed on raw data, needs horizontal scalability. Wide-column stores are ideal for this time-series-like append workload.</p>
<p><strong>Indexes:</strong></p>
<ul>
    <li><code>click_id</code>: Hash index (partition key) for O(1) point lookups and idempotent writes.</li>
    <li><code>(ad_id, timestamp)</code>: Composite index (sorted by timestamp) for efficient range scans when reconciling all clicks for a given ad within a time range. This is critical for the nightly batch reconciliation job.</li>
</ul>
<p><strong>Sharding:</strong> Sharded by <code>click_id</code> (hash-based sharding) for uniform data distribution. Using <code>ad_id</code> for sharding would create hotspots (popular ads would overload a single shard). Hash-based sharding on <code>click_id</code> distributes writes evenly across all shards.</p>
<p><strong>Read events:</strong> Nightly batch reconciliation job, ad-hoc audit queries, billing dispute investigations.</p>
<p><strong>Write events:</strong> Every valid click processed by the Aggregation Workers (after dedup and fraud check).</p>

<h4>Table: <code>minute_aggregates</code> (NoSQL &mdash; Wide-Column Store)</h4>
<table>
    <tr><th>Column</th><th>Type</th><th>Constraints</th><th>Description</th></tr>
    <tr><td><code>entity_type</code></td><td>VARCHAR(16)</td><td><span class="tag pk">PK (partition key, part 1)</span></td><td>"ad", "campaign", or "advertiser"</td></tr>
    <tr><td><code>entity_id</code></td><td>VARCHAR(64)</td><td><span class="tag pk">PK (partition key, part 2)</span></td><td>ad_id, campaign_id, or advertiser_id</td></tr>
    <tr><td><code>time_window</code></td><td>BIGINT</td><td><span class="tag pk">PK (sort/clustering key)</span></td><td>Minute-aligned Unix epoch</td></tr>
    <tr><td><code>click_count</code></td><td>BIGINT (counter)</td><td></td><td>Total clicks in this minute</td></tr>
    <tr><td><code>unique_click_count</code></td><td>BIGINT (counter)</td><td></td><td>Unique users who clicked</td></tr>
    <tr><td><code>last_updated</code></td><td>BIGINT</td><td></td><td>Last counter update timestamp</td></tr>
</table>
<p><strong>Why NoSQL:</strong> Extremely high write throughput (every click increments multiple counters). NoSQL wide-column stores support native atomic counter increments. The access pattern is a simple composite key lookup + range scan &mdash; no joins needed. Horizontal scaling via sharding is essential.</p>
<p><strong>Composite key design:</strong> The partition key <code>(entity_type, entity_id)</code> co-locates all time windows for the same entity on the same node, and the clustering key <code>time_window</code> sorts them chronologically. This enables efficient range scans: &ldquo;give me all minute aggregates for ad_9001 between 17:00 and 18:00.&rdquo;</p>
<p><strong>Sharding:</strong> Sharded by <code>(entity_type, entity_id)</code> with consistent hashing. This distributes different ads/campaigns across different shards while keeping all time windows for a single entity on the same shard for efficient range queries. Hot ads may still cause uneven load; this is mitigated by the Query Cache absorbing read traffic and counter batching on the write side.</p>
<p><strong>Read events:</strong> Query Service serves advertiser dashboard requests for real-time minute-level data.</p>
<p><strong>Write events:</strong> Every valid click processed by the Aggregation Workers increments the counter for the appropriate minute bucket.</p>

<h4>Table: <code>hour_aggregates</code> (NoSQL &mdash; Wide-Column Store)</h4>
<p>Same schema as <code>minute_aggregates</code> but <code>time_window</code> is hour-aligned. Populated by a roll-up job that sums minute aggregates into hourly buckets every hour, and also directly incremented by Aggregation Workers.</p>
<p><strong>Read events:</strong> Hourly dashboard views, billing reports.</p>
<p><strong>Write events:</strong> Aggregation Workers (real-time increment) + hourly roll-up job (batch correction).</p>

<h4>Table: <code>day_aggregates</code> (NoSQL &mdash; Wide-Column Store)</h4>
<p>Same schema but <code>time_window</code> is day-aligned. Populated by a roll-up job and directly incremented.</p>
<p><strong>Read events:</strong> Daily reports, billing, long-range analytics queries.</p>
<p><strong>Write events:</strong> Aggregation Workers + daily roll-up job + nightly batch reconciliation.</p>

<div class="warn-box">
    <strong>&#9888;&#65039; Denormalization Summary:</strong><br/>
    The aggregation tables maintain <strong>separate rows for each entity level</strong> (ad, campaign, advertiser) rather than requiring roll-up joins at query time. When a click for <code>ad_9001</code> (in <code>camp_42</code>, owned by <code>adv_7</code>) is processed, <strong>three counter increments</strong> happen: one for the ad row, one for the campaign row, and one for the advertiser row. This triple-write denormalization trades write amplification (3&times;) for read simplicity (no joins, O(1) lookups). This is worthwhile because: (a) reads vastly outnumber distinct writes after aggregation, (b) dashboard latency requirements are strict (&lt;500&thinsp;ms), and (c) NoSQL wide-column stores handle the write amplification efficiently with batched atomic increments.
</div>
</div>

<!-- ============================================================ -->
<div class="section" id="cdn-cache">
<h2 style="margin-top:0;">8. CDN &amp; Cache Deep Dive</h2>

<h3>CDN Assessment</h3>
<p>A CDN is <strong>not appropriate for the core API endpoints</strong> of the ad click aggregator. The reasons are:</p>
<ul>
    <li><strong>Click Tracker (<code>/click</code>):</strong> Every request must be recorded server-side. CDN caching would prevent click events from reaching the tracking service, causing data loss.</li>
    <li><strong>Query API (<code>/api/v1/aggregates</code>):</strong> Responses are personalized per-advertiser, per-time-range, and change frequently (real-time data). CDN caching would serve stale/incorrect data.</li>
</ul>
<p>However, a CDN <strong>is appropriate for static assets</strong> of the advertiser dashboard UI (HTML, CSS, JavaScript bundles, images). These are cacheable, infrequently changing, and benefit from edge distribution for global advertisers.</p>

<h3>Dedup Cache &mdash; Deep Dive</h3>
<table>
    <tr><th>Property</th><th>Value</th><th>Rationale</th></tr>
    <tr><td>Purpose</td><td>Filter duplicate clicks before aggregation</td><td>Prevents double-counting from rapid double-clicks and message queue redeliveries</td></tr>
    <tr><td>Technology</td><td>Distributed in-memory cache</td><td>Sub-millisecond lookups needed in the hot processing path</td></tr>
    <tr><td>Key</td><td><code>hash(user_id + ad_id + floor(ts/60))</code></td><td>Groups by user, ad, and 1-minute window</td></tr>
    <tr><td>Value</td><td><code>click_id</code> (first click in window)</td><td>Audit trail for which click was accepted</td></tr>
    <tr><td>Caching strategy</td><td><strong>Write-through</strong></td><td>On every new (non-duplicate) click, the fingerprint is written to the cache immediately. This ensures subsequent duplicates within the window are caught. Write-through is chosen because the cache is the primary dedup mechanism &mdash; we cannot afford a cache miss to cause a false negative (duplicate counted).</td></tr>
    <tr><td>Eviction policy</td><td><strong>TTL-based expiration</strong> (no LRU)</td><td>Entries represent temporal dedup windows. Once the window passes, the entry is no longer needed. LRU would risk evicting active windows under memory pressure, causing false negatives. TTL guarantees deterministic expiration.</td></tr>
    <tr><td>TTL</td><td>120 seconds</td><td>2&times; the 60-second dedup window to account for clock skew between Click Tracker instances and processing delay in the message queue.</td></tr>
    <tr><td>Populated by</td><td>Aggregation Workers</td><td>When a worker processes a click and the fingerprint is not found, it writes the fingerprint before proceeding.</td></tr>
    <tr><td>Failure mode</td><td>Fallback to Raw Click Log lookup</td><td>If the cache is down, the worker queries the Raw Click Log for the same <code>(user_id, ad_id, minute_bucket)</code>. This is slower (~50&thinsp;ms vs ~1&thinsp;ms) but correct.</td></tr>
    <tr><td>Memory estimate</td><td>~40 bytes/entry &times; 6M entries (100K clicks/s &times; 60s) = ~240 MB</td><td>Very small footprint. A single cache node can handle this; replicate for HA.</td></tr>
</table>

<h3>Query Cache &mdash; Deep Dive</h3>
<table>
    <tr><th>Property</th><th>Value</th><th>Rationale</th></tr>
    <tr><td>Purpose</td><td>Reduce read load on the Aggregation DB and serve low-latency responses to the Query Service</td><td>Dashboards auto-refresh frequently; many advertisers query the same popular ads/campaigns</td></tr>
    <tr><td>Technology</td><td>Distributed in-memory cache</td><td>Sub-10ms reads required for dashboard SLA</td></tr>
    <tr><td>Key format</td><td><code>agg:{entity_type}:{entity_id}:{granularity}:{time_window}</code></td><td>E.g., <code>agg:ad:ad_9001:hour:2024-02-13T17</code></td></tr>
    <tr><td>Value</td><td>Serialized JSON: <code>{click_count, unique_click_count}</code></td><td>Pre-serialized to avoid repeated serialization</td></tr>
    <tr><td>Caching strategy</td><td><strong>Read-through + write-behind hybrid</strong></td><td><strong>Read-through:</strong> On a cache miss during a query, the Query Service fetches from the Aggregation DB and populates the cache. This ensures the cache is lazily populated with actually-requested data.<br/><strong>Write-behind (from Aggregation Workers):</strong> After incrementing the Aggregation DB counter, the worker updates the corresponding cache entry if it exists. This keeps hot entries fresh without waiting for the next read miss. Write-behind avoids adding latency to the processing path (the cache update can be async/fire-and-forget).</td></tr>
    <tr><td>Eviction policy</td><td><strong>LRU</strong> (Least Recently Used)</td><td>The cache should prioritize keeping frequently-queried aggregates (popular ads, active campaigns). LRU naturally retains hot entries and evicts cold ones (old campaigns, paused ads).</td></tr>
    <tr><td>Expiration policy (TTL)</td><td>Varies by granularity:<br/>&bull; <strong>Minute:</strong> 30-second TTL<br/>&bull; <strong>Hour:</strong> 5-minute TTL<br/>&bull; <strong>Day:</strong> 1-hour TTL<br/>&bull; <strong>Historical (older than 7 days):</strong> 24-hour TTL</td><td>Minute-level aggregates change rapidly and must reflect near-real-time data &mdash; short TTL. Hourly data changes less frequently &mdash; moderate TTL. Daily and historical data is relatively stable &mdash; longer TTL reduces DB load. The increasing TTLs match the decreasing rate of change at coarser granularities.</td></tr>
    <tr><td>Populated by</td><td>1. Read-through (cache miss from Query Service)<br/>2. Aggregation Workers (proactive update of hot entries)</td><td>Dual population ensures both frequently-read and actively-written entries stay fresh.</td></tr>
    <tr><td>Cache invalidation</td><td>No explicit invalidation &mdash; TTL-based expiration only</td><td>Click aggregates are additive (counters only increase). Stale data means slightly-behind counts, not incorrect data. TTL naturally refreshes entries. Explicit invalidation would add complexity and coordination overhead for minimal benefit.</td></tr>
    <tr><td>Memory estimate</td><td>~200 bytes/entry &times; 10M entries = ~2 GB</td><td>Covers active ads &times; time granularities. Well within single-node capacity; replicated for HA.</td></tr>
</table>
</div>

<!-- ============================================================ -->
<div class="section" id="scaling">
<h2 style="margin-top:0;">9. Scaling Considerations</h2>

<h3>Load Balancers</h3>
<p>Load balancers are critical at two points:</p>
<ol>
    <li><strong>Ingestion Load Balancer</strong> (between users and Click Tracker Service):
        <ul>
            <li>Handles the highest traffic volume (every ad click).</li>
            <li>Algorithm: <strong>Round-robin</strong> (Click Tracker instances are stateless and homogeneous).</li>
            <li>Layer 7 (HTTP-aware) for URL-based routing and health checks.</li>
            <li>TLS termination here to offload crypto from Click Tracker instances.</li>
            <li>Auto-scaling: Scale the number of Click Tracker instances based on requests-per-second. The load balancer dynamically adds/removes instances.</li>
            <li>Geographic distribution: Deploy load balancers in multiple regions (anycast) to minimize click latency globally.</li>
        </ul>
    </li>
    <li><strong>Query Load Balancer</strong> (between advertisers/dashboards and Query Service):
        <ul>
            <li>Lower traffic volume than ingestion but still significant during business hours when dashboards are active.</li>
            <li>Algorithm: <strong>Least-connections</strong> (query processing time varies; this prevents slow queries from piling up on one instance).</li>
            <li>Rate limiting: Per-API-key rate limiting to prevent abuse.</li>
            <li>Auto-scaling: Scale Query Service instances based on request latency and CPU utilization.</li>
        </ul>
    </li>
</ol>

<h3>Horizontal Scaling Strategy by Component</h3>
<table>
    <tr><th>Component</th><th>Scaling Strategy</th><th>Scaling Trigger</th></tr>
    <tr><td>Click Tracker Service</td><td>Stateless; add instances behind LB</td><td>Requests/sec &gt; threshold</td></tr>
    <tr><td>Message Queue</td><td>Add partitions (and brokers) to topic</td><td>Consumer lag &gt; threshold; throughput limit</td></tr>
    <tr><td>Aggregation Workers</td><td>Add consumers (up to 1 per partition)</td><td>Consumer lag &gt; threshold</td></tr>
    <tr><td>Dedup Cache</td><td>Add replicas for HA; shard if memory exceeds single node</td><td>Memory utilization &gt; 70%; latency spike</td></tr>
    <tr><td>Fraud Detection</td><td>Stateless RPC; add instances</td><td>Latency &gt; threshold</td></tr>
    <tr><td>Raw Click Log</td><td>Add nodes/shards to NoSQL cluster</td><td>Storage utilization &gt; 70%; write latency</td></tr>
    <tr><td>Aggregation DB</td><td>Add shards (consistent hashing)</td><td>Write latency &gt; threshold; hot partition detected</td></tr>
    <tr><td>Query Cache</td><td>Add replicas for read throughput; shard for capacity</td><td>Cache hit rate drops; latency spike</td></tr>
    <tr><td>Query Service</td><td>Stateless; add instances behind LB</td><td>p99 latency &gt; 500ms</td></tr>
</table>

<h3>Hot Partition Mitigation</h3>
<p>A viral ad can create a &ldquo;hot partition&rdquo; in both the Message Queue and Aggregation DB. Mitigation strategies:</p>
<ul>
    <li><strong>Message Queue:</strong> Use a compound partition key <code>ad_id + random_suffix</code> to spread a single ad&rsquo;s clicks across multiple partitions during spikes. The Aggregation Workers then merge results.</li>
    <li><strong>Aggregation DB:</strong> Use a split-counter pattern: instead of one row per <code>(ad_id, time_window)</code>, maintain N sub-counters <code>(ad_id, time_window, shard_0..N-1)</code>. The Query Service sums them at read time. This trades read complexity for write distribution.</li>
</ul>

<h3>Data Volume Estimates</h3>
<ul>
    <li>100K clicks/second &times; 86,400 seconds/day = ~8.6 billion raw click events/day.</li>
    <li>Raw click size: ~500 bytes &rarr; ~4.3 TB/day raw storage.</li>
    <li>Aggregation entries: ~10M active ads &times; 1,440 minutes/day = ~14.4 billion minute-aggregate entries/day. Tiny row size (~100 bytes) &rarr; ~1.4 TB/day.</li>
    <li>With compression (typical 4&times;): ~1 TB/day raw + ~350 GB/day aggregates.</li>
    <li>1-year retention: ~365 TB raw + ~128 TB aggregates. At this scale, tiered storage (hot/warm/cold) is essential.</li>
</ul>
</div>

<!-- ============================================================ -->
<div class="section" id="tradeoffs">
<h2 style="margin-top:0;">10. Tradeoffs &amp; Deep Dives</h2>

<h3>Message Queue &mdash; Deep Dive</h3>
<p>The message queue is the backbone of the system, decoupling ingestion from processing.</p>
<ul>
    <li><strong>Why a message queue and not direct DB writes:</strong>
        <ul>
            <li>Click traffic is bursty (Super Bowl ads, flash sales). The queue absorbs spikes that would overwhelm a direct-write database.</li>
            <li>Decouples the user-facing latency (must be &lt;100ms) from processing latency (can tolerate seconds of delay).</li>
            <li>Enables multiple consumers: the same click event can be consumed by the Aggregation Workers, a separate Fraud Analytics pipeline, a Billing pipeline, etc., without duplicating the ingestion path.</li>
            <li>Provides replay capability: if a bug is found in the aggregation logic, events can be replayed from the queue (7-day retention) to recompute aggregates.</li>
        </ul>
    </li>
    <li><strong>Why not a pub/sub system:</strong> A traditional pub/sub (fan-out to all subscribers) wastes bandwidth when only one consumer group needs each event. The message queue&rsquo;s consumer-group model ensures each event is processed exactly once per consumer group, with load distributed across workers in the group. We can still have multiple consumer groups (aggregation, fraud, billing) each independently consuming the same topic &mdash; this is the best of both models.</li>
    <li><strong>Why not polling the database:</strong> Polling introduces latency (polling interval), wastes resources when there are no new events, and creates lock contention on the database.</li>
    <li><strong>How messages are put on the queue:</strong> The Click Tracker Service uses a producer client library to publish events to the <code>raw-click-events</code> topic. The producer batches messages (e.g., 100 messages or 50ms window) for throughput, and uses the <code>ad_id</code> as the partition key to determine which partition receives the message. The producer waits for an acknowledgment from the broker that the message has been replicated to at least 2 replicas before considering the publish successful.</li>
    <li><strong>How messages are removed from the queue:</strong> Aggregation Workers (in a consumer group) pull batches of messages. After processing a batch (dedup, fraud check, write to DB), the worker commits the offset for that batch. The broker marks those messages as consumed for that consumer group. Messages are not physically deleted &mdash; they are retained for the 7-day retention period regardless of consumption, enabling replay.</li>
    <li><strong>Protocol:</strong> TCP-based binary protocol. TCP is required (not UDP) because message delivery must be reliable, ordered, and acknowledged. UDP&rsquo;s unreliability is unacceptable when every click represents billable revenue.</li>
</ul>

<h3>At-Least-Once + Dedup = Exactly-Once Counting</h3>
<p>True exactly-once delivery in a distributed system is impractical without two-phase commits (expensive). Instead, we use:</p>
<ul>
    <li><strong>At-least-once delivery</strong> from the message queue: if a worker crashes before acknowledging, the message is redelivered.</li>
    <li><strong>Idempotent consumers:</strong> The Aggregation Worker checks the Dedup Cache and uses <code>click_id</code>-based idempotency in the Aggregation DB. A redelivered message with the same <code>click_id</code> is discarded or its counter increment is a no-op.</li>
    <li>This achieves <strong>effectively exactly-once counting</strong> without the performance cost of distributed transactions.</li>
</ul>

<h3>Real-Time vs. Batch Accuracy</h3>
<p>The system has two accuracy tiers:</p>
<ul>
    <li><strong>Real-time (fast path):</strong> Aggregation Workers update counters as events arrive. This path is fast (&lt;5 second delay) but may have minor inaccuracies if the Dedup Cache experiences brief failures or if race conditions occur in counter increments.</li>
    <li><strong>Batch (slow path):</strong> A nightly Batch Reconciliation job reads the entire day&rsquo;s events from the Raw Click Log, recomputes exact aggregates from scratch, and overwrites the Aggregation DB values. This corrects any drift. For billing, only reconciled values are used.</li>
</ul>
<p><strong>Tradeoff:</strong> The fast path prioritizes speed and availability over perfect accuracy. The slow path prioritizes accuracy over timeliness. Together, they satisfy both the dashboard&rsquo;s need for near-real-time data and billing&rsquo;s need for exact counts.</p>

<h3>Counter Increment Strategy</h3>
<p>Atomic counter increments in a distributed NoSQL store can become a bottleneck for hot keys (popular ads). Strategies:</p>
<ul>
    <li><strong>Batched increments:</strong> The Aggregation Worker accumulates increments in memory (e.g., for 1 second) and flushes a single +N increment to the DB instead of N individual +1 increments. This reduces write IOPS by ~100-1000&times;.</li>
    <li><strong>Split counters:</strong> For extremely hot ads, split the counter into K sub-counters (e.g., K=10). Workers randomly pick a sub-counter to increment. Reads sum all K sub-counters. This distributes write load at the cost of read amplification (K reads instead of 1).</li>
</ul>

<h3>Unique Click Counting</h3>
<p>Counting unique users who clicked an ad (unique_click_count) is harder than total clicks because it requires set cardinality.</p>
<ul>
    <li><strong>Exact approach:</strong> Maintain a set of user_ids per (ad_id, time_window). Expensive in memory and storage.</li>
    <li><strong>Approximate approach (recommended):</strong> Use a probabilistic data structure like <strong>HyperLogLog</strong> (HLL). Each aggregation row stores an HLL sketch (~12 KB). Workers add user_ids to the sketch. At query time, the HLL provides a cardinality estimate with ~2% error. This is acceptable for dashboard display and drastically reduces memory.</li>
    <li><strong>Tradeoff:</strong> HLL introduces a small error in unique counts but reduces storage from potentially gigabytes (exact sets for popular ads) to kilobytes.</li>
</ul>

<h3>TCP for All Inter-Service Communication</h3>
<p>All internal communication uses TCP:</p>
<ul>
    <li><strong>Click Tracker → Message Queue:</strong> TCP. Reliable delivery is mandatory; a lost click event is lost revenue.</li>
    <li><strong>Aggregation Workers → Dedup Cache:</strong> TCP. Cache responses must be reliable to prevent false negatives.</li>
    <li><strong>Aggregation Workers → Aggregation DB:</strong> TCP. Database writes must be acknowledged.</li>
    <li><strong>Query Service → Cache/DB:</strong> TCP. Query results must be complete and uncorrupted.</li>
    <li><strong>Why not UDP:</strong> No component in this system can tolerate data loss. UDP&rsquo;s speed advantage is irrelevant when every operation must be acknowledged. The latency difference between TCP and UDP is negligible on modern data center networks (~0.1ms).</li>
</ul>

<h3>WebSocket / SSE for Dashboard (Optional Enhancement)</h3>
<p>The baseline design uses standard HTTP polling (the dashboard calls <code>GET /api/v1/aggregates</code> every N seconds). For a more responsive real-time dashboard:</p>
<ul>
    <li><strong>Server-Sent Events (SSE):</strong> The Query Service could expose an SSE endpoint <code>GET /api/v1/aggregates/stream?ad_id=X</code>. The server pushes updated aggregates whenever the Query Cache is updated by the Aggregation Workers. SSE is preferred over WebSockets here because the data flow is unidirectional (server → client). It's simpler, works over HTTP/2, and doesn't require a persistent bidirectional connection.</li>
    <li><strong>Why not WebSockets:</strong> WebSockets add complexity (connection management, reconnection logic, state tracking) for a use case that only needs server-to-client pushes. SSE achieves the same result with less overhead.</li>
    <li><strong>Why this is optional:</strong> Most ad dashboards are fine with 10-30 second polling intervals. Real-time streaming is a nice-to-have, not a core requirement.</li>
</ul>
</div>

<!-- ============================================================ -->
<div class="section" id="alternatives">
<h2 style="margin-top:0;">11. Alternative Approaches</h2>

<div class="alt-box">
<strong>Alternative 1: Direct database writes (no message queue)</strong><br/>
The Click Tracker Service writes directly to the Aggregation DB and Raw Click Log synchronously.<br/><br/>
<strong>Why rejected:</strong> This couples user-facing latency to database write latency. Under spike traffic (e.g., Super Bowl), the database becomes a bottleneck, causing 503 errors and lost clicks. There is no burst buffering, no replay capability, and no ability to add new consumers (e.g., a fraud analytics pipeline) without modifying the Click Tracker. The message queue solves all of these problems.
</div>

<div class="alt-box">
<strong>Alternative 2: Pre-aggregation at the Click Tracker (edge aggregation)</strong><br/>
Each Click Tracker instance maintains in-memory counters and flushes aggregates to the database every N seconds, instead of publishing individual events to the queue.<br/><br/>
<strong>Why rejected:</strong> If a Click Tracker instance crashes, all in-memory counters are lost &mdash; data loss is unacceptable for billing. This also makes deduplication extremely difficult (duplicates would need to be detected across all Click Tracker instances). And it eliminates the ability to reprocess raw events (there are no raw events to replay). However, this approach <em>can</em> be used as an optimization layer <em>in front of</em> the message queue: batch multiple events into a single queue message to reduce per-event overhead. The tradeoff is a small window of potential data loss.
</div>

<div class="alt-box">
<strong>Alternative 3: Time-series database for aggregation</strong><br/>
Use a dedicated time-series database instead of a NoSQL wide-column store for the aggregation tables.<br/><br/>
<strong>Why rejected (as primary):</strong> Time-series databases are excellent for read-heavy analytical queries but many struggle with the extreme write throughput of atomic counter increments at 100K+ operations/second. NoSQL wide-column stores are better optimized for this write pattern. However, a time-series database could be used as a <strong>secondary store</strong> fed by the batch reconciliation job for complex analytical queries (e.g., click trends, anomaly detection).
</div>

<div class="alt-box">
<strong>Alternative 4: Lambda Architecture (separate real-time + batch pipelines)</strong><br/>
Run two completely separate pipelines: a real-time stream processor for approximate counts and a batch MapReduce job for exact counts. Query both and merge results.<br/><br/>
<strong>Why rejected:</strong> Full Lambda Architecture requires maintaining two separate codebases for the same logic (aggregation), which is a maintenance burden. Our design achieves the same result more simply: the Aggregation Workers provide the &ldquo;speed layer&rdquo; and the nightly batch reconciliation provides the &ldquo;batch layer,&rdquo; but both share the same data store (Aggregation DB). The reconciliation job overwrites the speed layer&rsquo;s values rather than requiring a merge at query time. This is sometimes called the &ldquo;Kappa Architecture&rdquo; with a reconciliation step.
</div>

<div class="alt-box">
<strong>Alternative 5: CQRS with event sourcing</strong><br/>
Use full event sourcing where the Raw Click Log is the source of truth and all aggregation state is derived from replaying events.<br/><br/>
<strong>Why partially adopted:</strong> We <em>do</em> use elements of CQRS (separate read and write models: write to Aggregation DB via workers, read via Query Service + cache). We also <em>do</em> store raw events (Raw Click Log) as a form of event source. However, we don&rsquo;t require <em>full</em> event replay for every read &mdash; that would be far too slow. Pre-computed aggregates are essential for the &lt;500ms query SLA. The batch reconciliation job is our &ldquo;event replay&rdquo; but it runs offline.
</div>

<div class="alt-box">
<strong>Alternative 6: Client-side aggregation with JavaScript SDK</strong><br/>
Embed a JavaScript SDK in the ad creative that batches clicks client-side and sends aggregated data.<br/><br/>
<strong>Why rejected:</strong> Client-side code can be tampered with (inflated counts for fraud), blocked by ad blockers, or disabled by browser settings. The redirect-based server-side tracking approach is more reliable, universal, and tamper-resistant. Client-side SDKs can complement server-side tracking (for viewability, engagement metrics) but should not be the primary click counting mechanism.
</div>
</div>

<!-- ============================================================ -->
<div class="section" id="additional">
<h2 style="margin-top:0;">12. Additional Considerations</h2>

<h3>Data Retention &amp; Tiered Storage</h3>
<ul>
    <li><strong>Hot tier (0&ndash;7 days):</strong> Minute-level aggregates and recent raw clicks on fast SSDs in the NoSQL cluster.</li>
    <li><strong>Warm tier (7&ndash;90 days):</strong> Hour/day-level aggregates on standard disks. Minute-level aggregates are compacted into hourly.</li>
    <li><strong>Cold tier (90 days &ndash; 3 years):</strong> Raw click logs and day-level aggregates moved to object storage. Queryable via batch jobs but not real-time APIs.</li>
    <li>This tiered approach balances cost (cold storage is ~10&times; cheaper) with access speed requirements.</li>
</ul>

<h3>Idempotency in Counter Increments</h3>
<p>The Aggregation DB supports idempotent increments using a <code>click_id</code> dedup log. Each increment operation is tagged with the <code>click_id</code>. The DB checks a small in-memory Bloom filter of recently-seen <code>click_id</code> values; if the ID is likely already processed, it queries a persistent dedup table before applying the increment. This prevents double-counting even if the Dedup Cache is bypassed.</p>

<h3>Monitoring &amp; Alerting</h3>
<ul>
    <li><strong>Consumer lag:</strong> Alert if message queue consumer lag exceeds 5 minutes (aggregation is falling behind).</li>
    <li><strong>Click drop rate:</strong> Alert if the ratio of published events to processed events drops below 99.9%.</li>
    <li><strong>Fraud rate:</strong> Alert if the percentage of flagged clicks exceeds 5% (possible attack).</li>
    <li><strong>Reconciliation drift:</strong> Alert if the nightly batch reconciliation corrects aggregates by more than 0.1%.</li>
    <li><strong>p99 latency:</strong> Alert if Click Tracker p99 &gt; 100ms or Query Service p99 &gt; 500ms.</li>
</ul>

<h3>Privacy &amp; Compliance</h3>
<ul>
    <li><strong>PII handling:</strong> <code>user_id</code> and <code>ip_address</code> are PII. Raw click logs must be encrypted at rest. Access is restricted to authorized billing and fraud teams.</li>
    <li><strong>GDPR right-to-erasure:</strong> Must support deleting all click records for a given <code>user_id</code>. This requires a background job that scans the raw click log and aggregation tables. Aggregation counters are decremented by the count of deleted clicks.</li>
    <li><strong>Cookie consent:</strong> In jurisdictions requiring consent for tracking cookies, the <code>user_id</code> may not be available. Clicks are still counted but deduplication and unique counts degrade (IP-based fingerprinting as fallback).</li>
</ul>

<h3>Geo-Distribution</h3>
<p>For a global ad platform, deploy the ingestion layer (Load Balancer + Click Tracker + Message Queue producers) in multiple geographic regions to minimize click latency. The processing layer (Aggregation Workers, databases) can be centralized in fewer regions since it is asynchronous. Use the message queue&rsquo;s cross-region replication to funnel events from regional producers to a central processing cluster.</p>

<h3>Graceful Degradation</h3>
<ul>
    <li>If the Aggregation DB is down: Aggregation Workers buffer increments in the message queue (stop committing offsets). Events accumulate and are processed when the DB recovers. Click recording continues unaffected.</li>
    <li>If the Dedup Cache is down: Workers fall back to the Raw Click Log for dedup (slower but correct). Alternatively, accept slightly higher duplicate risk and correct in batch reconciliation.</li>
    <li>If the Query Cache is down: Query Service reads directly from the Aggregation DB. Latency increases but functionality is preserved.</li>
</ul>
</div>

<!-- ============================================================ -->
<div class="section" id="vendors">
<h2 style="margin-top:0;">13. Vendor Recommendations</h2>
<p>The following vendor-specific technologies would be strong choices for implementing each component. The design remains vendor-agnostic in its architecture.</p>

<table>
    <tr><th>Component</th><th>Vendor Options</th><th>Rationale</th></tr>
    <tr>
        <td>Message Queue</td>
        <td>Apache Kafka, Amazon Kinesis, Apache Pulsar</td>
        <td><strong>Kafka:</strong> Industry standard for high-throughput event streaming. Supports partitioning by key, consumer groups, 7+ day retention, and replay. Battle-tested at ad-tech scale (LinkedIn, Uber). <strong>Kinesis:</strong> Managed alternative if running on AWS; lower ops burden. <strong>Pulsar:</strong> Better multi-tenancy and tiered storage if needed.</td>
    </tr>
    <tr>
        <td>NoSQL (Aggregation DB, Raw Click Log)</td>
        <td>Apache Cassandra, ScyllaDB, Apache HBase</td>
        <td><strong>Cassandra/ScyllaDB:</strong> Wide-column stores with native counter support, tunable consistency, and linear horizontal scaling. ScyllaDB is a higher-performance C++ rewrite of Cassandra. <strong>HBase:</strong> Good for very large datasets with Hadoop ecosystem integration (useful for batch reconciliation).</td>
    </tr>
    <tr>
        <td>SQL (Metadata: ads, campaigns, advertisers)</td>
        <td>PostgreSQL, MySQL/Aurora</td>
        <td><strong>PostgreSQL:</strong> Robust ACID compliance, rich indexing, excellent for relational metadata with low write volume. <strong>Aurora:</strong> Managed MySQL-compatible with automatic scaling if on AWS.</td>
    </tr>
    <tr>
        <td>In-Memory Cache (Dedup + Query)</td>
        <td>Redis, Memcached, Dragonfly</td>
        <td><strong>Redis:</strong> Supports TTL, atomic operations, HyperLogLog natively (useful for unique counts), and pub/sub for cache invalidation. <strong>Memcached:</strong> Simpler, slightly faster for pure key-value caching. <strong>Dragonfly:</strong> Redis-compatible with better multi-threaded performance.</td>
    </tr>
    <tr>
        <td>Object Storage (Cold Tier)</td>
        <td>Amazon S3, Google Cloud Storage, Azure Blob Storage</td>
        <td>Industry-standard object stores for cold archival of raw click logs. Extremely low cost ($0.023/GB/month for S3 Standard), 99.999999999% durability, and integration with batch processing frameworks.</td>
    </tr>
    <tr>
        <td>Stream Processing (Aggregation Workers)</td>
        <td>Apache Flink, Apache Spark Structured Streaming, Kafka Streams</td>
        <td><strong>Flink:</strong> Best-in-class for stateful stream processing with exactly-once semantics, windowed aggregation, and watermark handling. <strong>Kafka Streams:</strong> Simpler if already using Kafka; runs as a library (no separate cluster). <strong>Spark Structured Streaming:</strong> Good if batch reconciliation also uses Spark.</td>
    </tr>
    <tr>
        <td>Load Balancer</td>
        <td>NGINX, HAProxy, AWS ALB, Envoy</td>
        <td><strong>NGINX/HAProxy:</strong> High-performance, widely deployed, configurable. <strong>ALB:</strong> Managed option on AWS with auto-scaling integration. <strong>Envoy:</strong> Modern L7 proxy with advanced observability and service mesh support.</td>
    </tr>
    <tr>
        <td>Fraud Detection (ML models)</td>
        <td>Custom service using TensorFlow Serving, PyTorch Serve, or rule engine</td>
        <td>Start with a rule-based engine for low latency; graduate to ML models (gradient boosted trees or neural nets) as training data accumulates. <strong>TensorFlow Serving / PyTorch Serve</strong> can serve models with &lt;10ms inference latency.</td>
    </tr>
</table>
</div>

<script>
    mermaid.initialize({
        startOnLoad: true,
        theme: 'default',
        flowchart: {
            useMaxWidth: true,
            htmlLabels: true,
            curve: 'basis'
        }
    });
</script>

</body>
</html>
