<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>System Design: Vending Machine</title>
    <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
    <style>
        :root {
            --bg: #0d1117;
            --surface: #161b22;
            --border: #30363d;
            --text: #e6edf3;
            --muted: #8b949e;
            --accent: #58a6ff;
            --green: #3fb950;
            --orange: #d29922;
            --red: #f85149;
            --purple: #bc8cff;
        }
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
            background: var(--bg);
            color: var(--text);
            line-height: 1.7;
            padding: 2rem;
            max-width: 1200px;
            margin: 0 auto;
        }
        h1 { font-size: 2.4rem; margin-bottom: 0.5rem; color: var(--accent); border-bottom: 2px solid var(--border); padding-bottom: 0.75rem; }
        h2 { font-size: 1.8rem; margin-top: 2.5rem; margin-bottom: 1rem; color: var(--green); }
        h3 { font-size: 1.35rem; margin-top: 1.8rem; margin-bottom: 0.7rem; color: var(--purple); }
        h4 { font-size: 1.1rem; margin-top: 1.2rem; margin-bottom: 0.5rem; color: var(--orange); }
        p { margin-bottom: 1rem; color: var(--text); }
        ul, ol { margin-left: 1.5rem; margin-bottom: 1rem; }
        li { margin-bottom: 0.4rem; }
        code {
            background: var(--surface);
            border: 1px solid var(--border);
            border-radius: 4px;
            padding: 0.15rem 0.4rem;
            font-size: 0.9em;
            color: var(--orange);
        }
        pre {
            background: var(--surface);
            border: 1px solid var(--border);
            border-radius: 8px;
            padding: 1rem;
            overflow-x: auto;
            margin-bottom: 1rem;
        }
        pre code { border: none; padding: 0; background: none; }
        .diagram-container {
            background: #ffffff;
            border-radius: 12px;
            padding: 1.5rem;
            margin: 1.5rem 0;
            border: 1px solid var(--border);
        }
        .example-box {
            background: var(--surface);
            border-left: 4px solid var(--accent);
            border-radius: 0 8px 8px 0;
            padding: 1rem 1.2rem;
            margin: 1rem 0;
        }
        .example-box strong { color: var(--accent); }
        .callout {
            background: var(--surface);
            border-left: 4px solid var(--green);
            border-radius: 0 8px 8px 0;
            padding: 1rem 1.2rem;
            margin: 1rem 0;
        }
        .callout-warn {
            border-left-color: var(--orange);
        }
        .callout-red {
            border-left-color: var(--red);
        }
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 1rem 0 1.5rem 0;
            font-size: 0.95rem;
        }
        th, td {
            border: 1px solid var(--border);
            padding: 0.6rem 0.8rem;
            text-align: left;
        }
        th {
            background: var(--surface);
            color: var(--accent);
            font-weight: 600;
        }
        td { background: var(--bg); }
        hr { border: none; border-top: 1px solid var(--border); margin: 2.5rem 0; }
        .tag {
            display: inline-block;
            padding: 0.15rem 0.5rem;
            border-radius: 12px;
            font-size: 0.8rem;
            font-weight: 600;
            margin-right: 0.3rem;
        }
        .tag-sql { background: #1f3a5f; color: #58a6ff; }
        .tag-nosql { background: #3b2f1a; color: #d29922; }
        .tag-ts { background: #2a1f3a; color: #bc8cff; }
        .tag-pk { background: #1a3a2a; color: #3fb950; }
        .tag-fk { background: #3a1a1a; color: #f85149; }
    </style>
</head>
<body>

<h1>üè≠ System Design: Vending Machine</h1>
<p>A comprehensive design for a distributed, network-connected vending machine fleet with real-time monitoring, multi-modal payments, and offline resilience.</p>

<!-- ============================================================ -->
<h2>1. Functional Requirements</h2>
<!-- ============================================================ -->
<ol>
    <li><strong>Browse Products:</strong> Users can view available products and their prices on the machine's display screen.</li>
    <li><strong>Select Product:</strong> Users can select a product from an available slot.</li>
    <li><strong>Accept Payment:</strong> The machine accepts payment via cash (coins/bills), credit/debit card, or contactless/NFC mobile payment.</li>
    <li><strong>Dispense Product:</strong> Upon successful payment, the machine dispenses the selected product.</li>
    <li><strong>Return Change:</strong> For cash payments, the machine returns the correct change.</li>
    <li><strong>Cancel Transaction:</strong> Users can cancel a transaction at any point before dispensing and receive a refund.</li>
    <li><strong>Display Errors:</strong> The machine displays meaningful error messages (e.g., out of stock, exact change only, payment failed).</li>
    <li><strong>Remote Inventory Monitoring:</strong> Operators can view real-time inventory levels of all machines from a central dashboard.</li>
    <li><strong>Low-Inventory Alerts:</strong> Operators receive alerts when inventory drops below a configurable threshold.</li>
    <li><strong>Restock Inventory:</strong> Operators can physically restock a machine and the system updates inventory records.</li>
    <li><strong>Transaction History:</strong> All transactions are recorded and queryable for audit and analytics.</li>
    <li><strong>Machine Health Monitoring:</strong> The system monitors machine health (temperature, power, connectivity, mechanical status).</li>
    <li><strong>Dynamic Pricing:</strong> Operators can update product prices remotely per-machine or fleet-wide.</li>
</ol>

<!-- ============================================================ -->
<h2>2. Non-Functional Requirements</h2>
<!-- ============================================================ -->
<ol>
    <li><strong>Availability (99.9%):</strong> Machines must remain operational even during backend outages via offline mode for cash payments.</li>
    <li><strong>Low Latency:</strong> End-to-end purchase transaction should complete in &lt; 3 seconds (including payment processing).</li>
    <li><strong>Strong Consistency (Inventory):</strong> On-machine inventory must be strongly consistent to avoid selling items that are not physically present.</li>
    <li><strong>Eventual Consistency (Backend Sync):</strong> Backend inventory records can be eventually consistent with the physical machine state.</li>
    <li><strong>Security:</strong> Payment processing must be PCI-DSS compliant. All backend communication encrypted via TLS.</li>
    <li><strong>Scalability:</strong> Support 100,000+ machines generating millions of daily transactions.</li>
    <li><strong>Fault Tolerance:</strong> Gracefully handle power outages, network failures, and mechanical jams.</li>
    <li><strong>Durability:</strong> Zero transaction data loss ‚Äî transactions stored locally and synced to backend.</li>
    <li><strong>Observability:</strong> Full telemetry pipeline for dashboards, alerting, and anomaly detection.</li>
    <li><strong>Offline Resilience:</strong> Cash-based purchases work without network connectivity. Transactions sync when connectivity is restored.</li>
</ol>

<!-- ============================================================ -->
<h2>3. System Design Diagrams &amp; Flows</h2>
<!-- ============================================================ -->

<!-- ===================== FLOW 1 ===================== -->
<h3>Flow 1: Product Purchase (User Buying a Product)</h3>
<p>This is the primary flow. A user walks up to the machine, selects a product, pays, and receives the item.</p>

<div class="diagram-container">
    <pre class="mermaid">
flowchart LR
    subgraph User
        A["üë§ User at Machine"]
    end

    subgraph VendingMachine["Vending Machine (Embedded Software)"]
        B["Display Controller"]
        C["Local Inventory Store"]
        D["Payment Hardware\n(Coin/Bill Acceptor,\nCard Reader, NFC)"]
        E["Dispenser\nMechanism"]
        F["Local Transaction\nLog"]
    end

    subgraph Backend["Backend Services"]
        G["API Gateway\n+ Load Balancer"]
        H["Payment Service"]
        I["Transaction Service"]
        J["Inventory Service"]
    end

    subgraph External
        K["Payment Gateway\n(Card/NFC Processor)"]
    end

    subgraph DataStores
        L[("Transaction DB\n(NoSQL)")]
        M[("Inventory DB\n(SQL)")]
    end

    A -- "1. Select product" --> B
    B -- "2. Check stock" --> C
    C -- "3. Stock OK" --> B
    B -- "4. Prompt payment" --> A
    A -- "5. Insert payment" --> D
    D -- "6a. Cash validated\nlocally" --> E
    D -- "6b. Card/NFC\nrequest" --> G
    G -- "7. Process\npayment" --> H
    H -- "8. Authorize" --> K
    K -- "9. Approved" --> H
    H -- "10. Success" --> G
    G -- "11. Approved" --> D
    D -- "12. Trigger\ndispense" --> E
    E -- "13. Product\ndispensed" --> A
    E -- "14. Log locally" --> F
    F -- "15. Sync\ntransaction" --> G
    G -- "16. Record" --> I
    I --> L
    G -- "17. Update\ninventory" --> J
    J --> M
    </pre>
</div>

<h4>Examples</h4>

<div class="example-box">
    <strong>Example 1 ‚Äî Cash Payment (Happy Path):</strong><br>
    Alice walks up to Vending Machine #VM-4421 at the office lobby. She sees "Sparkling Water ‚Äî $2.00" in Slot B3. She presses B3 on the keypad. The Display Controller checks the Local Inventory Store and confirms 5 units remain. The screen shows "Insert $2.00." Alice inserts two $1 coins. The Coin Acceptor validates the coins, confirms $2.00 received, and triggers the Dispenser Mechanism, which pushes the Sparkling Water out of Slot B3. The Local Inventory Store is decremented to 4 units. The transaction is written to the Local Transaction Log. When connectivity is available, the machine sends an HTTP POST to the API Gateway, which routes to the Transaction Service (records the sale in the Transaction DB) and the Inventory Service (decrements Slot B3 quantity in the Inventory DB).
</div>

<div class="example-box">
    <strong>Example 2 ‚Äî Card Payment (Happy Path):</strong><br>
    Bob taps his credit card on the NFC reader of Machine #VM-8832 after selecting a $3.50 energy drink in Slot A1. The machine sends an HTTP POST to the API Gateway, which forwards it to the Payment Service. The Payment Service sends an authorization request to the external Payment Gateway. The Payment Gateway responds with "Approved." The Payment Service returns success to the machine. The Dispenser Mechanism releases the energy drink. The transaction is logged locally and synced to the Transaction Service and Inventory Service on the backend.
</div>

<div class="example-box">
    <strong>Example 3 ‚Äî Out of Stock:</strong><br>
    Carol selects Slot C2 (Chips ‚Äî $1.50) on Machine #VM-1003. The Display Controller queries the Local Inventory Store, which returns quantity = 0 for Slot C2. The screen displays "Sorry, this item is out of stock. Please select another product." No payment is prompted. Carol selects a different slot.
</div>

<div class="example-box">
    <strong>Example 4 ‚Äî Payment Failure (Card Declined):</strong><br>
    Dave selects a $4.00 sandwich from Machine #VM-5590 and taps his card. The machine sends an authorization request through the API Gateway ‚Üí Payment Service ‚Üí Payment Gateway. The Payment Gateway returns "Declined ‚Äî Insufficient Funds." The Payment Service returns a failure to the machine. The screen displays "Payment declined. Please try another payment method." No product is dispensed. Dave inserts cash instead and completes the purchase.
</div>

<div class="example-box">
    <strong>Example 5 ‚Äî Transaction Cancellation:</strong><br>
    Eve selects a $2.50 juice on Machine #VM-7712 and inserts $1.00 in coins. She changes her mind and presses the "Cancel" button. The machine returns the $1.00 via the coin return mechanism. The partial transaction is logged locally as "CANCELLED" and no product is dispensed. No backend sync is needed for cancelled transactions with cash returned.
</div>

<div class="example-box">
    <strong>Example 6 ‚Äî Change Returned:</strong><br>
    Frank selects a $1.75 cookie on Machine #VM-3301 and inserts a $5 bill. The Bill Acceptor validates the bill. The machine calculates $3.25 in change. It dispenses the cookie and returns $3.25 in coins via the change mechanism. If the machine cannot make exact change (e.g., low coin inventory), it would have shown "Exact Change Only" before accepting the bill.
</div>

<h4>Deep Dive: Components (Flow 1)</h4>

<h4>Display Controller (Embedded)</h4>
<p>The on-machine software driving the LCD/touchscreen. It renders the product grid (name, price, slot, availability), accepts keypad/touch input for product selection, and shows status messages. It communicates with the Local Inventory Store via in-process function calls (no network). It is the "brain" of the local machine coordinating all hardware modules.</p>

<h4>Local Inventory Store (Embedded)</h4>
<p>A lightweight embedded key-value store (e.g., a simple file-backed store or embedded database on the machine's SBC ‚Äî single-board computer). Keyed by <code>slot_number</code>, it stores <code>product_id</code>, <code>quantity</code>, <code>price</code>, and <code>max_capacity</code>. This is the source of truth for what the machine physically contains. Updated immediately on dispense or restock.</p>

<h4>Payment Hardware (Embedded)</h4>
<p>Three sub-modules:</p>
<ul>
    <li><strong>Coin Acceptor:</strong> Validates coins by weight/diameter/electromagnetic signature. Fully offline-capable.</li>
    <li><strong>Bill Acceptor:</strong> Validates bills using optical and magnetic sensors. Fully offline-capable.</li>
    <li><strong>Card/NFC Reader:</strong> Reads card chip or NFC token. Requires network to authorize via the Payment Service. Uses TLS-encrypted communication.</li>
</ul>

<h4>Dispenser Mechanism (Embedded)</h4>
<p>Electromechanical motor system that physically pushes the product out of the slot. Includes an infrared drop sensor at the dispensing tray to confirm the item actually fell. If the sensor does not detect a drop within a timeout (e.g., 5 seconds), the machine reports a jam, refunds the payment, and logs a mechanical error.</p>

<h4>Local Transaction Log (Embedded)</h4>
<p>An append-only log stored on the machine's local persistent storage. Each entry contains: <code>transaction_id</code>, <code>slot_number</code>, <code>product_id</code>, <code>amount</code>, <code>payment_method</code>, <code>status</code>, <code>timestamp</code>, and <code>synced</code> (boolean). A background sync daemon reads un-synced entries and POSTs them to the backend when connectivity is available.</p>

<h4>API Gateway + Load Balancer</h4>
<p>The single entry point for all machine-to-backend communication. Handles TLS termination, authentication (each machine has an API key / client certificate), rate limiting, and request routing. Distributes traffic across backend service instances using round-robin or least-connections.</p>

<h4>Payment Service</h4>
<ul>
    <li><strong>Protocol:</strong> HTTP/REST</li>
    <li><strong>Endpoint:</strong> <code>POST /api/v1/payments/authorize</code></li>
    <li><strong>Input:</strong> <code>{ machine_id, transaction_id, amount, currency, payment_token }</code></li>
    <li><strong>Output:</strong> <code>{ status: "APPROVED" | "DECLINED", authorization_code, decline_reason? }</code></li>
    <li>Acts as a proxy to the external Payment Gateway. Stores no card data (PCI compliance). Implements retry with idempotency keys to avoid double-charges.</li>
</ul>

<h4>Transaction Service</h4>
<ul>
    <li><strong>Protocol:</strong> HTTP/REST</li>
    <li><strong>Endpoint:</strong> <code>POST /api/v1/transactions</code></li>
    <li><strong>Input:</strong> <code>{ transaction_id, machine_id, product_id, slot_number, amount, payment_method, status, timestamp }</code></li>
    <li><strong>Output:</strong> <code>{ success: true }</code></li>
    <li><strong>Read Endpoint:</strong> <code>GET /api/v1/transactions?machine_id=X&start=T1&end=T2</code></li>
    <li><strong>Output:</strong> <code>{ transactions: [...] }</code></li>
    <li>Writes to the Transaction DB (NoSQL). Supports idempotent writes using <code>transaction_id</code> to handle retries from the machine sync daemon safely.</li>
</ul>

<h4>Inventory Service</h4>
<ul>
    <li><strong>Protocol:</strong> HTTP/REST</li>
    <li><strong>Endpoint:</strong> <code>PATCH /api/v1/machines/{machine_id}/slots/{slot_number}/inventory</code></li>
    <li><strong>Input:</strong> <code>{ quantity_delta: -1 }</code> (decrement by 1 on sale)</li>
    <li><strong>Output:</strong> <code>{ slot_number, new_quantity }</code></li>
    <li><strong>Read Endpoint:</strong> <code>GET /api/v1/machines/{machine_id}/inventory</code></li>
    <li><strong>Output:</strong> <code>{ slots: [{ slot_number, product_id, quantity, max_capacity, price }] }</code></li>
    <li>Updates the Inventory DB (SQL). The backend inventory is a mirror of the machine's local state ‚Äî the machine is the source of truth, and the backend is updated asynchronously.</li>
</ul>

<hr>

<!-- ===================== FLOW 2 ===================== -->
<h3>Flow 2: Inventory Restocking (Operator Restocking a Machine)</h3>
<p>An operator physically arrives at the machine, opens it, adds products, and the system records the restock event.</p>

<div class="diagram-container">
    <pre class="mermaid">
flowchart LR
    subgraph Operator
        A["üîß Field Operator"]
    end

    subgraph VendingMachine["Vending Machine"]
        B["Operator Panel\n(Keypad/Screen)"]
        C["Local Inventory\nStore"]
        D["Local Transaction\nLog"]
    end

    subgraph Backend["Backend Services"]
        E["API Gateway\n+ Load Balancer"]
        F["Inventory Service"]
        G["Notification Service"]
    end

    subgraph DataStores
        H[("Inventory DB\n(SQL)")]
    end

    subgraph OperatorDashboard
        I["üìä Operator\nDashboard (Web)"]
    end

    A -- "1. Authenticate\n(badge/PIN)" --> B
    B -- "2. Enter\nrestock mode" --> C
    A -- "3. Physically add\nproducts to slots" --> B
    B -- "4. Confirm restock\nper slot\n(slot, qty added)" --> C
    C -- "5. Update local\ninventory" --> C
    C -- "6. Log restock\nevent" --> D
    D -- "7. Sync restock\nevent" --> E
    E -- "8. Update\ninventory" --> F
    F --> H
    F -- "9. Clear low-stock\nalert" --> G
    G -- "10. Dashboard\nrefresh" --> I
    </pre>
</div>

<h4>Examples</h4>

<div class="example-box">
    <strong>Example 1 ‚Äî Standard Restock:</strong><br>
    Operator Maria arrives at Machine #VM-4421 with a supply cart. She scans her operator badge on the machine's operator panel. The screen switches to "Restock Mode." She opens the machine, fills Slot B3 (Sparkling Water) from 2 units to its max capacity of 10 units, and fills Slot A1 (Energy Drink) from 0 to 8 units. She confirms each slot's new quantity on the operator panel. The Local Inventory Store updates: B3 ‚Üí 10, A1 ‚Üí 8. A restock event is written to the Local Transaction Log and synced to the backend via HTTP POST to the Inventory Service, which updates the Inventory DB. The Notification Service clears the "low stock" alert for VM-4421 Slot A1. The operator dashboard refreshes to show the new inventory levels.
</div>

<div class="example-box">
    <strong>Example 2 ‚Äî Restock While Offline:</strong><br>
    Operator Jake restocks Machine #VM-9001 located in a parking garage with poor connectivity. He authenticates, enters restock mode, and adds products. The Local Inventory Store is updated immediately. The restock event is written to the Local Transaction Log with <code>synced = false</code>. When the machine regains connectivity (e.g., 30 minutes later), the sync daemon sends the restock event to the backend. The Inventory DB is updated retroactively.
</div>

<h4>Deep Dive: Components (Flow 2)</h4>

<h4>Operator Panel (Embedded)</h4>
<p>A secondary interface on the machine (behind the door or a separate panel) for operators only. Supports authentication via badge scan (RFID) or PIN. Allows entering restock mode, confirming quantities per slot, running diagnostics, and viewing error logs. Communicates with the Local Inventory Store and Local Transaction Log via in-process calls.</p>

<h4>Notification Service</h4>
<ul>
    <li><strong>Protocol:</strong> HTTP/REST (internal)</li>
    <li><strong>Endpoint (send alert):</strong> <code>POST /api/v1/notifications</code></li>
    <li><strong>Input:</strong> <code>{ machine_id, alert_type: "LOW_STOCK" | "MACHINE_ERROR" | "OFFLINE", message, severity }</code></li>
    <li><strong>Output:</strong> <code>{ notification_id }</code></li>
    <li><strong>Endpoint (clear alert):</strong> <code>DELETE /api/v1/notifications/{notification_id}</code></li>
    <li>Sends push notifications, emails, or SMS to operators. Also pushes real-time updates to the Operator Dashboard via WebSocket (see Flow 3).</li>
</ul>

<h4>Operator Dashboard (Web Application)</h4>
<p>A web application used by operators and fleet managers to monitor all machines. Shows a map of machine locations, inventory levels per machine/slot, transaction history, revenue reports, and active alerts. Communicates with backend services via HTTP REST APIs. Receives real-time updates via WebSocket connections (see Flow 3).</p>

<hr>

<!-- ===================== FLOW 3 ===================== -->
<h3>Flow 3: Fleet Monitoring &amp; Alerting</h3>
<p>Machines continuously report their health status. The backend aggregates telemetry, detects anomalies, and alerts operators in real time.</p>

<div class="diagram-container">
    <pre class="mermaid">
flowchart LR
    subgraph Machines["Vending Machines (Fleet)"]
        A["VM-4421"]
        B["VM-8832"]
        C["VM-1003"]
    end

    subgraph Backend["Backend Services"]
        D["API Gateway\n+ Load Balancer"]
        E["Machine\nManagement\nService"]
        F["Notification\nService"]
        G["Analytics\nService"]
    end

    subgraph DataStores
        H[("Telemetry DB\n(Time-Series)")]
        I[("Machine DB\n(SQL)")]
    end

    subgraph MessageBus
        J[["Message Queue"]]
    end

    subgraph Operators
        K["üìä Operator\nDashboard"]
        L["üì± Operator\nMobile App"]
    end

    A -- "Heartbeat\n(every 60s)" --> D
    B -- "Heartbeat\n(every 60s)" --> D
    C -- "Heartbeat\n(every 60s)" --> D
    D --> E
    E -- "Store\ntelemetry" --> H
    E -- "Update\nmachine status" --> I
    E -- "Anomaly\ndetected?" --> J
    J -- "Alert\nmessage" --> F
    F -- "WebSocket\npush" --> K
    F -- "Push\nnotification" --> L
    E -- "Aggregate\nmetrics" --> G
    G -- "Dashboard\ndata" --> K
    </pre>
</div>

<h4>Examples</h4>

<div class="example-box">
    <strong>Example 1 ‚Äî Low Inventory Alert:</strong><br>
    Machine #VM-4421 sends its periodic heartbeat (every 60 seconds) via HTTP POST to the API Gateway. The payload includes: <code>{ machine_id: "VM-4421", temperature: 4¬∞C, power: "OK", network: "OK", slots: [{slot: "B3", qty: 1}, {slot: "A1", qty: 8}, ...] }</code>. The Machine Management Service processes the heartbeat, writes the telemetry to the Time-Series DB, and updates the machine's <code>last_heartbeat_at</code> in the Machine DB. It detects that Slot B3 has quantity = 1 (below the threshold of 2). It publishes a <code>LOW_STOCK</code> alert message to the Message Queue. The Notification Service consumes the message and sends a WebSocket push to the Operator Dashboard, which shows a yellow warning badge on VM-4421. It also sends a push notification to operator Maria's mobile app: "VM-4421 Slot B3 (Sparkling Water) low stock: 1 unit remaining."
</div>

<div class="example-box">
    <strong>Example 2 ‚Äî Machine Offline Alert:</strong><br>
    Machine #VM-1003 loses power. It stops sending heartbeats. The Machine Management Service runs a periodic check (every 5 minutes) against the Machine DB and finds that VM-1003's <code>last_heartbeat_at</code> is over 3 minutes stale. It marks VM-1003 as "OFFLINE" in the Machine DB and publishes a <code>MACHINE_OFFLINE</code> alert to the Message Queue. The Notification Service pushes a red alert to the Operator Dashboard and sends an SMS to the assigned operator: "URGENT: VM-1003 (123 Main St) is offline. Last heartbeat: 3m ago."
</div>

<div class="example-box">
    <strong>Example 3 ‚Äî Temperature Anomaly:</strong><br>
    Machine #VM-8832 (a refrigerated machine) sends a heartbeat with <code>temperature: 12¬∞C</code>. The Machine Management Service detects this exceeds the safe threshold of 8¬∞C. It publishes a <code>TEMPERATURE_ALERT</code> to the Message Queue. The Notification Service alerts the operator: "VM-8832 temperature is 12¬∞C (threshold: 8¬∞C). Refrigeration unit may be failing." The machine's status is updated to "MAINTENANCE_REQUIRED" in the Machine DB.
</div>

<h4>Deep Dive: Components (Flow 3)</h4>

<h4>Machine Management Service</h4>
<ul>
    <li><strong>Protocol:</strong> HTTP/REST</li>
    <li><strong>Heartbeat Endpoint:</strong> <code>POST /api/v1/machines/{machine_id}/heartbeat</code></li>
    <li><strong>Input:</strong> <code>{ machine_id, timestamp, temperature, power_status, network_status, cash_level, slot_quantities: [...], error_codes: [...] }</code></li>
    <li><strong>Output:</strong> <code>{ ack: true, config_updates?: {...} }</code></li>
    <li>The heartbeat response can optionally include configuration updates (price changes, firmware update commands) ‚Äî this is a "pull-based config" model piggybacked on the heartbeat.</li>
    <li>Also runs a background cron job every 5 minutes to detect machines that haven't sent heartbeats beyond the expected interval (stale detection).</li>
</ul>

<h4>Message Queue</h4>
<p>Used for decoupling alert generation from alert delivery. The Machine Management Service publishes alert messages to the queue. The Notification Service consumes them asynchronously. This ensures that even if the Notification Service is temporarily down, alerts are not lost ‚Äî they remain in the queue until consumed.</p>
<ul>
    <li><strong>Why Message Queue over direct HTTP call?</strong> Decoupling, reliability (messages persist in the queue), and the ability to add multiple consumers later (e.g., an SMS service, an email service, a Slack bot) without modifying the producer.</li>
    <li><strong>Why not Pub/Sub?</strong> A message queue with competing consumers is sufficient here since each alert needs to be processed exactly once. Pub/Sub would be appropriate if multiple independent systems needed to process the same alert, but here the Notification Service is the single consumer that handles all delivery channels.</li>
    <li><strong>Message format:</strong> <code>{ alert_id, machine_id, alert_type, severity, message, timestamp, metadata }</code></li>
    <li><strong>Delivery guarantee:</strong> At-least-once. The Notification Service acknowledges messages after successful delivery. Failed messages are retried with exponential backoff and eventually moved to a dead-letter queue after max retries.</li>
</ul>

<h4>Analytics Service</h4>
<ul>
    <li><strong>Protocol:</strong> HTTP/REST</li>
    <li><strong>Endpoint:</strong> <code>GET /api/v1/analytics/revenue?machine_id=X&period=weekly</code></li>
    <li><strong>Output:</strong> <code>{ total_revenue, transactions_count, top_products: [...], revenue_by_day: [...] }</code></li>
    <li>Reads from the Transaction DB (NoSQL) and Telemetry DB (Time-Series) to compute aggregated metrics. Supports pre-computed rollups for dashboard performance.</li>
</ul>

<h4>WebSocket Connection (Dashboard Real-Time Updates)</h4>
<p>The Operator Dashboard maintains a persistent WebSocket connection to the Notification Service for real-time updates.</p>
<ul>
    <li><strong>Connection Establishment:</strong> When the operator opens the dashboard, the browser initiates an HTTP Upgrade request to <code>wss://api.vendingfleet.com/ws/notifications</code>. The API Gateway routes this to the Notification Service. Upon successful authentication (JWT token in the handshake), the connection is upgraded to WebSocket.</li>
    <li><strong>Connection Storage:</strong> The Notification Service maintains an in-memory map of <code>operator_id ‚Üí WebSocket connection</code>. If the Notification Service is horizontally scaled, a shared Pub/Sub channel (internal) is used so that any instance can publish to the correct operator's connection regardless of which instance holds it.</li>
    <li><strong>Message Format:</strong> JSON frames: <code>{ type: "ALERT", alert_id, machine_id, alert_type, severity, message, timestamp }</code></li>
    <li><strong>Why WebSocket over polling?</strong> Operators need real-time alerts (e.g., machine offline, temperature spike). Polling every few seconds would add unnecessary load and latency. WebSocket provides instant push delivery with a single persistent connection.</li>
    <li><strong>Why not Server-Sent Events (SSE)?</strong> SSE would also work here since the communication is server ‚Üí client only. WebSocket was chosen because the dashboard may also send commands back to the server (e.g., acknowledge an alert, trigger remote diagnostics), making bidirectional communication useful.</li>
    <li><strong>Heartbeat/Keepalive:</strong> The WebSocket connection sends ping/pong frames every 30 seconds to detect stale connections. If a pong is not received within 10 seconds, the connection is closed and the client auto-reconnects.</li>
</ul>

<hr>

<!-- ===================== COMBINED FLOW ===================== -->
<h3>Combined Overall System Diagram</h3>
<p>This diagram merges all three flows into a single unified architecture view.</p>

<div class="diagram-container">
    <pre class="mermaid">
flowchart TB
    subgraph Users["Users & Operators"]
        U["üë§ User"]
        OP["üîß Operator"]
        DASH["üìä Operator Dashboard\n(Web App)"]
        MOB["üì± Operator Mobile App"]
    end

    subgraph VendingMachine["Vending Machine (Embedded)"]
        DISP["Display Controller"]
        LIS["Local Inventory Store"]
        PAY_HW["Payment Hardware\n(Coin/Bill/Card/NFC)"]
        DISPENSER["Dispenser Mechanism"]
        LTXN["Local Transaction Log"]
        OPANEL["Operator Panel"]
    end

    subgraph Backend["Backend Services"]
        APIGW["API Gateway + Load Balancer"]
        PAY_SVC["Payment Service"]
        TXN_SVC["Transaction Service"]
        INV_SVC["Inventory Service"]
        MACHINE_SVC["Machine Management Service"]
        NOTIF_SVC["Notification Service"]
        ANALYTICS_SVC["Analytics Service"]
    end

    subgraph Messaging
        MQ[["Message Queue"]]
    end

    subgraph ExternalPayment["External"]
        PG["Payment Gateway"]
    end

    subgraph DataStores["Data Stores"]
        TXN_DB[("Transaction DB\n(NoSQL)")]
        INV_DB[("Inventory DB\n(SQL)")]
        MACHINE_DB[("Machine DB\n(SQL)")]
        TELEM_DB[("Telemetry DB\n(Time-Series)")]
        CACHE[("In-Memory Cache")]
        CDN["CDN\n(Static Assets +\nProduct Images)"]
    end

    %% User Purchase Flow
    U -- "Select product" --> DISP
    DISP -- "Check stock" --> LIS
    U -- "Insert payment" --> PAY_HW
    PAY_HW -- "Card/NFC auth" --> APIGW
    APIGW --> PAY_SVC
    PAY_SVC --> PG
    PAY_HW -- "Dispense" --> DISPENSER
    DISPENSER -- "Log" --> LTXN
    LTXN -- "Sync" --> APIGW

    %% Backend routing
    APIGW --> TXN_SVC
    APIGW --> INV_SVC
    TXN_SVC --> TXN_DB
    INV_SVC --> INV_DB
    INV_SVC --> CACHE

    %% Operator Restock Flow
    OP -- "Authenticate\n& restock" --> OPANEL
    OPANEL --> LIS
    OPANEL --> LTXN

    %% Fleet Monitoring Flow
    DISP -- "Heartbeat\n(60s)" --> APIGW
    APIGW --> MACHINE_SVC
    MACHINE_SVC --> TELEM_DB
    MACHINE_SVC --> MACHINE_DB
    MACHINE_SVC -- "Alert" --> MQ
    MQ --> NOTIF_SVC
    NOTIF_SVC -- "WebSocket" --> DASH
    NOTIF_SVC -- "Push notification" --> MOB

    %% Analytics
    MACHINE_SVC --> ANALYTICS_SVC
    ANALYTICS_SVC --> DASH

    %% Dashboard data
    DASH -- "REST API\ncalls" --> APIGW
    CDN -- "Static assets\n& images" --> DASH
    APIGW --> ANALYTICS_SVC
    CACHE -- "Read-through" --> INV_SVC
    </pre>
</div>

<h4>Combined Flow Examples</h4>

<div class="example-box">
    <strong>Example ‚Äî End-to-End Lifecycle:</strong><br>
    <strong>Morning (Monitoring):</strong> Operator Maria logs into the Operator Dashboard. Her browser opens a WebSocket connection to the Notification Service. The dashboard loads machine data via <code>GET /api/v1/machines?operator_id=maria</code> through the API Gateway ‚Üí Machine Management Service ‚Üí Machine DB + Cache. Static dashboard assets and product images load from the CDN.<br><br>
    <strong>Mid-day (Purchase):</strong> User Alice buys a $2.00 Sparkling Water from VM-4421 with cash. The machine handles it entirely locally ‚Äî validates coins, dispenses product, decrements local inventory, logs the transaction. The sync daemon POSTs the transaction to the API Gateway ‚Üí Transaction Service ‚Üí Transaction DB, and PATCHes inventory via Inventory Service ‚Üí Inventory DB. The cache for VM-4421's inventory is invalidated.<br><br>
    <strong>Afternoon (Alert):</strong> After 8 more sales from Slot B3, VM-4421's heartbeat reports Slot B3 quantity = 1 (below threshold = 2). The Machine Management Service publishes a <code>LOW_STOCK</code> alert to the Message Queue. The Notification Service consumes it, pushes a WebSocket message to Maria's dashboard (which shows a yellow badge on VM-4421) and sends a push notification to her phone.<br><br>
    <strong>Evening (Restock):</strong> Maria drives to VM-4421, scans her badge on the Operator Panel, enters restock mode, fills Slot B3 to 10 units, and confirms. The local inventory updates to 10. The restock event syncs to the backend ‚Äî Inventory Service updates the Inventory DB, and the Notification Service clears the low-stock alert. Maria's dashboard refreshes to show all-green status for VM-4421.<br><br>
    <strong>Night (Analytics):</strong> Maria opens the Analytics page on the dashboard. It calls <code>GET /api/v1/analytics/revenue?period=daily</code> through the API Gateway ‚Üí Analytics Service, which queries the Transaction DB for today's sales and returns: 47 transactions, $142.50 revenue, top product: Energy Drink (12 sold). The data is served from a pre-computed cache for sub-second response.
</div>

<hr>

<!-- ============================================================ -->
<h2>4. Database Schema</h2>
<!-- ============================================================ -->

<!-- ---- SQL: machines ---- -->
<h3><span class="tag tag-sql">SQL</span> Table: <code>machines</code></h3>
<table>
    <tr><th>Column</th><th>Type</th><th>Key</th><th>Description</th></tr>
    <tr><td>machine_id</td><td>UUID</td><td><span class="tag tag-pk">PK</span></td><td>Unique machine identifier</td></tr>
    <tr><td>location_address</td><td>VARCHAR(255)</td><td></td><td>Human-readable address</td></tr>
    <tr><td>latitude</td><td>DECIMAL(10,7)</td><td></td><td>GPS latitude</td></tr>
    <tr><td>longitude</td><td>DECIMAL(10,7)</td><td></td><td>GPS longitude</td></tr>
    <tr><td>status</td><td>ENUM</td><td></td><td>ACTIVE, INACTIVE, MAINTENANCE, OFFLINE</td></tr>
    <tr><td>operator_id</td><td>UUID</td><td><span class="tag tag-fk">FK ‚Üí operators</span></td><td>Assigned operator</td></tr>
    <tr><td>last_heartbeat_at</td><td>TIMESTAMP</td><td></td><td>Last heartbeat received</td></tr>
    <tr><td>installed_at</td><td>TIMESTAMP</td><td></td><td>When the machine was installed</td></tr>
    <tr><td>model</td><td>VARCHAR(100)</td><td></td><td>Machine model/type</td></tr>
    <tr><td>firmware_version</td><td>VARCHAR(50)</td><td></td><td>Current firmware version</td></tr>
</table>

<p><strong>Why SQL:</strong> The machines table has a well-defined relational schema, requires ACID transactions (status updates must be atomic), and has moderate cardinality (100K machines). Relational integrity is needed for the foreign key to operators. Read and write patterns are balanced (reads from dashboard, writes from heartbeats and status changes).</p>

<p><strong>Read events:</strong> Operator opens dashboard (load all machines for their region), Machine Management Service checks for stale heartbeats (background cron).</p>
<p><strong>Write events:</strong> Machine heartbeat updates <code>last_heartbeat_at</code>, status changes (OFFLINE detection, MAINTENANCE marking), new machine registration.</p>

<p><strong>Indexes:</strong></p>
<ul>
    <li><code>operator_id</code> ‚Äî <strong>B-tree index</strong> ‚Äî Operators frequently query machines assigned to them. B-tree supports efficient equality lookups and range scans on UUIDs.</li>
    <li><code>status</code> ‚Äî <strong>B-tree index</strong> ‚Äî The cron job for stale heartbeat detection queries <code>WHERE status = 'ACTIVE' AND last_heartbeat_at < threshold</code>. B-tree supports this range condition.</li>
    <li><code>(latitude, longitude)</code> ‚Äî <strong>R-tree (spatial) index</strong> ‚Äî Supports geo-queries like "find all machines within 5 miles" for the operator map view.</li>
</ul>

<p><strong>Sharding:</strong> Not required at 100K machines. A single SQL instance (with read replicas) handles this comfortably.</p>

<!-- ---- SQL: operators ---- -->
<h3><span class="tag tag-sql">SQL</span> Table: <code>operators</code></h3>
<table>
    <tr><th>Column</th><th>Type</th><th>Key</th><th>Description</th></tr>
    <tr><td>operator_id</td><td>UUID</td><td><span class="tag tag-pk">PK</span></td><td>Unique operator identifier</td></tr>
    <tr><td>name</td><td>VARCHAR(100)</td><td></td><td>Full name</td></tr>
    <tr><td>email</td><td>VARCHAR(100)</td><td></td><td>Email address</td></tr>
    <tr><td>phone</td><td>VARCHAR(20)</td><td></td><td>Phone number</td></tr>
    <tr><td>region</td><td>VARCHAR(50)</td><td></td><td>Assigned region</td></tr>
    <tr><td>created_at</td><td>TIMESTAMP</td><td></td><td>Account creation time</td></tr>
</table>

<p><strong>Why SQL:</strong> Small, relational dataset with well-defined schema. Needs referential integrity with the <code>machines</code> table. Very low cardinality (hundreds or low thousands of operators).</p>

<p><strong>Read events:</strong> Dashboard login, notification delivery (lookup operator contact info).</p>
<p><strong>Write events:</strong> New operator registration, profile updates (rare).</p>

<!-- ---- SQL: products ---- -->
<h3><span class="tag tag-sql">SQL</span> Table: <code>products</code></h3>
<table>
    <tr><th>Column</th><th>Type</th><th>Key</th><th>Description</th></tr>
    <tr><td>product_id</td><td>UUID</td><td><span class="tag tag-pk">PK</span></td><td>Unique product identifier</td></tr>
    <tr><td>name</td><td>VARCHAR(100)</td><td></td><td>Product name</td></tr>
    <tr><td>description</td><td>TEXT</td><td></td><td>Product description</td></tr>
    <tr><td>category</td><td>VARCHAR(50)</td><td></td><td>Product category (Beverages, Snacks, etc.)</td></tr>
    <tr><td>base_price</td><td>DECIMAL(6,2)</td><td></td><td>Default price (can be overridden per machine)</td></tr>
    <tr><td>image_url</td><td>VARCHAR(500)</td><td></td><td>URL to product image (served via CDN)</td></tr>
    <tr><td>is_active</td><td>BOOLEAN</td><td></td><td>Whether the product is currently offered</td></tr>
</table>

<p><strong>Why SQL:</strong> Well-defined relational schema, small cardinality (hundreds to low thousands of products), referenced by inventory and transactions. Needs relational integrity.</p>

<p><strong>Read events:</strong> Machine boots and downloads product catalog, dashboard displays product info.</p>
<p><strong>Write events:</strong> Admin adds/updates products (rare).</p>

<p><strong>Indexes:</strong></p>
<ul>
    <li><code>category</code> ‚Äî <strong>B-tree index</strong> ‚Äî Supports filtering products by category in the dashboard.</li>
</ul>

<!-- ---- SQL: machine_inventory ---- -->
<h3><span class="tag tag-sql">SQL</span> Table: <code>machine_inventory</code></h3>
<table>
    <tr><th>Column</th><th>Type</th><th>Key</th><th>Description</th></tr>
    <tr><td>machine_id</td><td>UUID</td><td><span class="tag tag-pk">PK (composite)</span> <span class="tag tag-fk">FK ‚Üí machines</span></td><td>Which machine</td></tr>
    <tr><td>slot_number</td><td>INT</td><td><span class="tag tag-pk">PK (composite)</span></td><td>Physical slot in the machine</td></tr>
    <tr><td>product_id</td><td>UUID</td><td><span class="tag tag-fk">FK ‚Üí products</span></td><td>Product stocked in this slot</td></tr>
    <tr><td>quantity</td><td>INT</td><td></td><td>Current quantity in the slot</td></tr>
    <tr><td>max_capacity</td><td>INT</td><td></td><td>Max items the slot can hold</td></tr>
    <tr><td>price</td><td>DECIMAL(6,2)</td><td></td><td>Per-machine price (overrides base_price)</td></tr>
    <tr><td>low_stock_threshold</td><td>INT</td><td></td><td>Alert when quantity falls below this</td></tr>
    <tr><td>last_restocked_at</td><td>TIMESTAMP</td><td></td><td>Last restock time</td></tr>
</table>

<p><strong>Why SQL:</strong> This is a relational dataset that references both <code>machines</code> and <code>products</code>. Requires ACID transactions for inventory updates (decrement must be atomic to avoid negative inventory). The composite primary key <code>(machine_id, slot_number)</code> naturally models the physical layout. Cardinality is moderate: 100K machines √ó ~20 slots = ~2M rows, well within SQL capacity.</p>

<p><strong>Read events:</strong> Machine heartbeat processing (compare quantities to thresholds), operator dashboard load (show inventory per machine), analytics queries.</p>
<p><strong>Write events:</strong> Product sale (decrement quantity by 1), restock (set quantity to new value), price update (remote pricing change).</p>

<p><strong>Indexes:</strong></p>
<ul>
    <li><code>machine_id</code> ‚Äî <strong>B-tree index</strong> ‚Äî All inventory queries for a specific machine use this as the leading key. Already covered by the composite PK but worth noting the access pattern.</li>
    <li><code>product_id</code> ‚Äî <strong>B-tree index</strong> ‚Äî Supports "which machines stock Product X?" queries for analytics and restocking route planning.</li>
    <li><code>quantity</code> ‚Äî <strong>B-tree index</strong> ‚Äî Supports efficient querying of low-stock slots: <code>WHERE quantity <= low_stock_threshold</code>.</li>
</ul>

<p><strong>Sharding:</strong> If the fleet grows beyond what a single SQL instance can handle (e.g., 1M+ machines = 20M+ rows with frequent writes), shard by <code>machine_id</code>. This ensures all slots for a given machine are co-located on the same shard, avoiding cross-shard queries for the primary access pattern (get all inventory for a machine). Consistent hashing on <code>machine_id</code> distributes load evenly.</p>

<!-- ---- NoSQL: transactions ---- -->
<h3><span class="tag tag-nosql">NoSQL (Wide-Column)</span> Table: <code>transactions</code></h3>
<table>
    <tr><th>Column</th><th>Type</th><th>Key</th><th>Description</th></tr>
    <tr><td>transaction_id</td><td>UUID</td><td><span class="tag tag-pk">PK (Partition Key)</span></td><td>Unique transaction identifier (generated on the machine)</td></tr>
    <tr><td>machine_id</td><td>UUID</td><td></td><td>Which machine processed the transaction</td></tr>
    <tr><td>product_id</td><td>UUID</td><td></td><td>Which product was purchased</td></tr>
    <tr><td>slot_number</td><td>INT</td><td></td><td>Which slot was dispensed from</td></tr>
    <tr><td>amount</td><td>DECIMAL(6,2)</td><td></td><td>Total amount charged</td></tr>
    <tr><td>payment_method</td><td>STRING</td><td></td><td>CASH, CARD, NFC</td></tr>
    <tr><td>payment_status</td><td>STRING</td><td></td><td>SUCCESS, FAILED, REFUNDED</td></tr>
    <tr><td>dispensed</td><td>BOOLEAN</td><td></td><td>Whether the product was physically dispensed</td></tr>
    <tr><td>created_at</td><td>TIMESTAMP</td><td></td><td>When the transaction occurred</td></tr>
    <tr><td>synced_at</td><td>TIMESTAMP</td><td></td><td>When it was synced to the backend (null if offline)</td></tr>
</table>

<p><strong>Why NoSQL (Wide-Column):</strong> Transactions are write-heavy (millions per day across the fleet), append-only (never updated after creation), and do not require joins. The access patterns are: (1) write a single transaction, (2) query transactions by <code>machine_id</code> + time range, (3) query all transactions for analytics. NoSQL provides high write throughput, horizontal scalability, and efficient time-range scans. No ACID requirements ‚Äî each transaction is independent.</p>

<p><strong>Read events:</strong> Operator views transaction history for a machine, Analytics Service aggregates daily/weekly revenue.</p>
<p><strong>Write events:</strong> Machine syncs a completed transaction to the backend (via POST to Transaction Service).</p>

<p><strong>Denormalization:</strong> The <code>machine_id</code> is stored directly in the transaction record rather than requiring a join to a separate table. This is intentional denormalization for read performance ‚Äî when querying "all transactions for machine X," the wide-column store can scan a single partition without any joins. Similarly, <code>product_id</code> and <code>amount</code> are stored per-transaction rather than looked up from the products table, because the price at time of sale may differ from the current price.</p>

<p><strong>Indexes / Access Patterns:</strong></p>
<ul>
    <li><strong>Primary access:</strong> Partition key = <code>transaction_id</code> for idempotent upserts during sync.</li>
    <li><strong>Secondary index on <code>(machine_id, created_at)</code></strong> ‚Äî Supports the most common query: "get all transactions for machine X between time T1 and T2." This is a composite index where <code>machine_id</code> is the partition key and <code>created_at</code> is the sort/clustering key, enabling efficient range scans within a machine's partition.</li>
    <li><strong>Secondary index on <code>(product_id, created_at)</code></strong> ‚Äî Supports analytics queries: "how many units of Product X sold this week?"</li>
</ul>

<p><strong>Sharding:</strong> Shard by <code>machine_id</code>. This co-locates all transactions for a given machine on the same shard, making per-machine queries efficient. With 100K machines, traffic is naturally distributed across shards. If a single machine generates disproportionate traffic (hot partition), the wide-column store's automatic splitting handles this.</p>

<!-- ---- Time-Series: machine_telemetry ---- -->
<h3><span class="tag tag-ts">Time-Series</span> Table: <code>machine_telemetry</code></h3>
<table>
    <tr><th>Column</th><th>Type</th><th>Key</th><th>Description</th></tr>
    <tr><td>machine_id</td><td>UUID</td><td><span class="tag tag-pk">Partition Key</span></td><td>Which machine</td></tr>
    <tr><td>timestamp</td><td>TIMESTAMP</td><td><span class="tag tag-pk">Sort Key</span></td><td>When the measurement was taken</td></tr>
    <tr><td>temperature</td><td>FLOAT</td><td></td><td>Internal temperature (¬∞C)</td></tr>
    <tr><td>power_status</td><td>STRING</td><td></td><td>OK, LOW_BATTERY, POWER_OUTAGE</td></tr>
    <tr><td>network_status</td><td>STRING</td><td></td><td>CONNECTED, DEGRADED, OFFLINE</td></tr>
    <tr><td>cash_level</td><td>DECIMAL(8,2)</td><td></td><td>Total cash in the machine</td></tr>
    <tr><td>coin_inventory</td><td>JSON</td><td></td><td>Coin counts by denomination for change-making</td></tr>
    <tr><td>error_codes</td><td>LIST&lt;STRING&gt;</td><td></td><td>Active error codes</td></tr>
</table>

<p><strong>Why Time-Series:</strong> Telemetry data is time-stamped, append-only, and queried by time ranges. Time-series databases are optimized for: high write throughput for continuous ingestion, efficient time-range scans, automatic downsampling/retention policies (e.g., keep 1-minute granularity for 7 days, 1-hour rollups for 1 year), and compression of time-series data. This is a textbook time-series workload.</p>

<p><strong>Read events:</strong> Operator views machine health history on dashboard, Machine Management Service checks for anomalies, Analytics Service computes uptime metrics.</p>
<p><strong>Write events:</strong> Every machine heartbeat (60-second intervals). At 100K machines, that's ~100K writes per minute = ~1,667 writes/second sustained.</p>

<p><strong>Retention Policy:</strong> Raw data retained for 30 days. Hourly rollups retained for 1 year. Daily rollups retained indefinitely.</p>

<hr>

<!-- ============================================================ -->
<h2>5. CDN &amp; In-Memory Cache</h2>
<!-- ============================================================ -->

<h3>CDN</h3>
<p><strong>Verdict: Appropriate and recommended.</strong></p>

<p>A CDN is used for two purposes:</p>
<ol>
    <li><strong>Product Images:</strong> Product images are uploaded to object storage and served via CDN. Machines download product images on boot or catalog refresh. The dashboard also loads product images from the CDN. Images rarely change, making them ideal CDN candidates with high cache-hit ratios.</li>
    <li><strong>Operator Dashboard Static Assets:</strong> The web dashboard's JavaScript, CSS, fonts, and icons are served via CDN for fast global loading. These assets are versioned and cache-busted on deploy.</li>
</ol>

<p><strong>Why CDN is appropriate:</strong> Product images and dashboard assets are static, read-heavy, and accessed globally (machines and operators are geographically distributed). CDN edge caching reduces latency and offloads traffic from origin servers.</p>

<h3>In-Memory Cache</h3>
<p><strong>Verdict: Appropriate for specific access patterns.</strong></p>

<h4>What is cached:</h4>
<table>
    <tr><th>Cache Key</th><th>Cached Data</th><th>Justification</th></tr>
    <tr>
        <td><code>inventory:{machine_id}</code></td>
        <td>Full inventory snapshot for a machine (all slots)</td>
        <td>The Operator Dashboard's machine list page queries inventory for many machines. Caching avoids repeated SQL reads. Invalidated on inventory updates.</td>
    </tr>
    <tr>
        <td><code>products:catalog</code></td>
        <td>Full product catalog</td>
        <td>Read-heavy, write-rare. Every machine boot and dashboard page load reads the catalog. Caching avoids database reads for a rarely-changing dataset.</td>
    </tr>
    <tr>
        <td><code>machine:{machine_id}:status</code></td>
        <td>Machine status and last heartbeat</td>
        <td>Dashboard map view queries status for all machines. Caching reduces load on the Machine DB.</td>
    </tr>
    <tr>
        <td><code>analytics:{period}:{scope}</code></td>
        <td>Pre-computed analytics rollups</td>
        <td>Analytics queries are expensive aggregations. Pre-computed results are cached and refreshed periodically.</td>
    </tr>
</table>

<h4>Caching Strategy: Write-Through (for Inventory)</h4>
<p>When the Inventory Service receives a PATCH to update inventory, it writes to the SQL database AND updates the cache simultaneously. This ensures the cache is always consistent with the database.</p>
<ul>
    <li><strong>Why write-through?</strong> Inventory data is critical for the dashboard. Stale cache data would show incorrect inventory levels to operators. Write-through ensures consistency. The write overhead is acceptable because inventory updates are moderate frequency (~1 write per sale per machine).</li>
    <li><strong>Why not write-behind (write-back)?</strong> Write-behind risks data loss if the cache crashes before flushing to the database. For inventory data, durability is important.</li>
    <li><strong>Why not cache-aside (lazy loading)?</strong> Cache-aside would work but introduces a window where the cache is stale after an inventory update until the next cache miss. For a real-time dashboard, this staleness is undesirable.</li>
</ul>

<h4>Caching Strategy: Cache-Aside (for Product Catalog &amp; Analytics)</h4>
<p>For the product catalog and analytics rollups, cache-aside is used. On a cache miss, the service reads from the database, populates the cache, and returns the result. On product updates (rare), the cache key is explicitly invalidated.</p>
<ul>
    <li><strong>Why cache-aside here?</strong> Products and analytics rollups are read-heavy and write-rare. Cache-aside is simpler and avoids writing to the cache on every read that's already cached. The brief staleness window on product updates is acceptable (products change infrequently).</li>
</ul>

<h4>Eviction Policy: LRU (Least Recently Used)</h4>
<p>When cache memory is full, the least recently accessed entries are evicted first.</p>
<ul>
    <li><strong>Why LRU?</strong> Active machines (which are most relevant) are accessed frequently and will stay in cache. Inactive or decommissioned machines' data will naturally be evicted since no one queries them. LRU is a good fit when access patterns have temporal locality, which is the case here ‚Äî operators tend to monitor the same set of machines repeatedly.</li>
</ul>

<h4>Expiration Policy (TTL)</h4>
<table>
    <tr><th>Cache Key Pattern</th><th>TTL</th><th>Reason</th></tr>
    <tr><td><code>inventory:{machine_id}</code></td><td>5 minutes</td><td>Even with write-through, a TTL ensures eventual consistency if a write-through fails silently. Short TTL balances freshness with cache hit rate.</td></tr>
    <tr><td><code>products:catalog</code></td><td>1 hour</td><td>Products rarely change. 1-hour TTL is sufficient with explicit invalidation on update.</td></tr>
    <tr><td><code>machine:{machine_id}:status</code></td><td>2 minutes</td><td>Status changes with every heartbeat (60s). 2-minute TTL ensures the dashboard never shows data more than 2 minutes stale.</td></tr>
    <tr><td><code>analytics:{period}:{scope}</code></td><td>15 minutes</td><td>Analytics don't need real-time accuracy. 15-minute TTL reduces expensive aggregation queries while keeping data reasonably fresh.</td></tr>
</table>

<hr>

<!-- ============================================================ -->
<h2>6. Scaling Considerations</h2>
<!-- ============================================================ -->

<h3>Load Balancers</h3>
<p>Load balancers are critical for scaling the backend services. They are placed at the following points:</p>

<h4>Load Balancer 1: API Gateway (External-Facing)</h4>
<ul>
    <li><strong>Position:</strong> Between vending machines/operator dashboard and the API Gateway cluster.</li>
    <li><strong>Type:</strong> Layer 7 (HTTP/HTTPS) load balancer with TLS termination.</li>
    <li><strong>Algorithm:</strong> Least connections ‚Äî distributes traffic to the API Gateway instance with the fewest active connections. This is preferred over round-robin because heartbeat requests (long-lived processing) and quick purchase requests have different durations.</li>
    <li><strong>Health checks:</strong> HTTP GET to <code>/health</code> on each API Gateway instance every 10 seconds. Unhealthy instances are removed from the rotation.</li>
    <li><strong>Sticky sessions:</strong> Not needed ‚Äî all backend services are stateless.</li>
    <li><strong>SSL termination:</strong> TLS is terminated at the load balancer. Internal traffic between API Gateway and backend services uses unencrypted HTTP (within a private VPC) for lower latency.</li>
</ul>

<h4>Load Balancer 2: Internal Service Load Balancer</h4>
<ul>
    <li><strong>Position:</strong> Between the API Gateway and the individual backend services (Payment, Transaction, Inventory, Machine Management, Notification, Analytics).</li>
    <li><strong>Type:</strong> Layer 4 (TCP) load balancer for efficiency ‚Äî no need to inspect HTTP headers for internal routing since the API Gateway already handles routing logic.</li>
    <li><strong>Algorithm:</strong> Round-robin ‚Äî internal services have homogeneous instances and similar request processing times.</li>
    <li><strong>Purpose:</strong> Allows each backend service to be horizontally scaled independently. For example, during peak hours, the Payment Service might scale to 10 instances while the Analytics Service stays at 3.</li>
</ul>

<h3>Horizontal Scaling by Service</h3>
<table>
    <tr><th>Service</th><th>Scaling Trigger</th><th>Strategy</th></tr>
    <tr><td>API Gateway</td><td>Requests per second &gt; threshold</td><td>Auto-scale behind external LB. Stateless ‚Äî any instance handles any request.</td></tr>
    <tr><td>Payment Service</td><td>Payment requests per second</td><td>Auto-scale. Each instance is stateless with idempotency keys preventing duplicate charges.</td></tr>
    <tr><td>Transaction Service</td><td>Write throughput to Transaction DB</td><td>Auto-scale. Writes are independent and parallelizable.</td></tr>
    <tr><td>Inventory Service</td><td>Dashboard read load + sale write load</td><td>Auto-scale. Cache absorbs read spikes.</td></tr>
    <tr><td>Machine Management Service</td><td>Heartbeat volume (= number of machines)</td><td>Auto-scale. Each heartbeat is independent.</td></tr>
    <tr><td>Notification Service</td><td>Number of active WebSocket connections</td><td>Scale based on connection count. Each instance holds a subset of WebSocket connections.</td></tr>
    <tr><td>Analytics Service</td><td>Query complexity and dashboard traffic</td><td>Scale for read-heavy workloads. Pre-compute rollups to reduce per-request computation.</td></tr>
</table>

<h3>Database Scaling</h3>
<ul>
    <li><strong>SQL (Machines, Inventory, Products, Operators):</strong> Start with a single primary + read replicas. If write throughput exceeds a single primary's capacity, shard <code>machine_inventory</code> by <code>machine_id</code>. The other SQL tables (machines, products, operators) are small enough to remain unsharded.</li>
    <li><strong>NoSQL (Transactions):</strong> Already designed for horizontal scaling. Partition by <code>machine_id</code>. Add nodes to the cluster as transaction volume grows.</li>
    <li><strong>Time-Series (Telemetry):</strong> Inherently scalable via time-based partitioning. Add nodes as the fleet grows. Retention policies keep the dataset manageable.</li>
</ul>

<h3>Estimated Traffic at Scale (100K machines)</h3>
<table>
    <tr><th>Event</th><th>Frequency</th><th>Requests/Second</th></tr>
    <tr><td>Heartbeats</td><td>100K machines √ó 1/minute</td><td>~1,667 r/s</td></tr>
    <tr><td>Purchases</td><td>~10 sales/machine/day</td><td>~116 r/s (avg), ~580 r/s (5√ó peak)</td></tr>
    <tr><td>Dashboard reads</td><td>~1,000 operators √ó 1 req/10s</td><td>~100 r/s</td></tr>
    <tr><td>Total backend</td><td></td><td>~2,000 r/s average, ~3,000 r/s peak</td></tr>
</table>
<p>This is a moderate load comfortably handled by a handful of service instances behind load balancers.</p>

<hr>

<!-- ============================================================ -->
<h2>7. Tradeoffs &amp; Deep Dives</h2>
<!-- ============================================================ -->

<h3>Tradeoff 1: Offline-First vs. Online-Only</h3>
<p><strong>Chosen: Offline-First.</strong> Machines can complete cash transactions without connectivity and sync later.</p>
<ul>
    <li><strong>Pro:</strong> Higher availability ‚Äî machines keep generating revenue even during backend outages or network issues. Essential for machines in locations with poor connectivity (parking garages, rural areas).</li>
    <li><strong>Con:</strong> Backend inventory is eventually consistent with the physical machine. There's a window where the dashboard shows stale inventory. More complex sync logic with conflict resolution.</li>
    <li><strong>Mitigation:</strong> The machine is the source of truth for its own inventory. The backend is a mirror. When syncing, the machine's data always wins. Sync is idempotent (using transaction_id as the idempotency key).</li>
</ul>

<h3>Tradeoff 2: Local Inventory as Source of Truth vs. Backend as Source of Truth</h3>
<p><strong>Chosen: Local machine is the source of truth.</strong></p>
<ul>
    <li><strong>Pro:</strong> The machine physically knows what's in its slots. No network latency for availability checks. Works offline.</li>
    <li><strong>Con:</strong> If the machine's local store corrupts (e.g., power outage during write), inventory data may be lost. Physical discrepancies (e.g., theft, mechanical miscounts) are not detectable without manual audit.</li>
    <li><strong>Mitigation:</strong> WAL (write-ahead logging) on the local store for crash recovery. Operators perform periodic physical audits and can manually correct inventory via the operator panel.</li>
</ul>

<h3>Tradeoff 3: Pull-Based Config (Piggybacking on Heartbeat) vs. Push-Based Config</h3>
<p><strong>Chosen: Pull-based via heartbeat response.</strong></p>
<ul>
    <li><strong>Pro:</strong> No need for inbound connections to the machine (machines are behind NAT/firewalls). Simpler networking. Config updates are delivered within 60 seconds (next heartbeat). No need for WebSocket or persistent connection on the machine side.</li>
    <li><strong>Con:</strong> Up to 60-second delay for config propagation. Cannot push urgent config changes instantly.</li>
    <li><strong>Why not push (e.g., WebSocket on machine)?</strong> Vending machines are resource-constrained embedded devices. Maintaining a persistent WebSocket connection consumes resources and complicates the machine firmware. The 60-second heartbeat interval is sufficient for non-urgent updates (price changes, firmware updates). Truly urgent actions (e.g., remote shutdown) can be achieved by setting a flag in the heartbeat response checked on the next heartbeat.</li>
</ul>

<h3>Tradeoff 4: Synchronous vs. Asynchronous Transaction Syncing</h3>
<p><strong>Chosen: Asynchronous syncing via background daemon.</strong></p>
<ul>
    <li><strong>Pro:</strong> The user's purchase experience is not blocked by backend communication. The machine can dispense immediately after local payment validation. Backend unavailability doesn't affect the purchase flow.</li>
    <li><strong>Con:</strong> Transactions may be delayed in appearing on the dashboard. If the machine is destroyed before syncing, unsynced transactions are lost.</li>
    <li><strong>Mitigation:</strong> Transactions are synced as soon as connectivity is available (not batched by time). The local transaction log uses durable storage (flash) that survives power cycles.</li>
</ul>

<h3>Deep Dive: Idempotent Sync</h3>
<p>When the machine's sync daemon sends transactions to the backend, network errors may cause retries. To prevent duplicate records, the Transaction Service uses <code>transaction_id</code> (generated on the machine at purchase time using UUID) as an idempotency key. If a POST is received with a <code>transaction_id</code> that already exists in the Transaction DB, the request is silently acknowledged without creating a duplicate. This "upsert" behavior is critical for at-least-once delivery semantics to be safe.</p>

<h3>Deep Dive: Mechanical Failure Handling</h3>
<p>If the infrared drop sensor does not detect the product within 5 seconds of the dispenser motor activating (jam detected), the machine:</p>
<ol>
    <li>Stops the motor to prevent damage.</li>
    <li>Refunds the user (cash returned to tray, card payment voided via Payment Service).</li>
    <li>Marks the slot as "JAMMED" in the Local Inventory Store (disables further sales from that slot).</li>
    <li>Logs a mechanical error in the Local Transaction Log with error code "DISPENSE_JAM."</li>
    <li>Publishes the error in the next heartbeat, triggering a maintenance alert to the operator.</li>
    <li>Displays "This item is temporarily unavailable. Please select another." to the user.</li>
</ol>

<hr>

<!-- ============================================================ -->
<h2>8. Alternative Approaches</h2>
<!-- ============================================================ -->

<h3>Alternative 1: Fully Online Architecture (No Local State)</h3>
<p><strong>Description:</strong> All logic lives in the backend. The machine is a thin client that sends every user action to the backend and waits for instructions.</p>
<p><strong>Why rejected:</strong> Network dependency makes the machine useless during outages. Many vending machines are in locations with unreliable connectivity (basements, transit stations, rural areas). Cash payments should always work. The latency of a round-trip to the backend for every interaction would degrade the user experience.</p>

<h3>Alternative 2: No Backend at All (Fully Standalone Machines)</h3>
<p><strong>Description:</strong> Each machine operates entirely independently with no central system.</p>
<p><strong>Why rejected:</strong> Operators have no visibility into inventory levels, leading to inefficient restocking routes (wasted trips to full machines, missed empty machines). No transaction reporting or analytics. No remote monitoring for failures. No ability to update prices without physically visiting each machine. This doesn't scale for fleet management.</p>

<h3>Alternative 3: gRPC Instead of HTTP/REST for Machine-to-Backend Communication</h3>
<p><strong>Description:</strong> Use gRPC with Protocol Buffers for the machine's communication with the backend.</p>
<p><strong>Why rejected:</strong> gRPC offers better performance (binary serialization, HTTP/2 multiplexing), but adds complexity to the embedded firmware. HTTP/REST is simpler to implement and debug on resource-constrained devices. The traffic volume per machine (~1 heartbeat/minute + a few transactions/day) doesn't justify the complexity. HTTP/REST also makes it easier to debug and inspect traffic with standard tools (curl, browser, Postman).</p>

<h3>Alternative 4: Pub/Sub Instead of Message Queue for Alerts</h3>
<p><strong>Description:</strong> Use a Pub/Sub system where the Machine Management Service publishes alert events and multiple subscribers consume them.</p>
<p><strong>Why rejected for the current scale:</strong> We have a single consumer (Notification Service) that handles all delivery channels (WebSocket, push, SMS, email). A message queue with a single consumer group is simpler. However, if the system evolves to have multiple independent consumers (e.g., a billing system, an insurance system, a third-party analytics system), migrating to Pub/Sub would be a good evolution. The current design is easy to evolve since the Message Queue interface is abstracted behind the Machine Management Service's publish call.</p>

<h3>Alternative 5: Polling Instead of WebSocket for the Dashboard</h3>
<p><strong>Description:</strong> The Operator Dashboard polls the backend every N seconds for updates instead of maintaining a WebSocket connection.</p>
<p><strong>Why rejected:</strong> Polling introduces latency (up to N seconds) for critical alerts. With 1,000 operators polling every 5 seconds, that's 200 requests/second just for polling ‚Äî much of which returns no new data ("empty polls"). WebSocket pushes updates instantly with zero empty requests. For a monitoring dashboard where real-time awareness is critical (machine offline, temperature spike), WebSocket is the right choice.</p>

<h3>Alternative 6: Event Sourcing for Transactions</h3>
<p><strong>Description:</strong> Instead of storing the current state (quantity, status), store a sequence of immutable events (ITEM_DISPENSED, SLOT_RESTOCKED, etc.) and derive current state by replaying events.</p>
<p><strong>Why rejected:</strong> Event sourcing is powerful but adds significant complexity (event store, projection builders, replay logic). For vending machines, the domain is simple enough that direct state storage works well. The Local Transaction Log already provides an append-only audit trail, which gives most of the benefits of event sourcing without the full complexity. If audit and compliance requirements grow, this could be reconsidered.</p>

<hr>

<!-- ============================================================ -->
<h2>9. Additional Considerations</h2>
<!-- ============================================================ -->

<h3>Security</h3>
<ul>
    <li><strong>Machine Authentication:</strong> Each machine is provisioned with a unique client certificate (mTLS) for backend communication. API keys alone are insufficient ‚Äî if stolen, they can be used to impersonate a machine. Client certificates are bound to the machine's hardware security module (HSM).</li>
    <li><strong>Payment Security:</strong> Card data never touches the machine's application layer. The card reader module communicates directly with the Payment Gateway using point-to-point encryption (P2PE). The machine only receives an opaque payment token. This minimizes PCI-DSS scope.</li>
    <li><strong>Operator Authentication:</strong> Multi-factor: badge scan (something you have) + PIN (something you know) for physical machine access. SSO with MFA for dashboard access.</li>
    <li><strong>Network Security:</strong> All machine-to-backend traffic over TLS 1.3. Backend services communicate within a private VPC. API Gateway applies rate limiting (100 requests/minute per machine) to prevent abuse.</li>
</ul>

<h3>Firmware Updates</h3>
<p>Firmware updates are delivered via the heartbeat response. The Machine Management Service includes an <code>update_available</code> field with a download URL (pointing to object storage via CDN). The machine downloads the update in the background, verifies the checksum, and applies it during the next idle period (no active transaction). A rollback mechanism is in place: if the new firmware fails to boot twice, the machine reverts to the previous version.</p>

<h3>Restocking Route Optimization</h3>
<p>The Analytics Service can compute optimal restocking routes by combining inventory data (which machines need restocking) with the R-tree spatial index on the machines table (geographic proximity). This helps operators minimize driving time and restock the most critical machines first.</p>

<h3>Disaster Recovery</h3>
<ul>
    <li><strong>Machine-level:</strong> Local storage uses WAL for crash recovery. If a machine's SBC fails entirely, it can be re-provisioned with a fresh image and sync its inventory from the last known backend state (with manual verification).</li>
    <li><strong>Backend-level:</strong> SQL databases use synchronous replication to a standby with automatic failover. NoSQL and Time-Series databases are inherently distributed with built-in replication. The Message Queue is configured with replication factor ‚â• 3.</li>
</ul>

<h3>Multi-Currency / Multi-Region</h3>
<p>The schema stores <code>price</code> as a decimal with an implicit currency determined by the machine's locale. For multi-region deployments, a <code>currency</code> column can be added to the <code>machine_inventory</code> table. Backend services are deployed per-region with a global control plane for fleet-wide operations.</p>

<hr>

<!-- ============================================================ -->
<h2>10. Vendor Considerations</h2>
<!-- ============================================================ -->
<p><em>The design is vendor-agnostic. Below are potential vendor choices for each infrastructure component.</em></p>

<table>
    <tr><th>Component</th><th>Potential Vendors</th><th>Rationale</th></tr>
    <tr>
        <td>SQL Database</td>
        <td>PostgreSQL, MySQL, CockroachDB</td>
        <td>PostgreSQL is feature-rich with excellent spatial index support (PostGIS) for machine location queries. CockroachDB if multi-region SQL is needed. MySQL for simpler deployments.</td>
    </tr>
    <tr>
        <td>NoSQL (Wide-Column)</td>
        <td>Apache Cassandra, ScyllaDB, Amazon DynamoDB, Google Bigtable</td>
        <td>Cassandra/ScyllaDB for high write throughput and tunable consistency. DynamoDB for fully managed with predictable pricing. Bigtable for integration with analytics tooling.</td>
    </tr>
    <tr>
        <td>Time-Series Database</td>
        <td>InfluxDB, TimescaleDB, Amazon Timestream</td>
        <td>InfluxDB is purpose-built for telemetry with excellent retention policies. TimescaleDB if you want to stay in the PostgreSQL ecosystem. Timestream for fully managed.</td>
    </tr>
    <tr>
        <td>In-Memory Cache</td>
        <td>Redis, Memcached, Hazelcast</td>
        <td>Redis for rich data structures (sorted sets for leaderboards, pub/sub for cache invalidation). Memcached if only simple key-value caching is needed (lower overhead).</td>
    </tr>
    <tr>
        <td>Message Queue</td>
        <td>RabbitMQ, Amazon SQS, Apache ActiveMQ</td>
        <td>RabbitMQ for flexible routing and acknowledgment semantics. SQS for fully managed with built-in dead-letter queues. ActiveMQ for JMS compatibility.</td>
    </tr>
    <tr>
        <td>CDN</td>
        <td>Cloudflare, Amazon CloudFront, Fastly, Akamai</td>
        <td>Cloudflare for global edge network with DDoS protection. CloudFront for tight AWS integration. Fastly for instant purge capabilities (useful for firmware update delivery).</td>
    </tr>
    <tr>
        <td>Object Storage</td>
        <td>Amazon S3, Google Cloud Storage, Azure Blob Storage, MinIO</td>
        <td>S3 is the industry standard for object storage. MinIO for on-premise/self-hosted. Any S3-compatible store works due to the S3 API being a de facto standard.</td>
    </tr>
    <tr>
        <td>Payment Gateway</td>
        <td>Stripe, Square, Adyen, Worldpay</td>
        <td>Stripe for developer-friendly APIs and broad payment method support. Square for integration with physical POS hardware. Adyen for enterprise-scale global payments.</td>
    </tr>
    <tr>
        <td>Load Balancer</td>
        <td>NGINX, HAProxy, AWS ALB/NLB, Envoy</td>
        <td>NGINX/HAProxy for self-hosted with fine-grained control. AWS ALB for managed L7 with auto-scaling integration. Envoy for service mesh architectures.</td>
    </tr>
    <tr>
        <td>Embedded Database (On-Machine)</td>
        <td>SQLite, LevelDB, BerkeleyDB</td>
        <td>SQLite for a lightweight, file-backed, ACID-compliant database on the machine's SBC. Zero configuration, zero administration, and extremely reliable for embedded use.</td>
    </tr>
</table>

<hr>

<p style="text-align: center; color: var(--muted); margin-top: 3rem;">‚Äî End of System Design Document ‚Äî</p>

<script>
    mermaid.initialize({
        startOnLoad: true,
        theme: 'default',
        flowchart: {
            useMaxWidth: true,
            htmlLabels: true,
            curve: 'basis'
        }
    });
</script>

</body>
</html>
