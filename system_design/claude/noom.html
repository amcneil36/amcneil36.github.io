<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>System Design: Noom</title>
    <script src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script>
    <style>
        :root {
            --bg: #fdfdfd;
            --text: #1a1a2e;
            --heading: #16213e;
            --accent: #0f3460;
            --border: #ddd;
            --code-bg: #f4f4f8;
            --table-header: #0f3460;
            --table-header-text: #fff;
            --table-stripe: #f8f9fc;
            --callout-bg: #eef2ff;
            --callout-border: #4361ee;
            --warn-bg: #fff8e1;
            --warn-border: #ffa000;
            --green: #2d6a4f;
            --yellow: #e09f3e;
            --red: #c1121f;
        }
        * { box-sizing: border-box; margin: 0; padding: 0; }
        body {
            font-family: 'Segoe UI', system-ui, -apple-system, sans-serif;
            line-height: 1.7;
            color: var(--text);
            background: var(--bg);
            max-width: 1100px;
            margin: 0 auto;
            padding: 2rem 2.5rem 4rem;
        }
        h1 { font-size: 2.2rem; color: var(--heading); border-bottom: 3px solid var(--accent); padding-bottom: .5rem; margin-bottom: 1.5rem; }
        h2 { font-size: 1.6rem; color: var(--accent); margin-top: 2.5rem; margin-bottom: 1rem; border-left: 4px solid var(--accent); padding-left: .75rem; }
        h3 { font-size: 1.25rem; color: var(--heading); margin-top: 1.8rem; margin-bottom: .6rem; }
        h4 { font-size: 1.05rem; color: var(--accent); margin-top: 1.3rem; margin-bottom: .4rem; }
        p, li { margin-bottom: .55rem; }
        ul, ol { padding-left: 1.6rem; margin-bottom: 1rem; }
        code {
            background: var(--code-bg);
            padding: 2px 6px;
            border-radius: 4px;
            font-size: .92em;
            font-family: 'SF Mono', 'Fira Code', 'Consolas', monospace;
        }
        pre { background: var(--code-bg); padding: 1rem; border-radius: 6px; overflow-x: auto; margin-bottom: 1rem; }
        pre code { background: none; padding: 0; }
        table { width: 100%; border-collapse: collapse; margin-bottom: 1.5rem; font-size: .93rem; }
        th { background: var(--table-header); color: var(--table-header-text); padding: 10px 14px; text-align: left; }
        td { padding: 9px 14px; border-bottom: 1px solid var(--border); }
        tr:nth-child(even) { background: var(--table-stripe); }
        .mermaid { margin: 1.5rem 0; text-align: center; }
        .callout {
            background: var(--callout-bg);
            border-left: 4px solid var(--callout-border);
            padding: 1rem 1.2rem;
            border-radius: 0 6px 6px 0;
            margin: 1.2rem 0;
        }
        .warn {
            background: var(--warn-bg);
            border-left: 4px solid var(--warn-border);
            padding: 1rem 1.2rem;
            border-radius: 0 6px 6px 0;
            margin: 1.2rem 0;
        }
        .example {
            background: #f0fdf4;
            border-left: 4px solid var(--green);
            padding: 1rem 1.2rem;
            border-radius: 0 6px 6px 0;
            margin: 1rem 0;
        }
        .badge {
            display: inline-block;
            padding: 2px 10px;
            border-radius: 12px;
            font-size: .82rem;
            font-weight: 600;
            margin-right: 4px;
        }
        .badge-green  { background: #d8f3dc; color: var(--green); }
        .badge-yellow { background: #fff3cd; color: #7c5e10; }
        .badge-red    { background: #fce4ec; color: var(--red); }
        .badge-blue   { background: #dbeafe; color: #1e40af; }
        .toc { background: var(--code-bg); padding: 1.5rem 2rem; border-radius: 8px; margin-bottom: 2rem; }
        .toc a { text-decoration: none; color: var(--accent); }
        .toc a:hover { text-decoration: underline; }
        .toc ol { margin-bottom: 0; }
        .toc li { margin-bottom: .3rem; }
        hr { border: none; border-top: 1px solid var(--border); margin: 2rem 0; }
        .grid-2 { display: grid; grid-template-columns: 1fr 1fr; gap: 1rem; }
        @media (max-width: 768px) { .grid-2 { grid-template-columns: 1fr; } body { padding: 1rem; } }
    </style>
</head>
<body>

<h1>System Design: Noom</h1>
<p><strong>A psychology-based health and weight management platform</strong> combining food logging with a color-coded system, personalized 1-on-1 coaching, group support, daily educational curriculum, weight tracking, and activity tracking to help users build sustainable healthy habits.</p>

<nav class="toc">
<strong>Table of Contents</strong>
<ol>
    <li><a href="#fr">Functional Requirements</a></li>
    <li><a href="#nfr">Non-Functional Requirements</a></li>
    <li><a href="#flow1">Flow 1 â€” Food Logging &amp; Calorie Tracking</a></li>
    <li><a href="#flow2">Flow 2 â€” Coaching Messaging</a></li>
    <li><a href="#flow3">Flow 3 â€” Curriculum &amp; Daily Lessons</a></li>
    <li><a href="#flow4">Flow 4 â€” Weight &amp; Progress Tracking</a></li>
    <li><a href="#combined">Combined Overall Diagram</a></li>
    <li><a href="#schema">Database Schema</a></li>
    <li><a href="#cache">CDN &amp; Caching Deep Dive</a></li>
    <li><a href="#ws">WebSocket Deep Dive</a></li>
    <li><a href="#mq">Message Queue Deep Dive</a></li>
    <li><a href="#lb">Load Balancer Deep Dive</a></li>
    <li><a href="#scaling">Scaling Considerations</a></li>
    <li><a href="#tradeoffs">Tradeoffs &amp; Deep Dives</a></li>
    <li><a href="#alternatives">Alternative Approaches</a></li>
    <li><a href="#additional">Additional Considerations</a></li>
    <li><a href="#vendors">Vendor Recommendations</a></li>
</ol>
</nav>

<!-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• -->
<h2 id="fr">1. Functional Requirements</h2>

<ol>
    <li><strong>User Onboarding &amp; Goal Setting</strong> â€” Users create an account, complete a health-assessment quiz (current weight, goal weight, height, age, activity level, motivation), and receive a personalized daily calorie budget and program plan.</li>
    <li><strong>Food Logging &amp; Calorie Tracking</strong> â€” Users search a comprehensive food database, log meals (breakfast, lunch, dinner, snack), and see real-time calorie/macro consumption. Each food is color-coded:
        <span class="badge badge-green">Green</span> low calorie-density (fruits, vegetables, whole grains) â€”
        <span class="badge badge-yellow">Yellow</span> moderate calorie-density (lean meats, legumes) â€”
        <span class="badge badge-red">Red</span> high calorie-density (sweets, oils, nuts).
    </li>
    <li><strong>Weight Logging &amp; Progress Tracking</strong> â€” Users log daily weigh-ins. The system displays progress graphs over time against their goal weight.</li>
    <li><strong>Daily Curriculum / Lessons</strong> â€” Deliver sequenced, psychology-based educational content (articles, quizzes, interactive exercises) on a per-day schedule aligned with the user's program week.</li>
    <li><strong>1-on-1 Coaching</strong> â€” Each premium user is assigned a personal health coach. Users and coaches exchange messages in real time. Coaches can view user logs and progress.</li>
    <li><strong>Group Support</strong> â€” Users join a small peer-support group facilitated by a group coach. Members can post messages, reactions, and encouragement.</li>
    <li><strong>Activity / Exercise Tracking</strong> â€” Users manually log exercises or sync data from wearable devices and health platform APIs (Apple HealthKit, Google Fit). Calories burned adjust the daily net calorie budget.</li>
    <li><strong>Push Notifications &amp; Reminders</strong> â€” Configurable reminders for meal logging, weigh-ins, lessons, and coach messages.</li>
    <li><strong>Barcode Scanning</strong> â€” Users scan food product barcodes to quickly look up nutritional information.</li>
</ol>

<!-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• -->
<h2 id="nfr">2. Non-Functional Requirements</h2>

<table>
<tr><th>Requirement</th><th>Target</th><th>Rationale</th></tr>
<tr><td>Availability</td><td>99.9 %</td><td>Health-logging is time-sensitive; users log meals immediately after eating</td></tr>
<tr><td>Food Search Latency</td><td>&lt; 200 ms p95</td><td>Users search food dozens of times/day; sluggish search degrades UX critically</td></tr>
<tr><td>Messaging Latency</td><td>&lt; 300 ms p95</td><td>Real-time coaching feel requires near-instant message delivery</td></tr>
<tr><td>Write Throughput</td><td>~50K meal-log writes/sec peak</td><td>Millions of users logging 3â€“5 meals/day with meal-time spikes</td></tr>
<tr><td>Data Durability</td><td>99.999999999 % (11 nines)</td><td>Health data is irreplaceable and users depend on historical trends</td></tr>
<tr><td>HIPAA Compliance</td><td>Full</td><td>Weight, dietary, and health-goal data qualifies as protected health information (PHI)</td></tr>
<tr><td>Scalability</td><td>100 M+ registered users</td><td>Noom targets a global audience across iOS and Android</td></tr>
<tr><td>Offline Support</td><td>Graceful degradation</td><td>Users must be able to log meals offline with sync on reconnection</td></tr>
<tr><td>Data Consistency</td><td>Eventual for logs; strong for billing / accounts</td><td>Slight delay in daily summary update is acceptable; account operations need ACID</td></tr>
<tr><td>Security</td><td>Encryption at rest &amp; in transit (TLS 1.3)</td><td>PHI protection requirement; all APIs over HTTPS</td></tr>
</table>

<!-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• -->
<h2 id="flow1">3. Flow 1 â€” Food Logging &amp; Calorie Tracking</h2>

<p>This is the most frequent user interaction. A user searches the food database, selects an item, chooses a portion size, and logs the meal. The system calculates calories, categorizes the food by color, updates the daily calorie budget, and refreshes the dashboard.</p>

<div class="mermaid">
flowchart LR
    A["ðŸ“± Mobile App"] -->|"HTTPS"| B["Load Balancer"]
    B --> C["Food Search<br/>Service"]
    B --> D["Meal Logging<br/>Service"]
    C --> E[("Cache<br/>(Food Search)")]
    C --> F[("Search Index")]
    C --> G[("Food Catalog DB<br/>(NoSQL Document)")]
    D --> H[("Meal Log DB<br/>(NoSQL)")]
    D --> I["Message Queue"]
    I --> J["Daily Summary<br/>Service"]
    J --> K[("Daily Summary DB<br/>(NoSQL)")]
    J --> L[("Cache<br/>(Daily Summary)")]
</div>

<h3>Examples</h3>

<div class="example">
<strong>Example 1 â€” Logging a common food (cache hit):</strong><br>
User opens the app at lunchtime and taps "Log Meal â†’ Lunch." They type "chicken breast" into the search bar. This triggers an <code>HTTP GET /api/v1/foods/search?q=chicken+breast</code> to the Food Search Service via the Load Balancer. The Food Search Service first checks the Cache â€” "chicken breast" is an extremely popular query so it's a cache hit. The top results (with calorie density, color category, and macros) are returned in &lt;50 ms. The user selects "Grilled Chicken Breast, 6 oz" (a <span class="badge badge-yellow">Yellow</span> food at 280 kcal). The app sends an <code>HTTP POST /api/v1/meals</code> to the Meal Logging Service with <code>{user_id, food_id, meal_type: "lunch", servings: 1, date: "2025-02-13"}</code>. The service writes the log to the Meal Log DB and enqueues a message on the Message Queue containing the calorie delta (+280 kcal, yellow). The Daily Summary Service consumes this message, reads the user's current daily summary, increments <code>total_calories_consumed</code> by 280 and <code>yellow_count</code> by 1, writes the updated summary to the Daily Summary DB, and invalidates/updates the Daily Summary Cache. The app polls or receives a push update and the dashboard now shows 280/1,500 kcal consumed with one yellow dot.
</div>

<div class="example">
<strong>Example 2 â€” Logging via barcode scan (cache miss, DB lookup):</strong><br>
User scans the barcode on a granola bar package. The app extracts the barcode string "049000042566" and sends <code>HTTP GET /api/v1/foods/barcode/049000042566</code> to the Food Search Service. The service checks the Cache â€” this barcode is uncommon, so it's a cache miss. The service then queries the Food Catalog DB by barcode. The document is found: "Nature Valley Oats & Honey Bar, 42g, 190 kcal, <span class="badge badge-red">Red</span>." The result is returned to the user and also written into the Cache for future lookups. The user confirms and logs it as a snack. The remaining flow is identical to Example 1: an <code>HTTP POST /api/v1/meals</code> is sent, the Meal Log DB is written, the message queue is enqueued, and the daily summary is updated asynchronously.
</div>

<div class="example">
<strong>Example 3 â€” Searching for a food not in the database:</strong><br>
User types "homemade avocado toast with egg." The Food Search Service checks the Cache (miss), then queries the Search Index using full-text search. No exact match is found. The service returns partial matches ("avocado toast," "fried egg," "avocado") and suggests the user log each component individually or create a custom food entry. The user selects "Create Custom Food," enters nutritional info manually, and saves. This triggers an <code>HTTP POST /api/v1/foods</code> to add the custom food to the Food Catalog DB (marked <code>source: "user_submitted"</code>, <code>verified: false</code>). The user then logs the custom food as a meal as in Example 1.
</div>

<h3>Component Deep Dive</h3>

<h4>Mobile App (Client)</h4>
<p>Native iOS and Android applications. Handles UI rendering, local caching for offline meal logging, barcode scanning via the device camera, and syncing queued logs when connectivity resumes. Communicates with backend exclusively over HTTPS (TLS 1.3) via REST APIs.</p>

<h4>Load Balancer</h4>
<p>Layer 7 (application-level) load balancer. Routes requests to healthy instances of the Food Search Service and Meal Logging Service using round-robin with health checks. Terminates TLS. See <a href="#lb">Load Balancer Deep Dive</a> for full details.</p>

<h4>Food Search Service</h4>
<table>
<tr><th>Property</th><th>Detail</th></tr>
<tr><td>Protocol</td><td>HTTP/REST</td></tr>
<tr><td>Endpoints</td><td>
    <code>GET /api/v1/foods/search?q={query}&limit={n}</code> â€” full-text food search<br>
    <code>GET /api/v1/foods/barcode/{barcode}</code> â€” barcode lookup<br>
    <code>GET /api/v1/foods/{food_id}</code> â€” single food detail<br>
    <code>POST /api/v1/foods</code> â€” create custom food
</td></tr>
<tr><td>Input (search)</td><td><code>q</code>: search string, <code>limit</code>: max results (default 20)</td></tr>
<tr><td>Output (search)</td><td>JSON array of food objects: <code>[{food_id, name, brand, calories_per_serving, serving_size, serving_unit, color_category, macros}]</code></td></tr>
<tr><td>Data Flow</td><td>Cache â†’ Search Index â†’ Food Catalog DB (waterfall on miss)</td></tr>
</table>

<h4>Meal Logging Service</h4>
<table>
<tr><th>Property</th><th>Detail</th></tr>
<tr><td>Protocol</td><td>HTTP/REST</td></tr>
<tr><td>Endpoints</td><td>
    <code>POST /api/v1/meals</code> â€” log a meal<br>
    <code>GET /api/v1/meals?user_id={id}&date={date}</code> â€” get meals for a user on a date<br>
    <code>DELETE /api/v1/meals/{log_id}</code> â€” delete a meal entry<br>
    <code>PATCH /api/v1/meals/{log_id}</code> â€” update portion/servings
</td></tr>
<tr><td>Input (POST)</td><td><code>{user_id, food_id, meal_type, servings, date}</code></td></tr>
<tr><td>Output (POST)</td><td><code>{log_id, user_id, food_id, food_name, meal_type, servings, calories, color_category, macros, logged_at}</code></td></tr>
<tr><td>Side Effect</td><td>Enqueues calorie-delta message onto Message Queue for async summary update</td></tr>
</table>

<h4>Search Index</h4>
<p>A dedicated full-text search engine with an <strong>inverted index</strong> over food names, brands, and aliases. Supports fuzzy matching, prefix search, and synonym expansion (e.g., "chicken" matches "poultry"). Indexed fields: <code>name</code>, <code>brand</code>, <code>aliases</code>, <code>barcode</code>. This is a read-optimized store that is periodically synced from the Food Catalog DB whenever new foods are added or verified.</p>

<h4>Food Catalog DB (NoSQL Document Store)</h4>
<p>Stores the canonical food catalog (~10M+ food items). Each document has a flexible schema to accommodate varying nutritional detail levels. See <a href="#schema">Schema</a> for full details.</p>

<h4>Meal Log DB (NoSQL)</h4>
<p>Append-heavy store for meal entries. Partitioned by <code>user_id</code> with <code>date</code> as the sort key for efficient "get all meals for user X on date Y" queries. See <a href="#schema">Schema</a> for details.</p>

<h4>Cache (Food Search)</h4>
<p>In-memory key-value cache storing popular food search results. See <a href="#cache">CDN &amp; Caching Deep Dive</a>.</p>

<h4>Message Queue</h4>
<p>Durable, at-least-once delivery queue. The Meal Logging Service produces messages; the Daily Summary Service consumes them. See <a href="#mq">Message Queue Deep Dive</a>.</p>

<h4>Daily Summary Service</h4>
<table>
<tr><th>Property</th><th>Detail</th></tr>
<tr><td>Protocol</td><td>Internal â€” consumes from Message Queue (no external HTTP API)</td></tr>
<tr><td>Input</td><td>Message from queue: <code>{user_id, date, calorie_delta, color_category, event_type: "meal_logged|meal_deleted|activity_logged"}</code></td></tr>
<tr><td>Output</td><td>Updated daily summary written to Daily Summary DB and Cache</td></tr>
<tr><td>Logic</td><td>Reads current summary, applies delta, writes back. Idempotent via <code>log_id</code> deduplication.</td></tr>
</table>

<h4>Daily Summary DB (NoSQL)</h4>
<p>Stores pre-computed daily aggregates per user. This is a <strong>denormalized</strong> view. See <a href="#schema-daily-summary">Schema â€” daily_summaries</a> for why.</p>

<!-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• -->
<h2 id="flow2">4. Flow 2 â€” Coaching Messaging</h2>

<p>Noom's core differentiator is human coaching. Premium users have a dedicated coach they can message at any time. Coaches use a web-based dashboard. Messages should feel real-time when both parties are online and fall back to push notifications when offline.</p>

<div class="mermaid">
flowchart LR
    UA["ðŸ“± User App"] -->|"WebSocket<br/>(wss://)"| WG["WebSocket<br/>Gateway"]
    CD["ðŸ’» Coach<br/>Dashboard"] -->|"WebSocket<br/>(wss://)"| WG
    WG --> CS[("Connection<br/>Store<br/>(Cache)")]
    WG --> MS["Messaging<br/>Service"]
    MS --> MDB[("Message DB<br/>(NoSQL)")]
    MS --> CONV[("Conversation DB<br/>(NoSQL)")]
    MS --> MQ["Message Queue"]
    MQ --> NS["Notification<br/>Service"]
    NS --> APNS["Push Notification<br/>Provider<br/>(APNs / FCM)"]
</div>

<h3>Examples</h3>

<div class="example">
<strong>Example 1 â€” Both user and coach online (real-time delivery):</strong><br>
User "Alice" opens the coaching chat in the Noom app. The app establishes a WebSocket connection (<code>wss://ws.noom.com/connect?token=JWT</code>) to the WebSocket Gateway. The Gateway authenticates Alice's JWT, registers her connection in the Connection Store under key <code>user:alice_123</code>, and holds the socket open. Alice types "I'm struggling with late-night snacking" and sends the message. The WebSocket Gateway receives the frame and forwards it to the Messaging Service via an internal RPC call: <code>{conversation_id: "conv_abc", sender_id: "alice_123", sender_type: "user", content: "I'm struggling with late-night snacking"}</code>. The Messaging Service writes the message to the Message DB, updates the Conversation DB's <code>last_message_preview</code> and <code>last_message_at</code>, then asks the Connection Store: "Is coach <code>coach_jane_456</code> connected?" The answer is yes â€” Coach Jane has her dashboard open. The Messaging Service instructs the WebSocket Gateway to push the message to Coach Jane's socket. Jane sees the message appear instantly (&lt;300 ms). She replies, "Let's talk about some strategies â€” have you tried keeping healthy snacks prepared?" The same flow happens in reverse: Gateway â†’ Messaging Service â†’ Message DB â†’ Connection Store lookup â†’ push to Alice's socket. No push notification is needed because both are online.
</div>

<div class="example">
<strong>Example 2 â€” User sends message while coach is offline (push notification):</strong><br>
User "Bob" sends a message: "I hit my goal weight this week!" The WebSocket Gateway forwards it to the Messaging Service, which writes it to the Message DB and updates the Conversation DB. The service checks the Connection Store for Coach Mike â€” Coach Mike is not connected (no active WebSocket). The Messaging Service enqueues a notification message onto the Message Queue: <code>{type: "coach_message", recipient_id: "coach_mike_789", sender_name: "Bob", preview: "I hit my goal weight this week!", conversation_id: "conv_def"}</code>. The Notification Service consumes this message and sends a push notification to Coach Mike's devices via APNs/FCM. When Coach Mike opens the dashboard later, a new WebSocket connection is established. The dashboard calls <code>HTTP GET /api/v1/conversations?coach_id=coach_mike_789</code> to load conversations sorted by <code>last_message_at</code> and then <code>HTTP GET /api/v1/messages?conversation_id=conv_def&limit=50</code> to load message history from the Message DB.
</div>

<div class="example">
<strong>Example 3 â€” WebSocket disconnects and reconnects:</strong><br>
Alice's phone loses cellular connection while on the subway. The WebSocket connection drops. The Gateway detects the disconnect (via TCP keepalive / heartbeat timeout) and removes Alice's entry from the Connection Store. If Coach Jane sends a message during this time, the Connection Store lookup finds Alice offline, so a push notification is queued. When Alice's phone regains connectivity, the app re-establishes the WebSocket connection with a <code>last_received_message_id</code> parameter. The Gateway re-registers Alice in the Connection Store. The client requests any missed messages since <code>last_received_message_id</code> via <code>HTTP GET /api/v1/messages?conversation_id=conv_abc&after_id=msg_xyz</code>, and the Messaging Service returns them from the Message DB. Alice sees the missed messages seamlessly.
</div>

<h3>Component Deep Dive</h3>

<h4>WebSocket Gateway</h4>
<p>A stateful edge service that manages long-lived WebSocket connections. Each Gateway instance maintains thousands of concurrent sockets. Responsibilities include:</p>
<ul>
    <li><strong>Connection lifecycle:</strong> Accept WebSocket upgrade requests (HTTP â†’ WebSocket via <code>wss://</code>), authenticate JWT tokens, register connections in the Connection Store, detect disconnections via heartbeat (ping/pong every 30 seconds), and clean up stale entries.</li>
    <li><strong>Message routing:</strong> Receive outbound messages from the Messaging Service and push them to the correct client socket.</li>
    <li><strong>Scaling:</strong> Multiple Gateway instances behind the Load Balancer. The Connection Store (shared cache) maps each user to the specific Gateway instance holding their socket, so the Messaging Service knows which Gateway to notify.</li>
</ul>
<p>Protocol: WebSocket over TLS (<code>wss://</code>). The initial handshake is an HTTP/1.1 Upgrade request. After upgrade, communication is bidirectional binary/text frames over a persistent TCP connection.</p>

<h4>Connection Store (Cache)</h4>
<p>An in-memory key-value cache mapping <code>user_id â†’ {gateway_instance_id, socket_id, connected_at}</code>. Entries have a TTL of 5 minutes, refreshed on every heartbeat. If a user disconnects and the Gateway fails to clean up, the TTL ensures the entry expires. This store enables the Messaging Service to locate the correct Gateway instance for message delivery across a horizontally-scaled fleet of Gateway instances.</p>

<h4>Messaging Service</h4>
<table>
<tr><th>Property</th><th>Detail</th></tr>
<tr><td>Protocol</td><td>Internal gRPC (from WebSocket Gateway); HTTP/REST (for history retrieval from clients)</td></tr>
<tr><td>Endpoints</td><td>
    <code>gRPC SendMessage(conversation_id, sender_id, sender_type, content, message_type)</code><br>
    <code>GET /api/v1/messages?conversation_id={id}&limit={n}&after_id={id}</code> â€” paginated message history<br>
    <code>GET /api/v1/conversations?user_id={id}</code> â€” list conversations<br>
    <code>PATCH /api/v1/messages/{message_id}/read</code> â€” mark as read
</td></tr>
<tr><td>Input (SendMessage)</td><td><code>{conversation_id, sender_id, sender_type, content, message_type}</code></td></tr>
<tr><td>Output (SendMessage)</td><td><code>{message_id, sent_at, delivery_status}</code></td></tr>
<tr><td>Side Effects</td><td>1) Write to Message DB, 2) Update Conversation DB, 3) Check Connection Store, 4) Push via Gateway OR enqueue notification on Message Queue</td></tr>
</table>

<h4>Message DB (NoSQL)</h4>
<p>Stores all chat messages. Partitioned by <code>conversation_id</code> with <code>sent_at</code> as sort key for efficient chronological pagination. Append-only write pattern. See <a href="#schema">Schema</a>.</p>

<h4>Conversation DB (NoSQL)</h4>
<p>Stores conversation metadata: participants, type (coaching vs. group), last message preview. Enables "inbox" view sorted by most recent activity.</p>

<h4>Notification Service</h4>
<table>
<tr><th>Property</th><th>Detail</th></tr>
<tr><td>Protocol</td><td>Internal â€” consumes from Message Queue</td></tr>
<tr><td>Input</td><td>Notification message: <code>{type, recipient_id, title, body, data_payload}</code></td></tr>
<tr><td>Output</td><td>Push notification delivered via APNs (iOS) or FCM (Android)</td></tr>
<tr><td>Logic</td><td>Looks up recipient's device tokens from the User DB, respects notification preferences (quiet hours, disabled categories), applies rate limiting (max 10 pushes/hour per user), and sends via the appropriate push provider.</td></tr>
</table>

<h4>Push Notification Provider (APNs / FCM)</h4>
<p>Apple Push Notification service and Firebase Cloud Messaging. These are external platform services â€” the Notification Service uses their respective HTTP/2 APIs to deliver push notifications to user devices. This is the only vendor-specific component in the design (required by iOS and Android platforms).</p>

<!-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• -->
<h2 id="flow3">5. Flow 3 â€” Curriculum &amp; Daily Lessons</h2>

<p>Noom delivers a multi-week program of psychology-based educational content. Each day, users receive a set of articles, quizzes, and interactive exercises. Content is sequential â€” completing today's lessons unlocks tomorrow's. Content is static (authored by content teams) but delivery is personalized based on each user's program week/day.</p>

<div class="mermaid">
flowchart LR
    A["ðŸ“± Mobile App"] -->|"HTTPS"| B["Load Balancer"]
    B --> CS["Curriculum<br/>Service"]
    CS --> CC[("Cache<br/>(Content)")]
    CS --> CDB[("Curriculum Content DB<br/>(NoSQL Document)")]
    CS --> PDB[("Curriculum Progress DB<br/>(NoSQL)")]
    CS --> CDN["CDN"]
    CDN --> OBJ[("Object Storage<br/>(Media Files)")]
</div>

<h3>Examples</h3>

<div class="example">
<strong>Example 1 â€” User opens daily lessons:</strong><br>
User "Alice" opens the Noom app and taps the "Lessons" tab. The app sends <code>HTTP GET /api/v1/curriculum/today?user_id=alice_123</code> to the Curriculum Service. The service reads Alice's progress from the Curriculum Progress DB â€” she is on Week 3, Day 2. It then fetches the content for Week 3, Day 2 from the Cache (this content is likely cached since other users in the same program week accessed it recently â€” cache hit). The response includes 3 lesson cards: an article titled "Understanding Emotional Eating," a quiz on "Identifying Trigger Foods," and a mindfulness exercise. Each card has a <code>content_id</code>, title, estimated reading time, and thumbnail image URL pointing to the CDN (<code>https://cdn.noom.com/curriculum/w3d2/emotional-eating-thumb.jpg</code>). The app renders the lesson list. Alice taps the first article. The app loads the full article body (already included in the initial payload for efficiency), and images are loaded from the CDN. Alice finishes reading and taps "Complete." The app sends <code>HTTP POST /api/v1/curriculum/progress</code> with <code>{user_id: "alice_123", content_id: "lesson_w3d2_01", status: "completed"}</code>. The service updates the Curriculum Progress DB. When Alice completes all 3 lessons for the day, her <code>current_day</code> advances to Day 3.
</div>

<div class="example">
<strong>Example 2 â€” User takes a quiz:</strong><br>
Alice opens the quiz "Identifying Trigger Foods." The app renders the quiz questions (returned as part of the curriculum content payload â€” an array of <code>{question, options, correct_answer_index}</code>). Alice answers each question. Upon submission, the app sends <code>HTTP POST /api/v1/curriculum/progress</code> with <code>{user_id: "alice_123", content_id: "quiz_w3d2_01", status: "completed", quiz_answers: [1, 2, 0, 3], quiz_score: 3}</code>. The service stores her answers and score in the Curriculum Progress DB. If she scores below a threshold, the service may flag this content for the coach to discuss.
</div>

<div class="example">
<strong>Example 3 â€” Content cache miss (new program week):</strong><br>
It's Monday and Alice just entered Week 4. She's among the first users to reach this week. The Curriculum Service checks the Cache for Week 4, Day 1 content â€” cache miss. The service queries the Curriculum Content DB for all content where <code>week_number = 4</code> and <code>day_number = 1</code>, ordered by <code>order</code>. The documents are returned, populated into the Cache with a 7-day TTL (content doesn't change within a program cycle), and sent to Alice. Subsequent users reaching Week 4, Day 1 will hit the cache.
</div>

<h3>Component Deep Dive</h3>

<h4>Curriculum Service</h4>
<table>
<tr><th>Property</th><th>Detail</th></tr>
<tr><td>Protocol</td><td>HTTP/REST</td></tr>
<tr><td>Endpoints</td><td>
    <code>GET /api/v1/curriculum/today?user_id={id}</code> â€” get today's lessons for user<br>
    <code>GET /api/v1/curriculum/content/{content_id}</code> â€” get single lesson<br>
    <code>POST /api/v1/curriculum/progress</code> â€” mark lesson/quiz complete<br>
    <code>GET /api/v1/curriculum/progress?user_id={id}</code> â€” get user's overall progress
</td></tr>
<tr><td>Input (today)</td><td><code>user_id</code> (from auth token)</td></tr>
<tr><td>Output (today)</td><td>JSON array of lesson objects: <code>[{content_id, title, content_type, body, media_urls, quiz_questions, estimated_minutes, order}]</code></td></tr>
<tr><td>Logic</td><td>1) Read user progress to determine current week/day, 2) Fetch content for that week/day (cache â†’ DB), 3) Replace media paths with CDN URLs</td></tr>
</table>

<h4>Curriculum Content DB (NoSQL Document Store)</h4>
<p>Stores all educational content authored by the Noom content team. Each document represents one lesson/quiz/exercise with flexible schema (articles have body text; quizzes have question arrays; exercises have step lists). Read-heavy, very infrequent writes (only when content team publishes new material). See <a href="#schema">Schema</a>.</p>

<h4>Curriculum Progress DB (NoSQL)</h4>
<p>Tracks each user's progress through the program. Partitioned by <code>user_id</code> for fast per-user lookups. Records which lessons are completed, quiz scores, and the user's current week/day position. See <a href="#schema">Schema</a>.</p>

<h4>CDN (Content Delivery Network)</h4>
<p>Serves static curriculum media globally: article images, illustrations, infographics, optional video content. Origin is Object Storage. Content is long-lived with aggressive caching (Cache-Control: max-age=2592000, 30 days). See <a href="#cache">CDN &amp; Caching Deep Dive</a>.</p>

<h4>Object Storage</h4>
<p>Durable blob storage for all curriculum media files. Serves as the CDN origin. Files are versioned and immutable (new versions get new paths). Write access is restricted to the content-management pipeline.</p>

<!-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• -->
<h2 id="flow4">6. Flow 4 â€” Weight &amp; Progress Tracking</h2>

<p>Users log their weight daily. The system records the measurement, updates the user's progress graph, optionally notifies the coach, and updates the daily summary.</p>

<div class="mermaid">
flowchart LR
    A["ðŸ“± Mobile App"] -->|"HTTPS"| B["Load Balancer"]
    B --> WS["Weight<br/>Service"]
    WS --> WDB[("Weight Log DB<br/>(NoSQL)")]
    WS --> MQ["Message Queue"]
    MQ --> DSS["Daily Summary<br/>Service"]
    MQ --> NS["Notification<br/>Service"]
    DSS --> DDB[("Daily Summary DB<br/>(NoSQL)")]
    NS --> APNS["Push Notification<br/>Provider"]
    B --> PS["Progress<br/>Service"]
    PS --> WDB
    PS --> DDB
    PS --> C[("Cache<br/>(Progress)")]
</div>

<h3>Examples</h3>

<div class="example">
<strong>Example 1 â€” User logs a normal weigh-in:</strong><br>
User "Alice" steps on her scale in the morning and taps "Log Weight" in the app. She enters 72.3 kg. The app sends <code>HTTP POST /api/v1/weight</code> with <code>{user_id: "alice_123", weight_kg: 72.3, date: "2025-02-13", source: "manual"}</code> to the Weight Service. The service writes the entry to the Weight Log DB (partition key: <code>user_id</code>, sort key: <code>date</code>). The service enqueues two messages on the Message Queue: (1) a daily-summary update <code>{user_id, date, event_type: "weight_logged", weight_kg: 72.3}</code> for the Daily Summary Service, and (2) an optional coach notification <code>{type: "weight_logged", user_id: "alice_123", weight_kg: 72.3, goal_weight: 68.0}</code> for the Notification Service (if the coach has opted into weight-log alerts). The Daily Summary Service updates <code>weight_logged: true</code> in the daily summary. Alice sees her weight graph update with the new data point.
</div>

<div class="example">
<strong>Example 2 â€” User views progress graph:</strong><br>
Alice taps the "Progress" tab. The app sends <code>HTTP GET /api/v1/progress?user_id=alice_123&range=90d</code> to the Progress Service. The service checks the Cache for Alice's 90-day progress data. On a cache miss, it queries the Weight Log DB for all entries for Alice in the last 90 days, fetches her goal weight from the Users table, and computes the progress graph data (dates, weights, trend line, distance to goal). The result is cached for 1 hour (since weight can only change once/day, short TTL is fine). The response includes: <code>{data_points: [{date, weight_kg}...], trend: [{date, smoothed_weight}...], starting_weight: 78.0, current_weight: 72.3, goal_weight: 68.0, total_lost: 5.7}</code>. The app renders a line chart with the raw data points, a smoothed trend line, and the goal weight as a horizontal reference line.
</div>

<div class="example">
<strong>Example 3 â€” User hits a milestone (coach notified):</strong><br>
Alice logs 68.0 kg â€” she has reached her goal weight! The Weight Service detects this by comparing the logged weight against her <code>goal_weight</code> in the Users table. The service enqueues a high-priority notification: <code>{type: "milestone_reached", user_id: "alice_123", milestone: "goal_weight_reached", weight_kg: 68.0}</code>. The Notification Service sends a congratulatory push notification to Alice: "ðŸŽ‰ Congratulations! You've reached your goal weight of 68 kg!" It also sends a notification to Coach Jane: "Alice has reached her goal weight!" The coach can then send a personalized congratulations message via the coaching chat.
</div>

<h3>Component Deep Dive</h3>

<h4>Weight Service</h4>
<table>
<tr><th>Property</th><th>Detail</th></tr>
<tr><td>Protocol</td><td>HTTP/REST</td></tr>
<tr><td>Endpoints</td><td>
    <code>POST /api/v1/weight</code> â€” log a weight entry<br>
    <code>GET /api/v1/weight?user_id={id}&start_date={d}&end_date={d}</code> â€” get weight history<br>
    <code>DELETE /api/v1/weight/{log_id}</code> â€” delete an erroneous entry
</td></tr>
<tr><td>Input (POST)</td><td><code>{user_id, weight_kg, date, source}</code></td></tr>
<tr><td>Output (POST)</td><td><code>{log_id, user_id, weight_kg, date, source, logged_at}</code></td></tr>
<tr><td>Side Effects</td><td>Enqueue daily-summary update + optional coach notification + milestone detection</td></tr>
</table>

<h4>Progress Service</h4>
<table>
<tr><th>Property</th><th>Detail</th></tr>
<tr><td>Protocol</td><td>HTTP/REST</td></tr>
<tr><td>Endpoints</td><td>
    <code>GET /api/v1/progress?user_id={id}&range={7d|30d|90d|all}</code> â€” get progress data for graphing
</td></tr>
<tr><td>Input</td><td><code>user_id</code>, <code>range</code></td></tr>
<tr><td>Output</td><td><code>{data_points, trend, starting_weight, current_weight, goal_weight, total_lost, bmi_current, bmi_start}</code></td></tr>
<tr><td>Logic</td><td>Aggregates from Weight Log DB, computes moving-average trend, compares to goal. Caches result.</td></tr>
</table>

<h4>Weight Log DB (NoSQL)</h4>
<p>Time-series-style document store. One entry per weigh-in. Partitioned by <code>user_id</code>, sorted by <code>date</code>. Efficient range queries for progress graphing. See <a href="#schema">Schema</a>.</p>

<!-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• -->
<h2 id="combined">7. Combined Overall Diagram</h2>

<p>This diagram combines all four flows into one unified architecture showing how all services interact.</p>

<div class="mermaid">
flowchart TB
    subgraph Clients
        UA["ðŸ“± User App<br/>(iOS / Android)"]
        CD["ðŸ’» Coach Dashboard<br/>(Web)"]
    end

    subgraph Edge
        LB["Load Balancer"]
        WSG["WebSocket<br/>Gateway"]
    end

    subgraph Core Services
        FSS["Food Search<br/>Service"]
        MLS["Meal Logging<br/>Service"]
        WTS["Weight<br/>Service"]
        PRS["Progress<br/>Service"]
        CRS["Curriculum<br/>Service"]
        MSG["Messaging<br/>Service"]
        DSS["Daily Summary<br/>Service"]
        NTS["Notification<br/>Service"]
        USR["User<br/>Service"]
        ACT["Activity<br/>Service"]
    end

    subgraph Data Stores
        USERDB[("Users DB<br/>(SQL)")]
        FOODDB[("Food Catalog DB<br/>(NoSQL Doc)")]
        SIDX[("Search Index")]
        MEALDB[("Meal Log DB<br/>(NoSQL)")]
        WEIGHTDB[("Weight Log DB<br/>(NoSQL)")]
        ACTDB[("Activity Log DB<br/>(NoSQL)")]
        DAILYDB[("Daily Summary DB<br/>(NoSQL)")]
        CURDB[("Curriculum<br/>Content DB<br/>(NoSQL Doc)")]
        PROGDB[("Curriculum<br/>Progress DB<br/>(NoSQL)")]
        MSGDB[("Message DB<br/>(NoSQL)")]
        CONVDB[("Conversation DB<br/>(NoSQL)")]
    end

    subgraph Infra
        CACHE[("Cache")]
        MQ["Message Queue"]
        CDN_NODE["CDN"]
        OBJ[("Object<br/>Storage")]
        CONN[("Connection<br/>Store")]
    end

    subgraph External
        APNS["APNs / FCM"]
        HEALTH["HealthKit /<br/>Google Fit"]
    end

    UA -->|"HTTPS"| LB
    CD -->|"HTTPS"| LB
    UA -->|"WSS"| WSG
    CD -->|"WSS"| WSG
    UA -.->|"Sync"| HEALTH

    LB --> FSS
    LB --> MLS
    LB --> WTS
    LB --> PRS
    LB --> CRS
    LB --> USR
    LB --> ACT

    WSG --> CONN
    WSG --> MSG

    FSS --> CACHE
    FSS --> SIDX
    FSS --> FOODDB

    MLS --> MEALDB
    MLS --> MQ

    WTS --> WEIGHTDB
    WTS --> MQ

    ACT --> ACTDB
    ACT --> MQ
    ACT -.-> HEALTH

    PRS --> WEIGHTDB
    PRS --> DAILYDB
    PRS --> CACHE

    CRS --> CACHE
    CRS --> CURDB
    CRS --> PROGDB
    CRS --> CDN_NODE

    MSG --> MSGDB
    MSG --> CONVDB
    MSG --> MQ

    MQ --> DSS
    MQ --> NTS

    DSS --> DAILYDB
    DSS --> CACHE

    NTS --> APNS

    CDN_NODE --> OBJ

    USR --> USERDB
</div>

<h3>Combined Flow Examples</h3>

<div class="example">
<strong>Example 1 â€” A user's complete morning routine:</strong><br>
Alice wakes up and opens the Noom app. <strong>(1) Weight logging:</strong> She weighs herself and taps "Log Weight" â€” <code>POST /api/v1/weight</code> â†’ Weight Service â†’ Weight Log DB + Message Queue â†’ Daily Summary Service updates her daily summary. <strong>(2) Lessons:</strong> She navigates to the Lessons tab â€” <code>GET /api/v1/curriculum/today</code> â†’ Curriculum Service â†’ Cache hit â†’ returns 3 lessons. She reads an article (images from CDN) and completes it â€” <code>POST /api/v1/curriculum/progress</code> â†’ Curriculum Progress DB updated. <strong>(3) Breakfast logging:</strong> She searches "oatmeal with blueberries" â€” <code>GET /api/v1/foods/search?q=oatmeal</code> â†’ Food Search Service â†’ Cache hit â†’ results returned. She logs "Steel Cut Oatmeal, 1 cup" (<span class="badge badge-green">Green</span>, 150 kcal) and "Blueberries, 1/2 cup" (<span class="badge badge-green">Green</span>, 42 kcal) â€” two <code>POST /api/v1/meals</code> calls â†’ Meal Log DB â†’ Message Queue â†’ Daily Summary now shows 192/1,400 kcal. <strong>(4) Coach message:</strong> Alice messages her coach: "Had a great morning!" â€” WebSocket frame â†’ WebSocket Gateway â†’ Messaging Service â†’ Message DB. Coach Jane is offline, so Message Queue â†’ Notification Service â†’ FCM push to Coach Jane's phone. The entire morning involved all four flows seamlessly.
</div>

<div class="example">
<strong>Example 2 â€” Coach reviews a struggling user:</strong><br>
Coach Jane opens her dashboard (HTTPS â†’ Load Balancer â†’ WebSocket established). She views her client list via <code>GET /api/v1/coach/clients?coach_id=jane_456</code> â†’ User Service â†’ Users DB. She notices Bob hasn't logged meals in 3 days (the Notification Service had sent her an alert earlier via push). She pulls up Bob's progress: <code>GET /api/v1/progress?user_id=bob_789&range=30d</code> â†’ Progress Service â†’ Weight Log DB + Daily Summary DB â†’ sees a weight plateau. She opens the coaching chat â€” <code>GET /api/v1/messages?conversation_id=conv_bob</code> â†’ Messaging Service â†’ Message DB â†’ displays history. She sends an encouraging message: "Hey Bob, I noticed you've been quiet. No judgment â€” let's talk about what's going on." â†’ WebSocket Gateway â†’ Messaging Service â†’ Message DB â†’ Connection Store (Bob is offline) â†’ Message Queue â†’ Notification Service â†’ APNs push notification to Bob's iPhone.
</div>

<div class="example">
<strong>Example 3 â€” Activity sync from wearable:</strong><br>
Alice's smartwatch syncs 8,200 steps and a 30-min jog (estimated 320 kcal burned) to Apple HealthKit. The Noom app's background sync detects the new data and sends <code>POST /api/v1/activities</code> â†’ Activity Service â†’ Activity Log DB + Message Queue (calorie-burn delta: -320 kcal). The Daily Summary Service consumes the message and updates Alice's daily summary: net calories = 1,200 consumed - 320 burned = 880 net. Her calorie budget effectively increases, giving her more room for dinner. The dashboard refreshes to show the updated budget.
</div>

<!-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• -->
<h2 id="schema">8. Database Schema</h2>

<h3>SQL Tables</h3>

<h4>users</h4>
<p><span class="badge badge-blue">SQL</span> <strong>Why SQL:</strong> User data is highly relational â€” it references coaches, groups, and billing. Account operations (signup, plan changes, billing) require ACID transactions. The schema is stable and well-defined. Read/write ratio is moderate.</p>

<table>
<tr><th>Column</th><th>Type</th><th>Constraints</th><th>Description</th></tr>
<tr><td><code>user_id</code></td><td>UUID</td><td><strong>Primary Key</strong></td><td>Unique user identifier</td></tr>
<tr><td><code>email</code></td><td>VARCHAR(255)</td><td>UNIQUE, NOT NULL</td><td>Login email</td></tr>
<tr><td><code>password_hash</code></td><td>VARCHAR(255)</td><td>NOT NULL</td><td>Bcrypt hash</td></tr>
<tr><td><code>first_name</code></td><td>VARCHAR(100)</td><td>NOT NULL</td><td></td></tr>
<tr><td><code>last_name</code></td><td>VARCHAR(100)</td><td>NOT NULL</td><td></td></tr>
<tr><td><code>date_of_birth</code></td><td>DATE</td><td></td><td>For calorie calculation</td></tr>
<tr><td><code>gender</code></td><td>ENUM</td><td></td><td>For calorie calculation</td></tr>
<tr><td><code>height_cm</code></td><td>DECIMAL(5,1)</td><td></td><td></td></tr>
<tr><td><code>starting_weight_kg</code></td><td>DECIMAL(5,1)</td><td></td><td>Weight at signup</td></tr>
<tr><td><code>goal_weight_kg</code></td><td>DECIMAL(5,1)</td><td></td><td>Target weight</td></tr>
<tr><td><code>daily_calorie_budget</code></td><td>INT</td><td></td><td>Computed from profile + goals</td></tr>
<tr><td><code>activity_level</code></td><td>ENUM</td><td></td><td>sedentary / light / moderate / active</td></tr>
<tr><td><code>plan_type</code></td><td>ENUM</td><td>NOT NULL</td><td>free / premium</td></tr>
<tr><td><code>plan_start_date</code></td><td>DATE</td><td></td><td>Program start date</td></tr>
<tr><td><code>coach_id</code></td><td>UUID</td><td><strong>Foreign Key â†’ coaches.coach_id</strong></td><td>Assigned coach (nullable for free users)</td></tr>
<tr><td><code>group_id</code></td><td>UUID</td><td><strong>Foreign Key â†’ groups.group_id</strong></td><td>Assigned support group</td></tr>
<tr><td><code>timezone</code></td><td>VARCHAR(50)</td><td></td><td>For notification scheduling</td></tr>
<tr><td><code>device_tokens</code></td><td>JSON</td><td></td><td>APNs/FCM push tokens</td></tr>
<tr><td><code>created_at</code></td><td>TIMESTAMP</td><td>NOT NULL</td><td></td></tr>
<tr><td><code>updated_at</code></td><td>TIMESTAMP</td><td>NOT NULL</td><td></td></tr>
</table>

<p><strong>Indexes:</strong></p>
<ul>
    <li><strong>B-Tree index on <code>email</code></strong> â€” Used for login lookups. B-Tree supports equality and uniqueness constraint efficiently. Email lookups are exact-match queries, and B-Tree provides O(log n) lookup which is optimal for this access pattern.</li>
    <li><strong>B-Tree index on <code>coach_id</code></strong> â€” Coaches frequently query "all my clients." Without this index, every such query would full-table scan. B-Tree supports equality lookups efficiently.</li>
    <li><strong>B-Tree index on <code>group_id</code></strong> â€” Group-membership queries need to find all users in a group.</li>
</ul>

<p><strong>Read triggers:</strong> User login, profile view, coach viewing client list, any API call requiring user context (calorie budget, goal weight).<br>
<strong>Write triggers:</strong> User signup, profile update, plan change, coach/group assignment, device token refresh.</p>

<p><strong>Sharding:</strong> The users table is sharded by <code>user_id</code> (hash-based sharding). This distributes users evenly across shards. Since most queries include <code>user_id</code> (lookups by a specific user), shard routing is straightforward â€” hash the <code>user_id</code> to find the target shard. The <code>email</code> unique constraint requires a global secondary index or a separate emailâ†’user_id lookup table to handle cross-shard uniqueness. Coach-based queries (<code>WHERE coach_id = X</code>) may scatter across shards, but this is acceptable since coaches have a limited number of clients (~100) and this query is infrequent.</p>

<h4>coaches</h4>
<p><span class="badge badge-blue">SQL</span> <strong>Why SQL:</strong> Coach data is relational (linked to users and groups). Client count management needs transactional consistency to avoid over-assignment.</p>

<table>
<tr><th>Column</th><th>Type</th><th>Constraints</th><th>Description</th></tr>
<tr><td><code>coach_id</code></td><td>UUID</td><td><strong>Primary Key</strong></td><td>Unique coach identifier</td></tr>
<tr><td><code>name</code></td><td>VARCHAR(200)</td><td>NOT NULL</td><td></td></tr>
<tr><td><code>email</code></td><td>VARCHAR(255)</td><td>UNIQUE, NOT NULL</td><td></td></tr>
<tr><td><code>specialization</code></td><td>VARCHAR(100)</td><td></td><td>e.g., "diabetes management", "general wellness"</td></tr>
<tr><td><code>max_clients</code></td><td>INT</td><td>NOT NULL</td><td>Capacity limit</td></tr>
<tr><td><code>current_client_count</code></td><td>INT</td><td>NOT NULL, DEFAULT 0</td><td>Denormalized count for fast capacity checks</td></tr>
<tr><td><code>is_active</code></td><td>BOOLEAN</td><td>NOT NULL</td><td></td></tr>
<tr><td><code>created_at</code></td><td>TIMESTAMP</td><td></td><td></td></tr>
</table>

<div class="callout">
<strong>Denormalization note â€” <code>current_client_count</code>:</strong> This field is denormalized (could be computed as <code>SELECT COUNT(*) FROM users WHERE coach_id = ?</code>). We store it directly because coach assignment happens frequently during onboarding surges and requires a fast capacity check. The count is updated transactionally within the same SQL transaction that assigns a coach to a user, so it stays consistent. The alternative (computing via COUNT on every assignment) would require a full index scan of the users table filtered by <code>coach_id</code>, which under high concurrency could create contention.
</div>

<p><strong>Read triggers:</strong> Coach login, admin assigning coaches, user onboarding (capacity check).<br>
<strong>Write triggers:</strong> Coach creation, client count update (on assignment/unassignment).</p>

<h4>groups</h4>
<p><span class="badge badge-blue">SQL</span> <strong>Why SQL:</strong> Relational data linking coaches and users. Membership management needs consistency.</p>

<table>
<tr><th>Column</th><th>Type</th><th>Constraints</th><th>Description</th></tr>
<tr><td><code>group_id</code></td><td>UUID</td><td><strong>Primary Key</strong></td><td></td></tr>
<tr><td><code>group_name</code></td><td>VARCHAR(200)</td><td>NOT NULL</td><td></td></tr>
<tr><td><code>coach_id</code></td><td>UUID</td><td><strong>Foreign Key â†’ coaches.coach_id</strong></td><td>Group facilitator</td></tr>
<tr><td><code>program_type</code></td><td>VARCHAR(50)</td><td></td><td>e.g., "weight_loss", "diabetes"</td></tr>
<tr><td><code>max_members</code></td><td>INT</td><td>NOT NULL</td><td></td></tr>
<tr><td><code>current_member_count</code></td><td>INT</td><td>NOT NULL, DEFAULT 0</td><td>Denormalized for fast capacity checks</td></tr>
<tr><td><code>created_at</code></td><td>TIMESTAMP</td><td></td><td></td></tr>
</table>

<div class="callout">
<strong>Denormalization note â€” <code>current_member_count</code>:</strong> Same rationale as <code>current_client_count</code> in the coaches table. Group assignment during onboarding needs to check capacity quickly without counting users table rows.
</div>

<p><strong>Read triggers:</strong> User viewing group info, coach viewing their groups, admin managing groups.<br>
<strong>Write triggers:</strong> Group creation, member count update on join/leave.</p>

<hr>

<h3>NoSQL Tables</h3>

<h4>food_catalog</h4>
<p><span class="badge badge-yellow">NoSQL Document Store</span> <strong>Why NoSQL:</strong> The food catalog contains ~10M+ items with highly variable schemas â€” some foods have detailed micronutrient breakdowns, allergen info, and ingredients lists while others only have basic macros. A document store handles this schema variability naturally without nullable columns. The access pattern is read-heavy (millions of searches/day) with rare writes (new foods added by content team or user submissions). No joins are needed â€” each food document is self-contained.</p>

<table>
<tr><th>Field</th><th>Type</th><th>Key</th><th>Description</th></tr>
<tr><td><code>food_id</code></td><td>String</td><td><strong>Primary Key</strong></td><td>Unique food identifier</td></tr>
<tr><td><code>name</code></td><td>String</td><td></td><td>"Grilled Chicken Breast"</td></tr>
<tr><td><code>brand</code></td><td>String (optional)</td><td></td><td>"Tyson" â€” null for generic foods</td></tr>
<tr><td><code>barcode</code></td><td>String (optional)</td><td></td><td>UPC barcode string</td></tr>
<tr><td><code>serving_size</code></td><td>Number</td><td></td><td>6</td></tr>
<tr><td><code>serving_unit</code></td><td>String</td><td></td><td>"oz"</td></tr>
<tr><td><code>calories_per_serving</code></td><td>Number</td><td></td><td>280</td></tr>
<tr><td><code>calorie_density</code></td><td>Number</td><td></td><td>kcal per gram â€” used for color coding</td></tr>
<tr><td><code>color_category</code></td><td>String</td><td></td><td>"green" | "yellow" | "red"</td></tr>
<tr><td><code>macros</code></td><td>Object</td><td></td><td><code>{protein_g, carbs_g, fat_g, fiber_g}</code></td></tr>
<tr><td><code>micronutrients</code></td><td>Object (optional)</td><td></td><td><code>{sodium_mg, potassium_mg, vitamin_c_mg, ...}</code></td></tr>
<tr><td><code>allergens</code></td><td>Array (optional)</td><td></td><td><code>["gluten", "dairy"]</code></td></tr>
<tr><td><code>ingredients</code></td><td>String (optional)</td><td></td><td>Raw ingredient list for packaged foods</td></tr>
<tr><td><code>verified</code></td><td>Boolean</td><td></td><td>Whether nutritionist-verified</td></tr>
<tr><td><code>source</code></td><td>String</td><td></td><td>"official" | "user_submitted"</td></tr>
<tr><td><code>created_at</code></td><td>Timestamp</td><td></td><td></td></tr>
</table>

<p><strong>Indexes:</strong></p>
<ul>
    <li><strong>Inverted index (full-text) on <code>name</code> and <code>brand</code></strong> â€” This is the critical index for the food search feature. An inverted index maps each token (word) to a list of document IDs containing that token. It supports prefix matching ("chick" â†’ "chicken"), fuzzy matching (edit distance for typos), and ranked relevance scoring. This is the same indexing technique used by search engines. The inverted index is maintained in a separate Search Index service and synced from the Food Catalog DB.</li>
    <li><strong>Hash index on <code>barcode</code></strong> â€” Barcode lookups are exact-match queries. A hash index provides O(1) lookup which is ideal. Barcode scanning needs to be fast (&lt;100 ms) for good UX.</li>
</ul>

<p><strong>Read triggers:</strong> User searching for food, scanning barcode, viewing food detail, Meal Logging Service looking up calorie info.<br>
<strong>Write triggers:</strong> Content team adding/updating foods, user submitting custom food, bulk data imports from nutrition databases.</p>

<h4>meal_logs</h4>
<p><span class="badge badge-yellow">NoSQL (Wide-Column / Document)</span> <strong>Why NoSQL:</strong> Extremely high write volume â€” every user logs 3â€“5 meals/day, producing 50K+ writes/sec at peak (meal times). The access pattern is simple: write one entry, read all entries for a user on a given date. No joins needed. The data is naturally partitionable by user. NoSQL's horizontal scaling handles the write throughput that would strain a single SQL server.</p>

<table>
<tr><th>Field</th><th>Type</th><th>Key</th><th>Description</th></tr>
<tr><td><code>log_id</code></td><td>UUID</td><td><strong>Primary Key</strong></td><td>Unique log entry ID</td></tr>
<tr><td><code>user_id</code></td><td>UUID</td><td><strong>Partition Key</strong></td><td>Owning user</td></tr>
<tr><td><code>date</code></td><td>Date</td><td><strong>Sort Key</strong></td><td>Log date</td></tr>
<tr><td><code>meal_type</code></td><td>String</td><td></td><td>"breakfast" | "lunch" | "dinner" | "snack"</td></tr>
<tr><td><code>food_id</code></td><td>String</td><td></td><td>Reference to food_catalog</td></tr>
<tr><td><code>food_name</code></td><td>String</td><td></td><td><strong>Denormalized</strong> from food_catalog</td></tr>
<tr><td><code>servings</code></td><td>Number</td><td></td><td>e.g., 1.5</td></tr>
<tr><td><code>calories</code></td><td>Number</td><td></td><td><strong>Denormalized</strong>: calories_per_serving Ã— servings</td></tr>
<tr><td><code>color_category</code></td><td>String</td><td></td><td><strong>Denormalized</strong> from food_catalog</td></tr>
<tr><td><code>macros</code></td><td>Object</td><td></td><td><strong>Denormalized</strong>: scaled by servings</td></tr>
<tr><td><code>logged_at</code></td><td>Timestamp</td><td></td><td>Actual timestamp of logging</td></tr>
</table>

<div class="callout">
<strong>Denormalization note â€” <code>food_name</code>, <code>calories</code>, <code>color_category</code>, <code>macros</code>:</strong> These fields are copied from the food_catalog at write time rather than looked up on read. This is intentional for three reasons: (1) <strong>Read performance:</strong> Displaying a user's daily meal list is a hot-path operation that happens every time the dashboard loads â€” it must not require a fan-out join to the food_catalog for each entry. (2) <strong>Historical accuracy:</strong> If a food's calorie count is corrected in the catalog, a user's past logs should reflect what they actually logged at the time, not the corrected value. (3) <strong>Decoupling:</strong> The meal_logs store can operate independently of the food_catalog store, improving availability.
</div>

<p><strong>Sharding:</strong> Shard by <code>user_id</code> (consistent hash). This ensures all of a user's meal logs land on the same shard, making per-user queries (get all meals for today) single-shard. The distribution is even because user IDs are UUIDs. The risk of hot shards is low â€” even the most prolific logger produces only tens of entries/day. Meal-time spikes (12â€“1 PM, 6â€“7 PM per timezone) are distributed across all users and thus across all shards.</p>

<p><strong>Read triggers:</strong> User opening dashboard (today's meals), user viewing meal history, Daily Summary Service reconciliation.<br>
<strong>Write triggers:</strong> User logging a meal, user editing a meal entry, user deleting a meal entry.</p>

<h4>weight_logs</h4>
<p><span class="badge badge-yellow">NoSQL</span> <strong>Why NoSQL:</strong> Time-series access pattern (query a range of dates for one user). Simple schema with no joins. High write volume across user base (each user logs 0â€“1 times/day, but across 100M users this is still significant).</p>

<table>
<tr><th>Field</th><th>Type</th><th>Key</th><th>Description</th></tr>
<tr><td><code>log_id</code></td><td>UUID</td><td><strong>Primary Key</strong></td><td></td></tr>
<tr><td><code>user_id</code></td><td>UUID</td><td><strong>Partition Key</strong></td><td></td></tr>
<tr><td><code>date</code></td><td>Date</td><td><strong>Sort Key</strong></td><td></td></tr>
<tr><td><code>weight_kg</code></td><td>Decimal</td><td></td><td></td></tr>
<tr><td><code>source</code></td><td>String</td><td></td><td>"manual" | "smart_scale"</td></tr>
<tr><td><code>logged_at</code></td><td>Timestamp</td><td></td><td></td></tr>
</table>

<p><strong>Sharding:</strong> Shard by <code>user_id</code> (consistent hash). Same rationale as meal_logs â€” all weight data for one user on one shard for efficient range queries.</p>

<p><strong>Read triggers:</strong> User viewing progress graph, coach viewing client progress, Progress Service computing trends.<br>
<strong>Write triggers:</strong> User logging weight, smart scale sync.</p>

<h4>activity_logs</h4>
<p><span class="badge badge-yellow">NoSQL</span> <strong>Why NoSQL:</strong> Flexible schema â€” different activity types have different fields (running has distance, swimming has laps, step counting has step count). Time-series pattern. High write volume from wearable syncs (can produce many entries throughout the day).</p>

<table>
<tr><th>Field</th><th>Type</th><th>Key</th><th>Description</th></tr>
<tr><td><code>log_id</code></td><td>UUID</td><td><strong>Primary Key</strong></td><td></td></tr>
<tr><td><code>user_id</code></td><td>UUID</td><td><strong>Partition Key</strong></td><td></td></tr>
<tr><td><code>date</code></td><td>Date</td><td><strong>Sort Key</strong></td><td></td></tr>
<tr><td><code>activity_type</code></td><td>String</td><td></td><td>"running", "walking", "cycling", etc.</td></tr>
<tr><td><code>duration_minutes</code></td><td>Number</td><td></td><td></td></tr>
<tr><td><code>calories_burned</code></td><td>Number</td><td></td><td></td></tr>
<tr><td><code>steps</code></td><td>Number (optional)</td><td></td><td>Only for step-based activities</td></tr>
<tr><td><code>distance_km</code></td><td>Number (optional)</td><td></td><td>For running, cycling, etc.</td></tr>
<tr><td><code>source</code></td><td>String</td><td></td><td>"manual" | "healthkit" | "google_fit" | "wearable"</td></tr>
<tr><td><code>logged_at</code></td><td>Timestamp</td><td></td><td></td></tr>
</table>

<p><strong>Sharding:</strong> Shard by <code>user_id</code>.</p>
<p><strong>Read triggers:</strong> User viewing activity history, Daily Summary Service.<br>
<strong>Write triggers:</strong> User logging exercise manually, wearable background sync.</p>

<h4 id="schema-daily-summary">daily_summaries</h4>
<p><span class="badge badge-yellow">NoSQL</span> <strong>Why NoSQL:</strong> High read frequency (loaded every time user opens the app) and updated multiple times per day as meals/activities are logged. Simple key-value access pattern (get summary for user X on date Y). No joins.</p>

<table>
<tr><th>Field</th><th>Type</th><th>Key</th><th>Description</th></tr>
<tr><td><code>user_id</code></td><td>UUID</td><td><strong>Partition Key</strong></td><td></td></tr>
<tr><td><code>date</code></td><td>Date</td><td><strong>Sort Key</strong></td><td></td></tr>
<tr><td><code>total_calories_consumed</code></td><td>Number</td><td></td><td>Sum of all meal calories</td></tr>
<tr><td><code>total_calories_burned</code></td><td>Number</td><td></td><td>Sum of all activity calories</td></tr>
<tr><td><code>net_calories</code></td><td>Number</td><td></td><td>consumed - burned</td></tr>
<tr><td><code>calorie_budget</code></td><td>Number</td><td></td><td>User's daily budget (snapshot)</td></tr>
<tr><td><code>green_count</code></td><td>Number</td><td></td><td># green foods logged</td></tr>
<tr><td><code>yellow_count</code></td><td>Number</td><td></td><td># yellow foods logged</td></tr>
<tr><td><code>red_count</code></td><td>Number</td><td></td><td># red foods logged</td></tr>
<tr><td><code>meals_logged</code></td><td>Number</td><td></td><td>Total meal entries today</td></tr>
<tr><td><code>weight_logged</code></td><td>Boolean</td><td></td><td>Did user log weight today?</td></tr>
<tr><td><code>lessons_completed</code></td><td>Number</td><td></td><td># lessons finished today</td></tr>
<tr><td><code>steps</code></td><td>Number</td><td></td><td>Total steps today</td></tr>
<tr><td><code>updated_at</code></td><td>Timestamp</td><td></td><td>Last update time</td></tr>
</table>

<div class="callout">
<strong>Denormalization note â€” entire table:</strong> This table is entirely a denormalized pre-computed aggregate. The source-of-truth data lives in meal_logs, weight_logs, and activity_logs. We maintain this denormalized view because:
<ul>
    <li><strong>Read amplification prevention:</strong> Without this table, rendering the dashboard would require: (1) query all meal_logs for today â†’ sum calories, count colors, (2) query activity_logs for today â†’ sum calories burned, (3) query weight_logs for today â†’ check if logged, (4) query curriculum_progress for today â†’ count completed. That's 4 queries across 4 different tables on every app open. With the denormalized summary, it's 1 query.</li>
    <li><strong>Consistency guarantee:</strong> The summary is updated asynchronously via the Message Queue. This means there can be a brief delay (typically &lt;1 second) between logging a meal and seeing the summary update. This eventual consistency is acceptable because the user can still see the individual meal they just logged; the summary will catch up almost immediately.</li>
    <li><strong>Reconciliation:</strong> A nightly batch job reconciles daily summaries against the source-of-truth tables to fix any drift caused by message processing failures.</li>
</ul>
</div>

<p><strong>Sharding:</strong> Shard by <code>user_id</code>.</p>
<p><strong>Read triggers:</strong> Every app open (dashboard load), widget refresh, coach viewing client summary.<br>
<strong>Write triggers:</strong> Daily Summary Service processing meal_logged / activity_logged / weight_logged / lesson_completed events from Message Queue.</p>

<h4>messages</h4>
<p><span class="badge badge-yellow">NoSQL</span> <strong>Why NoSQL:</strong> Messaging generates very high write volume (append-only). Access pattern is conversation-based pagination (get messages for conversation X, newest first, limit 50). No joins. NoSQL's partition-based design aligns perfectly with conversation-based partitioning.</p>

<table>
<tr><th>Field</th><th>Type</th><th>Key</th><th>Description</th></tr>
<tr><td><code>message_id</code></td><td>UUID</td><td><strong>Primary Key</strong></td><td></td></tr>
<tr><td><code>conversation_id</code></td><td>UUID</td><td><strong>Partition Key</strong></td><td></td></tr>
<tr><td><code>sender_id</code></td><td>UUID</td><td></td><td>User or coach ID</td></tr>
<tr><td><code>sender_type</code></td><td>String</td><td></td><td>"user" | "coach"</td></tr>
<tr><td><code>content</code></td><td>String</td><td></td><td>Message text</td></tr>
<tr><td><code>message_type</code></td><td>String</td><td></td><td>"text" | "image" | "system"</td></tr>
<tr><td><code>sent_at</code></td><td>Timestamp</td><td><strong>Sort Key</strong></td><td>Enables chronological ordering</td></tr>
<tr><td><code>read_at</code></td><td>Timestamp (nullable)</td><td></td><td>When recipient read it</td></tr>
</table>

<p><strong>Sharding:</strong> Shard by <code>conversation_id</code> (consistent hash). All messages for a conversation live on one shard, enabling efficient paginated queries. One-on-one coaching conversations are bounded in size (typically hundreds to low thousands of messages over weeks), so no individual partition grows unbounded. Group conversations are larger but still manageable.</p>

<p><strong>Read triggers:</strong> User/coach opening a chat (paginated load), scrolling up for history, reconnection after offline.<br>
<strong>Write triggers:</strong> User or coach sending a message, marking message as read.</p>

<h4>conversations</h4>
<p><span class="badge badge-yellow">NoSQL</span> <strong>Why NoSQL:</strong> Flexible participant list (1-on-1 or group). Access pattern is per-user "inbox" listing sorted by recency. Updated on every message send.</p>

<table>
<tr><th>Field</th><th>Type</th><th>Key</th><th>Description</th></tr>
<tr><td><code>conversation_id</code></td><td>UUID</td><td><strong>Primary Key</strong></td><td></td></tr>
<tr><td><code>type</code></td><td>String</td><td></td><td>"coaching" | "group"</td></tr>
<tr><td><code>participant_ids</code></td><td>Array</td><td></td><td>List of user/coach IDs</td></tr>
<tr><td><code>last_message_preview</code></td><td>String</td><td></td><td><strong>Denormalized</strong> from last message</td></tr>
<tr><td><code>last_message_at</code></td><td>Timestamp</td><td></td><td><strong>Denormalized</strong> for sort-by-recent</td></tr>
<tr><td><code>created_at</code></td><td>Timestamp</td><td></td><td></td></tr>
</table>

<div class="callout">
<strong>Denormalization note â€” <code>last_message_preview</code> and <code>last_message_at</code>:</strong> These are denormalized from the messages table to avoid an expensive query ("for each conversation, find the most recent message") when rendering the inbox/conversation list. The inbox is loaded frequently and must be fast. The Messaging Service updates these fields atomically when a new message is sent.
</div>

<p><strong>Indexes:</strong></p>
<ul>
    <li><strong>Secondary index on <code>participant_ids</code> (inverted index)</strong> â€” To answer "find all conversations where user X is a participant." This is an inverted index because <code>participant_ids</code> is an array, and we need to find documents where the array contains a specific value. Without this, listing a user's conversations would require scanning all conversations.</li>
</ul>

<p><strong>Read triggers:</strong> User/coach opening inbox (list conversations by recency).<br>
<strong>Write triggers:</strong> Conversation created on coach/group assignment, <code>last_message_preview/at</code> updated on every message.</p>

<h4>curriculum_content</h4>
<p><span class="badge badge-yellow">NoSQL Document Store</span> <strong>Why NoSQL:</strong> Different content types (articles, quizzes, exercises) have radically different structures. Articles have body text, quizzes have question arrays, exercises have step lists. A document store handles this polymorphism naturally. Read-heavy, very infrequent writes (only when the content team publishes).</p>

<table>
<tr><th>Field</th><th>Type</th><th>Key</th><th>Description</th></tr>
<tr><td><code>content_id</code></td><td>String</td><td><strong>Primary Key</strong></td><td></td></tr>
<tr><td><code>week_number</code></td><td>Number</td><td></td><td>Program week (1â€“16)</td></tr>
<tr><td><code>day_number</code></td><td>Number</td><td></td><td>Day within the week (1â€“7)</td></tr>
<tr><td><code>title</code></td><td>String</td><td></td><td></td></tr>
<tr><td><code>content_type</code></td><td>String</td><td></td><td>"article" | "quiz" | "exercise"</td></tr>
<tr><td><code>body</code></td><td>String</td><td></td><td>Rich text / markdown for articles</td></tr>
<tr><td><code>media_urls</code></td><td>Array</td><td></td><td>CDN paths for images/videos</td></tr>
<tr><td><code>quiz_questions</code></td><td>Array (optional)</td><td></td><td><code>[{question, options, correct_answer_index, explanation}]</code></td></tr>
<tr><td><code>exercise_steps</code></td><td>Array (optional)</td><td></td><td><code>[{instruction, duration_seconds}]</code></td></tr>
<tr><td><code>estimated_minutes</code></td><td>Number</td><td></td><td>Estimated completion time</td></tr>
<tr><td><code>order</code></td><td>Number</td><td></td><td>Display order within the day</td></tr>
<tr><td><code>created_at</code></td><td>Timestamp</td><td></td><td></td></tr>
<tr><td><code>updated_at</code></td><td>Timestamp</td><td></td><td></td></tr>
</table>

<p><strong>Indexes:</strong></p>
<ul>
    <li><strong>Composite B-Tree index on (<code>week_number</code>, <code>day_number</code>, <code>order</code>)</strong> â€” The primary query pattern is "get all content for week X, day Y, sorted by order." A composite index on these three fields makes this query an efficient range scan.</li>
</ul>

<p><strong>Read triggers:</strong> User opening daily lessons, Curriculum Service populating cache.<br>
<strong>Write triggers:</strong> Content team publishing or updating content (rare â€” weekly at most).</p>

<h4>curriculum_progress</h4>
<p><span class="badge badge-yellow">NoSQL</span> <strong>Why NoSQL:</strong> Per-user progress tracking. Access pattern is user-centric (get all progress for user X). Simple schema, no joins.</p>

<table>
<tr><th>Field</th><th>Type</th><th>Key</th><th>Description</th></tr>
<tr><td><code>user_id</code></td><td>UUID</td><td><strong>Partition Key</strong></td><td></td></tr>
<tr><td><code>content_id</code></td><td>String</td><td><strong>Sort Key</strong></td><td></td></tr>
<tr><td><code>status</code></td><td>String</td><td></td><td>"not_started" | "in_progress" | "completed"</td></tr>
<tr><td><code>quiz_answers</code></td><td>Array (optional)</td><td></td><td>User's answers for quiz content</td></tr>
<tr><td><code>quiz_score</code></td><td>Number (optional)</td><td></td><td></td></tr>
<tr><td><code>completed_at</code></td><td>Timestamp (nullable)</td><td></td><td></td></tr>
<tr><td><code>current_week</code></td><td>Number</td><td></td><td><strong>Denormalized:</strong> user's current program week</td></tr>
<tr><td><code>current_day</code></td><td>Number</td><td></td><td><strong>Denormalized:</strong> user's current program day</td></tr>
</table>

<div class="callout">
<strong>Denormalization note â€” <code>current_week</code> and <code>current_day</code>:</strong> These could be computed by scanning all progress entries and finding the latest completed lesson's week/day + 1. Storing them directly avoids this scan on every "get today's lessons" request (which happens for every user, every day). They are updated whenever a user completes the last lesson of a day.
</div>

<p><strong>Sharding:</strong> Shard by <code>user_id</code>.</p>
<p><strong>Read triggers:</strong> Curriculum Service determining which lessons to show today.<br>
<strong>Write triggers:</strong> User completing a lesson or quiz.</p>

<!-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• -->
<h2 id="cache">9. CDN &amp; Caching Deep Dive</h2>

<h3>CDN</h3>

<p><strong>What it serves:</strong> All static curriculum media (article images, illustrations, infographics, videos), food images, and app assets (icons, UI illustrations). These files are stored in Object Storage and distributed globally via CDN edge nodes.</p>

<p><strong>Why a CDN is appropriate:</strong></p>
<ul>
    <li>Curriculum images are served to millions of users across the globe. Without a CDN, every image request would hit the origin Object Storage, creating a bottleneck and adding latency for geographically distant users.</li>
    <li>The same images are served to many users (everyone on Week 3, Day 2 sees the same article images), making CDN caching highly effective.</li>
    <li>Media files are immutable (versioned URLs like <code>/curriculum/v2/w3d2/image1.jpg</code>), so aggressive cache TTLs are safe.</li>
</ul>

<p><strong>Cache-Control headers:</strong> <code>Cache-Control: public, max-age=2592000, immutable</code> (30 days). Content is versioned via URL path, so stale cache is never an issue â€” new content gets a new URL.</p>

<h3>In-Memory Cache</h3>

<p>Three distinct cache use cases, each with its own strategy:</p>

<h4>Cache 1: Food Search Cache</h4>
<table>
<tr><th>Property</th><th>Detail</th></tr>
<tr><td><strong>What is cached</strong></td><td>Search query â†’ top-N food results (e.g., key: <code>"search:chicken breast"</code> â†’ value: top 20 food objects)</td></tr>
<tr><td><strong>Caching strategy</strong></td><td><strong>Cache-Aside (Lazy Loading)</strong>: The Food Search Service first checks the cache. On a miss, it queries the Search Index/Food Catalog DB, returns results to the client, and populates the cache with the results. This is chosen over write-through because food data changes infrequently but search queries are highly varied â€” we only want to cache queries that are actually being made.</td></tr>
<tr><td><strong>Eviction policy</strong></td><td><strong>LRU (Least Recently Used)</strong>: Food searches follow a Zipf distribution â€” a small set of common foods ("chicken," "banana," "rice") account for a large proportion of searches. LRU keeps these hot queries in cache and evicts the long tail of rarely-searched foods. LRU is preferred over LFU because new trending foods (e.g., a newly popular health food) should be cacheable quickly even without a high historical frequency.</td></tr>
<tr><td><strong>Expiration policy (TTL)</strong></td><td><strong>24 hours</strong>. Food nutritional data rarely changes. A 24-hour TTL balances freshness (newly verified foods appear within a day) and hit rate (long enough that popular queries stay cached through daily usage patterns).</td></tr>
<tr><td><strong>Population trigger</strong></td><td>Cache-miss during a food search query. Also optionally pre-warmed with the top 1,000 most common queries during deployment.</td></tr>
<tr><td><strong>Estimated size</strong></td><td>~500 MB (top 100K queries Ã— ~5 KB avg result set)</td></tr>
</table>

<h4>Cache 2: Daily Summary Cache</h4>
<table>
<tr><th>Property</th><th>Detail</th></tr>
<tr><td><strong>What is cached</strong></td><td>User's daily summary for today (key: <code>"summary:{user_id}:{date}"</code> â†’ value: daily_summaries document)</td></tr>
<tr><td><strong>Caching strategy</strong></td><td><strong>Write-Through</strong>: When the Daily Summary Service updates the daily summary (after processing a message queue event), it writes to both the Daily Summary DB and the cache simultaneously. This is chosen because the daily summary is read far more frequently than it's written (read on every app open, written only when a meal/activity/weight is logged). Write-through ensures the cache is always warm for the most critical read path (dashboard load).</td></tr>
<tr><td><strong>Eviction policy</strong></td><td><strong>LRU</strong>: Users who haven't opened the app recently will have their summaries evicted in favor of active users. This naturally prioritizes active users who generate the most reads.</td></tr>
<tr><td><strong>Expiration policy (TTL)</strong></td><td><strong>26 hours</strong>. Slightly longer than one day to cover the full day plus a buffer for time zone differences and late-night logging. At midnight (user's timezone), the next day's summary will be created and cached, and the previous day's entry will eventually expire or be evicted.</td></tr>
<tr><td><strong>Population trigger</strong></td><td>Write-through from Daily Summary Service. Also populated on cache-miss if a user opens the app and their summary isn't cached (e.g., after a cache restart).</td></tr>
<tr><td><strong>Estimated size</strong></td><td>~2 GB for 10M daily active users Ã— ~200 bytes per summary</td></tr>
</table>

<h4>Cache 3: Curriculum Content Cache</h4>
<table>
<tr><th>Property</th><th>Detail</th></tr>
<tr><td><strong>What is cached</strong></td><td>Curriculum lesson content for a week/day (key: <code>"curriculum:{week}:{day}"</code> â†’ value: array of lesson documents)</td></tr>
<tr><td><strong>Caching strategy</strong></td><td><strong>Cache-Aside</strong>: The Curriculum Service checks cache first. On miss, queries the Curriculum Content DB and caches the result. Cache-aside is chosen because the total content set is small enough (~16 weeks Ã— 7 days = 112 day-groups) that nearly all will be cached, and content changes are very rare.</td></tr>
<tr><td><strong>Eviction policy</strong></td><td><strong>LRU</strong>: Earlier weeks' content is accessed by more users (as all users pass through weeks 1â€“4), so LRU naturally keeps the most-accessed weeks cached.</td></tr>
<tr><td><strong>Expiration policy (TTL)</strong></td><td><strong>7 days</strong>. Content is static and published on a weekly cycle at most. A 7-day TTL ensures content team changes propagate within a week while keeping a very high cache hit rate.</td></tr>
<tr><td><strong>Population trigger</strong></td><td>Cache-miss when a user on a particular week/day requests content. Can also be pre-warmed by the content publishing pipeline.</td></tr>
<tr><td><strong>Estimated size</strong></td><td>~50 MB (112 day-groups Ã— ~500 KB avg content set, excluding media which is served via CDN)</td></tr>
</table>

<!-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• -->
<h2 id="ws">10. WebSocket Deep Dive</h2>

<h3>Why WebSocket?</h3>
<p>Coaching messaging requires bidirectional, low-latency communication. Here's why WebSocket was chosen over alternatives:</p>

<table>
<tr><th>Alternative</th><th>Why Not Chosen</th></tr>
<tr><td><strong>HTTP Long Polling</strong></td><td>Each "poll" is a full HTTP request/response cycle. For active conversations, this means a new TCP connection (or at least a new HTTP request) every few seconds. This is wasteful and introduces 1â€“5 second latency for message delivery depending on poll interval. It also generates unnecessary server load from constant polling by inactive users.</td></tr>
<tr><td><strong>Server-Sent Events (SSE)</strong></td><td>SSE is server-to-client only (unidirectional). While we could pair it with HTTP POST for client-to-server, this creates an asymmetric design. More importantly, SSE uses a long-lived HTTP connection that counts against the browser's per-domain connection limit (6 connections in most browsers), which is problematic for the coach dashboard where multiple conversations may be open.</td></tr>
<tr><td><strong>Short Polling</strong></td><td>Polling every N seconds is the simplest approach but creates unacceptable latency (average latency = poll_interval/2) and massive wasted traffic. With millions of users, most polls return empty responses, yet each consumes server resources.</td></tr>
<tr><td><strong>WebRTC</strong></td><td>Designed for peer-to-peer media streaming. Overkill for text messaging and adds unnecessary complexity (STUN/TURN servers, NAT traversal). Not suitable for text-based coaching chat.</td></tr>
</table>

<h3>Connection Establishment Flow</h3>
<ol>
    <li><strong>Client initiates:</strong> The mobile app or coach dashboard sends an HTTP/1.1 Upgrade request to the WebSocket Gateway:
        <pre><code>GET /ws/connect HTTP/1.1
Host: ws.noom.com
Upgrade: websocket
Connection: Upgrade
Sec-WebSocket-Key: dGhlIHNhbXBsZSBub25jZQ==
Sec-WebSocket-Version: 13
Authorization: Bearer &lt;JWT&gt;</code></pre>
    </li>
    <li><strong>Authentication:</strong> The WebSocket Gateway validates the JWT token. If invalid, it responds with HTTP 401 and closes the connection. If valid, it extracts the <code>user_id</code> from the token.</li>
    <li><strong>Upgrade response:</strong> The Gateway responds with HTTP 101 Switching Protocols, and the connection is upgraded to WebSocket (bidirectional TCP with frame-based messaging).</li>
    <li><strong>Register in Connection Store:</strong> The Gateway writes to the in-memory Connection Store: <code>key: "conn:{user_id}" â†’ value: {gateway_instance_id: "gw-03", socket_id: "sk-abc123", connected_at: timestamp}</code> with a TTL of 5 minutes.</li>
    <li><strong>Heartbeat loop:</strong> The Gateway sends a WebSocket <code>ping</code> frame every 30 seconds. The client must respond with <code>pong</code>. On each successful pong, the Connection Store TTL is refreshed to 5 minutes. If 3 consecutive pings go unanswered (90 seconds), the Gateway assumes the client disconnected, closes the socket, and removes the Connection Store entry.</li>
</ol>

<h3>How Other WebSockets Are Found (Cross-Gateway Routing)</h3>
<p>When the Messaging Service needs to deliver a message to a user (say, Coach Jane), it:</p>
<ol>
    <li>Looks up <code>"conn:coach_jane_456"</code> in the Connection Store.</li>
    <li>If found, the entry contains <code>gateway_instance_id: "gw-07"</code>.</li>
    <li>The Messaging Service sends an internal RPC to Gateway instance <code>gw-07</code>: "Deliver this message to socket <code>sk-xyz789</code>."</li>
    <li>Gateway <code>gw-07</code> pushes the message to Coach Jane's WebSocket.</li>
    <li>If no Connection Store entry exists (Coach Jane is offline), the Messaging Service enqueues a push notification on the Message Queue instead.</li>
</ol>

<h3>Protocol Details</h3>
<p>WebSocket runs over TCP (not UDP) because message delivery must be reliable and ordered. TCP provides:</p>
<ul>
    <li><strong>Reliable delivery:</strong> Every message must arrive. A dropped coaching message is unacceptable â€” unlike video streaming where a dropped frame is tolerable.</li>
    <li><strong>Ordered delivery:</strong> Messages must arrive in the order they were sent for conversation coherence.</li>
    <li><strong>Congestion control:</strong> TCP's built-in congestion control prevents overwhelming the client on slow networks.</li>
</ul>
<p>UDP was not considered because text messaging does not benefit from UDP's lower overhead â€” the payload sizes are small (text messages) and latency requirements (~300 ms) are easily met by TCP.</p>

<!-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• -->
<h2 id="mq">11. Message Queue Deep Dive</h2>

<h3>Why a Message Queue?</h3>
<p>Several operations triggered by user actions (logging a meal, sending a message, logging weight) have downstream effects that should not block the user-facing response:</p>

<ul>
    <li><strong>Daily summary update</strong> â€” After logging a meal, the Meal Logging Service must update the daily summary. If done synchronously, the user waits for the summary computation + DB write before getting a response. With a queue, the meal log write returns immediately (~20 ms) and the summary updates asynchronously (~200 ms later).</li>
    <li><strong>Push notifications</strong> â€” Sending a push notification involves calling an external service (APNs/FCM) with variable latency (50â€“500 ms). This should never block the sender's chat experience.</li>
    <li><strong>Coach alerts</strong> â€” When a user logs weight or reaches a milestone, the coach may be notified. This is a fire-and-forget operation.</li>
    <li><strong>Analytics</strong> â€” Event tracking for product analytics should never impact user-facing latency.</li>
</ul>

<h3>Why Not Alternatives?</h3>
<table>
<tr><th>Alternative</th><th>Why Not Chosen</th></tr>
<tr><td><strong>Synchronous processing</strong></td><td>Blocks user-facing response. If the Daily Summary DB is slow or Notification Service is down, the meal logging API would fail or timeout â€” even though the meal log itself was written successfully. This violates the principle of partial failure isolation.</td></tr>
<tr><td><strong>Pub/Sub</strong></td><td>Pub/Sub is appropriate when multiple independent consumers need the same event (fan-out). For our use case, most events have a small, known number of consumers (1â€“3). A message queue with topic-based routing achieves the same result with simpler semantics and guaranteed processing (acknowledgments). That said, some implementations blend the two â€” our "message queue" can be thought of as a durable pub/sub with consumer groups if preferred.</td></tr>
<tr><td><strong>Direct async calls (fire-and-forget HTTP)</strong></td><td>No delivery guarantee. If the receiving service is temporarily down, the event is lost. A durable message queue persists events until they are successfully processed and acknowledged.</td></tr>
</table>

<h3>How Messages Are Produced and Consumed</h3>

<h4>Production (Enqueue)</h4>
<ol>
    <li>The producing service (e.g., Meal Logging Service) serializes the event as a JSON message with a topic/routing key:
        <pre><code>{
  "topic": "meal_events",
  "key": "user_123",
  "payload": {
    "event_type": "meal_logged",
    "user_id": "user_123",
    "date": "2025-02-13",
    "log_id": "log_abc",
    "calorie_delta": 280,
    "color_category": "yellow"
  },
  "timestamp": "2025-02-13T12:30:00Z",
  "idempotency_key": "log_abc"
}</code></pre>
    </li>
    <li>The service publishes the message to the queue via the queue's client library. The queue acknowledges receipt once the message is durably persisted (replicated to multiple broker nodes).</li>
    <li>The producing service considers the operation complete once the queue acknowledges. If the queue is unreachable, the service retries with exponential backoff (up to 3 attempts) and, as a last resort, writes the event to a local dead-letter store for later replay.</li>
</ol>

<h4>Consumption (Dequeue)</h4>
<ol>
    <li>Consumer services (Daily Summary Service, Notification Service) subscribe to specific topics. Each topic can have multiple consumer groups for independent processing.</li>
    <li>A consumer pulls (or receives via push) a batch of messages. For each message, it processes the event (e.g., updates the daily summary).</li>
    <li>After successful processing, the consumer <strong>acknowledges</strong> the message. The queue removes it from the pending state.</li>
    <li>If the consumer crashes or doesn't acknowledge within a timeout (e.g., 60 seconds), the queue re-delivers the message to another consumer instance (at-least-once delivery). This means consumers must be <strong>idempotent</strong> â€” processing the same <code>log_id</code> twice should have the same effect as processing it once. The <code>idempotency_key</code> field enables this via deduplication checks.</li>
</ol>

<h4>Topics and Consumer Groups</h4>
<table>
<tr><th>Topic</th><th>Producers</th><th>Consumer Groups</th></tr>
<tr><td><code>meal_events</code></td><td>Meal Logging Service</td><td>Daily Summary Service, Analytics Service</td></tr>
<tr><td><code>weight_events</code></td><td>Weight Service</td><td>Daily Summary Service, Notification Service (milestone alerts), Analytics Service</td></tr>
<tr><td><code>activity_events</code></td><td>Activity Service</td><td>Daily Summary Service, Analytics Service</td></tr>
<tr><td><code>notification_events</code></td><td>Messaging Service, Weight Service, Curriculum Service</td><td>Notification Service</td></tr>
<tr><td><code>curriculum_events</code></td><td>Curriculum Service</td><td>Daily Summary Service, Analytics Service</td></tr>
</table>

<h3>Ordering Guarantees</h3>
<p>Messages are partitioned by <code>user_id</code> (used as the partition key). This guarantees that all events for a given user are processed in order. For example, if a user logs a meal and then deletes it, the Daily Summary Service will process the "meal_logged" event before the "meal_deleted" event, preventing incorrect summary states. Messages for different users can be processed in parallel across partitions for throughput.</p>

<!-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• -->
<h2 id="lb">12. Load Balancer Deep Dive</h2>

<h3>Where Load Balancers Are Placed</h3>
<ol>
    <li><strong>LB-1: Between clients and the API Gateway / HTTP services</strong> â€” Routes HTTP/REST requests from mobile apps and coach dashboards to instances of the Food Search Service, Meal Logging Service, Weight Service, Progress Service, Curriculum Service, User Service, and Activity Service.</li>
    <li><strong>LB-2: Between clients and WebSocket Gateway instances</strong> â€” Routes WebSocket upgrade requests to available WebSocket Gateway instances. Uses a sticky-session or IP-hash strategy so that a client reconnecting (e.g., after a network blip) is routed to the same Gateway instance when possible, reducing the need to re-register in the Connection Store.</li>
    <li><strong>LB-3 (optional): Between internal services</strong> â€” If internal service-to-service communication uses HTTP/gRPC, an internal load balancer (or client-side load balancing via service discovery) distributes calls evenly. For example, the WebSocket Gateway calling the Messaging Service.</li>
</ol>

<h3>Load Balancing Strategy</h3>
<table>
<tr><th>Property</th><th>LB-1 (HTTP)</th><th>LB-2 (WebSocket)</th></tr>
<tr><td><strong>Layer</strong></td><td>Layer 7 (application)</td><td>Layer 7 (application)</td></tr>
<tr><td><strong>Algorithm</strong></td><td>Round-Robin with health checks</td><td>Least-Connections</td></tr>
<tr><td><strong>Why this algorithm</strong></td><td>HTTP requests are short-lived and roughly equal in cost. Round-Robin distributes evenly. Health checks (HTTP GET /health every 10s) remove unhealthy instances from the pool.</td><td>WebSocket connections are long-lived. Round-Robin could lead to uneven distribution if some instances have more stale connections. Least-Connections ensures new connections go to the instance with the fewest active connections, balancing the load over time.</td></tr>
<tr><td><strong>TLS termination</strong></td><td>Yes â€” terminates HTTPS at the LB, forwards HTTP internally to services.</td><td>Yes â€” terminates WSS at the LB, forwards WS internally to Gateway.</td></tr>
<tr><td><strong>Health checks</strong></td><td>HTTP GET /health â†’ 200 OK</td><td>TCP connect + HTTP GET /health â†’ 200 OK</td></tr>
<tr><td><strong>Sticky sessions</strong></td><td>Not needed â€” REST is stateless</td><td>IP-hash preferred â€” helps WebSocket reconnections</td></tr>
</table>

<h3>Why Load Balancers Help with Scale</h3>
<ul>
    <li><strong>Horizontal scaling:</strong> LBs enable adding more service instances to handle increased traffic without changing the client configuration.</li>
    <li><strong>Zero-downtime deployments:</strong> During rolling updates, the LB drains connections from old instances and routes new traffic to updated ones.</li>
    <li><strong>Failure isolation:</strong> If one instance crashes, the LB detects it via health checks and stops routing to it within seconds.</li>
    <li><strong>Geographic distribution:</strong> DNS-based load balancing can route users to the nearest data center (e.g., US East, US West, Europe) before the LB within each data center handles per-instance routing.</li>
</ul>

<!-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• -->
<h2 id="scaling">13. Scaling Considerations</h2>

<h3>Service Scaling</h3>
<table>
<tr><th>Component</th><th>Scaling Strategy</th><th>Key Metric for Auto-Scaling</th></tr>
<tr><td>Food Search Service</td><td>Horizontal â€” add instances behind LB</td><td>Request rate + p95 latency</td></tr>
<tr><td>Meal Logging Service</td><td>Horizontal â€” add instances behind LB</td><td>Request rate (spikes at meal times)</td></tr>
<tr><td>Weight Service</td><td>Horizontal</td><td>Request rate (morning spike)</td></tr>
<tr><td>Curriculum Service</td><td>Horizontal</td><td>Request rate</td></tr>
<tr><td>WebSocket Gateway</td><td>Horizontal â€” add instances behind LB-2</td><td>Active connections per instance (target: 50K per instance)</td></tr>
<tr><td>Messaging Service</td><td>Horizontal</td><td>Message throughput</td></tr>
<tr><td>Daily Summary Service</td><td>Horizontal â€” add consumer instances</td><td>Queue depth / consumer lag</td></tr>
<tr><td>Notification Service</td><td>Horizontal â€” add consumer instances</td><td>Queue depth</td></tr>
</table>

<h3>Database Scaling</h3>
<table>
<tr><th>Database</th><th>Strategy</th><th>Details</th></tr>
<tr><td>Users DB (SQL)</td><td>Vertical + Read replicas + Sharding</td><td>Vertical scaling for write leader; read replicas for read-heavy coach/admin queries; hash-based sharding by user_id for beyond-vertical scale.</td></tr>
<tr><td>Food Catalog DB</td><td>Read replicas</td><td>Write traffic is negligible (new foods are rare). Scale reads with replicas. The dataset fits in memory on modern instances (~10 GB for 10M foods).</td></tr>
<tr><td>Meal Log DB</td><td>Sharding by user_id</td><td>This is the highest-volume write table. Sharding distributes writes evenly. Each shard handles a subset of users.</td></tr>
<tr><td>Daily Summary DB</td><td>Sharding by user_id</td><td>Same write pattern as meal logs (updated on every meal/activity/weight log).</td></tr>
<tr><td>Message DB</td><td>Sharding by conversation_id</td><td>Conversations are independent. Sharding by conversation_id keeps all messages for a chat on one shard.</td></tr>
<tr><td>Search Index</td><td>Horizontal partitioning (index shards)</td><td>The food index can be partitioned by food_id ranges. Each search query is fanned out to all partitions (scatter-gather), but each partition's response is fast.</td></tr>
<tr><td>Cache</td><td>Horizontal (consistent hashing)</td><td>Cache keys are distributed across nodes using consistent hashing. Adding a node invalidates only 1/N of keys.</td></tr>
</table>

<h3>Traffic Patterns and Auto-Scaling</h3>
<p>Noom has predictable traffic spikes aligned with meal times and morning routines:</p>
<ul>
    <li><strong>6:00â€“8:00 AM:</strong> Weight logging surge + breakfast logging + morning lessons</li>
    <li><strong>11:30 AMâ€“1:30 PM:</strong> Lunch logging peak</li>
    <li><strong>5:30â€“7:30 PM:</strong> Dinner logging peak</li>
    <li><strong>Evening:</strong> Coach messaging activity (users check in after work)</li>
</ul>
<p>Auto-scaling rules should pre-warm instances 30 minutes before expected peaks (predictive scaling based on historical patterns) in addition to reactive scaling based on CPU/request rate thresholds. This is particularly important for the Meal Logging Service, Food Search Service, and Daily Summary Service consumer fleet.</p>

<h3>Geographic Distribution</h3>
<p>For a global user base, deploy in at least 3 regions (e.g., US, Europe, Asia-Pacific). Each region has its own:</p>
<ul>
    <li>Set of all services (Food Search, Meal Logging, etc.)</li>
    <li>Read replicas of all databases</li>
    <li>Shard of the cache cluster</li>
    <li>CDN edge nodes (handled by CDN provider)</li>
</ul>
<p>Write operations are routed to the user's home region (where their data shard lives). This avoids cross-region write latency. The tradeoff is that users who travel internationally may experience slightly higher write latency until their requests are routed to their home region.</p>

<!-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• -->
<h2 id="tradeoffs">14. Tradeoffs &amp; Deep Dives</h2>

<h3>1. Denormalization vs. Consistency in Daily Summaries</h3>
<p><strong>Decision:</strong> We heavily denormalize via the <code>daily_summaries</code> table and denormalized fields in <code>meal_logs</code>.</p>
<p><strong>Tradeoff:</strong> <em>Read performance and simplicity</em> vs. <em>data consistency and write complexity</em>. Every update to a meal, weight, or activity log must also update the daily summary. If the Message Queue loses a message or a consumer crashes before acknowledging, the summary drifts from reality. We mitigate this with idempotent consumers, dead-letter queues for failed messages, and a nightly reconciliation batch job that recomputes summaries from source-of-truth tables.</p>
<p><strong>Why this tradeoff is acceptable:</strong> The dashboard is the #1 most-viewed screen. Its load time directly impacts user retention. A &lt;1 second inconsistency window is invisible to users, while the read-time savings (1 query vs. 4) are substantial.</p>

<h3>2. Microservices vs. Monolith</h3>
<p><strong>Decision:</strong> Microservice architecture with separate services for food search, meal logging, weight, curriculum, messaging, notifications, and daily summary.</p>
<p><strong>Tradeoff:</strong> <em>Independent scaling and deployment</em> vs. <em>operational complexity</em>. Microservices require service discovery, distributed tracing, API versioning, and careful handling of distributed transactions. A monolith would be simpler to develop and deploy but would mean that a spike in food searches (which happen during meal times) would compete for resources with the curriculum service (which users access throughout the day).</p>
<p><strong>Why this tradeoff is acceptable:</strong> Noom's services have very different scaling profiles: Food Search is read-heavy with caching, Meal Logging is write-heavy with meal-time spikes, WebSocket Gateway is connection-heavy, and Curriculum is mostly cacheable static content. Independent scaling per service saves significant infrastructure cost.</p>

<h3>3. WebSocket vs. Polling for Coaching Chat</h3>
<p><strong>Decision:</strong> WebSocket for real-time coaching messaging.</p>
<p><strong>Tradeoff:</strong> <em>Real-time experience with low latency</em> vs. <em>infrastructure complexity</em> (managing stateful connections, Connection Store, cross-gateway routing, heartbeats). Polling would be simpler but would introduce 2â€“5 second latency and generate massive unnecessary traffic from inactive users.</p>
<p><strong>Why this tradeoff is acceptable:</strong> The coach-user relationship is central to Noom's value proposition. A real-time, responsive chat experience builds trust and engagement. The complexity of WebSocket infrastructure is a one-time investment that directly supports the product's core differentiator.</p>

<h3>4. NoSQL for Logs vs. SQL with Time-Series Extension</h3>
<p><strong>Decision:</strong> NoSQL for meal_logs, weight_logs, and activity_logs.</p>
<p><strong>Tradeoff:</strong> <em>Write throughput and horizontal scalability</em> vs. <em>loss of SQL features</em> (joins, complex aggregations, flexible ad-hoc queries). If a product team wants to answer "what is the average calorie intake for users in week 4 who lost more than 5 lbs?" this requires a separate analytics pipeline reading from the NoSQL tables into an analytical data warehouse, rather than a simple SQL JOIN.</p>
<p><strong>Why this tradeoff is acceptable:</strong> The operational access patterns for logs are simple (write one entry, read entries for one user on one date). Complex analytical queries are handled by a separate batch/streaming analytics pipeline, which is the standard approach at this scale.</p>

<h3>5. Event-Driven Summary Updates vs. Synchronous Computation</h3>
<p><strong>Decision:</strong> Asynchronous daily summary updates via Message Queue.</p>
<p><strong>Tradeoff:</strong> <em>Fast API responses and fault isolation</em> vs. <em>eventual consistency</em>. When a user logs a meal, they see the response immediately, but the calorie counter on their dashboard may take ~1 second to update. If the Message Queue or Daily Summary Service is temporarily down, summaries become stale (though individual log entries are still correctly stored).</p>
<p><strong>Why this tradeoff is acceptable:</strong> Users see the individual meal log appear in their meal list instantly. The summary counter updating a moment later feels natural. During outages, the stale summary is "wrong but not dangerously wrong" â€” the user can still see their individual logs. The nightly reconciliation corrects any drift.</p>

<!-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• -->
<h2 id="alternatives">15. Alternative Approaches</h2>

<h3>1. Monolithic Architecture</h3>
<p><strong>Approach:</strong> A single deployable application containing all services.</p>
<p><strong>Why not chosen:</strong> Noom's services have dramatically different scaling profiles. The Food Search Service needs to handle burst read traffic during meal times, while the Curriculum Service handles steady, cacheable reads. In a monolith, scaling for food-search spikes means scaling everything, wasting resources. Additionally, a bug in the notification code could bring down meal logging â€” unacceptable for a health app where users need to log meals on time.</p>

<h3>2. SQL for All Tables</h3>
<p><strong>Approach:</strong> Use a SQL database for everything, including meal_logs and messages.</p>
<p><strong>Why not chosen:</strong> At 50K writes/sec for meal logs alone, a single SQL database would require aggressive vertical scaling and sophisticated sharding with limited tooling support. NoSQL databases are purpose-built for this access pattern (partition by user_id, sort by date, high write throughput). SQL's relational features (joins, transactions) are not needed for log data â€” the access pattern is always scoped to a single user on a single date.</p>

<h3>3. Time-Series Database for Logs</h3>
<p><strong>Approach:</strong> Use a dedicated time-series database for meal_logs, weight_logs, and activity_logs.</p>
<p><strong>Why not chosen:</strong> Time-series databases are optimized for high-cardinality metric data (e.g., server CPU every second). Our log data, while time-ordered, is more document-like (each entry has a food_id, name, macros, etc.) than metric-like. A document-oriented NoSQL store with time-based sort keys provides the same efficient range queries while better accommodating the complex nested fields (macros, allergens). That said, a time-series DB could work for weight_logs specifically (simple numeric values over time), but the operational benefit of using one data store for all log types outweighs the marginal efficiency gain.</p>

<h3>4. Server-Sent Events (SSE) + HTTP POST for Messaging</h3>
<p><strong>Approach:</strong> Use SSE for serverâ†’client message delivery and HTTP POST for clientâ†’server message sending.</p>
<p><strong>Why not chosen:</strong> While this avoids WebSocket complexity, it creates an asymmetric architecture where real-time delivery and message sending use different protocols and code paths. Debugging delivery issues becomes harder. Additionally, SSE connections count against the browser's per-domain connection limit, which is problematic for the coach dashboard that may need multiple real-time feeds (multiple client chats, notifications, etc.). WebSocket multiplexes all communication over a single bidirectional connection.</p>

<h3>5. On-the-Fly Daily Summary Computation</h3>
<p><strong>Approach:</strong> Compute the daily summary on every dashboard load by aggregating meal_logs, activity_logs, and weight_logs in real-time. No denormalized summary table.</p>
<p><strong>Why not chosen:</strong> A user who logs 15 items across meals, snacks, and activities would require reading 15+ rows from the meal_logs table, several from activity_logs, and one from weight_logs, then aggregating them â€” on every single app open. With 10M+ daily active users and an average of 5 app opens per day, this is 50M aggregation queries per day. The denormalized approach trades ~200 bytes of storage per user per day for eliminating all of these queries. At scale, the storage cost (~2 GB/day for 10M users) is negligible compared to the compute cost of real-time aggregation.</p>

<h3>6. Graph Database for Social/Group Features</h3>
<p><strong>Approach:</strong> Use a graph database for group memberships and coach-user relationships.</p>
<p><strong>Why not chosen:</strong> The social graph in Noom is simple: each user has one coach (1:1) and belongs to one group (1:many). There are no "friends of friends" traversals, no recommendation algorithms requiring graph traversal, and no complex relationship queries. A simple foreign key in the users table (<code>coach_id</code>, <code>group_id</code>) handles these relationships efficiently in SQL. A graph database would add operational complexity without meaningful query-pattern benefits.</p>

<h3>7. Push Notifications Only (No WebSocket) for Coaching</h3>
<p><strong>Approach:</strong> Deliver all coaching messages via push notifications and let users read/reply via HTTP when they open the app.</p>
<p><strong>Why not chosen:</strong> This eliminates the real-time conversational feel. When both user and coach are online, each message would take 2â€“10 seconds to round-trip through APNs/FCM, versus &lt;300 ms over WebSocket. Coaching conversations are most effective when they feel like a real-time dialogue â€” push-only reduces them to asynchronous email-like exchanges, undermining Noom's core coaching value proposition.</p>

<!-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• -->
<h2 id="additional">16. Additional Considerations</h2>

<h3>HIPAA Compliance</h3>
<p>Noom handles protected health information (PHI): body weight, dietary intake, health goals, and coaching conversations about health conditions. HIPAA compliance requires:</p>
<ul>
    <li><strong>Encryption at rest:</strong> All databases must encrypt data at rest using AES-256. This includes NoSQL stores, SQL databases, object storage, and cache (if persisted).</li>
    <li><strong>Encryption in transit:</strong> All communication uses TLS 1.3 â€” client-to-LB (HTTPS/WSS), LB-to-service (HTTP/WS over TLS), service-to-database (TLS), service-to-message-queue (TLS).</li>
    <li><strong>Access controls:</strong> Role-based access control (RBAC) ensures coaches can only see their assigned clients' data. Admins have audit-logged access. No engineer has direct production database access without an approved access request.</li>
    <li><strong>Audit logging:</strong> Every data access (especially PHI reads) is logged with who, what, when, and why. These logs are stored immutably for 6 years per HIPAA requirements.</li>
    <li><strong>Business Associate Agreements (BAAs):</strong> All third-party services processing PHI (cloud provider, CDN, push notification relay if message content is included) must sign BAAs.</li>
    <li><strong>Data minimization:</strong> Push notification payloads should not contain PHI (e.g., "You have a new message from your coach" rather than the actual message content).</li>
</ul>

<h3>Offline Support</h3>
<p>Users frequently log meals in situations with poor connectivity (restaurants, basements, airplanes). The mobile app implements offline support via:</p>
<ul>
    <li><strong>Local food cache:</strong> The top 1,000 most commonly logged foods are cached locally on-device, enabling food search without connectivity.</li>
    <li><strong>Offline log queue:</strong> Meal, weight, and activity logs created offline are stored in a local SQLite database on the device. When connectivity resumes, the app syncs the queue to the server, replaying each log as an API call with the original <code>logged_at</code> timestamp.</li>
    <li><strong>Conflict resolution:</strong> If the same date/meal_type entry exists both locally and on the server (e.g., user logged on two devices), the server uses last-write-wins based on <code>logged_at</code> timestamp. This is a rare edge case since users typically use one device.</li>
    <li><strong>Curriculum offline reading:</strong> The app pre-fetches tomorrow's lessons while online, allowing users to read lessons without connectivity.</li>
</ul>

<h3>Rate Limiting</h3>
<p>To protect against abuse and ensure fair usage:</p>
<ul>
    <li><strong>API rate limiting:</strong> Per-user rate limits enforced at the API Gateway / Load Balancer level. Example: 100 food search requests/minute, 50 meal log writes/minute, 200 message sends/hour.</li>
    <li><strong>Push notification rate limiting:</strong> Max 10 push notifications per user per hour to prevent notification fatigue.</li>
    <li><strong>Coach messaging rate limiting:</strong> Coaches are limited to 500 messages/hour across all clients to prevent spam and encourage quality interactions.</li>
</ul>

<h3>Data Retention and Deletion</h3>
<ul>
    <li><strong>Active users:</strong> All data retained indefinitely while the account is active (users want to see long-term weight trends).</li>
    <li><strong>Account deletion (GDPR/CCPA):</strong> When a user requests account deletion, all PII and PHI are hard-deleted within 30 days. Aggregated, anonymized data (for product analytics) may be retained. This requires cascading deletes across all NoSQL tables partitioned by <code>user_id</code> â€” a benefit of user-based partitioning.</li>
    <li><strong>Inactive accounts:</strong> Accounts inactive for 3+ years are flagged for data archival to cold storage, reducing active database size.</li>
</ul>

<h3>Monitoring and Observability</h3>
<ul>
    <li><strong>Distributed tracing:</strong> Each request gets a unique trace ID propagated across all services. This enables end-to-end debugging (e.g., "why did this meal log take 2 seconds?" â†’ trace shows the Message Queue was slow).</li>
    <li><strong>Key metrics dashboards:</strong> Food search p50/p95/p99 latency, meal log write throughput, WebSocket active connections, Message Queue depth/lag, daily summary update latency, cache hit rates.</li>
    <li><strong>Alerts:</strong> Page on-call if: food search p95 &gt; 500 ms, Message Queue consumer lag &gt; 10,000 messages, WebSocket Gateway active connections drop by &gt;20% in 5 minutes (indicates mass disconnection), daily summary reconciliation finds &gt;1% drift.</li>
</ul>

<h3>Content Management Pipeline</h3>
<p>The Noom content team (psychologists, nutritionists, writers) authors curriculum content through an internal CMS (Content Management System). Published content flows through a pipeline: CMS â†’ review/approval â†’ Curriculum Content DB write â†’ cache invalidation â†’ CDN pre-warm for media assets. This pipeline is decoupled from the user-facing system and operates on a weekly publishing cadence.</p>

<h3>A/B Testing and Personalization</h3>
<p>Noom's curriculum and food color-coding may be A/B tested (e.g., testing different lesson orderings or different calorie-density thresholds for color categories). The Curriculum Service supports variant assignment via an experiment configuration service. Each user is assigned to a variant at program start, and the variant ID is stored in the <code>users</code> table. The Curriculum Service fetches variant-specific content from the Curriculum Content DB.</p>

<!-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• -->
<h2 id="vendors">17. Vendor Recommendations</h2>

<p>The design above is vendor-agnostic. Below are recommended vendors for each infrastructure component, with rationale.</p>

<h3>SQL Database</h3>
<table>
<tr><th>Vendor</th><th>Rationale</th></tr>
<tr><td><strong>PostgreSQL</strong> (self-managed or cloud-managed)</td><td>Mature, feature-rich SQL database with excellent JSON support for the <code>device_tokens</code> field. Strong community, extensive tooling, and well-supported managed offerings across all major clouds. HIPAA-compliant deployment guides available.</td></tr>
<tr><td><strong>Amazon Aurora (PostgreSQL-compatible)</strong></td><td>If on AWS: Aurora provides automatic storage scaling, up to 15 read replicas with &lt;20 ms replication lag, and automatic failover. HIPAA-eligible. Good fit for the users/coaches/groups tables.</td></tr>
<tr><td><strong>Google Cloud SQL for PostgreSQL</strong></td><td>If on GCP: Managed PostgreSQL with automatic backups, high availability, and HIPAA compliance.</td></tr>
</table>

<h3>NoSQL Database</h3>
<table>
<tr><th>Vendor</th><th>Rationale</th></tr>
<tr><td><strong>Amazon DynamoDB</strong></td><td>Fully managed, auto-scaling, single-digit millisecond latency at any scale. Its partition key + sort key model maps directly to our schema (user_id partition, date sort). On-demand capacity mode handles meal-time spikes without pre-provisioning. DynamoDB Streams can replace or supplement the Message Queue for change data capture. HIPAA-eligible with encryption at rest.</td></tr>
<tr><td><strong>Apache Cassandra</strong> (self-managed or DataStax Astra)</td><td>Excellent write throughput and linear horizontal scaling. Partition key + clustering key aligns with our data model. Good choice if multi-cloud or on-prem deployment is needed. Requires more operational expertise than DynamoDB.</td></tr>
<tr><td><strong>Google Cloud Firestore / Bigtable</strong></td><td>If on GCP: Firestore for document store needs (food_catalog, curriculum_content); Bigtable for high-throughput time-series data (meal_logs, weight_logs, activity_logs).</td></tr>
</table>

<h3>Search Index</h3>
<table>
<tr><th>Vendor</th><th>Rationale</th></tr>
<tr><td><strong>Elasticsearch</strong></td><td>Industry-standard full-text search engine. Excellent inverted index with fuzzy matching, prefix queries, and relevance scoring. Well-suited for food name/brand search. Can be deployed as a managed service (Elastic Cloud, Amazon OpenSearch).</td></tr>
<tr><td><strong>Typesense</strong></td><td>Lighter-weight alternative optimized for instant search (typo-tolerance, prefix search). Lower operational overhead than Elasticsearch. Good fit if the food search use case doesn't require Elasticsearch's advanced analytics features.</td></tr>
<tr><td><strong>Algolia</strong></td><td>Fully managed search-as-a-service with excellent typo tolerance and speed. Higher cost but zero operational overhead. Good for faster time-to-market.</td></tr>
</table>

<h3>In-Memory Cache</h3>
<table>
<tr><th>Vendor</th><th>Rationale</th></tr>
<tr><td><strong>Redis</strong></td><td>Most popular in-memory cache/data store. Supports key-value, hash, sorted set, and pub/sub. Can serve as both the food search cache and the WebSocket Connection Store (using key expiration for TTL). Cluster mode for horizontal scaling. HIPAA-eligible in managed offerings (Amazon ElastiCache, Redis Cloud).</td></tr>
<tr><td><strong>Memcached</strong></td><td>Simpler alternative if only key-value caching is needed (no Connection Store). Higher throughput per node due to multi-threaded architecture. Less feature-rich than Redis.</td></tr>
</table>

<h3>Message Queue</h3>
<table>
<tr><th>Vendor</th><th>Rationale</th></tr>
<tr><td><strong>Apache Kafka</strong></td><td>High-throughput, durable, partitioned log. Partition-by-user_id aligns with our ordering requirements. Consumer groups enable multiple independent consumers (Daily Summary, Notification, Analytics) from the same topic. Excellent for event-driven architectures at Noom's scale. Managed options: Confluent Cloud, Amazon MSK.</td></tr>
<tr><td><strong>Amazon SQS + SNS</strong></td><td>If on AWS: SQS for point-to-point queuing with dead-letter queues; SNS for fan-out to multiple SQS queues. Simpler operationally than Kafka but less flexible for stream processing. Good for smaller scale or faster time-to-market.</td></tr>
<tr><td><strong>RabbitMQ</strong></td><td>Feature-rich message broker with flexible routing. Good fit if advanced routing patterns are needed. Lower throughput ceiling than Kafka at extreme scale.</td></tr>
</table>

<h3>Object Storage</h3>
<table>
<tr><th>Vendor</th><th>Rationale</th></tr>
<tr><td><strong>Amazon S3</strong></td><td>Industry standard for object storage. 11 nines durability, lifecycle policies for archival, versioning, and event notifications. HIPAA-eligible. Integrates with all major CDN providers as an origin.</td></tr>
<tr><td><strong>Google Cloud Storage</strong></td><td>If on GCP: equivalent to S3 with strong consistency and similar durability guarantees.</td></tr>
<tr><td><strong>Azure Blob Storage</strong></td><td>If on Azure: HIPAA-compliant, tiered storage for cost optimization.</td></tr>
</table>

<h3>CDN</h3>
<table>
<tr><th>Vendor</th><th>Rationale</th></tr>
<tr><td><strong>CloudFront</strong></td><td>If on AWS: tight integration with S3, Lambda@Edge for dynamic content at the edge, global PoP network.</td></tr>
<tr><td><strong>Cloudflare</strong></td><td>Cloud-agnostic CDN with excellent performance, DDoS protection, and Workers for edge compute. Good pricing model.</td></tr>
<tr><td><strong>Fastly</strong></td><td>Real-time purging (useful if food images or curriculum content need immediate updates), high-performance edge network.</td></tr>
</table>

<h3>Push Notification Relay</h3>
<table>
<tr><th>Vendor</th><th>Rationale</th></tr>
<tr><td><strong>Firebase Cloud Messaging (FCM)</strong></td><td>Required for Android push notifications. Free. Also supports iOS (as a proxy to APNs). Simplifies cross-platform notification delivery.</td></tr>
<tr><td><strong>Apple Push Notification service (APNs)</strong></td><td>Required for iOS push notifications. Direct integration provides the most reliable iOS delivery.</td></tr>
</table>

<hr>

<p style="text-align:center; color:#888; margin-top:3rem; font-size:.9rem;">
    System Design Document â€” Noom &nbsp;|&nbsp; Generated February 2025
</p>

</body>
</html>
