<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>System Design: Instacart</title>
    <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
    <script>mermaid.initialize({startOnLoad:true, theme:'default', securityLevel:'loose'});</script>
    <style>
        :root {
            --bg: #ffffff;
            --fg: #1a1a2e;
            --accent: #16213e;
            --border: #ddd;
            --code-bg: #f5f5f5;
            --highlight: #e8f4f8;
            --table-header: #2c3e50;
            --table-alt: #f9f9f9;
            --green: #27ae60;
            --blue: #2980b9;
            --orange: #e67e22;
        }
        * { box-sizing: border-box; margin: 0; padding: 0; }
        body { font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; color: var(--fg); line-height: 1.7; max-width: 1100px; margin: 0 auto; padding: 40px 24px; background: var(--bg); }
        h1 { font-size: 2.4em; border-bottom: 4px solid var(--accent); padding-bottom: 12px; margin-bottom: 8px; }
        h2 { font-size: 1.8em; margin-top: 48px; margin-bottom: 12px; border-bottom: 2px solid var(--border); padding-bottom: 8px; color: var(--accent); }
        h3 { font-size: 1.35em; margin-top: 32px; margin-bottom: 8px; color: #34495e; }
        h4 { font-size: 1.1em; margin-top: 20px; margin-bottom: 6px; color: #555; }
        p, li { margin-bottom: 8px; }
        ul, ol { padding-left: 24px; margin-bottom: 16px; }
        .subtitle { color: #666; font-size: 1.1em; margin-bottom: 32px; }
        .toc { background: var(--highlight); border-radius: 8px; padding: 20px 28px; margin: 24px 0 40px 0; }
        .toc h2 { margin-top: 0; border: none; font-size: 1.4em; }
        .toc ol { margin-bottom: 0; }
        .toc li { margin-bottom: 4px; }
        .toc a { color: var(--blue); text-decoration: none; }
        .toc a:hover { text-decoration: underline; }
        .diagram-container { background: #fafafa; border: 1px solid var(--border); border-radius: 8px; padding: 20px; margin: 20px 0; overflow-x: auto; }
        .example-box { background: #fffbea; border-left: 4px solid var(--orange); padding: 16px 20px; margin: 16px 0; border-radius: 0 8px 8px 0; }
        .example-box strong { color: var(--orange); }
        .component-box { background: var(--highlight); border-left: 4px solid var(--blue); padding: 16px 20px; margin: 12px 0; border-radius: 0 8px 8px 0; }
        .component-box h4 { margin-top: 0; color: var(--blue); }
        table { width: 100%; border-collapse: collapse; margin: 16px 0 24px 0; font-size: 0.95em; }
        th { background: var(--table-header); color: #fff; padding: 10px 14px; text-align: left; }
        td { padding: 10px 14px; border-bottom: 1px solid var(--border); }
        tr:nth-child(even) { background: var(--table-alt); }
        code { background: var(--code-bg); padding: 2px 6px; border-radius: 4px; font-size: 0.92em; }
        .tag { display: inline-block; background: var(--blue); color: #fff; padding: 2px 10px; border-radius: 12px; font-size: 0.82em; margin-right: 4px; }
        .tag.green { background: var(--green); }
        .tag.orange { background: var(--orange); }
        .callout { background: #eaf7ea; border: 1px solid #b7e4b7; padding: 14px 18px; border-radius: 8px; margin: 16px 0; }
        .callout.warn { background: #fff3e0; border-color: #ffe0b2; }
        .schema-section { margin-bottom: 32px; }
        .schema-section h4 { border-bottom: 1px dashed var(--border); padding-bottom: 4px; }
        hr { border: none; border-top: 1px solid var(--border); margin: 32px 0; }
    </style>
</head>
<body>

<h1>System Design: Instacart</h1>
<p class="subtitle">On-demand grocery delivery platform â€” comprehensive system architecture</p>

<!-- ============================================================ -->
<!-- TABLE OF CONTENTS -->
<!-- ============================================================ -->
<div class="toc">
    <h2>Table of Contents</h2>
    <ol>
        <li><a href="#fr">Functional Requirements</a></li>
        <li><a href="#nfr">Non-Functional Requirements</a></li>
        <li><a href="#flow1">Flow 1 â€” Store Discovery &amp; Product Browsing / Search</a></li>
        <li><a href="#flow2">Flow 2 â€” Cart Management &amp; Order Placement</a></li>
        <li><a href="#flow3">Flow 3 â€” Order Fulfillment &amp; Real-Time Tracking</a></li>
        <li><a href="#flow4">Flow 4 â€” Shopper-Customer Real-Time Chat</a></li>
        <li><a href="#combined">Combined Overall Flow Diagram</a></li>
        <li><a href="#schema">Database Schema</a></li>
        <li><a href="#cdn-cache">CDN &amp; Cache Deep Dive</a></li>
        <li><a href="#websocket">WebSocket Deep Dive</a></li>
        <li><a href="#mq">Message Queue Deep Dive</a></li>
        <li><a href="#lb">Load Balancer Deep Dive</a></li>
        <li><a href="#scaling">Scaling Considerations</a></li>
        <li><a href="#tradeoffs">Tradeoffs &amp; Deep Dives</a></li>
        <li><a href="#alternatives">Alternative Approaches</a></li>
        <li><a href="#additional">Additional Considerations</a></li>
        <li><a href="#vendors">Vendor Section</a></li>
    </ol>
</div>

<!-- ============================================================ -->
<!-- 1. FUNCTIONAL REQUIREMENTS -->
<!-- ============================================================ -->
<h2 id="fr">1. Functional Requirements</h2>
<ol>
    <li><strong>Store Discovery</strong> â€” Users can discover nearby grocery stores based on their delivery address (geolocation-based lookup).</li>
    <li><strong>Product Browsing &amp; Search</strong> â€” Users can browse store catalogs by category/subcategory and perform full-text search with typo tolerance.</li>
    <li><strong>Cart Management</strong> â€” Users can add, update, and remove items from a per-store shopping cart.</li>
    <li><strong>Order Placement &amp; Payment</strong> â€” Users can checkout, select a delivery window (immediate or scheduled), apply promo codes, tip the shopper, and pay via stored payment methods.</li>
    <li><strong>Shopper Matching &amp; Assignment</strong> â€” The system matches placed orders with available personal shoppers based on proximity, workload, and batch optimization.</li>
    <li><strong>Order Fulfillment</strong> â€” Shoppers accept orders, shop items in-store (marking items found/not-found), suggest substitutions, checkout, and deliver.</li>
    <li><strong>Item Substitution Flow</strong> â€” When a shopper finds an item unavailable, they propose a substitute; the customer approves or rejects in real time.</li>
    <li><strong>Real-Time Order Tracking</strong> â€” Customers track the shopper's progress through each stage (shopping â†’ checkout â†’ en route â†’ delivered) with live map location during delivery.</li>
    <li><strong>Shopper-Customer Chat</strong> â€” During an active order, the shopper and customer can exchange text and photo messages in real time.</li>
    <li><strong>Ratings &amp; Reviews</strong> â€” After delivery, customers rate the shopper and can rate individual products.</li>
    <li><strong>Notifications</strong> â€” Push notifications for order status changes, item substitutions, promotions, and chat messages when the app is backgrounded.</li>
    <li><strong>Order History</strong> â€” Customers can view past orders and quickly re-order.</li>
</ol>

<!-- ============================================================ -->
<!-- 2. NON-FUNCTIONAL REQUIREMENTS -->
<!-- ============================================================ -->
<h2 id="nfr">2. Non-Functional Requirements</h2>
<ol>
    <li><strong>Low Latency</strong> â€” Product search results must be returned in &lt;200 ms; store browsing pages in &lt;300 ms; real-time tracking updates delivered in &lt;1 s.</li>
    <li><strong>High Availability</strong> â€” 99.99% uptime target. Order placement and payment flows must be resilient to individual service failures.</li>
    <li><strong>Scalability</strong> â€” Must handle 10Ã— traffic spikes (holidays, severe weather, pandemic surges). Individual services scale independently.</li>
    <li><strong>Data Durability &amp; Consistency</strong> â€” Orders and payments require strong consistency (ACID). Product catalog and inventory can tolerate eventual consistency.</li>
    <li><strong>Fault Tolerance</strong> â€” Graceful degradation: if Search Service is down, browsing by category still works. If chat is down, push notifications still deliver substitution requests.</li>
    <li><strong>Security</strong> â€” All traffic over TLS. Payment data PCI-DSS compliant. User data GDPR/CCPA compliant. Token-based authentication (JWT/OAuth 2.0).</li>
    <li><strong>Real-Time Communication</strong> â€” WebSocket connections for order tracking and chat must support millions of concurrent connections across server fleet.</li>
    <li><strong>Geo-Distribution</strong> â€” CDN edge caches and multi-region deployment to minimize latency for nationwide coverage.</li>
</ol>

<!-- ============================================================ -->
<!-- 3. FLOW 1 â€” STORE DISCOVERY & PRODUCT BROWSING / SEARCH -->
<!-- ============================================================ -->
<h2 id="flow1">3. Flow 1 â€” Store Discovery &amp; Product Browsing / Search</h2>

<div class="diagram-container">
<pre class="mermaid">
graph LR
    Client["ðŸ“± Client App"] -->|HTTPS| LB["âš–ï¸ Load Balancer"]
    LB --> GW["ðŸ”€ API Gateway"]

    GW -->|"GET /stores?lat=x&lng=y"| SS["Store Service"]
    GW -->|"GET /stores/{id}/products?category=x"| CS["Catalog Service"]
    GW -->|"GET /search?q=term&store_id=x"| SRS["Search Service"]

    SS --> SC["ðŸ—‚ï¸ Cache"]
    SC -.->|cache miss| SDB[("Store DB\n(SQL)")]

    CS --> PC["ðŸ—‚ï¸ Cache"]
    PC -.->|cache miss| PDB[("Product DB\n(Document NoSQL)")]
    CS --> CDN["ðŸŒ CDN"]
    CDN --> OS[("Object Storage")]

    SRS --> SI[("Search Index\n(Inverted Index)")]
    PDB -.->|CDC / re-index| SI
</pre>
</div>

<h3>Examples</h3>

<div class="example-box">
    <strong>Example 1 â€” Store Discovery:</strong> Alice opens the Instacart app and enters her delivery address "123 Main St, San Francisco." The client sends an <code>HTTPS GET /stores?lat=37.78&amp;lng=-122.41&amp;radius=10</code> to the Load Balancer, which routes it through the API Gateway to the Store Service. The Store Service checks the Cache for nearby stores at this geolocation. On a cache miss, it queries the Store DB using a geospatial R-tree index on <code>(lat, lng)</code>, returning a list of 12 nearby stores (Safeway, Costco, Whole Foods, etc.) with logos, distances, and estimated delivery times. The response is written to the cache (TTL 10 min) and returned to Alice.
</div>

<div class="example-box">
    <strong>Example 2 â€” Category Browsing:</strong> Alice taps on "Whole Foods" and lands on the store home page. The client sends <code>HTTPS GET /stores/wf-123/products?category=fruits</code> to the Catalog Service. The Catalog Service checks the Cache (keyed by <code>store_id:category</code>). On a cache hit, it returns 48 fruit products immediately. Product image URLs in the response point to the CDN, which serves images from Object Storage at the nearest edge location to Alice.
</div>

<div class="example-box">
    <strong>Example 3 â€” Full-Text Search:</strong> Alice types "orgnic bnanas" (with typos) in the search bar. The client sends <code>HTTPS GET /search?q=orgnic+bnanas&amp;store_id=wf-123</code> to the Search Service. The Search Service queries the inverted index with fuzzy matching and typo tolerance, correctly resolving to "organic bananas." It returns ranked results including "Organic Bananas," "Organic Baby Bananas," and "Organic Banana Chips," each with price, image CDN URL, and availability status.
</div>

<h3>Component Deep Dive</h3>

<div class="component-box">
    <h4>Store Service</h4>
    <p><strong>Protocol:</strong> HTTP REST</p>
    <ul>
        <li><code>GET /stores?lat={lat}&amp;lng={lng}&amp;radius={miles}</code> â€” Returns list of active stores within radius. Input: latitude, longitude, radius. Output: array of <code>{store_id, name, chain_name, address, distance_miles, logo_url, estimated_delivery_min, operating_hours}</code>.</li>
        <li><code>GET /stores/{store_id}</code> â€” Returns detailed store information. Input: store_id. Output: full store object with operating hours, supported delivery windows, and minimum order amount.</li>
    </ul>
    <p>The Store Service performs geospatial queries against the SQL Store DB using an R-tree index on latitude/longitude. Results are cached per geohash bucket with a 10-minute TTL.</p>
</div>

<div class="component-box">
    <h4>Catalog Service</h4>
    <p><strong>Protocol:</strong> HTTP REST</p>
    <ul>
        <li><code>GET /stores/{store_id}/products?category={cat}&amp;page={n}&amp;size={n}</code> â€” Returns paginated product listings. Input: store_id, optional category/subcategory, pagination params. Output: array of <code>{product_id, name, brand, price, unit, image_url, is_available, is_organic}</code>.</li>
        <li><code>GET /products/{product_id}</code> â€” Returns full product details. Input: product_id. Output: <code>{product_id, name, description, brand, price, unit, image_urls[], nutritional_info{}, tags[], aisle_number, is_available}</code>.</li>
    </ul>
    <p>Reads from the Document NoSQL Product DB. Product listings are cached per <code>store_id:category</code> with a 5-minute TTL. Product images are served through the CDN backed by Object Storage.</p>
</div>

<div class="component-box">
    <h4>Search Service</h4>
    <p><strong>Protocol:</strong> HTTP REST</p>
    <ul>
        <li><code>GET /search?q={query}&amp;store_id={id}&amp;page={n}&amp;size={n}</code> â€” Full-text product search scoped to a store. Input: query string, store_id, pagination. Output: ranked array of product objects with relevance scores.</li>
    </ul>
    <p>Backed by a dedicated Search Index (inverted index) that is populated via Change Data Capture (CDC) from the Product DB. Supports fuzzy matching, stemming, synonym expansion (e.g., "soda" â†’ "soft drink"), and typo tolerance. The index is partitioned by <code>store_id</code> for efficient scoped searches.</p>
</div>

<div class="component-box">
    <h4>Cache (Product &amp; Store)</h4>
    <p>In-memory key-value cache sitting in front of the Store DB and Product DB. Strategy: <strong>cache-aside (lazy loading)</strong>. On a request, the service checks the cache first; on miss, it queries the DB and populates the cache. Details in the <a href="#cdn-cache">CDN &amp; Cache Deep Dive</a>.</p>
</div>

<div class="component-box">
    <h4>CDN &amp; Object Storage</h4>
    <p>Product images, store logos, and promotional banners are stored in Object Storage and served via a pull-based CDN. The CDN caches images at edge locations worldwide with long TTLs (24 hours). Details in the <a href="#cdn-cache">CDN &amp; Cache Deep Dive</a>.</p>
</div>

<!-- ============================================================ -->
<!-- 4. FLOW 2 â€” CART MANAGEMENT & ORDER PLACEMENT -->
<!-- ============================================================ -->
<h2 id="flow2">4. Flow 2 â€” Cart Management &amp; Order Placement</h2>

<div class="diagram-container">
<pre class="mermaid">
graph LR
    Client["ðŸ“± Client App"] -->|HTTPS| LB["âš–ï¸ Load Balancer"]
    LB --> GW["ðŸ”€ API Gateway"]

    GW -->|"POST/PUT/DELETE /cart/items"| CartS["Cart Service"]
    GW -->|"POST /orders"| OrdS["Order Service"]

    CartS --> CartDB[("Cart Store\n(Key-Value NoSQL)")]

    OrdS --> OrdDB[("Order DB\n(SQL)")]
    OrdS --> PayS["Payment Service"]
    PayS --> PayGW["Payment Gateway\n(External)"]

    OrdS --> MQ["ðŸ“¨ Message Queue"]
    MQ --> ShopMatch["Shopper Matching\nService"]
    MQ --> InvS["Inventory Service"]
    MQ --> NotifS["Notification\nService"]

    InvS --> InvDB[("Inventory DB\n(NoSQL)")]
    ShopMatch --> ShopDB[("Shopper DB\n(SQL)")]
    NotifS -->|push notification| ShopApp["ðŸ“± Shopper App"]
</pre>
</div>

<h3>Examples</h3>

<div class="example-box">
    <strong>Example 1 â€” Adding to Cart:</strong> Alice taps "Add to Cart" on Organic Bananas ($2.49/bunch). The client sends <code>HTTPS POST /cart/items</code> with body <code>{store_id: "wf-123", product_id: "ban-001", quantity: 2}</code>. The Cart Service upserts the item in Alice's cart in the Key-Value NoSQL Cart Store (keyed by <code>user_id</code>). The cart is ephemeral data with a 7-day TTL. If Alice adds another item from a different store, a new cart is created for that store. The response returns the updated cart with subtotal.
</div>

<div class="example-box">
    <strong>Example 2 â€” Order Placement (Happy Path):</strong> Alice taps "Checkout." The client sends <code>HTTPS POST /orders</code> with body <code>{cart_id: "cart-abc", delivery_address_id: "addr-1", delivery_window: "2pm-3pm", payment_method_id: "pm-visa", tip_amount: 5.00}</code>. The Order Service: (1) validates the cart contents against current inventory, (2) creates an order record in the SQL Order DB with status <code>CREATED</code>, (3) calls the Payment Service to authorize the total ($47.83), (4) on successful authorization, updates order status to <code>PENDING_ASSIGNMENT</code>, (5) publishes an <code>order.created</code> event to the Message Queue. The Shopper Matching Service consumes this event and finds an available shopper (Bob) within 3 miles of Whole Foods. It assigns Bob and updates order status to <code>ASSIGNED</code>. The Notification Service consumes the event and sends push notifications to both Alice ("Your order has been placed!") and Bob ("New order available at Whole Foods").
</div>

<div class="example-box">
    <strong>Example 3 â€” Payment Failure:</strong> Alice tries to checkout, but her credit card is declined. The Payment Service returns <code>status: DECLINED</code>. The Order Service does not create a permanent order record, returns HTTP 402 to the client with error details, and the client prompts Alice to try a different payment method. No message is published to the queue.
</div>

<div class="example-box">
    <strong>Example 4 â€” Scheduled Delivery:</strong> Alice selects "Tomorrow 10amâ€“11am" as the delivery window. The Order Service creates the order with <code>delivery_window_start: "2024-03-15T10:00"</code> and status <code>SCHEDULED</code>. The Shopper Matching Service does not immediately assign a shopper. Instead, a scheduled job triggers 30â€“45 minutes before the window opens, finds an available shopper, and proceeds with assignment.
</div>

<h3>Component Deep Dive</h3>

<div class="component-box">
    <h4>Cart Service</h4>
    <p><strong>Protocol:</strong> HTTP REST</p>
    <ul>
        <li><code>GET /cart?store_id={id}</code> â€” Returns current cart for a user+store. Input: store_id (from query param), user_id (from auth token). Output: <code>{cart_id, store_id, items[{product_id, name, quantity, unit_price}], subtotal, updated_at}</code>.</li>
        <li><code>POST /cart/items</code> â€” Adds an item to the cart. Input: <code>{store_id, product_id, quantity}</code>. Output: updated cart object.</li>
        <li><code>PUT /cart/items/{product_id}</code> â€” Updates quantity. Input: <code>{quantity}</code>. Output: updated cart object.</li>
        <li><code>DELETE /cart/items/{product_id}</code> â€” Removes item from cart. Output: updated cart object.</li>
    </ul>
    <p>Backed by a Key-Value NoSQL store for high throughput reads/writes. Cart data is ephemeral (7-day TTL). The cart is keyed by <code>user_id:store_id</code>. This is an excellent fit for a key-value store because the access pattern is always by user and the schema is simple.</p>
</div>

<div class="component-box">
    <h4>Order Service</h4>
    <p><strong>Protocol:</strong> HTTP REST</p>
    <ul>
        <li><code>POST /orders</code> â€” Places a new order. Input: <code>{cart_id, delivery_address_id, delivery_window, payment_method_id, tip_amount, special_instructions}</code>. Output: <code>{order_id, status, estimated_delivery, total_amount}</code>.</li>
        <li><code>GET /orders/{order_id}</code> â€” Returns order details. Output: full order object with items, status history, shopper info.</li>
        <li><code>GET /orders?user_id={id}&amp;page={n}</code> â€” Returns paginated order history.</li>
        <li><code>PATCH /orders/{order_id}/status</code> â€” Updates order status (used by Shopper App). Input: <code>{status, metadata}</code>. Output: updated order.</li>
        <li><code>PATCH /orders/{order_id}/items/{item_id}</code> â€” Updates item status (found/not-found/replaced). Input: <code>{status, replacement_product_id}</code>. Output: updated order item.</li>
    </ul>
    <p>Central orchestrator for order lifecycle. Uses SQL Order DB for ACID guarantees. Publishes events to the Message Queue for downstream consumers. Implements an order state machine: <code>CREATED â†’ PENDING_ASSIGNMENT â†’ ASSIGNED â†’ SHOPPING â†’ CHECKOUT â†’ DELIVERING â†’ DELIVERED</code> (with <code>CANCELLED</code> as a terminal state from any pre-delivery stage).</p>
</div>

<div class="component-box">
    <h4>Payment Service</h4>
    <p><strong>Protocol:</strong> HTTP REST (internal service-to-service)</p>
    <ul>
        <li><code>POST /payments/authorize</code> â€” Authorizes a payment hold. Input: <code>{order_id, user_id, amount, payment_method_id}</code>. Output: <code>{payment_id, status: AUTHORIZED|DECLINED, authorization_code}</code>.</li>
        <li><code>POST /payments/{payment_id}/capture</code> â€” Captures (finalizes) the authorized amount after delivery. Input: <code>{final_amount}</code> (may differ from authorization if items substituted). Output: <code>{status: CAPTURED}</code>.</li>
        <li><code>POST /payments/{payment_id}/refund</code> â€” Issues partial/full refund. Input: <code>{amount, reason}</code>. Output: <code>{status: REFUNDED}</code>.</li>
    </ul>
    <p>Wraps external Payment Gateway integration. Implements two-phase payment: authorize at checkout, capture after delivery (when final total is known after substitutions/refunds). All payment data stored in SQL for ACID compliance and PCI-DSS audit trails.</p>
</div>

<div class="component-box">
    <h4>Shopper Matching Service</h4>
    <p><strong>Protocol:</strong> Event-driven (consumes from Message Queue) + HTTP REST for internal queries</p>
    <p>Consumes <code>order.created</code> events from the queue. Matches orders to available shoppers using a scoring algorithm that considers: (1) shopper proximity to the store (geo distance), (2) shopper current workload, (3) shopper ratings, (4) batch optimization â€” can batch 2â€“3 orders from the same store to a single shopper for efficiency. Updates the Order DB with the assigned shopper and publishes <code>order.assigned</code> event.</p>
</div>

<div class="component-box">
    <h4>Inventory Service</h4>
    <p><strong>Protocol:</strong> Event-driven + HTTP REST</p>
    <ul>
        <li><code>GET /inventory?store_id={id}&amp;product_id={id}</code> â€” Checks current availability. Output: <code>{quantity_available, last_synced_at}</code>.</li>
    </ul>
    <p>Manages inventory data in a NoSQL key-value store keyed by <code>store_id:product_id</code>. Inventory is synced periodically from store partners (batch jobs) and updated in real-time when shoppers report items as unavailable. Because most grocery stores lack real-time inventory APIs, this service provides "best effort" availability â€” shoppers serve as the ground-truth validation layer.</p>
</div>

<div class="component-box">
    <h4>Notification Service</h4>
    <p><strong>Protocol:</strong> Event-driven (consumes from Message Queue)</p>
    <p>Consumes order lifecycle events and sends push notifications via APNs (Apple) and FCM (Google) to customer and shopper devices. Also triggers email/SMS for critical events (order confirmation, delivery complete). Notification preferences are stored per user. Details on the queue interaction in the <a href="#mq">Message Queue Deep Dive</a>.</p>
</div>

<!-- ============================================================ -->
<!-- 5. FLOW 3 â€” ORDER FULFILLMENT & REAL-TIME TRACKING -->
<!-- ============================================================ -->
<h2 id="flow3">5. Flow 3 â€” Order Fulfillment &amp; Real-Time Tracking</h2>

<div class="diagram-container">
<pre class="mermaid">
graph LR
    SA["ðŸ“± Shopper App"] -->|HTTPS| LB["âš–ï¸ Load Balancer"]
    LB --> GW["ðŸ”€ API Gateway"]

    GW -->|"PATCH /orders/{id}/status"| OrdS["Order Service"]
    GW -->|"PATCH /orders/{id}/items/{id}"| OrdS

    SA -->|"WebSocket: location updates"| WSS["WebSocket\nServer"]
    WSS --> LocS["Location Service"]
    LocS --> LocDB[("Location Store\n(Time-Series NoSQL)")]

    LocS --> PubSub["Pub/Sub"]
    PubSub --> WSS2["WebSocket\nServer(s)"]
    WSS2 -->|"real-time location\nto customer"| Client["ðŸ“± Client App"]

    OrdS --> OrdDB[("Order DB\n(SQL)")]
    OrdS --> MQ["ðŸ“¨ Message Queue"]
    MQ --> NotifS["Notification\nService"]
    NotifS -->|push notification| Client
    NotifS -->|push notification| SA
</pre>
</div>

<h3>Examples</h3>

<div class="example-box">
    <strong>Example 1 â€” Shopper Begins Shopping:</strong> Bob (the assigned shopper) taps "Start Shopping" on his Shopper App. The app sends <code>HTTPS PATCH /orders/ord-456/status</code> with <code>{status: "SHOPPING"}</code>. The Order Service updates the order in the SQL DB and publishes an <code>order.status_changed</code> event to the Message Queue. The Notification Service consumes this event and sends a push notification to Alice: "Your shopper Bob has started shopping at Whole Foods!" Alice opens the app and sees a live order status page.
</div>

<div class="example-box">
    <strong>Example 2 â€” Item Found / Checked Off:</strong> Bob locates the Organic Bananas and taps "Found" next to the item. The app sends <code>HTTPS PATCH /orders/ord-456/items/item-789</code> with <code>{status: "FOUND"}</code>. The Order Service updates the order item status. Alice sees the item checked off in real-time on her order tracking screen (via the WebSocket connection that carries order status micro-updates).
</div>

<div class="example-box">
    <strong>Example 3 â€” Item Substitution:</strong> Bob cannot find "Organic Whole Milk 1gal." He taps "Can't Find" and selects a substitute: "Organic 2% Milk 1gal ($5.99)." The app sends <code>HTTPS PATCH /orders/ord-456/items/item-790</code> with <code>{status: "SUBSTITUTE_PROPOSED", replacement_product_id: "milk-2pct"}</code>. The Order Service publishes a <code>substitution.proposed</code> event. Alice receives a push notification: "Bob suggests replacing Organic Whole Milk with Organic 2% Milk ($5.99). Approve?" Alice taps "Approve," sending <code>HTTPS PATCH /orders/ord-456/items/item-790</code> with <code>{status: "REPLACED"}</code>. If Alice doesn't respond within 10 minutes, the system auto-approves the substitution (configurable per user preference).
</div>

<div class="example-box">
    <strong>Example 4 â€” Real-Time Delivery Tracking:</strong> Bob finishes checkout and taps "Start Delivery." The order moves to <code>DELIVERING</code> status. Bob's Shopper App opens a persistent WebSocket connection to the WebSocket Server and begins sending GPS location updates every 5 seconds: <code>{shopper_id: "bob-1", lat: 37.785, lng: -122.409, order_id: "ord-456"}</code>. The WebSocket Server forwards these to the Location Service, which writes to the Time-Series Location Store and publishes the update to the Pub/Sub system. Alice's Client App, which has an open WebSocket connection (possibly on a different WebSocket server instance), receives these updates via Pub/Sub fan-out and renders Bob's position on a live map. Alice sees Bob moving toward her house in real time.
</div>

<div class="example-box">
    <strong>Example 5 â€” Delivery Completion:</strong> Bob arrives at Alice's door, hands off the groceries, and taps "Mark Delivered." <code>PATCH /orders/ord-456/status</code> with <code>{status: "DELIVERED"}</code>. The Order Service updates the order, triggers a <code>payment.capture</code> call to the Payment Service with the final total (adjusted for any substitutions), and publishes <code>order.delivered</code>. Alice receives a notification: "Your groceries have been delivered! Rate your experience." The WebSocket connections for this order are closed.
</div>

<h3>Component Deep Dive</h3>

<div class="component-box">
    <h4>Location Service</h4>
    <p><strong>Protocol:</strong> Receives data via WebSocket Server (internal) + HTTP REST for queries</p>
    <ul>
        <li>Receives real-time GPS coordinates from shoppers during active deliveries.</li>
        <li>Writes location data to a Time-Series NoSQL store (optimized for time-stamped append-only writes).</li>
        <li>Publishes location updates to the Pub/Sub system for real-time fan-out to customer WebSocket connections.</li>
        <li><code>GET /locations/shopper/{id}/latest</code> â€” Returns latest known location. Used for ETA calculations.</li>
    </ul>
</div>

<div class="component-box">
    <h4>WebSocket Server</h4>
    <p><strong>Protocol:</strong> WebSocket (WSS â€” WebSocket Secure over TLS)</p>
    <p>Maintains persistent bidirectional connections with client and shopper apps. Used for: (1) real-time delivery location tracking, (2) order status micro-updates (item checked off, etc.), (3) chat messages. Full deep dive in the <a href="#websocket">WebSocket Deep Dive</a> section.</p>
</div>

<div class="component-box">
    <h4>Pub/Sub System</h4>
    <p>An internal publish/subscribe system used to fan out real-time updates across the WebSocket Server fleet. When a location update or order status change occurs, it is published to a topic (e.g., <code>order:{order_id}</code>). All WebSocket Server instances subscribe to relevant topics and push updates to connected clients. This is necessary because the customer and shopper may be connected to different WebSocket Server instances. Full details in the <a href="#websocket">WebSocket Deep Dive</a>.</p>
</div>

<!-- ============================================================ -->
<!-- 6. FLOW 4 â€” SHOPPER-CUSTOMER REAL-TIME CHAT -->
<!-- ============================================================ -->
<h2 id="flow4">6. Flow 4 â€” Shopper-Customer Real-Time Chat</h2>

<div class="diagram-container">
<pre class="mermaid">
graph LR
    Client["ðŸ“± Client App"] -->|WebSocket| WSS["WebSocket\nServer(s)"]
    SA["ðŸ“± Shopper App"] -->|WebSocket| WSS

    WSS --> ChatS["Chat Service"]
    ChatS --> ChatDB[("Chat Store\n(Document NoSQL)")]

    ChatS --> PubSub["Pub/Sub"]
    PubSub --> WSS

    ChatS --> NotifS["Notification\nService"]
    NotifS -->|"push notification\n(if recipient offline)"| Client
    NotifS -->|"push notification\n(if recipient offline)"| SA
</pre>
</div>

<h3>Examples</h3>

<div class="example-box">
    <strong>Example 1 â€” Shopper Sends Text Message:</strong> While shopping, Bob wants to ask Alice about her banana preference. Bob types "Do you prefer green or yellow bananas?" in the chat. The message travels over Bob's existing WebSocket connection to the WebSocket Server, which routes it to the Chat Service. The Chat Service persists the message in the Document NoSQL Chat Store (partitioned by <code>order_id</code>) and publishes it to the Pub/Sub topic <code>chat:ord-456</code>. Alice's WebSocket Server instance receives the message via Pub/Sub and pushes it to Alice's Client App in real time. Alice sees the message instantly in the in-app chat window.
</div>

<div class="example-box">
    <strong>Example 2 â€” Customer is Offline (App Backgrounded):</strong> Bob sends a message, but Alice has the app backgrounded. The Chat Service detects that Alice's WebSocket connection is inactive (or the Pub/Sub delivery fails). The Chat Service triggers the Notification Service, which sends a push notification to Alice's device: "Bob: Do you prefer green or yellow bananas?" When Alice opens the app, her client re-establishes the WebSocket connection and fetches unread messages via <code>GET /chat/orders/ord-456/messages?since={last_seen_timestamp}</code> from the Chat Service to fill the gap.
</div>

<div class="example-box">
    <strong>Example 3 â€” Photo Message:</strong> Bob takes a photo of two banana options and sends it. The Shopper App first uploads the image to Object Storage via <code>POST /media/upload</code> (receiving back a CDN URL), then sends the chat message with <code>{type: "image", image_url: "https://cdn.../bananas.jpg"}</code> over WebSocket. Alice's app renders the image inline in the chat.
</div>

<h3>Component Deep Dive</h3>

<div class="component-box">
    <h4>Chat Service</h4>
    <p><strong>Protocol:</strong> Receives messages from WebSocket Server (internal) + HTTP REST for history</p>
    <ul>
        <li><strong>Message Handling:</strong> Receives chat messages from the WebSocket Server, validates them (order must be active, sender must be participant), persists to Chat Store, and publishes to Pub/Sub for delivery.</li>
        <li><code>GET /chat/orders/{order_id}/messages?since={ts}&amp;limit={n}</code> â€” Returns chat history for an order. Used when client reconnects to fetch missed messages. Input: order_id, since timestamp, limit. Output: array of <code>{message_id, sender_id, sender_type, content, message_type, timestamp, is_read}</code>.</li>
    </ul>
    <p>Chat messages are stored in a Document NoSQL database partitioned by <code>order_id</code>. Each order's chat is ephemeral in nature (only active during order lifecycle) but retained for 90 days for dispute resolution. Messages are sorted by timestamp within each partition.</p>
</div>

<!-- ============================================================ -->
<!-- 7. COMBINED OVERALL FLOW DIAGRAM -->
<!-- ============================================================ -->
<h2 id="combined">7. Combined Overall Flow Diagram</h2>

<div class="diagram-container">
<pre class="mermaid">
graph TB
    Client["ðŸ“± Client App"] -->|HTTPS| LB["âš–ï¸ Load Balancer"]
    SA["ðŸ“± Shopper App"] -->|HTTPS| LB
    Client -->|WebSocket| WSLB["âš–ï¸ WS Load Balancer"]
    SA -->|WebSocket| WSLB

    LB --> GW["ðŸ”€ API Gateway"]
    WSLB --> WSS["WebSocket Server Fleet"]

    GW --> StoreS["Store Service"]
    GW --> CatS["Catalog Service"]
    GW --> SearchS["Search Service"]
    GW --> CartS["Cart Service"]
    GW --> OrdS["Order Service"]
    GW --> PayS["Payment Service"]

    WSS --> ChatS["Chat Service"]
    WSS --> LocS["Location Service"]

    StoreS --> Cache["ðŸ—‚ï¸ Cache Layer"]
    CatS --> Cache
    Cache -.-> StoreDB[("Store DB\n(SQL)")]
    Cache -.-> ProdDB[("Product DB\n(Document NoSQL)")]

    SearchS --> SearchIdx[("Search Index\n(Inverted Index)")]
    CartS --> CartDB[("Cart Store\n(KV NoSQL)")]
    OrdS --> OrdDB[("Order DB\n(SQL)")]
    PayS --> PayGW["Payment Gateway\n(External)"]

    OrdS --> MQ["ðŸ“¨ Message Queue"]
    MQ --> MatchS["Shopper Matching\nService"]
    MQ --> InvS["Inventory Service"]
    MQ --> NotifS["Notification Service"]

    MatchS --> ShopDB[("Shopper DB\n(SQL)")]
    InvS --> InvDB[("Inventory DB\n(NoSQL)")]
    ChatS --> ChatDB[("Chat Store\n(Document NoSQL)")]
    LocS --> LocDB[("Location Store\n(Time-Series NoSQL)")]

    LocS --> PubSub["Pub/Sub"]
    ChatS --> PubSub
    PubSub --> WSS

    CatS --> CDN["ðŸŒ CDN"]
    CDN --> ObjStore[("Object Storage")]

    NotifS -->|push| Client
    NotifS -->|push| SA
</pre>
</div>

<h3>End-to-End Example</h3>

<div class="example-box">
    <strong>Full Journey â€” Alice orders groceries, Bob delivers:</strong>
    <ol>
        <li><strong>Discovery:</strong> Alice opens the app. <code>GET /stores?lat=37.78&amp;lng=-122.41</code> hits the Store Service â†’ Cache â†’ Store DB. She sees 12 nearby stores.</li>
        <li><strong>Browsing:</strong> Alice taps "Whole Foods." <code>GET /stores/wf-123/products?category=produce</code> hits the Catalog Service â†’ Cache â†’ Product DB. Product images load from CDN â†’ Object Storage.</li>
        <li><strong>Search:</strong> Alice searches "oat milk." <code>GET /search?q=oat+milk&amp;store_id=wf-123</code> hits Search Service â†’ Inverted Index. Returns 8 results.</li>
        <li><strong>Cart:</strong> Alice adds 6 items. Each <code>POST /cart/items</code> goes to Cart Service â†’ Key-Value Cart Store.</li>
        <li><strong>Checkout:</strong> Alice taps "Place Order." <code>POST /orders</code> goes to Order Service, which validates the cart, creates the order in SQL Order DB, calls Payment Service â†’ Payment Gateway (authorization: $52.47). On success, publishes <code>order.created</code> to the Message Queue.</li>
        <li><strong>Shopper Matching:</strong> Shopper Matching Service consumes the event, queries available shoppers near Whole Foods from Shopper DB, assigns Bob (highest score: proximity + rating + current capacity). Publishes <code>order.assigned</code>.</li>
        <li><strong>Notification:</strong> Notification Service consumes events: pushes "Order placed!" to Alice and "New order at Whole Foods" to Bob.</li>
        <li><strong>Shopping:</strong> Bob starts shopping. <code>PATCH /orders/ord-456/status {status: SHOPPING}</code>. Alice gets a notification. Bob checks off items one by one. For each item: <code>PATCH /orders/ord-456/items/{id} {status: FOUND}</code>.</li>
        <li><strong>Substitution:</strong> Bob can't find Organic Whole Milk. He proposes 2% Milk as a substitute. Alice receives a push notification and approves via the app.</li>
        <li><strong>Chat:</strong> Bob sends "Do you want a bag of ice?" over WebSocket â†’ Chat Service â†’ Pub/Sub â†’ Alice's WebSocket connection. Alice replies "Yes please!" following the same path.</li>
        <li><strong>Delivery:</strong> Bob checks out and starts delivering. His Shopper App sends GPS coordinates every 5 seconds over WebSocket â†’ Location Service â†’ Time-Series Store â†’ Pub/Sub â†’ Alice's WebSocket. Alice sees Bob on a live map.</li>
        <li><strong>Completion:</strong> Bob delivers and taps "Mark Delivered." Order moves to <code>DELIVERED</code>. Payment Service captures the final amount ($51.98 â€” milk substitution was cheaper). Alice gets a "Delivered!" notification and rates Bob 5 stars.</li>
    </ol>
</div>

<!-- ============================================================ -->
<!-- 8. DATABASE SCHEMA -->
<!-- ============================================================ -->
<h2 id="schema">8. Database Schema</h2>

<h3>SQL Tables</h3>

<div class="schema-section">
    <h4>8.1 â€” <code>users</code> (SQL)</h4>
    <table>
        <tr><th>Column</th><th>Type</th><th>Key</th><th>Notes</th></tr>
        <tr><td>user_id</td><td>UUID</td><td><span class="tag green">PK</span></td><td></td></tr>
        <tr><td>email</td><td>VARCHAR(255)</td><td>UNIQUE</td><td></td></tr>
        <tr><td>phone</td><td>VARCHAR(20)</td><td></td><td></td></tr>
        <tr><td>name</td><td>VARCHAR(100)</td><td></td><td></td></tr>
        <tr><td>password_hash</td><td>VARCHAR(255)</td><td></td><td>Bcrypt hash</td></tr>
        <tr><td>default_address_id</td><td>UUID</td><td><span class="tag orange">FK â†’ addresses</span></td><td>Nullable</td></tr>
        <tr><td>notification_preferences</td><td>JSON</td><td></td><td>Push, email, SMS toggles</td></tr>
        <tr><td>auto_approve_substitutions</td><td>BOOLEAN</td><td></td><td>Default: false</td></tr>
        <tr><td>created_at</td><td>TIMESTAMP</td><td></td><td></td></tr>
        <tr><td>updated_at</td><td>TIMESTAMP</td><td></td><td></td></tr>
    </table>
    <p><strong>Why SQL:</strong> Users have a well-defined, relational schema. Account operations (registration, login, profile update) require ACID guarantees to prevent duplicate accounts or data corruption.</p>
    <p><strong>Index:</strong> <strong>B-tree index on <code>email</code></strong> â€” for fast login lookup by email. B-tree is chosen because email lookups are exact-match equality queries, and B-tree supports efficient equality as well as range queries (useful for admin email-based searches).</p>
    <p><strong>Read:</strong> User login, profile view, order placement (fetch default address). <strong>Write:</strong> User registration, profile update.</p>
</div>

<div class="schema-section">
    <h4>8.2 â€” <code>addresses</code> (SQL)</h4>
    <table>
        <tr><th>Column</th><th>Type</th><th>Key</th><th>Notes</th></tr>
        <tr><td>address_id</td><td>UUID</td><td><span class="tag green">PK</span></td><td></td></tr>
        <tr><td>user_id</td><td>UUID</td><td><span class="tag orange">FK â†’ users</span></td><td></td></tr>
        <tr><td>label</td><td>VARCHAR(50)</td><td></td><td>"Home", "Office", etc.</td></tr>
        <tr><td>street</td><td>VARCHAR(255)</td><td></td><td></td></tr>
        <tr><td>apartment</td><td>VARCHAR(50)</td><td></td><td>Nullable</td></tr>
        <tr><td>city</td><td>VARCHAR(100)</td><td></td><td></td></tr>
        <tr><td>state</td><td>VARCHAR(50)</td><td></td><td></td></tr>
        <tr><td>zip_code</td><td>VARCHAR(10)</td><td></td><td></td></tr>
        <tr><td>lat</td><td>DECIMAL(9,6)</td><td></td><td></td></tr>
        <tr><td>lng</td><td>DECIMAL(9,6)</td><td></td><td></td></tr>
        <tr><td>delivery_instructions</td><td>TEXT</td><td></td><td>Nullable</td></tr>
        <tr><td>is_default</td><td>BOOLEAN</td><td></td><td></td></tr>
    </table>
    <p><strong>Why SQL:</strong> Relational to users (one-to-many). Addresses have a strict, structured schema. Needs referential integrity.</p>
    <p><strong>Index:</strong> <strong>B-tree index on <code>user_id</code></strong> â€” to efficiently retrieve all addresses for a given user. Every address lookup is scoped to a user.</p>
    <p><strong>Read:</strong> Order placement (select delivery address), store discovery (use lat/lng for nearby stores). <strong>Write:</strong> User adds/edits an address.</p>
</div>

<div class="schema-section">
    <h4>8.3 â€” <code>stores</code> (SQL)</h4>
    <table>
        <tr><th>Column</th><th>Type</th><th>Key</th><th>Notes</th></tr>
        <tr><td>store_id</td><td>UUID</td><td><span class="tag green">PK</span></td><td></td></tr>
        <tr><td>name</td><td>VARCHAR(200)</td><td></td><td>"Whole Foods Market â€” SoMa"</td></tr>
        <tr><td>chain_name</td><td>VARCHAR(100)</td><td></td><td>"Whole Foods"</td></tr>
        <tr><td>address</td><td>VARCHAR(500)</td><td></td><td></td></tr>
        <tr><td>lat</td><td>DECIMAL(9,6)</td><td></td><td></td></tr>
        <tr><td>lng</td><td>DECIMAL(9,6)</td><td></td><td></td></tr>
        <tr><td>operating_hours</td><td>JSON</td><td></td><td><code>{"mon": {"open":"7:00","close":"22:00"}, ...}</code></td></tr>
        <tr><td>min_order_amount</td><td>DECIMAL(8,2)</td><td></td><td></td></tr>
        <tr><td>delivery_fee</td><td>DECIMAL(6,2)</td><td></td><td></td></tr>
        <tr><td>logo_url</td><td>VARCHAR(500)</td><td></td><td>CDN URL</td></tr>
        <tr><td>is_active</td><td>BOOLEAN</td><td></td><td></td></tr>
        <tr><td>created_at</td><td>TIMESTAMP</td><td></td><td></td></tr>
    </table>
    <p><strong>Why SQL:</strong> Stores have a well-defined schema. The number of stores is relatively small (tens of thousands), so SQL handles it well. Need for geospatial queries is supported by SQL extensions.</p>
    <p><strong>Index:</strong> <strong>R-tree (geospatial) index on <code>(lat, lng)</code></strong> â€” R-tree is specifically designed for multi-dimensional spatial data. It allows efficient "find all stores within X miles of point (lat, lng)" queries. This is far superior to B-tree for bounding-box and radius queries because B-tree can only index one dimension at a time, while R-tree partitions 2D space into hierarchical rectangles.</p>
    <p><strong>Read:</strong> Store discovery (user opens app), browsing (user taps on a store). <strong>Write:</strong> Store onboarding (admin), operating hours updates (rare).</p>
</div>

<div class="schema-section">
    <h4>8.4 â€” <code>shoppers</code> (SQL)</h4>
    <table>
        <tr><th>Column</th><th>Type</th><th>Key</th><th>Notes</th></tr>
        <tr><td>shopper_id</td><td>UUID</td><td><span class="tag green">PK</span></td><td></td></tr>
        <tr><td>name</td><td>VARCHAR(100)</td><td></td><td></td></tr>
        <tr><td>email</td><td>VARCHAR(255)</td><td>UNIQUE</td><td></td></tr>
        <tr><td>phone</td><td>VARCHAR(20)</td><td></td><td></td></tr>
        <tr><td>vehicle_type</td><td>ENUM</td><td></td><td>car, bicycle, on_foot</td></tr>
        <tr><td>is_active</td><td>BOOLEAN</td><td></td><td>Account active</td></tr>
        <tr><td>is_available</td><td>BOOLEAN</td><td></td><td>Currently accepting orders</td></tr>
        <tr><td>current_zone_id</td><td>VARCHAR(20)</td><td></td><td>Geohash zone for matching</td></tr>
        <tr><td>active_order_count</td><td>INTEGER</td><td></td><td>Current batch count (0â€“3)</td></tr>
        <tr><td>rating_avg</td><td>DECIMAL(3,2)</td><td></td><td></td></tr>
        <tr><td>total_deliveries</td><td>INTEGER</td><td></td><td></td></tr>
        <tr><td>created_at</td><td>TIMESTAMP</td><td></td><td></td></tr>
    </table>
    <p><strong>Why SQL:</strong> Shopper data is structured and relational. Matching queries require complex filtering (availability + zone + capacity) that benefits from SQL query optimization.</p>
    <p><strong>Index:</strong> <strong>Composite B-tree index on <code>(is_available, current_zone_id, active_order_count)</code></strong> â€” The shopper matching query filters by <code>is_available = true</code>, then by geographic zone, then by shoppers with capacity (<code>active_order_count &lt; 3</code>). This composite index covers the most common matching query efficiently. B-tree is appropriate because the query involves equality checks on discrete values and a range comparison.</p>
    <p><strong>Read:</strong> Shopper Matching Service queries for available shoppers during order assignment. <strong>Write:</strong> Shopper goes online/offline, location zone updates, order count increments/decrements, rating updates.</p>
</div>

<div class="schema-section">
    <h4>8.5 â€” <code>orders</code> (SQL)</h4>
    <table>
        <tr><th>Column</th><th>Type</th><th>Key</th><th>Notes</th></tr>
        <tr><td>order_id</td><td>UUID</td><td><span class="tag green">PK</span></td><td></td></tr>
        <tr><td>user_id</td><td>UUID</td><td><span class="tag orange">FK â†’ users</span></td><td></td></tr>
        <tr><td>store_id</td><td>UUID</td><td><span class="tag orange">FK â†’ stores</span></td><td></td></tr>
        <tr><td>shopper_id</td><td>UUID</td><td><span class="tag orange">FK â†’ shoppers</span></td><td>Nullable until assigned</td></tr>
        <tr><td>status</td><td>ENUM</td><td></td><td>CREATED, PENDING_ASSIGNMENT, SCHEDULED, ASSIGNED, SHOPPING, CHECKOUT, DELIVERING, DELIVERED, CANCELLED</td></tr>
        <tr><td>delivery_address_id</td><td>UUID</td><td><span class="tag orange">FK â†’ addresses</span></td><td></td></tr>
        <tr><td>delivery_window_start</td><td>TIMESTAMP</td><td></td><td></td></tr>
        <tr><td>delivery_window_end</td><td>TIMESTAMP</td><td></td><td></td></tr>
        <tr><td>subtotal</td><td>DECIMAL(10,2)</td><td></td><td></td></tr>
        <tr><td>delivery_fee</td><td>DECIMAL(6,2)</td><td></td><td></td></tr>
        <tr><td>service_fee</td><td>DECIMAL(6,2)</td><td></td><td></td></tr>
        <tr><td>tip_amount</td><td>DECIMAL(6,2)</td><td></td><td></td></tr>
        <tr><td>total_amount</td><td>DECIMAL(10,2)</td><td></td><td></td></tr>
        <tr><td>special_instructions</td><td>TEXT</td><td></td><td>Nullable</td></tr>
        <tr><td>promo_code</td><td>VARCHAR(50)</td><td></td><td>Nullable</td></tr>
        <tr><td>created_at</td><td>TIMESTAMP</td><td></td><td></td></tr>
        <tr><td>updated_at</td><td>TIMESTAMP</td><td></td><td></td></tr>
    </table>
    <p><strong>Why SQL:</strong> Orders involve financial transactions that absolutely require ACID compliance. Order creation, payment authorization, and status updates must be atomic and consistent. SQL transactions protect against partial writes (e.g., order created but payment not recorded). Referential integrity ensures every order links to a valid user, store, and address.</p>
    <p><strong>Indexes:</strong></p>
    <ul>
        <li><strong>B-tree index on <code>user_id</code></strong> â€” Users frequently query their order history (<code>GET /orders?user_id=x</code>). B-tree is ideal because we need ordered results (by <code>created_at</code>) and the query is an equality match on <code>user_id</code>.</li>
        <li><strong>B-tree index on <code>shopper_id</code></strong> â€” Shoppers view their assigned orders. Same reasoning as above.</li>
        <li><strong>Composite B-tree index on <code>(status, delivery_window_start)</code></strong> â€” Used by the scheduled order job to find orders with status <code>SCHEDULED</code> whose delivery window is approaching. Also used for operational dashboards filtering by status.</li>
    </ul>
    <p><strong>Sharding:</strong> Shard by <code>user_id</code>. Rationale: the dominant access pattern is users viewing their own orders. Sharding by <code>user_id</code> ensures all of a user's orders are co-located on the same shard, making order-history queries efficient (single-shard reads). For shopper-side queries (<code>WHERE shopper_id = x</code>), we maintain a secondary global index on <code>shopper_id</code> that maps to the shard containing the order. Since shoppers only access a small number of active orders at a time, the cross-shard overhead is acceptable.</p>
    <p><strong>Read:</strong> User views order history, user views active order status, shopper views assigned orders. <strong>Write:</strong> Order placement (checkout button), every status transition (shopper actions).</p>
</div>

<div class="schema-section">
    <h4>8.6 â€” <code>order_items</code> (SQL)</h4>
    <table>
        <tr><th>Column</th><th>Type</th><th>Key</th><th>Notes</th></tr>
        <tr><td>order_item_id</td><td>UUID</td><td><span class="tag green">PK</span></td><td></td></tr>
        <tr><td>order_id</td><td>UUID</td><td><span class="tag orange">FK â†’ orders</span></td><td></td></tr>
        <tr><td>product_id</td><td>VARCHAR(100)</td><td></td><td>Reference to Product DB (cross-DB ref)</td></tr>
        <tr><td>product_name</td><td>VARCHAR(300)</td><td></td><td><span class="tag orange">Denormalized</span></td></tr>
        <tr><td>product_image_url</td><td>VARCHAR(500)</td><td></td><td><span class="tag orange">Denormalized</span></td></tr>
        <tr><td>quantity</td><td>INTEGER</td><td></td><td></td></tr>
        <tr><td>unit_price</td><td>DECIMAL(8,2)</td><td></td><td>Price at time of order</td></tr>
        <tr><td>status</td><td>ENUM</td><td></td><td>PENDING, FOUND, NOT_FOUND, SUBSTITUTE_PROPOSED, REPLACED, REFUNDED</td></tr>
        <tr><td>replacement_product_id</td><td>VARCHAR(100)</td><td></td><td>Nullable</td></tr>
        <tr><td>replacement_product_name</td><td>VARCHAR(300)</td><td></td><td>Nullable <span class="tag orange">Denormalized</span></td></tr>
        <tr><td>replacement_unit_price</td><td>DECIMAL(8,2)</td><td></td><td>Nullable</td></tr>
    </table>
    <p><strong>Why SQL:</strong> One-to-many relationship with orders. Must participate in the same transaction as the order (e.g., order creation inserts all order items atomically).</p>
    <p><strong>Index:</strong> <strong>B-tree index on <code>order_id</code></strong> â€” Order items are always queried by order. B-tree is efficient for this equality lookup and supports range scan for all items in an order.</p>
    <p><strong>Denormalization Explanation:</strong> <code>product_name</code>, <code>product_image_url</code>, and <code>replacement_product_name</code> are denormalized (copied) from the Product DB at the time the order is placed. This is done for two reasons: (1) <strong>Performance:</strong> When displaying order details, we avoid a cross-database join between the SQL Order DB and the NoSQL Product DB. This would require an application-level join across two different database systems, adding latency and complexity. (2) <strong>Historical accuracy:</strong> Product names and images may change over time (rebranding, updated packaging photos). The order record must preserve exactly what the customer ordered at that point in time. <code>unit_price</code> is similarly captured at order time rather than looked up dynamically.</p>
    <p><strong>Read:</strong> Viewing order details, shopper's pick list, receipt generation. <strong>Write:</strong> Order placement (bulk insert), shopper marks item found/not-found/replaced.</p>
</div>

<div class="schema-section">
    <h4>8.7 â€” <code>payments</code> (SQL)</h4>
    <table>
        <tr><th>Column</th><th>Type</th><th>Key</th><th>Notes</th></tr>
        <tr><td>payment_id</td><td>UUID</td><td><span class="tag green">PK</span></td><td></td></tr>
        <tr><td>order_id</td><td>UUID</td><td><span class="tag orange">FK â†’ orders</span></td><td></td></tr>
        <tr><td>user_id</td><td>UUID</td><td><span class="tag orange">FK â†’ users</span></td><td></td></tr>
        <tr><td>amount_authorized</td><td>DECIMAL(10,2)</td><td></td><td>Initial hold amount</td></tr>
        <tr><td>amount_captured</td><td>DECIMAL(10,2)</td><td></td><td>Final charged amount (may differ)</td></tr>
        <tr><td>payment_method</td><td>ENUM</td><td></td><td>credit_card, debit_card, apple_pay, google_pay</td></tr>
        <tr><td>payment_token</td><td>VARCHAR(255)</td><td></td><td>Tokenized reference to external gateway</td></tr>
        <tr><td>status</td><td>ENUM</td><td></td><td>PENDING, AUTHORIZED, CAPTURED, PARTIALLY_REFUNDED, REFUNDED, FAILED</td></tr>
        <tr><td>created_at</td><td>TIMESTAMP</td><td></td><td></td></tr>
        <tr><td>updated_at</td><td>TIMESTAMP</td><td></td><td></td></tr>
    </table>
    <p><strong>Why SQL:</strong> Financial data demands the strongest consistency guarantees. ACID transactions ensure that payment state transitions (authorize â†’ capture â†’ refund) are atomic and durable. Required for PCI-DSS audit trails.</p>
    <p><strong>Index:</strong> <strong>B-tree on <code>order_id</code></strong> (1:1 relationship, fast lookup for orderâ†’payment). <strong>B-tree on <code>user_id</code></strong> (payment history / billing queries).</p>
    <p><strong>Read:</strong> Order summary (display payment status), refund processing, financial reconciliation. <strong>Write:</strong> Order checkout (authorize), delivery complete (capture), customer requests refund.</p>
</div>

<div class="schema-section">
    <h4>8.8 â€” <code>ratings</code> (SQL)</h4>
    <table>
        <tr><th>Column</th><th>Type</th><th>Key</th><th>Notes</th></tr>
        <tr><td>rating_id</td><td>UUID</td><td><span class="tag green">PK</span></td><td></td></tr>
        <tr><td>order_id</td><td>UUID</td><td><span class="tag orange">FK â†’ orders</span></td><td>UNIQUE â€” one rating per order</td></tr>
        <tr><td>user_id</td><td>UUID</td><td><span class="tag orange">FK â†’ users</span></td><td></td></tr>
        <tr><td>shopper_id</td><td>UUID</td><td><span class="tag orange">FK â†’ shoppers</span></td><td></td></tr>
        <tr><td>shopper_rating</td><td>SMALLINT</td><td></td><td>1â€“5</td></tr>
        <tr><td>comment</td><td>TEXT</td><td></td><td>Nullable</td></tr>
        <tr><td>created_at</td><td>TIMESTAMP</td><td></td><td></td></tr>
    </table>
    <p><strong>Why SQL:</strong> Relational data tied to orders, users, and shoppers. Needs referential integrity and aggregation queries (AVG rating per shopper).</p>
    <p><strong>Index:</strong> <strong>B-tree on <code>shopper_id</code></strong> â€” Frequently compute average ratings per shopper. <strong>Unique index on <code>order_id</code></strong> â€” Prevents duplicate ratings per order.</p>
    <p><strong>Read:</strong> Display shopper's average rating during matching and on shopper profile. <strong>Write:</strong> Customer submits rating after delivery. The <code>rating_avg</code> field on the <code>shoppers</code> table is <span class="tag orange">denormalized</span> and updated asynchronously via the Message Queue when a new rating is submitted. This avoids computing AVG on every shopper-matching query.</p>
</div>

<hr>

<h3>NoSQL Tables</h3>

<div class="schema-section">
    <h4>8.9 â€” <code>products</code> (Document NoSQL)</h4>
    <table>
        <tr><th>Field</th><th>Type</th><th>Key</th><th>Notes</th></tr>
        <tr><td>product_id</td><td>String</td><td><span class="tag green">PK</span></td><td></td></tr>
        <tr><td>store_id</td><td>String</td><td><span class="tag">Partition Key</span></td><td></td></tr>
        <tr><td>name</td><td>String</td><td></td><td></td></tr>
        <tr><td>description</td><td>String</td><td></td><td></td></tr>
        <tr><td>category</td><td>String</td><td></td><td>"Produce", "Dairy", "Bakery"</td></tr>
        <tr><td>subcategory</td><td>String</td><td></td><td>"Fruits", "Milk", "Bread"</td></tr>
        <tr><td>brand</td><td>String</td><td></td><td></td></tr>
        <tr><td>price</td><td>Number</td><td></td><td></td></tr>
        <tr><td>unit</td><td>String</td><td></td><td>"each", "lb", "oz", "gal"</td></tr>
        <tr><td>image_urls</td><td>Array&lt;String&gt;</td><td></td><td>CDN URLs</td></tr>
        <tr><td>nutritional_info</td><td>Object</td><td></td><td><code>{calories, fat, protein, ...}</code> â€” varying fields</td></tr>
        <tr><td>dietary_tags</td><td>Array&lt;String&gt;</td><td></td><td>["organic", "gluten-free", "vegan"]</td></tr>
        <tr><td>aisle_number</td><td>String</td><td></td><td>Helps shoppers find items</td></tr>
        <tr><td>is_available</td><td>Boolean</td><td></td><td>Best-effort availability</td></tr>
        <tr><td>updated_at</td><td>Timestamp</td><td></td><td></td></tr>
    </table>
    <p><strong>Why Document NoSQL:</strong> Products have highly variable attributes. A carton of milk has nutritional_info fields (calories, fat, protein), but a cleaning product has safety_info fields instead. A document model naturally accommodates this schema flexibility without requiring NULLable columns for every possible attribute. Additionally, product data is read-heavy (browsing) and writes are batch-oriented (catalog syncs from store partners), which aligns well with document stores' read optimization.</p>
    <p><strong>Sharding:</strong> Partition by <code>store_id</code>. Products are always browsed and searched within the context of a single store, so all products for a store are co-located on the same partition. This ensures that <code>GET /stores/{id}/products</code> is always a single-partition read â€” extremely fast.</p>
    <p><strong>Read:</strong> Product browsing (user views category page), product detail view, search result hydration. <strong>Write:</strong> Batch catalog sync from store partners (daily/hourly ETL jobs), inventory status updates (shopper marks item unavailable).</p>
</div>

<div class="schema-section">
    <h4>8.10 â€” <code>cart</code> (Key-Value NoSQL)</h4>
    <table>
        <tr><th>Field</th><th>Type</th><th>Key</th><th>Notes</th></tr>
        <tr><td>cart_key</td><td>String</td><td><span class="tag green">PK</span></td><td>Format: <code>{user_id}:{store_id}</code></td></tr>
        <tr><td>items</td><td>Array&lt;Object&gt;</td><td></td><td><code>[{product_id, name, quantity, unit_price, image_url}]</code></td></tr>
        <tr><td>subtotal</td><td>Number</td><td></td><td>Pre-computed for quick display</td></tr>
        <tr><td>updated_at</td><td>Timestamp</td><td></td><td></td></tr>
        <tr><td>expires_at</td><td>Timestamp</td><td></td><td>TTL: 7 days</td></tr>
    </table>
    <p><strong>Why Key-Value NoSQL:</strong> Cart data is ephemeral, has a simple access pattern (always by user+store), and requires very high read/write throughput (users frequently add/remove items). The key-value model provides O(1) lookups by composite key. The built-in TTL support auto-expires abandoned carts. No relational joins are needed.</p>
    <p><strong>Read:</strong> User views their cart, checkout validation. <strong>Write:</strong> Every "Add to Cart" / quantity change / "Remove" action. Auto-deleted after 7 days of inactivity (TTL).</p>
</div>

<div class="schema-section">
    <h4>8.11 â€” <code>inventory</code> (Key-Value NoSQL)</h4>
    <table>
        <tr><th>Field</th><th>Type</th><th>Key</th><th>Notes</th></tr>
        <tr><td>inventory_key</td><td>String</td><td><span class="tag green">PK</span></td><td>Format: <code>{store_id}:{product_id}</code></td></tr>
        <tr><td>quantity_available</td><td>Number</td><td></td><td>Estimated stock level</td></tr>
        <tr><td>is_available</td><td>Boolean</td><td></td><td>Simplified availability flag</td></tr>
        <tr><td>last_synced_at</td><td>Timestamp</td><td></td><td>Last partner API sync</td></tr>
        <tr><td>last_verified_at</td><td>Timestamp</td><td></td><td>Last in-store shopper verification</td></tr>
    </table>
    <p><strong>Why Key-Value NoSQL:</strong> Inventory lookups are always by exact <code>store_id:product_id</code> key â€” a perfect key-value access pattern. Inventory updates are frequent (periodic syncs + real-time shopper reports), requiring high write throughput. Eventual consistency is acceptable (shoppers serve as ground-truth). No relational joins needed.</p>
    <p><strong>Read:</strong> Product availability display, order validation at checkout. <strong>Write:</strong> Periodic batch sync from store partner APIs (hourly), shopper marks item as out-of-stock during fulfillment.</p>
</div>

<div class="schema-section">
    <h4>8.12 â€” <code>shopper_locations</code> (Time-Series NoSQL)</h4>
    <table>
        <tr><th>Field</th><th>Type</th><th>Key</th><th>Notes</th></tr>
        <tr><td>shopper_id</td><td>String</td><td><span class="tag green">Partition Key</span></td><td></td></tr>
        <tr><td>timestamp</td><td>Timestamp</td><td><span class="tag green">Sort Key</span></td><td></td></tr>
        <tr><td>lat</td><td>Number</td><td></td><td></td></tr>
        <tr><td>lng</td><td>Number</td><td></td><td></td></tr>
        <tr><td>order_id</td><td>String</td><td></td><td>Active order being delivered</td></tr>
        <tr><td>speed_mph</td><td>Number</td><td></td><td>For ETA calculation</td></tr>
    </table>
    <p><strong>Why Time-Series NoSQL:</strong> Location data is inherently time-series â€” append-only, time-stamped, and queried by recency ("where is the shopper now?" = latest data point for a given shopper). Time-series databases are optimized for high-frequency writes (every 5 seconds per active shopper) and efficient "latest N" queries. They also support automatic data aging/compaction â€” location data older than 30 days can be downsampled or purged.</p>
    <p><strong>Read:</strong> Real-time tracking (latest location for shopper), ETA calculation (recent location history for speed/direction). <strong>Write:</strong> Shopper App sends GPS update every 5 seconds during active delivery.</p>
</div>

<div class="schema-section">
    <h4>8.13 â€” <code>chat_messages</code> (Document NoSQL)</h4>
    <table>
        <tr><th>Field</th><th>Type</th><th>Key</th><th>Notes</th></tr>
        <tr><td>message_id</td><td>String</td><td><span class="tag green">PK</span></td><td>ULID or UUID</td></tr>
        <tr><td>order_id</td><td>String</td><td><span class="tag green">Partition Key</span></td><td></td></tr>
        <tr><td>sender_id</td><td>String</td><td></td><td></td></tr>
        <tr><td>sender_type</td><td>String</td><td></td><td>"customer" or "shopper"</td></tr>
        <tr><td>content</td><td>String</td><td></td><td>Text content</td></tr>
        <tr><td>message_type</td><td>String</td><td></td><td>"text", "image"</td></tr>
        <tr><td>image_url</td><td>String</td><td></td><td>Nullable, CDN URL for photo messages</td></tr>
        <tr><td>timestamp</td><td>Timestamp</td><td><span class="tag green">Sort Key</span></td><td></td></tr>
        <tr><td>is_read</td><td>Boolean</td><td></td><td></td></tr>
    </table>
    <p><strong>Why Document NoSQL:</strong> Chat messages have a simple, document-oriented structure. The access pattern is always by order (all messages for an active order) with time-sorted reads. Document stores handle high write throughput well and support flexible message types (text, image, future: audio). No joins needed.</p>
    <p><strong>Sharding:</strong> Partition by <code>order_id</code>. All messages for an order are co-located, ensuring chat history retrieval is a single-partition query. Order-scoped chat means bounded partition sizes (dozens to hundreds of messages per order).</p>
    <p><strong>Data Retention:</strong> Messages are retained for 90 days for dispute resolution, then purged. TTL or a scheduled cleanup job handles this.</p>
    <p><strong>Read:</strong> Loading chat history when opening the chat window, reconnection gap-fill. <strong>Write:</strong> Every message sent by customer or shopper.</p>
</div>

<div class="schema-section">
    <h4>8.14 â€” Search Index (Inverted Index)</h4>
    <p>Not a traditional database table but a dedicated search index data structure.</p>
    <table>
        <tr><th>Concept</th><th>Details</th></tr>
        <tr><td>Indexed Fields</td><td><code>product.name</code>, <code>product.description</code>, <code>product.category</code>, <code>product.brand</code>, <code>product.dietary_tags</code></td></tr>
        <tr><td>Partition</td><td>By <code>store_id</code> â€” searches are always scoped to one store</td></tr>
        <tr><td>Index Type</td><td><strong>Inverted Index</strong> â€” maps terms (tokenized, stemmed, lowercased) â†’ list of <code>product_id</code>s. This is the standard structure for full-text search. Given a query term, the inverted index directly yields all matching documents in O(1) lookup time per term.</td></tr>
        <tr><td>Features</td><td>Fuzzy matching (edit distance â‰¤ 2), synonym expansion, stemming, relevance scoring (TF-IDF or BM25)</td></tr>
        <tr><td>Population</td><td>CDC (Change Data Capture) from the Product DB. When a product is added/updated/removed, a CDC event triggers re-indexing of that product.</td></tr>
    </table>
    <p><strong>Read:</strong> Every user search query. <strong>Write:</strong> Product catalog updates trigger asynchronous re-indexing via CDC pipeline.</p>
</div>

<!-- ============================================================ -->
<!-- 9. CDN & CACHE DEEP DIVE -->
<!-- ============================================================ -->
<h2 id="cdn-cache">9. CDN &amp; Cache Deep Dive</h2>

<h3>CDN (Content Delivery Network)</h3>

<p><strong>Why a CDN is appropriate:</strong> Instacart is an image-heavy application. Every product listing shows a product photo, every store shows a logo, and promotional banners are displayed on the home screen. Without a CDN, every image request would travel to the origin Object Storage, adding latency and creating a bottleneck. Since product images are <em>static</em> (they don't change per-user or per-request), they are ideal CDN candidates. Images are read millions of times but written (uploaded) rarely (only during catalog sync).</p>

<div class="callout">
    <strong>CDN Strategy &amp; Policies:</strong>
    <ul>
        <li><strong>Type:</strong> Pull-based CDN. When a client requests an image URL, the CDN edge checks its local cache. On miss, it pulls from Object Storage (origin), caches it, and serves future requests from edge.</li>
        <li><strong>Caching Strategy:</strong> Cache-through â€” the CDN transparently caches on first access. No explicit push/invalidation needed for most content.</li>
        <li><strong>Expiration Policy (TTL):</strong>
            <ul>
                <li>Product images: <strong>24 hours</strong> â€” product images rarely change. If a product photo is updated, we use URL versioning (append <code>?v=2</code>) to bust the cache.</li>
                <li>Store logos: <strong>7 days</strong> â€” extremely stable.</li>
                <li>Promotional banners: <strong>1 hour</strong> â€” may change for time-sensitive promotions.</li>
            </ul>
        </li>
        <li><strong>Eviction Policy:</strong> <strong>LRU (Least Recently Used)</strong> â€” when edge cache fills up, evict least recently accessed images. Products that haven't been viewed recently are evicted first. LRU is appropriate because popular products (milk, bananas, eggs) are viewed repeatedly and should remain cached, while niche products are accessed rarely.</li>
        <li><strong>Populated by:</strong> First client request for a given image URL triggers CDN population. The Catalog Service stores CDN-prefixed URLs in the Product DB so clients always request through the CDN.</li>
    </ul>
</div>

<h3>In-Memory Cache</h3>

<p><strong>Why an in-memory cache is appropriate:</strong> Product catalog browsing and store listings are extremely read-heavy (100:1 read-to-write ratio). The same store listings and product pages are requested by thousands of users. Without caching, every browse request would hit the database, which is unnecessary since catalog data changes infrequently (hourly batch syncs). An in-memory cache drastically reduces DB load and provides sub-millisecond response times.</p>

<div class="callout">
    <strong>Cache Strategy &amp; Policies:</strong>
    <ul>
        <li><strong>Caching Strategy: Cache-Aside (Lazy Loading)</strong>
            <ul>
                <li><strong>Read path:</strong> Service checks cache first. On hit â†’ return cached data. On miss â†’ query DB â†’ populate cache â†’ return data.</li>
                <li><strong>Write path:</strong> When product/store data is updated (batch sync), the DB is written to directly. The cache entry is invalidated (deleted), and the next read re-populates it.</li>
                <li><strong>Why cache-aside:</strong> Only data that is actually requested gets cached (no wasted memory on unpopular products). The application controls exactly what is cached and when. It's simple to implement and reason about. Write-through or write-behind would be overkill since writes are infrequent batch operations.</li>
            </ul>
        </li>
        <li><strong>Eviction Policy: LRU (Least Recently Used)</strong>
            <ul>
                <li>Popular items (stores near dense urban areas, frequently browsed products) stay in cache. Rarely accessed items are evicted when memory pressure occurs.</li>
                <li>Why LRU over LFU: LRU is simpler and performs well when access patterns shift (e.g., seasonal products become popular). LFU would keep stale-but-historically-popular items cached even after they become irrelevant.</li>
            </ul>
        </li>
        <li><strong>Expiration Policy (TTL):</strong>
            <ul>
                <li>Store listings: <strong>10 minutes</strong> â€” stores rarely change, but we want operating hours to reflect reasonably current state.</li>
                <li>Product category pages: <strong>5 minutes</strong> â€” product availability may change; 5 min staleness is acceptable since shoppers verify on-ground.</li>
                <li>Individual product details: <strong>5 minutes</strong> â€” same reasoning as category pages.</li>
                <li>Inventory counts: <strong>60 seconds</strong> â€” tighter TTL because we want to reduce the chance of showing out-of-stock items as available. However, we accept this is "best effort" â€” true availability is confirmed by the shopper.</li>
            </ul>
        </li>
        <li><strong>Cache Key Design:</strong>
            <ul>
                <li>Store listings: <code>stores:geohash:{hash}:radius:{r}</code></li>
                <li>Product category page: <code>products:store:{store_id}:cat:{category}:page:{n}</code></li>
                <li>Product detail: <code>product:{product_id}</code></li>
                <li>Inventory: <code>inventory:{store_id}:{product_id}</code></li>
            </ul>
        </li>
        <li><strong>Populated by:</strong> First request that misses the cache (lazy loading). On catalog batch sync completion, a cache warming job can proactively populate cache for the top 100 most-browsed stores and their top products to avoid a thundering herd of cache misses after invalidation.</li>
    </ul>
</div>

<!-- ============================================================ -->
<!-- 10. WEBSOCKET DEEP DIVE -->
<!-- ============================================================ -->
<h2 id="websocket">10. WebSocket Deep Dive</h2>

<h3>Why WebSocket?</h3>
<p>Instacart requires real-time, low-latency bidirectional communication for two features:</p>
<ol>
    <li><strong>Delivery tracking:</strong> The shopper's GPS location must stream to the customer's map view every 5 seconds. Latency must be &lt;1 second for a smooth map experience.</li>
    <li><strong>In-order chat:</strong> The customer and shopper exchange messages during the fulfillment process (item substitutions, special requests, delivery instructions). Messages must appear instantly.</li>
</ol>

<h3>Why Not Alternatives?</h3>
<table>
    <tr><th>Alternative</th><th>Why Not Chosen</th></tr>
    <tr><td><strong>HTTP Long Polling</strong></td><td>Each "poll" requires a full HTTP request/response cycle. At 5-second intervals for millions of active deliveries, this creates enormous overhead (TCP handshake, HTTP headers repeated every time). Server must hold connections open, wasting resources. Latency is bounded by poll interval â€” not truly real-time.</td></tr>
    <tr><td><strong>HTTP Short Polling</strong></td><td>Even worse than long polling. Constant requests every 1â€“5 seconds, most returning empty responses (no new data). Extremely wasteful of bandwidth and server resources.</td></tr>
    <tr><td><strong>Server-Sent Events (SSE)</strong></td><td>SSE provides server-to-client streaming, which would work for delivery tracking (server pushes location to customer). However, SSE is <em>unidirectional</em> â€” the client cannot send data back over the same connection. Chat requires <em>bidirectional</em> communication. We would need SSE for tracking + separate HTTP requests for chat messages, adding complexity. WebSocket handles both use cases with a single connection.</td></tr>
</table>

<h3>Connection Lifecycle</h3>
<ol>
    <li><strong>Connection Establishment:</strong> When a customer opens the order tracking screen (or the shopper starts an active order), the client initiates a WebSocket handshake:
        <ul>
            <li>Client sends an <code>HTTP GET</code> request with <code>Upgrade: websocket</code> and <code>Connection: Upgrade</code> headers to <code>wss://ws.instacart.com/connect</code>. The request includes an auth token (JWT) in the header for authentication.</li>
            <li>The WS Load Balancer (Layer 4 â€” TCP-level, using IP-hash or sticky sessions) routes the connection to a WebSocket Server instance.</li>
            <li>The WebSocket Server validates the JWT, accepts the upgrade, and the connection is established over TCP with TLS (WSS).</li>
        </ul>
    </li>
    <li><strong>Connection Registration:</strong> The WebSocket Server stores the connection in an <strong>in-memory connection registry</strong> â€” a hash map: <code>{user_id â†’ WebSocket connection object}</code>. It also subscribes to the relevant Pub/Sub topic(s): <code>order:{order_id}</code>, <code>chat:{order_id}</code>.</li>
    <li><strong>Cross-Server Delivery (Pub/Sub):</strong> The customer and shopper for the same order may be connected to <em>different</em> WebSocket Server instances (since load balancing distributes connections). To deliver messages across servers:
        <ul>
            <li>When the shopper sends a location update or chat message, the receiving WebSocket Server publishes it to the Pub/Sub system under the order's topic (e.g., <code>order:ord-456</code>).</li>
            <li>All WebSocket Server instances that have subscriptions to <code>order:ord-456</code> receive the message via Pub/Sub.</li>
            <li>The instance that holds the customer's connection pushes the message to the customer's WebSocket.</li>
            <li>This Pub/Sub fan-out pattern decouples the sender's and receiver's server instances.</li>
        </ul>
    </li>
    <li><strong>Heartbeat &amp; Keepalive:</strong> The server sends WebSocket ping frames every 30 seconds. If a client doesn't respond with a pong within 10 seconds, the connection is considered dead and cleaned up. The client also implements reconnection logic with exponential backoff.</li>
    <li><strong>Connection Termination:</strong> When the order is delivered (or cancelled), the server sends a final <code>order.completed</code> message and gracefully closes the WebSocket connection. The connection is removed from the registry, and the Pub/Sub subscription is unsubscribed.</li>
</ol>

<h3>Scaling WebSocket Servers</h3>
<p>Each WebSocket Server holds thousands of concurrent connections in memory. To scale:</p>
<ul>
    <li>Deploy a fleet of WebSocket Servers behind a sticky/IP-hash Layer 4 Load Balancer.</li>
    <li>The Pub/Sub system (see <a href="#mq">Message Queue section</a>) acts as the communication backbone between servers.</li>
    <li>Connection state is in-memory only (not persisted). If a server crashes, clients reconnect to a different server and re-subscribe. Missed messages are caught up via the Chat Service's <code>GET /messages?since={ts}</code> endpoint.</li>
    <li>Auto-scale WebSocket Server fleet based on active connection count (metric: connections per instance, threshold: ~50,000).</li>
</ul>

<!-- ============================================================ -->
<!-- 11. MESSAGE QUEUE DEEP DIVE -->
<!-- ============================================================ -->
<h2 id="mq">11. Message Queue Deep Dive</h2>

<h3>Why a Message Queue?</h3>
<p>The order placement flow involves multiple downstream actions that should not block the user-facing response: shopper matching, inventory updates, and notification delivery. Without a queue, the Order Service would have to synchronously call each downstream service, increasing latency and creating tight coupling (if Notification Service is slow, order placement is slow). A message queue provides:</p>
<ul>
    <li><strong>Decoupling:</strong> Order Service doesn't need to know about downstream consumers.</li>
    <li><strong>Asynchronous processing:</strong> User gets an instant order confirmation; downstream work happens in the background.</li>
    <li><strong>Buffering:</strong> During traffic spikes (holidays), the queue absorbs bursts that downstream services can process at their own pace.</li>
    <li><strong>Reliability:</strong> Messages are persisted; if a consumer crashes, it picks up unprocessed messages on restart.</li>
</ul>

<h3>Why Not Alternatives?</h3>
<table>
    <tr><th>Alternative</th><th>Why Not Chosen</th></tr>
    <tr><td><strong>Synchronous HTTP calls</strong></td><td>Tight coupling, higher latency, no buffering during spikes, cascade failures if one downstream service is slow/down.</td></tr>
    <tr><td><strong>Pure Pub/Sub (without durability)</strong></td><td>Pub/Sub delivers messages to active subscribers only. If the Notification Service is temporarily down, the message is lost. We need durability and at-least-once delivery guarantees for order events. (Note: We do use Pub/Sub for real-time WebSocket fan-out where transient loss is acceptable â€” a missed location update is superseded by the next one. But for order lifecycle events, we need guaranteed delivery.)</td></tr>
</table>

<h3>How It Works</h3>
<div class="callout">
    <strong>Message Production (Enqueue):</strong>
    <ul>
        <li>The Order Service publishes events to named topics on the message queue: <code>order.created</code>, <code>order.assigned</code>, <code>order.status_changed</code>, <code>order.delivered</code>, <code>substitution.proposed</code>, <code>rating.submitted</code>.</li>
        <li>Each event message contains: <code>{event_type, order_id, timestamp, payload}</code>. The payload varies by event type (e.g., <code>order.assigned</code> includes <code>shopper_id</code>).</li>
        <li>Messages are published with <strong>at-least-once delivery guarantee</strong>. The queue acknowledges receipt; if the publisher doesn't receive an ack (network issue), it retries.</li>
    </ul>
</div>

<div class="callout">
    <strong>Message Consumption (Dequeue):</strong>
    <ul>
        <li>Each downstream service runs a <strong>consumer group</strong> that subscribes to relevant topics:
            <ul>
                <li><strong>Shopper Matching Service</strong> consumes <code>order.created</code></li>
                <li><strong>Inventory Service</strong> consumes <code>order.created</code> (to decrement estimated inventory) and <code>order.cancelled</code> (to restore it)</li>
                <li><strong>Notification Service</strong> consumes all order lifecycle events</li>
                <li><strong>Analytics Pipeline</strong> consumes all events (for dashboards, ML training data)</li>
            </ul>
        </li>
        <li>A consumer reads a message, processes it, and <strong>acknowledges (ACKs)</strong> it. The queue removes acknowledged messages. If a consumer crashes mid-processing, the message times out and is <strong>redelivered</strong> to another consumer in the group.</li>
        <li><strong>Idempotency:</strong> Because of at-least-once delivery, consumers must be idempotent. They check if the event has already been processed (e.g., by checking <code>order.status</code> in the DB before applying a transition) to prevent duplicate side effects (double notifications, double payment captures).</li>
    </ul>
</div>

<div class="callout">
    <strong>Dead Letter Queue (DLQ):</strong>
    <ul>
        <li>Messages that fail processing after N retries (default: 3) are moved to a Dead Letter Queue for manual inspection and remediation.</li>
        <li>Alerts fire when DLQ depth exceeds a threshold, indicating a systemic issue.</li>
    </ul>
</div>

<h3>Pub/Sub for WebSocket Fan-Out</h3>
<p>Separate from the durable message queue, we use a lightweight <strong>Pub/Sub</strong> system for real-time WebSocket message distribution:</p>
<ul>
    <li>Topics are ephemeral: <code>order:{order_id}</code> exists only while the order is active.</li>
    <li>WebSocket Servers subscribe to order topics for their connected clients.</li>
    <li>When a location update or chat message is published, Pub/Sub fans it out to all subscribing WebSocket Servers in &lt;50ms.</li>
    <li>If a message is lost (server crash, network blip), it's acceptable â€” the next location update arrives in 5 seconds, and chat uses the reliable Message Queue as a fallback for offline delivery via push notifications.</li>
</ul>

<!-- ============================================================ -->
<!-- 12. LOAD BALANCER DEEP DIVE -->
<!-- ============================================================ -->
<h2 id="lb">12. Load Balancer Deep Dive</h2>

<h3>Where Load Balancers Are Placed</h3>
<ol>
    <li><strong>External Load Balancer (Layer 7 â€” HTTP)</strong> â€” Sits between client apps and the API Gateway. All HTTP/HTTPS traffic from Client App and Shopper App enters here.
        <ul>
            <li><strong>Algorithm:</strong> Round-robin with health checks. Layer 7 allows path-based routing (e.g., route <code>/search/*</code> to a dedicated Search API Gateway cluster if needed).</li>
            <li><strong>TLS termination:</strong> Handles TLS termination so internal traffic can be unencrypted (reducing per-request overhead within the data center).</li>
            <li><strong>Rate limiting:</strong> Enforces rate limits per API key/user to protect against abuse and DDoS.</li>
            <li><strong>Health checks:</strong> Pings API Gateway instances every 10 seconds; removes unhealthy instances from the rotation.</li>
        </ul>
    </li>
    <li><strong>WebSocket Load Balancer (Layer 4 â€” TCP)</strong> â€” Sits between client apps and the WebSocket Server fleet.
        <ul>
            <li><strong>Algorithm:</strong> IP-hash (sticky sessions). Since WebSocket connections are long-lived and stateful (the connection registry is in-memory on a specific server), we need to ensure a client's connection always routes to the same server for the duration of the session. IP-hash achieves this by hashing the client's IP to deterministically select a server.</li>
            <li><strong>Why Layer 4 (not Layer 7):</strong> WebSocket traffic is upgraded from HTTP to a raw TCP stream after the initial handshake. A Layer 7 LB would need to understand the WebSocket protocol (some do, but at reduced performance). A Layer 4 LB operates at the TCP level, simply forwarding packets â€” lower overhead for high-throughput streaming.</li>
            <li><strong>Health checks:</strong> TCP-level health checks on WebSocket Server ports.</li>
        </ul>
    </li>
    <li><strong>Internal Service Load Balancers (Layer 7 â€” HTTP)</strong> â€” Between the API Gateway and individual microservices (Store Service, Catalog Service, Order Service, etc.).
        <ul>
            <li><strong>Algorithm:</strong> Least-connections. Directs traffic to the service instance with the fewest active requests, providing better load distribution than round-robin when request processing times vary (e.g., a complex order creation takes longer than a simple product lookup).</li>
            <li><strong>Service discovery:</strong> Integrated with a service registry so new service instances are automatically registered and unhealthy ones deregistered.</li>
        </ul>
    </li>
</ol>

<!-- ============================================================ -->
<!-- 13. SCALING CONSIDERATIONS -->
<!-- ============================================================ -->
<h2 id="scaling">13. Scaling Considerations</h2>

<h3>Horizontal Scaling</h3>
<ul>
    <li><strong>Stateless services</strong> (Store, Catalog, Search, Cart, Order, Payment, Inventory, Notification, Shopper Matching): All designed to be stateless â€” no session data stored on the instance. Scale horizontally by adding more instances behind load balancers. Auto-scale policies based on CPU utilization (target: 60â€“70%) and request latency (P99 &lt; 500ms).</li>
    <li><strong>WebSocket Servers:</strong> Scale based on active connection count. Each instance handles ~50,000 connections. Pub/Sub ensures cross-server communication. When a new instance is added, clients naturally connect to it (LB distributes new connections).</li>
</ul>

<h3>Database Scaling</h3>
<ul>
    <li><strong>SQL databases (Orders, Users, Shoppers, Payments):</strong>
        <ul>
            <li><strong>Read replicas:</strong> Deploy read replicas for read-heavy queries (order history, user profiles). Write to primary, read from replicas. Acceptable for slightly stale reads (millisecond replication lag).</li>
            <li><strong>Sharding:</strong> The <code>orders</code> table is sharded by <code>user_id</code> (as detailed in the schema section). Sharding distributes write load across multiple database nodes. <code>payments</code> is co-located with <code>orders</code> on the same shard (same <code>user_id</code> partitioning) so that order+payment transactions remain local.</li>
            <li><strong>Connection pooling:</strong> Each service instance maintains a connection pool to the database to avoid connection overhead on each request.</li>
        </ul>
    </li>
    <li><strong>NoSQL databases (Products, Cart, Inventory, Locations, Chat):</strong>
        <ul>
            <li>NoSQL databases are designed for horizontal scaling natively. Products are sharded by <code>store_id</code>, chat by <code>order_id</code>, locations by <code>shopper_id</code>.</li>
            <li>Adding more nodes redistributes partitions automatically (consistent hashing).</li>
        </ul>
    </li>
    <li><strong>Search Index:</strong> Horizontally scaled by adding more shards. Each shard handles a subset of stores. Search queries are routed to the shard for the relevant <code>store_id</code>.</li>
</ul>

<h3>Cache Scaling</h3>
<ul>
    <li>The in-memory cache cluster scales by adding more nodes with consistent hashing. Cache keys are distributed across nodes; adding a node only invalidates ~1/N of the keys (minimal disruption).</li>
</ul>

<h3>Message Queue Scaling</h3>
<ul>
    <li>Partition topics by <code>order_id</code> (ensures all events for a single order go to the same partition, maintaining order). Add more partitions to increase throughput. Add more consumer instances (up to the number of partitions) for parallel processing.</li>
</ul>

<h3>Geographic Scaling</h3>
<ul>
    <li>Deploy in multiple regions (e.g., US-West, US-East) with data partitioned by geographic zone. Orders in San Francisco are handled by the US-West region. This reduces latency and provides disaster recovery.</li>
    <li>CDN edge locations provide geographic image caching automatically.</li>
</ul>

<h3>Traffic Spike Handling</h3>
<ul>
    <li><strong>Peak hours (5â€“7 PM weekdays):</strong> Auto-scaling responds within 2â€“3 minutes based on metrics.</li>
    <li><strong>Holidays / pandemics:</strong> Pre-scale before known events. The message queue absorbs burst traffic while downstream services catch up.</li>
    <li><strong>Rate limiting:</strong> At the API Gateway level, per-user rate limits prevent abuse. Circuit breakers on external Payment Gateway calls prevent cascading failures.</li>
</ul>

<!-- ============================================================ -->
<!-- 14. TRADEOFFS & DEEP DIVES -->
<!-- ============================================================ -->
<h2 id="tradeoffs">14. Tradeoffs &amp; Deep Dives</h2>

<h3>14.1 â€” Consistency vs. Availability for Inventory</h3>
<p><strong>Tradeoff:</strong> We chose <strong>eventual consistency</strong> for inventory data rather than strong consistency.</p>
<p><strong>Reasoning:</strong> Grocery store inventory is inherently imprecise. Most stores don't provide real-time inventory APIs â€” Instacart syncs periodically (hourly batch jobs) and supplements with shopper-reported data. Even with a perfectly consistent inventory DB, the data would be stale relative to the physical shelf. Making inventory strongly consistent would require distributed locking or two-phase commits, adding significant latency to every product browse page â€” unacceptable for the ~100:1 read-to-write ratio. Instead, we accept that a user might see an item as "available" that turns out to be out of stock, and handle this gracefully via the substitution flow. The shopper is the ground-truth.</p>

<h3>14.2 â€” Denormalization of Product Data in Order Items</h3>
<p><strong>Tradeoff:</strong> We denormalize <code>product_name</code>, <code>product_image_url</code>, and prices into the <code>order_items</code> table.</p>
<p><strong>What we gain:</strong> (1) No cross-database joins (SQL Order DB â†” NoSQL Product DB). Displaying order history is a single query to the orders shard. (2) Historical accuracy â€” the order record preserves the exact product name and price at the time of purchase, immune to future catalog changes.</p>
<p><strong>What we lose:</strong> Increased storage. If a product name changes, old orders still show the original name (which is actually desired behavior for order receipts).</p>
<p><strong>Why normalization was not chosen:</strong> Normalization would require an application-level join: query orders from SQL, extract product IDs, then query product details from the NoSQL Product DB. This adds latency, complexity, and creates a cross-system dependency. If the Product DB has an outage, users couldn't even view their order history. Denormalization makes the orders service self-contained.</p>

<h3>14.3 â€” Denormalization of Shopper Rating Average</h3>
<p><strong>Tradeoff:</strong> The <code>shoppers.rating_avg</code> field is a denormalized aggregate.</p>
<p><strong>Reasoning:</strong> The Shopper Matching Service runs on every order placement and needs the shopper's average rating to score candidates. Computing <code>AVG(shopper_rating)</code> from the <code>ratings</code> table on every matching query would be expensive (especially for shoppers with thousands of ratings). Instead, we maintain a pre-computed <code>rating_avg</code> on the <code>shoppers</code> table. When a new rating is submitted, a queue consumer updates this value. The slight staleness (seconds) is acceptable for matching purposes.</p>

<h3>14.4 â€” SQL for Orders &amp; Payments vs. NoSQL</h3>
<p><strong>Tradeoff:</strong> SQL adds horizontal scaling complexity (sharding is harder than NoSQL's native partitioning), but provides ACID guarantees essential for financial transactions.</p>
<p><strong>Reasoning:</strong> An order involves: creating an order record, inserting N order items, and creating a payment authorization â€” all atomically. If any step fails, all must roll back. SQL transactions provide this natively. In a NoSQL system, we'd need to implement saga patterns or compensating transactions at the application level, adding significant complexity and failure modes. The sharding overhead is a worthwhile price for transactional safety on monetary operations.</p>

<h3>14.5 â€” Microservices vs. Monolith</h3>
<p><strong>Tradeoff:</strong> Microservices add operational complexity (service discovery, distributed tracing, inter-service communication overhead) but enable independent scaling and deployment.</p>
<p><strong>Reasoning:</strong> Instacart's services have vastly different scaling profiles. The Search Service sees 10Ã— more traffic than the Payment Service. Product browsing peaks at different times than order placement. With a monolith, we'd scale everything together (wasteful) and deploy everything together (risky â€” a bug in search code could bring down payments). Microservices let us scale Search to 50 instances while keeping Payment at 5 instances, and deploy them independently.</p>

<h3>14.6 â€” Two-Phase Payment (Authorize then Capture)</h3>
<p><strong>Tradeoff:</strong> More complex than single-step payment, but handles Instacart's unique challenge: the final total is unknown at checkout.</p>
<p><strong>Reasoning:</strong> When Alice checks out, she sees an estimated total of $52.47. But during shopping, Bob might substitute Organic Whole Milk ($6.99) with Organic 2% Milk ($5.99), or an item might be unavailable and refunded. The final total could be $51.47. We authorize the estimated amount at checkout (placing a hold on Alice's card) and capture the actual amount after delivery. This prevents overcharging or undercharging.</p>

<h3>14.7 â€” Batch Order Optimization</h3>
<p><strong>Tradeoff:</strong> Batching 2â€“3 orders from the same store to a single shopper improves efficiency (fewer shoppers needed, lower cost per order) but may slightly delay individual order fulfillment.</p>
<p><strong>Reasoning:</strong> If the system assigns each order to a separate shopper, and three orders come in for the same Whole Foods within 10 minutes, three shoppers drive to the same store. This is inefficient. Batching allows one shopper to fulfill all three, shopping once, checking out once. The tradeoff is that order #1 might wait an extra 5â€“10 minutes for orders #2 and #3 to arrive before the batch is assigned. We mitigate this with a batching window (max 10 min wait) and user-facing transparency ("Your order may be batched for efficiency").</p>

<!-- ============================================================ -->
<!-- 15. ALTERNATIVE APPROACHES -->
<!-- ============================================================ -->
<h2 id="alternatives">15. Alternative Approaches</h2>

<h3>15.1 â€” GraphQL Instead of REST</h3>
<p><strong>Approach:</strong> Replace RESTful HTTP APIs with a single GraphQL endpoint that allows clients to request exactly the fields they need.</p>
<p><strong>Why not chosen:</strong> While GraphQL would reduce over-fetching (mobile clients could request only needed fields, saving bandwidth), it adds backend complexity: resolvers, schema management, N+1 query problems, and caching becomes harder (REST responses are easily cache-keyed by URL, while GraphQL POST bodies vary). For Instacart, the REST API contracts are well-defined and stable, and the BFF (Backend-for-Frontend) pattern can optimize responses per platform. The marginal bandwidth savings don't justify the complexity for this use case.</p>

<h3>15.2 â€” CQRS (Command Query Responsibility Segregation)</h3>
<p><strong>Approach:</strong> Separate the read and write models for orders. Writes go to a normalized SQL write store; reads come from a denormalized read store (NoSQL materialized view) optimized for query patterns.</p>
<p><strong>Why not chosen:</strong> CQRS adds significant complexity: maintaining two stores in sync, handling eventual consistency between write and read models, and a more complex deployment. For our scale, standard SQL with read replicas provides sufficient read performance. The order table's query patterns (by user_id, by shopper_id, by status) are well-served by proper indexing. CQRS would be justified at much higher scale (billions of orders) but is premature optimization here.</p>

<h3>15.3 â€” Event Sourcing for Order State</h3>
<p><strong>Approach:</strong> Instead of storing the current order state and overwriting it on each transition, store every state event (ORDER_CREATED, SHOPPER_ASSIGNED, ITEM_FOUND, etc.) and derive the current state by replaying events.</p>
<p><strong>Why not chosen:</strong> Event sourcing provides a perfect audit trail and the ability to "replay" order history. However, it's complex to implement correctly (event versioning, snapshot optimization for long event chains, debugging becomes harder). For our needs, a traditional state machine with an <code>order_status_history</code> audit log table (recording each status transition with timestamp) provides sufficient traceability without the architectural overhead of full event sourcing.</p>

<h3>15.4 â€” Server-Sent Events (SSE) for Tracking + REST for Chat</h3>
<p><strong>Approach:</strong> Use SSE for unidirectional server-to-client streaming (location updates) and regular REST endpoints for sending chat messages.</p>
<p><strong>Why not chosen:</strong> This would require two communication channels instead of one. SSE connections have limitations: no binary data, limited browser connection pools (6 per domain), and poor proxy/firewall traversal compared to WebSocket. Chat messages sent via REST would have higher latency than WebSocket. A single WebSocket connection handles both use cases cleanly with lower complexity.</p>

<h3>15.5 â€” Polling for Inventory Updates Instead of Cache + Best-Effort</h3>
<p><strong>Approach:</strong> Have the client poll the server every 10 seconds for updated inventory while browsing, ensuring the user always sees current availability.</p>
<p><strong>Why not chosen:</strong> With millions of users browsing products simultaneously, polling every 10 seconds would generate enormous server load (mostly wasted â€” inventory doesn't change that frequently). Our cache-aside approach with 60-second TTL provides a good balance: data is reasonably fresh, server load is manageable, and the ground-truth is always verified by the shopper during fulfillment.</p>

<h3>15.6 â€” Monolithic Architecture</h3>
<p><strong>Approach:</strong> Build all services (Store, Catalog, Search, Cart, Order, Payment, Matching, etc.) as a single deployable application.</p>
<p><strong>Why not chosen:</strong> A monolith would be simpler to develop initially and avoids inter-service communication overhead. However, Instacart's different components have dramatically different scaling needs (Search handles 100Ã— more requests than Payment), different deployment cadences (Catalog Service changes weekly, Payment changes quarterly with compliance review), and different reliability requirements (a Search bug shouldn't bring down Payment processing). Microservices enable independent scaling, deployment, and fault isolation.</p>

<!-- ============================================================ -->
<!-- 16. ADDITIONAL CONSIDERATIONS -->
<!-- ============================================================ -->
<h2 id="additional">16. Additional Considerations</h2>

<h3>16.1 â€” ETA Estimation</h3>
<p>Accurate delivery time estimation is critical for user experience. The ETA system considers: (1) estimated shopping time based on item count and historical store-specific data, (2) estimated checkout wait time based on time-of-day patterns, (3) estimated driving time using a routing/maps API based on the shopper's current location and customer's delivery address, accounting for real-time traffic. ETAs are recalculated in real-time as the order progresses (e.g., once shopping is complete, the ETA switches to driving-time only).</p>

<h3>16.2 â€” Fraud Detection</h3>
<p>An async fraud-detection pipeline (consuming events from the Message Queue) analyzes orders for: unusual purchasing patterns, rapid-fire order placement (account takeover), stolen credit cards, and shopper fraud (claiming items out-of-stock to keep them). ML models score each order; high-risk orders are flagged for manual review.</p>

<h3>16.3 â€” Idempotency</h3>
<p>All mutating API endpoints include an <code>Idempotency-Key</code> header. The Order Service stores idempotency keys in cache (TTL: 24 hours) to prevent duplicate order creation if the client retries a failed request. This is critical for the payment flow â€” we must never double-charge a customer.</p>

<h3>16.4 â€” Observability</h3>
<ul>
    <li><strong>Distributed tracing:</strong> Each request gets a unique <code>trace_id</code> propagated across all services via HTTP headers. This allows tracing a single order placement from API Gateway â†’ Order Service â†’ Payment Service â†’ Message Queue â†’ Shopper Matching.</li>
    <li><strong>Metrics:</strong> Each service emits latency percentiles (P50, P95, P99), error rates, and throughput metrics. Dashboards track: orders per minute, average delivery time, cache hit rate, queue depth.</li>
    <li><strong>Alerting:</strong> Alerts on: error rate spikes, queue depth exceeding threshold (consumers falling behind), payment failure rate increase, WebSocket connection count anomalies.</li>
</ul>

<h3>16.5 â€” Data Privacy &amp; Compliance</h3>
<ul>
    <li>Payment tokens are stored instead of raw card numbers (PCI-DSS compliance).</li>
    <li>User location data (delivery addresses) is encrypted at rest.</li>
    <li>Shopper GPS data is retained for 30 days (delivery verification), then purged.</li>
    <li>GDPR/CCPA: users can request data export and account deletion. Deletion cascades to all personal data across services (coordinated via an async deletion pipeline).</li>
</ul>

<h3>16.6 â€” Offline / Poor Connectivity Handling</h3>
<p>Shoppers in large stores often have poor cellular connectivity. The Shopper App implements:</p>
<ul>
    <li><strong>Offline queue:</strong> Item status updates (found/not-found) are queued locally on the device and synced when connectivity is restored.</li>
    <li><strong>Optimistic UI:</strong> The shopper's app immediately reflects the action locally; sync happens in the background.</li>
    <li><strong>Conflict resolution:</strong> Last-write-wins for item status; server timestamps determine order if conflicting updates arrive.</li>
</ul>

<h3>16.7 â€” Promo / Coupon System</h3>
<p>A lightweight Promo Service validates and applies promo codes at checkout. It maintains a table of active promotions with constraints (max usage count, min order amount, eligible stores, expiration date). Applied atomically with order creation to prevent race conditions on limited-use coupons (using optimistic locking or atomic decrement on usage counter).</p>

<!-- ============================================================ -->
<!-- 17. VENDOR SECTION -->
<!-- ============================================================ -->
<h2 id="vendors">17. Vendor Section</h2>
<p>The architecture above is vendor-agnostic by design. Below are potential vendor choices for each infrastructure component, with brief justifications:</p>

<table>
    <tr><th>Component</th><th>Potential Vendors</th><th>Rationale</th></tr>
    <tr>
        <td><strong>SQL Database</strong><br>(Users, Orders, Shoppers, Payments)</td>
        <td>PostgreSQL, MySQL, Amazon Aurora, Google Cloud Spanner</td>
        <td>PostgreSQL offers rich geospatial support (PostGIS) for the Store Service's R-tree queries, strong ACID compliance, and excellent JSON support for flexible columns. Aurora provides PostgreSQL-compatible managed scaling. Spanner offers global distribution with strong consistency if multi-region write is needed.</td>
    </tr>
    <tr>
        <td><strong>Document NoSQL</strong><br>(Products, Chat Messages)</td>
        <td>MongoDB, Amazon DynamoDB, Couchbase</td>
        <td>MongoDB's flexible document model naturally fits the variable product attributes and provides rich query/indexing capabilities. DynamoDB offers managed scaling with predictable performance. Couchbase provides built-in caching (memcached protocol) which could consolidate the cache layer.</td>
    </tr>
    <tr>
        <td><strong>Key-Value NoSQL</strong><br>(Cart, Inventory)</td>
        <td>Amazon DynamoDB, Redis (with persistence), Aerospike</td>
        <td>DynamoDB provides single-digit millisecond reads/writes with auto-scaling and built-in TTL. Redis offers sub-millisecond performance and is widely used for ephemeral data. Aerospike provides hybrid memory architecture (SSD-backed with in-memory indexes) for cost-effective high throughput.</td>
    </tr>
    <tr>
        <td><strong>Time-Series NoSQL</strong><br>(Shopper Locations)</td>
        <td>InfluxDB, TimescaleDB, Amazon Timestream</td>
        <td>InfluxDB is purpose-built for high-frequency time-stamped data with built-in downsampling and retention policies. TimescaleDB extends PostgreSQL with time-series optimizations (could share the SQL infra). Timestream is fully managed with automatic scaling.</td>
    </tr>
    <tr>
        <td><strong>In-Memory Cache</strong></td>
        <td>Redis, Memcached, Hazelcast</td>
        <td>Redis is the industry standard: supports strings, hashes, lists, and sorted sets (useful for ranked search caches). Provides cluster mode for horizontal scaling with consistent hashing. Memcached is simpler (pure key-value) and slightly faster for basic get/set if advanced data structures are unnecessary.</td>
    </tr>
    <tr>
        <td><strong>Search Index</strong></td>
        <td>Elasticsearch, Apache Solr, Meilisearch, Typesense</td>
        <td>Elasticsearch is the most widely adopted full-text search engine with fuzzy matching, synonym support, and relevance tuning out of the box. Highly scalable with sharding. Typesense is a lighter alternative with built-in typo tolerance and faster indexing for smaller datasets.</td>
    </tr>
    <tr>
        <td><strong>Message Queue</strong></td>
        <td>Apache Kafka, RabbitMQ, Amazon SQS/SNS, Google Pub/Sub</td>
        <td>Kafka provides durable, ordered, high-throughput event streaming with consumer groups â€” ideal for the order event pipeline. RabbitMQ is simpler for traditional task queuing. SQS+SNS provides managed queue + pub/sub if on AWS. For our use case (durable order events + multiple consumers), Kafka's log-based architecture is the best fit.</td>
    </tr>
    <tr>
        <td><strong>Pub/Sub (WebSocket fan-out)</strong></td>
        <td>Redis Pub/Sub, Apache Kafka, NATS</td>
        <td>Redis Pub/Sub is lightweight, in-memory, and provides sub-millisecond message delivery â€” ideal for ephemeral real-time WebSocket fan-out where durability isn't needed. NATS is even lighter and designed for cloud-native messaging. Kafka could serve double duty (durable queue + pub/sub) but has higher latency for real-time use cases.</td>
    </tr>
    <tr>
        <td><strong>Object Storage</strong></td>
        <td>Amazon S3, Google Cloud Storage, Azure Blob Storage</td>
        <td>S3 is the industry standard for object storage: 11 nines durability, lifecycle policies, and deep CDN integration. All three major cloud providers offer equivalent services.</td>
    </tr>
    <tr>
        <td><strong>CDN</strong></td>
        <td>Cloudflare, Amazon CloudFront, Fastly, Akamai</td>
        <td>Cloudflare offers the largest edge network and built-in DDoS protection. CloudFront integrates natively with S3. Fastly provides instant cache purging (useful for promotional banner updates). Akamai has the longest track record for enterprise CDN.</td>
    </tr>
    <tr>
        <td><strong>API Gateway</strong></td>
        <td>Kong, AWS API Gateway, NGINX, Envoy</td>
        <td>Kong provides a plugin ecosystem for rate limiting, authentication, and logging. NGINX is lightweight and battle-tested. Envoy is built for microservice architectures with advanced load balancing and observability. AWS API Gateway is fully managed if on AWS.</td>
    </tr>
    <tr>
        <td><strong>Load Balancer</strong></td>
        <td>AWS ALB/NLB, NGINX, HAProxy, Google Cloud Load Balancing</td>
        <td>AWS ALB (Layer 7) for HTTP traffic with path-based routing. AWS NLB (Layer 4) for WebSocket traffic with ultra-low latency. HAProxy is a high-performance open-source option for on-premise or multi-cloud. NGINX can double as both reverse proxy and load balancer.</td>
    </tr>
    <tr>
        <td><strong>Container Orchestration</strong></td>
        <td>Kubernetes, Amazon ECS, Docker Swarm</td>
        <td>Kubernetes is the industry standard for microservice orchestration: auto-scaling, rolling deployments, service discovery, and self-healing. ECS is simpler if on AWS-only. Kubernetes provides cloud-agnostic portability.</td>
    </tr>
    <tr>
        <td><strong>Observability</strong></td>
        <td>Datadog, Prometheus + Grafana, New Relic, Jaeger (tracing)</td>
        <td>Datadog provides unified metrics, traces, and logs in a single platform. Prometheus + Grafana is the open-source standard for metrics and dashboards. Jaeger provides distributed tracing for debugging cross-service request flows.</td>
    </tr>
</table>

</body>
</html>
